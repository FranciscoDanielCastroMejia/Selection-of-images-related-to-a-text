[
    "Una aguja para coser, con uno de sus extremos afilado en punta y un ojo para enhebrar el hilo de costura en el otro, podría parecer quizá un objeto insignificante. La aguja de costura, sin embargo, jugó un papel destacado decenas de miles de años atrás en el desarrollo social de nuestros antecesores. Esto, de acuerdo con un artículo publicado la pasada semana en la revista “Science Advances” por un grupo de investigadores encabezado por Ian Gilligan, de la Universidad de Sydney, Australia.  Retrocedamos 50,000 años en el tiempo. Encontraremos que nuestra especie había ya iniciado la migración desde el este de África hacia Europa y Asia. El planeta estaba entonces en medio de la última glaciación, y en la medida en que los inmigrantes avanzaban hacia el norte encontraban temperaturas cada vez más bajas. Para protegerse del frío, la ropa era esencial, y en este sentido una especie de poncho fabricado con la piel de animal proporcionaba alguna protección. Dicha protección, sin embargo, no era completa pues el aire se colaba por los huecos que dejaba el poncho. Una mejor opción era la ropa ajustada, si bien para esto hubieron de desarrollar técnicas y herramientas de costura. En este sentido, fabricaron punzones de hueso para hacer pequeñas perforaciones en piezas de piel de animal, lo que les permitía unirlas empleando fibras animales o vegetales como ligamentos. Un avance tecnológico con respecto al punzón de hueso fue la aguja de hueso, que es básicamente un punzón con un ojo u orificio en un extremo. En una aguja con ojo se combinan dos funciones: perforar orificios en la piel a coser e insertar la fibra o ligamento. Esto permitió a nuestros ancestros fabricar prendas más elaboradas y de manera más eficiente. La aguja con ojo constituyó un avance tecnológico que fue desarrollado solamente por nuestra especie. El registro arqueológico más antiguo de una aguja con ojo se localiza en Siberia y tiene una antigüedad de 40,000 años. En el Cáucaso y en Europa hay registros de 38,000 años y de 26,000 años de antigüedad, en forma respectiva. De acuerdo con Gilligan y colaboradores, la localización geográfica de los registros arqueológicos está relacionada con las temperaturas ambientales ahí imperantes, lo que indica que el desarrollo de la aguja de ojo fue impulsado por la necesidad de fabricar ropa ajustada al cuerpo. De manera adicional, como comentan los autores, en función de la temperatura imperante es necesario añadir capas adicionales de ropa interior. Esta ropa requiere de un proceso de fabricación más delicado, que implica un labor tardada y tediosa insertando fibras en pequeños agujeros. Todo esto hizo imperioso hacer más eficiente y delicado el proceso de fabricación de ropa con nuevas herramientas y técnicas de costura.Por otro lado, consideran Gilligan y colaboradores que no fue únicamente la necesidad de protección contra el frío lo que impulsó el desarrollo de la aguja con ojo, sino que dicho desarrollo fue también condicionado por factores sociales. Es decir, por el papel que la ropa juega como un elemento de identificación social y en la creación de sociedades complejas. En este sentido, los investigadores hacen notar la costumbre entre algunas sociedades de pintarse o tatuarse el cuerpo. En un clima frío esto no es posible, por lo que la pintura del cuerpo habría sido sustituida por la ropa y por los adornos cosidos sobre la misma. Con esto, la ropa se transforma en vestido, es decir, cambia su papel protector en contra del frío en un papel de integración y diferenciación social.Apuntan Gilligan y colaboradores que aun antes de la invención de la aguja con ojo se fabricaba ropa ajustada al cuerpo.  La importancia de dicha aguja, no obstante, radica en que posibilitó la fabricación de ropa más elaborada. En palabras de los investigadores: “La importancia de las agujas con ojo no radica en confeccionar ropa sino más bien en una mayor elaboración de la ropa confeccionada que, si bien tecnológicamente es un pequeño paso, iba a suponer un salto cuántico en las sociedades humanas donde la ropa se usó con regularidad… La transición a la ropa como vestido transformó la ropa de una necesidad física a una necesidad social, asegurando el uso continuo de la ropa hasta el presente”.Al margen de la importancia que la aguja de hueso con ojo haya tenido en el desarrollo social hace decenas de miles de años, cabe preguntarse cómo habría sido la historia del mundo sin su invención. Como apuntan Gilligan y colaboradores, la aguja con ojo facilitó la fabricación de ropa interior y ésta a su vez posibilitó la migración hacia Siberia y posteriormente hacia nuestro continente a través de Beringia. ¿Se habría entorpecido la migración hacia América sin la aguja de ojo? ¿En qué medida estamos en deuda con un objeto tan aparentemente insignificante?",
    "En su libro, “Pronto: Una historia atrasada de la procrastinación, desde Leonardo y Darwin hasta tú y yo”, el escritor Andrew Santaella se pregunta por qué Charles Darwin tardó 20 años en publicar su libro sobre el origen de las especies, el cual está basado en los descubrimientos que hizo durante su viaje de cinco años alrededor del mundo a bordo del “Beagle”. Como comenta Santaella, Darwin había ya llegado a la conclusión de que las especies no eran inmutables en fechas tan tempranas como 1836, poco después de regresar de su viaje, pero no fue sino hasta 1859 que publicó sus ideas al respecto.En lugar de dedicarse a escribir y dar a conocer sus conclusiones sobre la evolución de las especies, que sabía provocarían una revolución intelectual, Darwin dedicó su tiempo a otras investigaciones. En particular, relata Santaella, se dedicó durante ocho años al estudio de los percebes, crustáceos que le provocaban un entusiasmo cercano a la obsesión. Era tal el entusiasmo de Darwin por dichos crustáceos, que sus hijos habrían pensado que coleccionar especímenes de percebes en alcohol, como hacía su padre, era algo normal y que ocurría con todos los padres.    Así, se pregunta Santaella si el retraso de Darwin en escribir y publicar sus revolucionarias ideas fue un episodio de procrastinación. Es decir, un episodio en el que dejó lo que “tendría que haber hecho” por hacer algo más. Por otro lado, al margen de si Charles Darwin tenía o no el hábito de posponer o retrasar tareas, sabemos que la procrastinación está muy extendida y virtualmente todos hemos practicado de alguna u otra manera. Por ejemplo, difiriendo alguna actividad programada y que no nos es particularmente agradable, en favor de una actividad que nos cause una mayor satisfacción. Y dado que diferir actividades puede tener consecuencias negativas, ya sea individuales o sociales, la procrastinación ha tomado relevancia y ha sido motivo de investigaciones científicas para establecer las causas y mecanismos que la determinan. Un ejemplo de esto es un artículo publicado en 2024 en “Proceedings of the Annual Meeting of the Cognitive Science Society”, que describe los resultados de una investigación sobre procrastinación llevada a cabo con estudiantes universitarios. El artículo fue publicado por Sahiti Chebolu y Peter Dayan de Max Planck Institute for Biological Cybernetics, Alemania. Dado que la procrastinación se presenta de formas variadas e involucra diferentes comportamientos, para facilitar su investigación Chebolu y Dayan distinguieron tres modos fundamentales de procrastinación. En un primer modo, hay un apego a un retraso programado que deja un tiempo insuficiente para finalizar la tarea. Esto puede ser porque se busca una máxima utilidad o porque hay un mal cálculo del tiempo que en realidad toma en completar la tarea. En un segundo modo, hay un retraso deliberado a pesar de la intención inicial de actuar antes. Finalmente, en un tercer modo hay retrasos por no establecer un compromiso con un tiempo de acción y se falla con una fecha límite, o se pierde tiempo en el proceso.  Chebolu y Dayan enfocan la procrastinación como una serie de decisiones temporales que puede ser entendida en términos matemáticos. Una decisión puede tomarse en base a la recompensa inmediata y a la consecuencia futura que tendría dicha decisión. Por ejemplo, la decisión que tome un estudiante de asistir a una fiesta en lugar de estudiar para el examen que tendrá que presentar el día siguiente. El estudiante tendrá que escoger entre la recompensa inmediata que obtendrá al asistir a la fiesta y la recompensa que tendría días después al recibir su calificación. La decisión, por otro lado, estaría también determinada por el grado de incertidumbre que tendría el estudiante en cuanto a sus posibilidades de aprobar el examen.En su investigación, Chebolu y Dayan hicieron uso de datos sobre el desempeño académico de un grupo de 173 estudiantes de New York University. De manera específica, sobre su desempeño en un laboratorio que era parte de un curso de psicología. Los estudiantes debieron cumplir con un mínimo de 7 horas de laboratorio que podrían extender hasta por un máximo de 4 horas adicionales, lo que les daría puntos adicionales en su calificación final.  Encuentran que los estudiantes se dividen en grupos. Algunos de ellos llevaron a cabo su tarea de inmediato, mientras que otros espaciaron la tarea a lo largo del curso. Y, por supuesto, hubo otros que esperaron hasta el final del curso para llevarla a cabo. En todos los casos, no {obstante, Chebolu y Dayan pudieron encasillar el comportamiento de los estudiantes de acuerdo a su enfoque de investigación.En un mundo en el que se privilegia la eficiencia, la procrastinación se ve como un problema que hay que superar, y ciertamente lo es en cierta medida. Habría que mencionar, sin embargo, que algunos como Santaella se preguntan si la procrastinación de Darwin fue parte de su genio y si le habría ayudado para su descubrimiento revolucionario. En cuyo caso habría sido un rasgo positivo. Aun si así fuera, no obstante, sería peligroso que lo extrapolemos a los demás mortales. ¿O no?",
    "Un asunto de la mayor actualidad es, a no dudarlo, el cambio climático, con su acompañamiento de olas de calor, sequías, y huracanes de gran magnitud.  Como lo explican los expertos, el planeta se está calentando lentamente porque se ha alterado el delicado equilibrio que existía entre la energía solar que recibe la Tierra, y aquella que, por reflexión o emisión, es enviada de regreso al espacio. Así, para mitigar el cambio climático los expertos consideran tres acciones a seguir. Primeramente, habría que disminuir la emisión de gases de invernadero a la atmósfera, los cuales actúan como una capa que dificulta el paso hacia el espacio del calor emitido por, la superficie de planeta -del mismo modo que el parabrisas y los vidrios de las ventanas de un automóvil dificultan la salida del calor en su interior en un día caluroso. Como una segunda acción, habría que desarrollar tecnologías para remover de la atmósfera el dióxido de carbono -el principal gas de invernadero- ya acumulado. Y, finalmente, habría que incrementar la porción de radiación solar que es reflejada por la atmósfera de regreso al espacio.Con relación a esta última acción, habría que recordar que la emisión masiva de gases y de partículas a la atmósfera durante una erupción volcánica puede bloquear la radiación solar y producir descensos de temperatura significativos. Un ejemplo en este sentido es la erupción en 1815 del volcán Tambora en Indonesia, que hizo de 1816 “El año sin verano”. Recordemos que, en el verano de ese año, debido a las bajas temperaturas imperantes, Mary Shelly tuvo que permanecer encerrada en compañía de otros escritores, incluyendo a Lord Byron, en una villa cercana al lago Ginebra en Suiza. Producto de este encierro, a Shelly le llegó la inspiración para escribir su famosa novela “Frankenstein”.  Si bien el origen de la novela “Frankenstein” es sin duda fascinante, no es la intención de este artículo comentar sobre dicho origen. Y sí, por lo contrario, comentar sobre la mitigación del cambio climático mediante el bloqueo de la radiación solar que llega directamente del sol. La anécdota de “Frankenstein”, sin embargo, nos ilustra sobre una manera de lograr dicho bloqueo: por medio de una capa de partículas microscópicas suspendidas en la atmósfera que reflejen la radiación del sol.No se pretendería, por supuesto, suspender en la atmósfera gases y partículas contaminantes como las generadas en una erupción volcánica. Lejos de esto, los expertos consideran dispersar partículas microscópicas de sal dentro de las nubes, que se sabe están formadas por pequeñas gotas de agua y cristales de hielo que reflejan la luz del sol. Al dispersar partículas de sal en las nubes se incrementa el número de gotas de agua y con esto la fracción de luz solar reflejada hacia el espacio. Esto haría las nubes más brillantes y, en principio, permitiría regular el clima de la Tierra.No obstante, como apuntan los expertos, existen grandes obstáculos para llevar a cabo un proyecto de esta naturaleza. Un primer problema es que el clima de la Tierra es tan complejo que resulta difícil predecir los efectos de secundarios que tendría un proyecto de intervención del clima. Adicionalmente, un proyecto de ingeniería climática a nivel global requeriría la participación de todos los países de planeta, lo cual los expertos consideran es inviable.Así, es más viable un proyecto o proyectos de ingeniería climática a nivel regional, y en ese sentido un artículo aparecido esta semana en la revista “Nature Climate Change” analiza el efecto que tendría sobre el clima del oeste de los Estados Unidos -que ha sufrido de ondas severas de calor en los últimos años- un proyecto de intervención de nubes. El artículo fue publicado por un grupo de investigadores encabezado por Jessica Wan, de la Universidad de California en San Diego.De manera específica, Wan y colaboradores investigaron el efecto que tendría sobre el oeste norteamericano dispersar partículas de sal en nubes marinas en dos regiones del océano Pacífico: enfrente de la costa de California y cerca de las islas Aleutianas, a la altura de Alaska. Encuentran que las dos intervenciones reducen de manera significativa la probabilidad de que ocurran ondas de calor extremo. Si bien de manera contraintuitiva, pues la intervención cerca de Alaska resultaría más efectiva que la realizada en la costa californiana. Esta efectividad, sin embargo, se reduciría en ambos casos con el paso de los años en la medida en que avance el cambio climático e incluso podría ser negativa. Por otro lado, de manera más alarmante, ambas intervenciones podrían tener repercusiones negativas en regiones remotas. En Europa, por ejemplo, se intensificarían las olas de calor en el año 2050.Así, dado lo complejo de clima de la Tierra, una intervención de ingeniería climática en una cierta región podría provocar efectos negativos en lugares remotos. Tal pareciera entonces que la solución al cambio climático no va en esa dirección.",
    "Según el Popol Vuh, los gemelos héroes Hunapú e Imbalanqué bajaron al inframundo llamados por sus Señores principales, Hun-Camé y Vucub-Camé, quienes pretendían darles muerte molestos por el ruido que hacían jugando a la pelota. Un episodio similar había ocurrido tiempo atrás con el padre y el tío de los gemelos -igualmente gemelos- atraídos y muertos por los Señores del inframundo, también por jugar a la pelota. Con los gemelos héroes, sin embargo, Hun-Camé y Vucub-Camé no tuvieron éxito.Para cumplir con sus oscuros propósitos, los Señores del inframundo hicieron pasar a los gemelos por la Casa Oscura, obstáculo que sortearon con inteligencia. Los invitaron enseguida a un juego de pelota, con la seguridad de vencerlos, lo que no ocurrió. Igualmente, los hicieron pasar por la Casa de la Navajas, con la intención de que las navajas los destrozaran, lo cual tampoco ocurrió. No pudieron vencer a los gemelos héroes ni la Casa de Frío, ni la Casa de los Tigres, ni la Casa del Fuego. En la Casa de Murciélagos, Hun-Camé y Vucub-Camé tuvieron una victoria aparente, pues Hunapú resultó decapitado. Su hermano gemelo, no obstante, logró resucitarlo. Con la ayuda de dos adivinos, los gemelos hicieron un plan para en engañar y vencer de manera definitiva a los Señores del inframundo. Como parte de este plan, Hunapú e Imbalanqué se inmolaron en una hoguera e hicieron que sus huesos fueran molidos y arrojados al río. Con esto, Hun-Camé y Vucub-Camé clamaron victoria definitiva. Los gemelos, sin embargo, resucitaron al quinto día y fueron vistos en el agua con la apariencia de hombres-peces.Según el Popol Vuh, “Al día siguiente se presentaron dos pobres de rostro avejentado y aspecto miserable, vestidos de harapos, y cuya apariencia no los recomendaba”.  No obstante, a pesar de esta apariencia eran capaces de realizar muchos prodigios. Podían, por ejemplo, quemar una casa y luego volverla a su estado anterior. También, relata el Popol Vuh, “Se despedazaban a sí mismos; se mataban el uno al otro; se tendía como muerto el primero a quien habían matado y al instante lo resucitaba el otro”.Tan maravillados estaban Hun-Camé y Vucub-Camé de los prodigios de que eran capaces los pordioseros -que no eran otros que los héroes gemelos- que les pidieron que practicaran su magia con ellos mismos. Accedieron, pero de manera astuta la practicaron solo de manera parcial, pues si bien dieron muerte a los Señores del inframundo, ya no los resucitaron. Así, los héroes gemelos triunfaron de manera definitiva y vengaron las muertes de su padre y de su tío.  Luego, según el Popol Vuh, “Subieron en medio de la luz y al instante se elevaron al cielo. Al uno le tocó el sol y al otro la luna. Entonces se iluminó la bóveda del cielo y la faz de la tierra. Y ellos moran en el cielo”.  El mito maya quiché de los héroes gemelos está en el centro de una investigación reportada en un artículo publicado esta semana en la revista “Nature”. Dicho artículo fue publicado por un grupo de investigadores encabezado por Rodrigo Barquera del Max Planck Institute for Evolutionary Antropology, Alemania, y la Escuela Nacional de Antropología e Historia, México. En dicho artículo se reportan una investigación genética de los restos de 64 personas enterradas en una cueva cerca de cenote sagrado en Chichén Itzá, la mayor parte de ellas entre los años 800-1000 d.C., periodo de florecimiento de la ciudad.Entre otras cosas, Chichén Itzá es conocida por la práctica de los sacrificios humanos.  Existe también una idea extendida de que dichos sacrificios se practicaban comúnmente con mujeres jóvenes. El análisis genético realizado por Barquera y colaboradores, sin embargo, arroja que los restos de todas las 64 personas investigadas corresponden a niños del género masculino. Igualmente, encuentran que cuando menos el 25 por ciento de los niños enterrados en la cueva están emparentados con cuando menos otro niño en la misma cueva. Aún más, encuentran incluso dos parejas de gemelos, lo cual es improbable meramente por azar. Esto indica, según Barquera y colaboradores, que los sacrificios rituales se practicaban fundamentalmente con niños hombres con un cierto grado de parentesco. Y en cuanto a los gemelos, el hallazgo nos remite al mito de los héroes gemelos del Popol Vuh, que eran hijos de un padre también gemelo y que sufrieron episodios repetidos de muerte y resurrección.A la distancia, es difícil saber el estado de ánimo de los sacrificados antes de la experiencia, dado que el sacrificio ritual podría constituir una especie de honor a la luz del mito de los héroes gemelos. Hoy en día, excepto en muy pocos casos y al menos en nuestro medio, tendríamos posiblemente dificultades considerables para convencer a una persona para que se sometiera a la experiencia. Con todo y la posibilidad de terminar en el firmamento convertida en una estrella.",
    "Es posible que Roma haya sido la primera ciudad en la historia en alcanzar un millón de habitantes, hace unos dos mil años. Hoy en día, siguieron su ejemplo cerca de quinientas ciudades, que suman un cuarto de la población del mundo, y que superan esta población. Vivir en una ciudad tiene ventajas, y es por eso que las ciudades nacieron y crecieron, en algunos casos de manera desmesurada. Al mismo tiempo, sin embargo, la vida urbana tiene desventajas.En este sentido, consideremos que la infraestructura de una ciudad incluye estructuras artificiales de concreto u otros materiales que han desplazado a elementos naturales como árboles o estanques de agua Estos nuevos elementos interactúan con la radiación solar de manera diferente a como lo hacía con la vegetación o los cuerpos de agua originales, alterando el medio ambiente y contribuyendo al llamado efecto de isla de calor. ¿Cómo puede un árbol mitigar el calor? Una respuesta obvia es bloqueando los rayos del sol y haciendo sombra. Hay respuestas menos obvias, sin embargo.  Así, mientras que las hojas de los árboles reflejan hacia el cielo una cierta parte de la radiación solar incidente, una estructura de concreto puede absorber dicha radiación contribuyendo a un incremento en la temperatura ambiental. De la misma manera, los árboles evaporan al medio ambiente agua que toman del suelo y para esto absorben calor del medio ambiente, lo que produce un efecto refrigerante. Al reemplazar a la vegetación con concreto se pierde este efecto con el consecuente incremento de la temperatura ambiental.Al igual que la vegetación, los cuerpos de agua absorben la radiación solar incidente y contribuyen a disminuir la temperatura en sus inmediaciones. De manera adicional, y al igual que la evaporación de agua en las hojas de las plantas produce una disminución de temperatura, la evaporación de agua de los cuerpos produce un efecto refrigerante.Así, se podrían identificar tres culpables de la ola de calor que estamos padeciendo: el cambio climático causado por la emisión de gases de invernadero a la atmósfera, el fenómeno de El Niño, que afortunadamente se disiparía en los próximos meses, y el proceso de urbanización que destruye el medio ambiente natural y produce el efecto de isla de calor. Para mitigar este último efecto, los especialistas consideran el despliegue de una infraestructura urbana verde y azul, que llaman GBGI, por sus siglas en inglés, que de alguna manera sustituya a los elementos perdidos por la urbanización. La infraestructura verde comprende elementos basados en la vegetación, como árboles, césped y setos, mientras que la infraestructura azul comprende cuerpos de agua como estanques, lagos y ríos. Estas dos infraestructuras estarían complementadas por una infraestructura gris que incluye muros, fachadas y techos cubiertos con plantas. Dada la diversidad de regiones geográficas con climas particulares, la infraestructura GBGI óptima depende del clima considerado. En un artículo publicado el pasado mes de marzo en la revista “The Innovation”, se discute el uso de infraestructura GBGI para mitigar el calor urbano en cuatro climas: tropical, seco, templado y continental. El artículo fue publicado por un grupo de investigadores encabezado por Prashant Kumar de “University of Surrey”, Reino Unido. Entre los elementos que han sido efectivos para mitigar el calor urbano en diferentes climas, Kumar y colaboradores citan a: jardines de techo, setos, árboles en las calles, jardines botánicos, humedales, bosques, paredes recubiertas de vegetación y campos de golf. Según Kumar y colaboradores, los jardines de techo y las pérgolas han sido las infraestructuras más exitosas para mitigar el calentamiento urbano en Japón y Corea del Sur. En ciudades chinas, tuvieron éxito los jardines botánicos, los humedales, las paredes con vegetación y los estanques.    En su artículo, Kumar y colaboradores llegan a conclusiones basados en una revisión de más de 200 artículos sobre intervenciones con infraestructura GBGI a lo largo del planeta, llevadas a cabo para mitigar el calentamiento urbano. Afirman que la información que presentan puede servir para fijar políticas de combate a dicho calentamiento. Consideran incluso los probables cambios climáticos que se darán en el futuro por efecto del calentamiento global. Al margen de la información detallada presentada por Kumar y colaboradores, y dado que el problema de calentamiento urbano se originó por la destrucción de un medio ambiente natural y la construcción en su lugar de uno artificial, no es difícil entender que la solución del problema pasa por tratar de restaurar el medio ambiente original en la medida de lo posible. De otra manera, seguiremos teniendo veranos calientes por dos vías: el calentamiento global y el calentamiento urbano.",
    "Consideremos una situación hipotética en la cual colapsa la civilización tal como la conocemos y un arqueólogo del futuro -digamos, del siglo XL- que está investigando las causas que llevaron a dicho colapso. Dado que gran parte de la información documental habría sido destruida, el arqueólogo recurre al carbono 14 para fechar los acontecimientos que precedieron a la catástrofe. ¿Qué tanto podría averiguar? Como sabemos, la técnica del carbono 14 fue desarrollada en la Universidad de Chicago por el químico Willard Libby en la década de los años 40 y como muchas ideas geniales, su principio es muy simple. Libby sabía que los rayos cósmicos que inciden sobre nuestro planeta generan carbono 14 en las partes altas de la atmósfera a partir del nitrógeno. El carbono 14, sin embargo, no es estable y se desintegra en carbono 12 -que es una versión estable y más común que el carbono 14- de modo tal que en un periodo de 5,730 años la mitad del carbono 14 original se habrá convertido en carbono 12. Por otro lado, las plantas toman carbono de la atmósfera para llevar a cabo el proceso de fotosíntesis, y parte del carbono 14 generado por los rayos cósmicos termina en los tejidos de las plantas juntamente con carbón 12.  Mientras una planta se mantenga viva, el carbono 14 desintegrado es reemplazado de manera continua y su concentración en los tejidos de la planta se mantiene constante. Una vez muerta la planta, sin embargo, la proporción de carbono 14 disminuirá de manera paulatina hasta virtualmente extinguirse.  De este modo, a partir del porcentaje de carbono 14 remanente en los tejidos de una planta se podrá fechar el tiempo que ha transcurrido desde su muerte. Desafortunadamente, dicho fechado tiene un margen de incertidumbre de algunas décadas, según los especialistas, lo que podría ser excesivo en ciertos casos. Por ejemplo, si bien nuestro arqueólogo del siglo XL podrá sospechar que en el siglo XX hubo conflictos armados en gran escala dada la elevación que observa en la concentración de carbono 12 en la atmósfera -lo que le indica que hubo un incremento en la quema de combustibles fósiles, posiblemente por actividades guerreras- tendrá dificultades para llegar a la conclusión que en la primera mitad del siglo XX hubo dos guerras mundiales separadas por intervalo de dos décadas y no una sola guerra sin interrupción. Igualmente, tendrá dificultades para separar en el tiempo el incremento, que seguramente observará, en la concentración de carbono 14 en la atmósfera, mismo que sabemos fue producto de las pruebas nucleares que se llevaron a cabo durante la Guerra Fría en las décadas de los años 50 y 60.Frente a estas dificultades, nuestro arqueólogo del futuro tendría recursos, y podrá combinar al carbono 14 con la dendrocronología, la ciencia que permite develar los secretos del pasado grabados en los anillos del tronco de los árboles. En particular, podría hacer uso de los resultados de un artículo publicado esta semana en la revista “Nature Communications” por un grupo de investigadores encabezado por Andrej Maczkowski de la Universidad de Berna, Suiza. Lo podría hacer, por supuesto, en caso de que el manuscrito de dicho artículo haya sobrevivido a la catástrofe.En su artículo, Maczkowski y colaboradores fechan con una precisión de un año diversas actividades de tala de árboles y de construcción de estructuras de madera realizadas entre los años 5328-5140 a.C., en un sitio arqueológico de la prehistoria en el norte de Grecia. Los investigadores emplearon secuencias de anillos de árboles que anclaron en el tiempo haciendo uso del llamado evento Miyake, que se sabe que ocurrió en el año 5,259 a.C. Los eventos Miyake producen un incremento temporal en la generación de carbono 14 en las capas altas de la atmósfera por causas que no están completamente esclarecidas, pero que según los expertos, estarían asociadas a una elevación de la actividad solar. El incremento súbito del carbono 14 en la atmósfera por un evento Miyake queda grabada en el anillo o anillos del tronco de los árboles que corresponden a los años en que se produjo un evento particular, lo que permite fechar dichos anillos de manera muy precisa. A un grado tal de precisión que podemos saber en qué año, hace más de 7,000 años, se taló tal o cual árbol, que se usó para tal o cual construcción de madera.Es ocioso, por supuesto, especular sobre lo que podría llegar a averiguar un hipotético arqueólogo sobre una civilización que prosperó e hipotéticamente colapsó dos mil años antes. Dependiendo de la magnitud de la catástrofe, la civilización podría haber sido regresada a la prehistoria, en cuyo caso pudiera ser que en el siglo XL ni siquiera existieran arqueólogos investigando el pasado. Mas probablemente, sin embargo, la ciencia en dos mil años habrá avanzado a tal grado que los métodos empleados por los arqueólogos del presente le parecerían una curiosidad.",
    "Refiriéndose a acontecimientos de los que fue testigo en el año 536 d.C. el historiador Procopio escribió en su obra “Historia de las Guerras”: “Y aconteció durante este año que tuvo lugar un presagio de lo más terrible. Porque el sol dio su luz sin brillo, como la luna, durante todo este año, y se parecía mucho al sol en eclipse, porque los rayos que arrojaba no eran claros, como los que acostumbra arrojar. Y desde el momento en que sucedió esto, los hombres no quedaron libres de guerra ni de pestilencia ni de ninguna otra cosa que lleve a la muerte. Y era el momento en que Justiniano se encontraba en el décimo año de su reinado.” Según los especialistas, lo presenciado por Procopio fue producto de una erupción volcánica que arrojó a la atmosfera grandes cantidades de gases y ceniza que oscurecieron la luz del sol. El resultado fue un invierno volcánico que produjo el año más frío en los últimos dos milenios. El episodio relatado por Procopio no ha sido único a lo largo de la historia. Así, por ejemplo, el año 1916, conocido como el “Año que no tuvo verano”, fue particularmente frío por la erupción del volcán Tambora en Indonesia un año antes.  En años más recientes, la erupción en 1991 del volcán Pinatubo en Filipinas, produjo una disminución de la temperatura global, si bien no tan severa como la presenciada por Procopio.De manera similar a su disminución temporal por causas naturales, la temperatura global ha experimentado a lo largo de la historia incrementos temporales de manera natural. Además, desde hace unos dos siglos, a la variabilidad natural de la temperatura global se le ha añadido un incremento continuo por la emisión de gases de invernadero a la atmósfera. Y como consecuencia, el verano del año 2023 ha sido el más caluroso de los últimos 2,000 años. A esta conclusión llega un artículo publicado esta semana en la revista “Nature” por un grupo de investigadores encabezado por Jan Esper de Johannes Gutenberg University, en Mainz, Alemania.Para llegar a esta conclusión, Esper y colaboradores reconstruyeron la temperatura del aire durante los meses de junio a agosto a lo largo de los últimos 2,000 años. Dado que solamente se tienen mediciones de la temperatura del aire a partir de 1850, los investigadores tuvieron que reconstruir dichas temperatura dos mil años hacia atrás. Para esto hicieron uso de la información sobre el clima que está grabada en el tronco de los árboles.  Como es conocido, los troncos de los árboles muestran anillos, de colores claro y oscuro de manera alternada.  El color claro corresponde al crecimiento acelerado del tronco durante los meses de primavera y verano, mientras que el color oscuro se da por un crecimiento más lento durante el otoño y el invierno.   Así, cada par de anillos claro y oscuro corresponde a un año de vida del árbol, de modo tal que contando el número de anillos es posible averiguar la edad de un árbol vivo. En el caso de un árbol muerto, dicho número corresponde a la edad que tenía en el momento de morir. Además de la edad de un árbol, su tronco guarda información sobre el clima de la Tierra en el pasado. Esto es debido a que el grosor de un anillo depende de la temperatura y la humedad ambiental imperantes en el momento de crecer, de modo que estudiando dicho grosor es posible determinar las cambiantes condiciones climáticas bajo las cuales se desarrolló el árbol, particularmente en lo que se refiere a la temperatura ambiental. Como mencionan Esper y colaboradores, dado que en el hemisferio sur hay menos mediciones de temperatura que en el hemisferio norte y a que en las latitudes bajas las estaciones son menos marcadas y por tanto el patrón de anillos es menos informativo, limitaron su estudio a latitudes en el hemisferio norte entre los 30 y los 90 grados.La conjunción de temperaturas medidas y reconstruidas, empleando más de 10,000 árboles, llevó a Esper y colaboradores que el verano de 2023 ha sido el más caluroso en los últimos dos mil años, con una temperatura que sobrepasó cuando menos en 0.5 grados centígrados a la temperatura del año 246 d.C., que es la más alta ocurrida antes de que empezáramos a quemar combustibles fósiles. Además, comparada con la temperatura baja récord ocurrida durante el reinado de Justiniano, la temperatura de 2023 fue 4 grados centígrados más alta. Dados estos números, Esper y colaboradores concluyen: “Aunque 2023 es consistente con una tendencia de calentamiento inducida por los gases de efecto invernadero que se ve amplificada por el evento de El Niño, este extremo enfatiza la urgencia de implementar acuerdos internacionales para la reducción de las emisiones de carbono.”Aunque fuera esperado según la tendencia, y aun sin compartir el pesimismo de Procopio sobre las consecuencias que puede traer un cambio climático, no deja de impresionarnos que el año pasado haya sido el más caliente en dos mil años. Al mismo tiempo que cruzamos los dedos para que el actual no lo supere.",
    "Dos de las características de los tiempos que hoy corren son el cambio climático y la disminución de las tasas de fertilidad. Como nos consta, el cambio climático se hace cada vez más presente con eventos extremos, como el huracán Otis que devastó a Acapulco el pasado mes de octubre, y la ola de calor que está en curso. Las tasas de fertilidad, por otro lado, han estado a la baja desde la segunda mitad de siglo pasado, alcanzando en muchos países valores que están incluso por debajo de los 2.1 nacimientos por mujer, que se considera es la tasa mínima necesaria para mantener un equilibrio entre los que nacen y los que mueren. Este es el caso de países como Japón, Corea del Sur, Italia y España, entre otros, que enfrentan una disminución de población en las próximas décadas. Como sabemos, el cambio climático es debido a la emisión de gases a la atmósfera que ha llevado a un incremento de la temperatura global de 1.2 grados centígrados con respecto a sus valores preindustriales. La baja en la tasa de fertilidad, por su lado, tiene causas múltiples, incluyendo el desarrollo de los anticonceptivos en la década de los años sesenta del siglo pasado, y la incorporación de las mujeres al mercado de trabajo. En primera instancia, y debido a que obedecen a causas tan diversas, no pensaríamos que hubiera una conexión entre el cambio climático y la disminución de la tasa de fertilidad. Existe evidencia anecdótica, sin embargo, que la incertidumbre sobre lo que nos depara el cambio climático para el futuro produce un estado de ansiedad que tiene un efecto sobre los planes de las parejas para engendrar un hijo, que podría enfrentar un mundo colapsado por el desastre climático. Se han incluso inventado términos como eco-ansiedad y solastalgia. Este último, según la Wikipedia, “es un neologismo que describe una forma de angustia, estrés mental o existencial, causado por el deterioro medioambiental”.Con el objeto de ir más allá de la evidencia anecdótica y, en su caso, establecer sobre bases sólidas la influencia que tiene el cambio climático sobre la decisión de las parejas de tener hijos, un grupo de investigadores encabezado por Hope Dillarstone del University College London, en Londres, Reino Unido, se dio a la tarea de llevar a cabo una revisión documental exhaustiva para determinar cómo y por qué las preocupaciones medioambientales pueden afectar la decisión de tener o no un hijo. Los investigadores publicaron los resultados de su trabajo el pasado mes de noviembre en la revista “PLOS CLIMATE”.Dillarstone y colaboradores llevaron a cabo una revisión amplia de las publicaciones existentes sobre cambio climático, preocupaciones sobre salud mental y bienestar, y toma de decisiones reproductivas. Examinaron 446 documentos de los cuales escogieron 13 que corresponden a estudios realizados entre 2012 y 2022. Los estudios involucraron a 10,788 participantes y fueron llevados a cabo en los Estados Unidos, Canadá, Nueva Zelanda y en países europeos.   Los participantes en los estudios expresaron preocupaciones sobre la calidad de vida que enfrentaría en el futuro un niño que estaría por nacer, en un mundo devastado por el cambio climático. De la misma manera, expresaron preocupación por agravar con hijos, la situación de un mundo ya de por si sobrepoblado y sobreexplotado. De acuerdo con Dillarstone y colaboradores, en 12 de los 13 estudios analizados dichas preocupaciones constituyen un fuerte incentivo para tener menos hijos o para no tener ninguno.La investigación documental llevada a cabo por Dillarstone y colaboradores involucró fundamentalmente estudios realizados en países del llamado Norte Global, es decir los países industrializados. En estas circunstancias consideran que un estudio similar debe ser llevado a cabo con países de Sur Global, entre los que se encuentra México. Por los demás, al margen de la motivación o motivaciones de las parejas para limitar el número de hijos, el hecho es que la tasa de fertilidad se ha reducido drásticamente, tanto en países del Norte Global como del Sur Global. Podíamos concluir que, a reserva de lo que digan los especialistas, no podríamos quizá culpar enteramente de la caída en fertilidad al cambio climático, pues ésta se inició en el siglo pasado antes de que hubiera una conciencia pública de dicho cambio. Pero si hemos de creerle a Dillarstone y colaboradores, el cambio climático no pareciera estar tampoco totalmente libre de culpa.",
    "Según los especialistas, el carbón usado como combustible se originó a partir de plantas sepultadas durante el periodo carbonífero de la Tierra, hace unos 300-350 millones de años. De la misma manera, se asume comúnmente que el petróleo y el gas natural, también usados como combustibles, se habrían formado a partir de materia orgánica sepultada hace muchos millones de años, la cual fue transformada por las grandes presiones y altas temperaturas a las que fue sometida. Por lo demás, y al margen de las hipótesis sobre la formación de los combustibles fósiles, lo que sí es un hecho es que estuvieron atrapados bajo la superficie del planeta por millones de años, hasta que hace muy poco tiempo -en términos geológicos- se empezaron a extraer y a quemar en forma acelerada. Esto, como sabemos, ha originado una crisis climática.El elemento químico que está en el centro de la crisis climática es el carbono. Al quemar un combustible fósil, carbón, petróleo o gas natural, se produce dióxido de carbono que se acumula en la atmósfera y actúa como una cubierta que refleja el calor que emite la superficie de la tierra que se calienta en consecuencia. La mitigación de la crisis climática requiere así de la sustitución de los combustibles fósiles por fuentes no contaminantes como el sol o el viento que generan energía eléctrica. En términos generales, se habla de electrificar la economía como una medida de mitigación del cambio climático  En algunos casos, sin embargo, la energía eléctrica no es una solución. Por ejemplo, supongamos que se pretende hacer volar un avión comercial con energía solar. Dado que esta energía está muy diluida, no podría obtenerse al momento por medio de paneles solares colocados en el fuselaje del avión y habría que almacenarla previamente en un banco de baterías. Este banco, sin embargo, tendría un peso excesivo y todo el arreglo sería inviable. Sería así difícil o imposible prescindir de fuentes de alta densidad de energía, como lo son los combustibles fósiles.  Para aplicaciones en las que las energías renovables no proporcionen una solución factible y aun así pretendamos combatir el cambio climático, habría que recurrir a otras opciones. Por ejemplo, al uso de combustibles que no contengan carbón. Como otra posibilidad se podría “reusar” el carbón previamente generado, múltiples veces de ser posible, en lugar de extraer más carbón del subsuelo. En este respecto, en un artículo aparecido esta semana en la revista “Nature Reviews Chemistry” se discuten perspectivas para cerrar el círculo del carbono y “desfosilizar” segmentos de la economía estadounidense que son difíciles de electrificar. Dicho artículo fue publicado por un grupo de investigadores encabezado por Wendy Shaw de Pacific Northwest National Laboratory en el estado de Washington.Habría que señalar que Shaw y colaboradores no solamente consideran la desfosilización de la economía para resolver el problema de cambio climático, sino que consideran el problema más amplio de contaminación ambiental por carbono. En este sentido, sabemos que existe un problema grave de contaminación por plásticos, que son materiales que contienen carbón como uno de sus constituyentes. Igualmente, consideran la contaminación por desechos municipales, por desechos agrícolas y por gas metano, que son todos fuentes de contaminación por carbón. En particular, el metano está compuesto de carbón e hidrógeno y se sabe que es un gas de invernadero altamente contaminante, muchas veces más potente que el dióxido de carbono.      Shaw y colaboradores contemplan un mundo en el que los átomos de carbono no se desperdician, sino que se reciclan para una misma aplicación o como materia prima para otros procesos de fabricación.  Como fuentes de materias primas consideran a las principales fuentes de contaminación de carbono: el dióxido de carbono de la atmósfera, que es con mucho la mayor, los desechos municipales, los desechos agrícolas, los plásticos y el metano. Para extraer el carbono de estas materias primas sería necesario desarrollar las tecnologías correspondientes. Los autores contemplan también un mundo en el que los combustibles fósiles son sustituidos por otros combustibles sin carbono, como el hidrógeno o el amoniaco. El hidrógeno es particularmente atractivo pues se obtiene solamente agua como residuo de su combustión. A pesar de esta gran ventaja, sin embargo, el hidrógeno tiene también desventajas. Una de ellas es su temperatura de licuefacción extremadamente baja, de menos 253 grados centígrados. Así, no podríamos llenar de hidrógeno líquido el tanque de combustible de nuestro automóvil, como sí lo podemos llenar con gasolina.Por todo lo anterior, el carbono aparece como el villano de la historia, lo cual no deja de ser injusto y engañoso. Por un lado, al igual que todos los seres vivos, estamos en buena medida hechos de carbón. Por otro lado, el carbón estuvo plácidamente durmiendo por millones de años bajo la superficie de la tierra y no es de ninguna manera culpable del mal uso que hemos hecho de él.",
    "¿Cuánto pesa una ciudad? Es decir, ¿cuál es el peso total de sus edificios y construcciones? En un artículo publicado en enero de 2021 en la revista “Advancing Earth and Space Sciences”, Tom Parsons, del U.S. Geological Survey nos da una respuesta, relativa al área de la bahía de San Francisco, California. De acuerdo con Parsons, tomando en cuenta solamente los edificios y no las obras de infraestructura, San Francisco pesa 1,600 millones de toneladas.Si bien el trabajo que hay que realizar para calcular el peso de una ciudad parecería ocioso, hay que considerar que, según los especialistas, en el año 2050 el 70 por ciento de la población del planeta vivirá en ciudades, lo que implica que crecerá sustancialmente el volumen de construcciones urbanas. Si el piso fuera “firme como una roca”, no habría problemas en colocarle encima todo el peso que quisiéramos. Desafortunadamente, sabemos que esto no es necesariamente cierto y que muchas ciudades en el mundo se están hundiendo lentamente, por lo que cabe preguntarse si el peso de sus edificios juega algún papel en este sentido.Consideremos a las ciudades chinas, repletas de rascacielos. Según los expertos, el peso de los edificios urbanos es uno de los factores que contribuye al hundimiento del suelo. Otro factor es la extracción de agua subterránea para satisfacer las necesidades de agua potable de la población. De un modo u otro, para evaluar la gravedad del problema es necesario conocer la velocidad con la que se están hundiendo las ciudades chinas, y este es precisamente el tópico de un artículo aparecido el pasado 19 de abril en la revista “Science”, publicado por un grupo de investigadores encabezado por Zurui Ao de la South China Normal University.  Ao y colaboradores midieron el hundimiento del suelo de las mayores ciudades chinas a lo largo de los años 2015 a 2022. Específicamente, consideraron 82 ciudades que incluyen al 74 por ciento de la población urbana de China. Para su investigación, hicieron uso de una técnica conocida como “Interferometría de Radar de Apertura Sintética”, que permite medir movimientos verticales del suelo de unos pocos milímetros, desde un satélite a cientos de kilómetros sobre la superficie de la Tierra. La técnica, además, permite observar áreas extensas de terreno con un gran detalle.Encontraron que, de las áreas urbanas investigadas, el 45 por ciento se está hundiendo a una velocidad mayor a 3 milímetros por año, mientras que el 16 por ciento lo está haciendo a una velocidad mayor a 10 milímetros por año. Esto afecta a un 29 por ciento y a un 7 por ciento de la población urbana, de manera respectiva. De acuerdo con los autores: “El hundimiento parece estar asociado a varios factores, tales como la extracción del agua subterránea y el peso de los edificios”. Dado el tamaño de la población de China, los anteriores números resultan impactantes. En efecto, como hacen notar Ao y colaboradores, si sus resultados se escalan a toda la población urbana de China, 270 millones de personas están viviendo en un área que se hunde a una velocidad mayor a 3 milímetros por año, lo que representa un tercio de la población de Europa. Igualmente, 67 millones de personas en China están afectados por un hundimiento del terreno mayor a los 10 milímetros por año, equivalente a la población total de Francia.Además de estar afectadas por el hundimiento del terreno, las ciudades costeras en China están amenazadas de inundación por el calentamiento global, que está elevando el nivel del mar por la fusión del hielo polar y por la expansión del agua de los océanos. En estas circunstancias, los autores consideran que en 2120 habrá un alto riesgo de inundaciones en ciudades costeras, a menos que se tomen medidas como la construcción de diques de protección.Como estrategia para mitigar el hundimiento del terreno, Ao y colaboradores proponen que la clave para abordar el hundimiento de las ciudades de China podría estar en el control sostenido y a largo plazo de la extracción de aguas subterráneas. En ese sentido, hacen notar que el control de la extracción de agua a largo plazo ha logrado estabilizar el hundimiento en Tokio y Osaka en Japón, al mismo tiempo que un control incompleto y a corto plazo ha demostrado ser inefectivo, como lo ejemplifica la experiencia de Querétaro.Los hundimientos de terreno se dan en ciudades de todo el mundo y México no es la excepción. En particular, no lo es nuestra ciudad, que muestra signos de hundimiento en varias zonas. Podíamos quizá descartar que dicho hundimiento sea debido al peso de los edificios urbanos, que no son particularmente masivos. Así, como causa más probable, podemos pensar en la extracción del agua subterránea.",
    "El escritor norteamericano Ray Bradbury publicó en 1950 el libro “Crónicas Marcianas” en el que describe la colonización del planeta Marte. En dicho libro, Bradbury conjuntó una serie de cuentos cortos publicados con anterioridad de manera separada. Según el primero de estos cuentos, que lleva por título “El verano del cohete”, la colonización de Marte se inició en enero de 1999, cuando partió un cohete desde el estado norteamericano de Ohio con la primera misión con rumbo a Marte. En esa ocasión: “El cohete creaba el buen tiempo, y durante unos instantes fue verano en la Tierra”.Los colonizadores encontraron en arribar a Marte una antigua civilización, ya en decadencia. Si bien Bradbury no nos describe con detalle el aspecto físico de los marcianos, si señala diferencias con los humanos. Así, Ylla y el señor K, una pareja de marcianos que aparecen en el segundo cuento, “Tenían la tez clara, un poco parda, de todos los marcianos, los ojos amarillos y rasgados, las voces suaves y musicales”.  Además, los marcianos tenían facultades sensoriales más allá de las humanas.Otros autores, sin la sutileza y habilidades poéticas de Bradbury, han descrito a los extraterrestres de manera más explícita, en algunos casos posiblemente abusando de la imaginación. En este sentido, en un inicio los extraterrestres eran frecuentemente de color verde, si bien con los años adquirieron otros colores. Y en cuanto a su cuerpo, los extraterrestres tendrían forma vagamente humana, pero con características claramente no humanas.  Lo cierto, por supuesto, es que no sabemos con certeza que aspecto tienen los extraterrestres. De hecho, no tenemos la seguridad de que existan, aunque nos sorprendería que no fuera así, dada la inmensidad del universo y el número de planetas capaces de albergar vida.  Muy probablemente podríamos descartar la existencia de formas de vida superior en nuestro vecindario, en decir en nuestro sistema solar. Tendríamos así que buscar en el vecindario de otras estrellas. Para empezar, habría que identificar planetas con condiciones parecidas a las de la tierra. Al respecto, se han detectado más de 5,500 planetas fuera de nuestro sistema solar, de los cuales más de 30 son similares al nuestro. Enseguida, tomando como base lo que ocurre en nuestro planeta, los expertos deben buscar en dichos planetas la huella de la fotosíntesis, mediante la cual las plantas fabrican materia orgánica en nuestro medio. Esta huella es bien conocida y es característica de las plantas de color verde, que reflejan no solamente el verde, y de ahí su color, sin también la radiación infrarroja. De revelarse esta huella en un planeta lejano se tendría indicación de que alberga vida. Contrastamos esta estrategia que se apoya en la vida tal como la conocemos en la Tierra para buscar vida en otros planetas, con los puntos de vista de novelistas y directores de cine que asumen que la vida extraterrestre es diferente, en mayor o menor medida, a la que tenemos en la Tierra. En apoyo de estos últimos, sin embargo, un artículo aparecido esta semana aduce que no solamente debemos buscar signos de vida proporcionados por la fotosíntesis de las plantas verdes, sino que debemos extender la búsqueda a formas de fotosíntesis basadas en bacterias de color púrpura que imprimen una huella diferente. El artículo fue publicado esta semana en la revista “Monthly Notices of the Royal Astronomical Society”, por un grupo de investigadores encabezado por Ligia Fonseca Coelho de Cornell University.   De acuerdo con Fonseca Coelho: “El verde es el color que más asociamos con la vida en la superficie de la Tierra, donde las condiciones favorecieron la evolución de organismos que realizan la fotosíntesis productora de oxígeno utilizando el pigmento verde clorofila a. Pero un planeta similar a la Tierra que orbita otra estrella podría verse muy diferente, potencialmente cubierto por bacterias que reciben poca o ninguna luz visible, como en algunos entornos de la Tierra, y en su lugar utilizan radiación infrarroja invisible para impulsar la fotosíntesis. En lugar de verde, muchas de estas bacterias en la Tierra contienen pigmentos púrpuras, y los mundos púrpuras en los que son dominantes producirían una “huella digital luminosa” distintiva, detectable por los telescopios terrestres y espaciales de próxima generación”.De acuerdo con Fonseca Coelho y colaboradores, debemos buscar vida no solamente en los mundos verdes como el nuestro sino también en los potencialmente de color púrpura, lo que será posible en un futuro cercano en la medida en que se pongan en funcionamiento los nuevos telescopios. Desafortunadamente, aun si descubrimos que nuestro planeta no es el único lugar del Universo que alberga vida, con seguridad nos quedaremos todavía con la duda acerca del color de los habitantes de esos mundos, si es que los hubiera. ¿Serán verdes o morados? Esto podría ser relevante para novelistas y directores de cine.",
    "Como sabemos, esta semana se celebró un juego de futbol en la ciudad de Monterrey entre los equipos Monterrey e Inter Miami, con Leo Messi en sus filas.  Con motivo de este partido, visitó Monterrey el famoso jugador argentino de futbol Mario Kempes, quién fue campeón del mundo con Argentina en 1978. Durante su visita, Kempes se quejó del tráfico de la ciudad de la siguiente manera: “Lo que hemos visto está bien, lo que pasa es que bueno el tráfico generalmente aquí en México es bastante complicado, y Monterrey no es menos, espero que de cualquier manera que a medida que se vaya acercando esa gran posibilidad de Mundial, se hagan arreglos para que el ida y vuelta sea más fácil”. Por lo demás, sabemos que el tráfico de automóviles no es un problema exclusivo de nuestro país y que lo experimentan muchas ciudades a lo largo del mundo.  Por otro lado, si bien el tráfico no es la única incomodidad que enfrentamos quienes vivimos en áreas urbanas y nos faltarían dedos de las manos para enumerarlas, hemos decidido permanecer en las ciudades por las ventajas que también tienen. De hecho, las ciudades nacieron hace miles de años porque en algún momento resultó conveniente incrementar el contacto entre personas.En un artículo aparecido la pasada semana en la revista “Journal of Archaeological  Method and Theory”, se reportan los resultados de un estudio sobre las primeras etapas de desarrollo de áreas urbanas de baja densidad en Tongatapu. Tongatapu es la isla principal de Tonga, un país insular del Pacífico sur, de 100,000 habitantes, que por su  tamaño ocupa el lugar 186 entre los países del mundo. El artículo fue publicado por Phillip Parton y Geoffrey Clark de la Universidad Nacional de Australia.Como comentan Parton y Clark, Tonga fue primeramente ocupado en el año 900 a.C. y floreció desde el año 1300 d.C, hasta su colapso a inicios del siglo XIX, parcialmente por el impacto de enfermedades introducidas por extranjeros. Durante su esplendor, Tonga desarrolló arquitectura monumental, redes de tráfico comercial e instituciones políticas y sociales. Adicionalmente, dado su aislamiento en medio del océano Pacífico, los asentamientos urbanos primitivos de Tonga se desarrollaron sin influencia externa, y su estudio revela información valiosa sobre la evolución urbana. En su investigación, Parton y Clark utilizaron datos topográficos de Tongatapu obtenidos mediante la técnica de Lidar, que consiste en lanzar un haz de luz láser desde un avión hacia la superficie del terreno, midiendo el tiempo que tarda en regresar después de reflejarse en dicha superficie. A partir de este tiempo, es posible determinar las  elevaciones y depresiones del terreno. El uso del Lidar proporciona una enorme cantidad de datos que no es posible obtener por medio de los arqueológicos tradicionales. En particular, Parton y Clark estaban interesados en localizar montículos artificiales de tierra que son comunes en Tobgatapu y que se sabe fueron construidos con diferentes propósitos, ya sea como tumbas, o como plataformas para la construcción de casas habitación o espacios públicos. Además de los montículos, el Lidar revela redes de caminos que los enlazan, lo mismo que fortificaciones y construcciones para practicar deporte.Parton y Clark describen el proceso de urbanización de Tobgatapu, que se iniciaría cuando se incrementa la población dentro de los límites de las áreas pobladas  y genera lo que llama efectos de aglomeración: “La aglomeración causa cambios en la forma en la cual están construidos los asentamientos a medida que los pobladores empiezan a hacer un uso más eficiente del espacio y hacen un balance de las ventajas del cambio con los costos que implica. Efectos de aglomeración más grandes estimulan el desarrollo de instituciones sociales a medida que los asentamientos se adaptan a interacciones sociales cada vez mayores. Las instituciones sociales también provocan cambios en la forma en que está construido un asentamiento, al competir por espacios para realizar sus funciones con otros usos de la tierra, como el residencial, de subsistencia y otros usos productivos”. Basados en su estudio, Parton y Clark concluyen que los asentamientos en el Pacífico tienen un potencial considerable para contribuir a debates sobre la formación de asentamientos, la urbanización y la sostenibilidad, y contribuirán a nuestro conocimiento sobre urbanización y desarrollo de sociedades complejas. Ciertamente, entre mejor entendamos el proceso de formación de las ciudades podremos desarrollar medidas para contrarrestar las desventajas que representa vivir en una de ellas.  Una ciudad, sin embargo, es un objeto de estudio muy complejo -incluso más que el clima del planeta- y entender que botones presionar para que cambie en una dirección u otra, no será  algo que logremos en un futuro cercano. No obstante, esperemos que Monterrey pueda mejorar, aunque sea un poquito, el tráfico de la ciudad durante el mundial de futbol.",
    "En su poema “Oscuridad”, Lord Byron describe un mundo apocalíptico en el que,“El brillante sol se apagaba, y los astros Vagaban diluyéndose por el espacio eterno, Sin rayos, sin rutas y la helada tierraOscilaba ciega y oscureciéndose en el aire sin luna;”Byron escribió “Oscuridad” en 1816, año conocido como “El año que no tuvo verano”, que fue particularmente frío por una disminución de la radiación solar que alcanzó la superficie de la tierra. Este fenómeno fue debido a la erupción del volcán Tambora un año antes. El 5 de abril de 1815, el volcán Tambora, localizado, en la isla Sumbawa, en el sur de Indonesia, entró en erupción, alcanzando un máximo de actividad el 10 de abril, cuando ocurrió lo que fue la mayor explosión volcánica registrada en la historia, escuchada a miles de kilómetros de distancia. Antes de esta explosión, el volcán tenía una altura de 4,300 metros sobre el nivel del mar, misma que se redujo a 2,850 metros después de la explosión. La erupción del volcán Tambora lanzó a la atmósfera enormes cantidades de dióxido de azufre que originaron pequeñas gotas de ácido sulfúrico que permanecieron suspendidas en la estratósfera por largo tiempo. Estas gotas actuaron como una capa que reflejó parcialmente la radiación solar que disminuyó su intensidad al llegar a la superficie de nuestro planeta. Con esto, se provocó un descenso de temperatura en Europa y Norteamérica. Efectos similares se han producido con otras erupciones volcánicas. Por ejemplo, la erupción del volcán Krakatoa en agosto de 1883 produjo una disminución de 0.4 grados centígrados en el hemisferio norte, mientras que la erupción del volcán Pinatubo en junio de 1991 tuvo efectos similares a lo largo de varios meses.La disminución de temperatura que ha provocado la emisión de gases a la atmósfera durante las erupciones volcánicas ha inspirado a algunos científicos que han propuesto inyectar artificialmente aerosoles en la atmósfera como un medio para contrarrestar el calentamiento global. En 2017, David Keith y Frank Keutch de Harvard University, hicieron públicos sus planes para estudiar la propagación de aerosoles en la atmósfera. Para esto, propusieron elevar un globo y una góndola suspendida con una carga de varios kilogramos de aerosoles hasta una altitud de 20 kilómetros. Una vez alcanzada esta altitud, liberarían la carga y el globo, equipado con los instrumentos de medición adecuados, descendería a través de la nube de partículas para estudiar su dispersión en la atmósfera. Este proyecto serviría como antecedente para proyectos posteriores de dispersión de aerosoles con fines de reducir la radiación solar.La propuesta de Keith y Keutch, sin embargo, resultó muy controvertida. Sostuvieron sus críticos que el clima de la tierra es demasiado complejo y que reducir artificialmente la cantidad de radiación solar que llega a la tierra podría tener graves efectos secundarios de alteración del clima global que son difíciles de anticipar. Al respecto, en un artículo publicado esta semana en la revista MIT Technology Review se recogen opiniones expresadas por Jennie Stephens de Northeastern University: “El experimento planteado por Keith y Keutch fue particularmente peligroso, porque la financiación, la atención y el prestigio de Harvard confirieron legitimidad a intervenciones a escala planetaria que, en su opinión, nunca podrán gobernarse o controlarse de manera segura”. De manera adicional, Stephens sostiene que “incluso si los investigadores tuvieran las mejores intenciones, la geoingeniería solar en última instancia sería implementada por personas o naciones con dinero y poder de la manera que más beneficie a sus intereses, incluso si eso significara consecuencias desastrosas para otras áreas. Algunas investigaciones, por ejemplo, sugieren que la geoingeniería solar podría reducir significativamente las precipitaciones en determinadas zonas y reducir los rendimientos de algunos cultivos básicos. Si bien un bloque de naciones podría decidir utilizar la geoingeniería para aliviar las olas de calor, ¿qué pasaría si eso redujera los monzones de verano y los suministros de alimentos en partes de India o África occidental?”.Las criticas recibidas por el proyecto de Keith y Keutch hicieron su efecto y Harvard University anunció el pasado mes la cancelación definitiva del proyecto. Desafortunadamente, esto posiblemente no significa que proyectos similares no se llevarán a cabo en otros centros de investigación. Si bien, posiblemente no de manera tan abierta como procedió Harvard University para evitar críticas. En este sentido, esperemos que, después de doscientos años, nunca tengamos a un nuevo Lord escribiendo un poema apocalíptico sobre un desastre climático. Artificial. No natural.",
    "Ver a Ameca en acción, el robot humanoide publicitado como el más avanzado del mundo, produce una sensación extraña. Por un lado, para enfatizar su condición de robot, el cuerpo de Ameca deja ver todas sus circuitos y mecanismos que le permiten cobrar vida. Por otro lado, la cara del robot está cubierta por una piel de goma de color gris -color que, nuevamente, enfatiza su naturaleza no humana- que le permite gesticular y reproducir de manera notable las expresiones faciales humanas.  No obstante, si bien ver a Ameca gesticular e interactuar con humanos empleando la inteligencia artificial es todo un espectáculo, resulta claro que al robot le falta todavía un buen trecho para alcanzar la capacidad humana de comunicación no verbal, particularmente la capacidad de comunicación mediante expresiones faciales. De hecho, según los expertos, mientras que la comunicación verbal entre humanos y robots está progresando rápidamente por medio de la inteligencia artificial, la comunicación no verbal mediante gestos faciales no está avanzado con la misma rapidez. La comunicación no verbal entre humanos y robots es abordada en un artículo que apareció esta semana en la revista “Science Robotics”, publicado por un grupo de investigadores encabezado por Yuhang Hu de la Columbia University en Nueva York, que están particularmente interesados en la comunicación mediante intercambio de sonrisas.  Como apuntan en su artículo: “Pocos gestos son más afectuosos que una sonrisa. Pero, cuando dos personas se sonríen una a la otra de manera simultánea, el efecto se amplifica…. Poniéndolo en términos simples, si una sonrisa es simultánea, es más probable que sea genuina”. Así, un intercambio efectivo de sonrisas entre humanos y robots mejoraría su interacción y la haría más parecida a la interacción entre humanos.Para que un robot pueda sonreír simultáneamente con un humano tiene que anticipar su sonrisa. Es decir, tiene que distinguir los cambios sutiles en que ocurren en su rostro del humano que la anticipan. Y, por supuesto, debe saber cómo responder a la sonrisa con otra equivalente.Hu y colaboradores construyeron una cabeza robot antropomórfico que llamaron Emo, recubierta con una piel suave de silicón y con 26 actuadores para generar expresiones faciales que asemejen a las humanas. El robot fue equipado con cámaras en ambos ojos, lo que le dio una visión semejante a la humana.  La expresión facial está controlada por tres módulos para mover los ojos, la boca y el cuello. El módulo de ojos mueve los globos oculares, los párpados y las cejas, mientras que el módulo de boca permite reproducir los movimientos complejos de los labios con precisión.Emo fue inicialmente sujeto a un proceso de entrenamiento para que aprendiera a relacionar, viéndose en un espejo, una expresión facial dada con el accionar de un determinado actuador. El conocimiento que adquirió el robot durante el entrenamiento fue posteriormente utilizado durante su interacción con un humano cuando tuvo que responder a una determinada sonrisa. El robot fue igualmente sujeto a un segundo entrenamiento para que fuera capaz de identificar los pequeños cambios faciales que anteceden a una sonrisa humana. Para este propósito se emplearon 970 videos de 45 participantes humanos. El 80 por ciento de dichos videos fueron empleados en el entrenamiento, mientras que el resto fue utilizado para validarlo.  De acuerdo con Hu y colaboradores, sus resultados demuestran que es posible construir un robot con movimientos suficientemente complejos, el cual puede ser entrenado para generar una expresión facial anticipada en su interacción con un humano.  Reconocen, sin embargo, que el éxito de sus resultados solamente puede ser medido por la reacción que tengan los humanos a su interacción con el robot. En estas condiciones, consideran que: “Un paso futuro esencial es validar el efecto emocional de estas expresiones en las interacciones entre humanos y robots en diversos contextos en el mundo real para determinar sus efectos psicológicos”.Ciertamente, ver a Ameca con su cara azul pálido y su cuerpo de metal, gesticulando, moviendo sus brazos y contestando preguntas, produce una sensación extraña. ¿Cuál sería nuestra reacción ante un robot humanoide que se mueve y reacciona de una manera indistinguible de la de un humano?",
    "Se espera que con la transición energética la demanda de petróleo alcance en algún momento un valor máximo y que luego disminuya de manera paulatina en la medida en que los combustibles fósiles sean sustituidos por fuentes renovables de energía, como el sol y el viento. Sobre la base del crecimiento exponencial de las energías renovables y los automóviles eléctricos, la Agencia Internacional de la Energía de la Organización para la Cooperación y el Desarrollo Económico, pronostica en su reporte “Perspectiva Mundial de la Energía 2023” que la demanda máxima de petróleo se alcanzará al final de la presente década. Esto es fundamental para que el incremento de la temperatura del planeta no sobrepase el límite de 1.5 grados centígrados con respecto a sus valores preindustriales, límite que los expertos han fijado para evitar un desastre climático. No todo mundo es tan optimista, sin embargo. Por ejemplo, en la conferencia CERAWeek celebrada en Houston, TX la semana pasada, el director ejecutivo de la compañía petrolera Saudi Aramco calificó como una fantasía que se vaya a sustituir por completo el petróleo y el gas natural. De hecho, apunta que la demanda de dichos combustibles crecerá en los próximos años y que la Agencia Internacional de la Energía se equivoca, pues se enfoca solamente en los países industrializados y no en los países subdesarrollados en proceso de industrialización que necesitarán grandes cantidades de energía.  En el sitio de red “Super Spiked”, el analista Arjun Murti argumenta en la misma dirección. De acuerdo con Murti, cada uno de los mil millones de personas que viven en los Estados Unidos, Canadá, Europa Occidental, Japón, Australia y Nueva Zelanda -el mundo de los ricos-, usan anualmente 13 barriles de petróleo en promedio, mientras que los restantes 7,000 millones de personas que viven en el mundo subdesarrollado solo usan un promedio de 3 barriles por año. Así, aun si el mundo subdesarrollado no llegase a consumir una energía per cápita equivalente a la de los países ricos, habría una demanda futura de energía varias veces superior a la demanda actual. Podemos esperar, arguye Murti, que la demanda de petróleo continuará incrementándose cuando menos por los próximos diez años. Y concluye: “La idea de que el petróleo crudo no desempeñará ningún papel y disminuirá a nivel mundial es pura fantasía. Si miramos los números, no es algo que pueda sostenerse”.Por su lado, en su “Perspectiva Mundial de la Energía 2023”, la Agencia Internacional de la Energía escribe: “Hoy en día, la temperatura media mundial de la superficie ya está alrededor de 1.2 grados centígrados por encima de los niveles preindustriales, lo que provoca olas de calor y otros fenómenos meteorológicos extremos, y las emisiones de gases de efecto invernadero aún no han alcanzado su punto máximo. El sector energético es también la causa principal del aire contaminado que más del 90 por ciento de la población mundial se ve obligada a respirar, vinculado a más de 6 millones de muertes prematuras al año”.La Agencia Internacional de la Energía reconoce que el contexto energético mundial es complejo. Sin embargo, al mismo tiempo considera que el surgimiento de una nueva economía de energía limpia, encabezada por la energía solar fotovoltaica y los vehículos eléctricos, ofrece esperanzas sobre el camino a seguir para sortear la crisis energética. En este sentido, hace notar que las inversiones en energías limpias han crecido cuarenta por ciento desde 2020, y que estas inversiones no han tenido solamente el propósito de reducir las emisiones de gases de invernadero, sino otros objetivos también. Por ejemplo, el de incrementar la seguridad energética, particularmente de países importadores de petróleo.En unos cinco años podremos saber si la Agencia Internacional de la Energía tiene razón y la transición energética avanzará lo suficientemente rápido para que la demanda de petróleo alcance un pico máximo antes de que termine la década. O bien, si las compañías petroleras y los analistas escépticos están en lo cierto y el mercado de combustibles fósiles -y no el de las energías renovables- en los países en desarrollo crecerá con la suficiente rapidez para sostener el negocio. Y con esto agravar el problema del cambio climático.",
    "Como sabemos, con la irrupción de la inteligencia artificial se perderán numerosos trabajos debido a que la inteligencia artificial tiene habilidades que en algunos casos superan a las humanas. Para conocer estas habilidades consultamos a ChatGPT, que es producto, precisamente, de la inteligencia artificial.  De manera específica, le hicimos la siguiente pregunta: ¿cuáles son las cinco habilidades en las que la inteligencia artificial supera a los humanos? Como respuesta, el chat nos hace saber que somos superados por la inteligencia artificial en cuanto al procesado y análisis de grandes volúmenes de datos que puede realizar de una manera más rápida y eficiente. Igualmente, somos superados en la realización de tareas repetitivas, tanto en velocidad como en precisión, lo mismo que en el reconocimiento de patrones complejos de datos, de texto, imágenes o señales, y en juegos como el ajedrez y el Go, que requieren el despliegue de una estrategia compleja. Además, a diferencia nuestra, la inteligencia artificial hace su trabajo sin experimentar fatiga.Afortunadamente, los humanos tenemos habilidades en las que superamos (al menos por ahora) a la inteligencia artificial. Para conocer la opinión de ChatGPT al respecto le hicimos la siguiente pregunta: ¿Cuáles son las cinco habilidades en las que los humanos superamos a la inteligencia artificial? De acuerdo con el chat, los humanos tienen una capacidad de pensar de manera creativa, de generar ideas innovadoras y de encontrar soluciones a problemas complejos, que no han sido aun completamente replicados por la inteligencia artificial. Así mismo, los humanos superan a la inteligencia artificial en la inteligencia emocional que nos permite comprender nuestras emociones y las de los demás y guiar nuestra conducta en consecuencia. De acuerdo con ChatGPT, la inteligencia artificial carece de la capacidad de comprender y experimentar emociones de manera genuina. Entre otras ventajas que tenemos los humanos sobre la inteligencia artificial se incluye la capacidad para tomar decisiones apegadas a la ética. La inteligencia artificial, en contraste, solamente puede tomar decisiones basadas en datos y reglas prestablecidas. Igualmente, mientras que los humanos tenemos la flexibilidad suficiente para adaptarnos a entornos cambiantes, la inteligencia artificial aun no alcanza una adaptabilidad equivalente. Finalmente, los humanos practicamos la comunicación no verbal, que incluye el lenguaje corporal, el tono de voz y las expresiones faciales, habilidades que son esenciales para la interacción social. En este sentido, la inteligencia artificial tiene todavía una capacidad limitada.En el contexto anterior de fortalezas y debilidades, humanas y de algoritmos de inteligencia artificial, los expertos avizoran un mercado de trabajo en el que ambos, humanos y algoritmos, colaboren en diversos grados, dependiendo del tipo de trabajo. En este sentido, la red LinkedIn, propiedad de Microsoft, contempla los trabajos del futuro agrupados en tres grupos. Un primer grupo compuesto de trabajos en la que colaboran la inteligencia artificial y trabajadores humanos que aportan habilidades complementarias.  Un segundo grupo en que la inteligencia artificial tiene un papel dominante, con las actividades complementarias juagando un papel menor. Finalmente, en un tercer grupo el trabajo humano está relativamente protegido de la inteligencia artificial. En el escenario anterior, todas las generaciones de trabajadores están expuestas a la inteligencia artificial, sin bien para las generaciones más nuevas esta exposición es mayor. Así, de los miembros de LinkedIn que pertenecen a la generación “Baby Boomers” (1946-1964), el 53 por ciento está aislado de la inteligencia artificial, mientras que el 29 por ciento está fuertemente influenciado por la misma. En contraste, para la generación “Z” (1977-2012) los números correspondientes son 38 por ciento y 33 por ciento. El escenario que ha emergido con la inteligencia artificial es ciertamente disruptivo. Lo es al grado que plantea extrañas colaboraciones (ventajosas, por lo demás) entre humanos y algoritmos, contando estos últimos incluso con algunas capacidades que superan a las humanas. Ciertamente, como el mismo ChatGPT lo reconoce, los humanos contamos con capacidades que a la inteligencia artificial le costará superar. No obstante, que esto se mantenga en el futuro es algo que todavía está por verse.",
    "El 18 de mayo de 1854 partió de Inglaterra rumbo al ártico la expedición encabezada por John Franklin, con la misión de encontrar el llamado Paso del Noroeste, que conectaría a los océanos Atlántico y Pacífico por un laberinto de canales helados entre las islas al norte de Canadá. Como sabemos, la expedición, que estaba compuesta por los barcos Erebus y Terror y una tripulación de 129 hombres entre oficiales y marineros, tuvo un final trágico y nunca regreso a Inglaterra. No lo hicieron ni los barcos ni los expedicionarios, que murieron todos en el intento. La expedición fue avistada por última vez por barcos balleneros el 22 de julio en la bahía de Baffin y después desapareció al quedar los barcos atrapados en el hielo. Solo hasta fechas recientes se descubrieron los restos de los barcos hundidos en el ártico canadiense. En el año 2014 se encontró al Erebus y en 2016 al Terror. En cuanto a los tripulantes, se sabe ahora que, desesperados después de pasar dos inviernos atrapados en el hielo, decidieron abandonar los barcos y caminar hacia el sur tratando de alcanzar territorio continental canadiense. Sin embargo, no alcanzaron su objetivo y todos murieron en el camino. Se unieron así a los 24 tripulantes, incluyendo al capitán Franklin, que habían ya fallecido durante la larga espera.  Por lo demás, sabemos ahora que, de un modo u otro, la expedición muy probablemente no hubiera logrado encontrar el paso buscado, por el hielo que bloqueba la navegación entre las islas del ártico canadiense. De hecho, habría de transcurrir más de medio siglo antes de que el noruego Roald Amundsen lograra cruzarlo por primera vez en 1906, en una travesía que le tomó tres años. Si Franklin hubiera intentado realizar su búsqueda del Paso del Noroeste en la época actual -abordado una máquina del tiempo, por ejemplo-, habría encontrado mucho menos obstáculos, debido a que el cambio climático está ocasionando una disminución paulatina del volumen de hielo ártico lo que facilita el tránsito marino.  Con relación a esto último, un artículo aparecido el pasado 5 de marzo en la revista “Nature Reviews Earth and Environment”, hace una revisión del conocimiento que los expertos tienen acerca de la pérdida de hielo que está sufriendo el océano Ártico por el cambio climático y concluyen que podría haber ocasiones durante el verano en los que el océano Ártico esté libre de hielo antes de que termine la presente década. El artículo fue publicado por un grupo de investigadores encabezado por Alexandra Jahn de la Universidad de Colorado en Boulder.Habría que precisar que para Jahn y colaboradores un océano ártico libre de hielo no implica que el mar esté completamente desprovisto de hielo. En su lugar, para ellos el océano está libre de hielo cuando su superficie recubierta con hielo sea inferior a un millón de kilómetros cuadrados, equivalente a la mitad de la extensión territorial de nuestro país. Para poner esto en perspectiva, habría que señalar que millón de kilómetros cuadrados es apenas un 20 por ciento de la superficie del océano Ártico que estaba recubierta con hielo en el verano en la década de 1980.     Como señalan Jahn y colaboradores: “El hielo marino del Ártico ha disminuido sustancialmente desde el comienzo de las observaciones por satélite en 1978, y se prevé que continúe haciéndolo en el futuro. De hecho, podrían ocurrir condiciones sin hielo en el mes de septiembre en las décadas de 2020 o 2030, y a mediados del siglo presente es probable que sí ocurran, independientemente de cual fuera el escenario de emisión de gases de invernadero. Se espera que entre 2035 y 2067 se produzcan condiciones consistentemente sin hielo, lo que implica una transición a un Ártico frecuentemente libre de hielo. Esto, en un escenario de altas emisiones, con un pequeño retraso posible en escenarios de menores emisiones”.Si bien las predicciones que puedan hacer los expertos sobre algo tan complejo como es el cambio climático y el efecto preciso que pueda tener sobre el hielo ártico, es un hecho que el volumen de dicho hielo está disminuyendo. Y que dicha disminución ha tenido una consecuencia inesperada: está dificultando las labores de investigación que está llevando a cabo la agencia gubernamental canadiense Parks Canadá con los restos del Erebus y el Terror para desvelar los misterios que todavía encierran acerca de la malograda expedición de John Franklin. En efecto, de acuerdo con un reporte del periódico británico The Guardian, “El trabajo con los restos de los barcos hundidos es particularmente urgente, pues, al estar en aguas poco profundas, están siendo azotados y maltratados por las tormentas cada vez más severas a medida que el cambio climático se apodera de la región”.  Así, de manera por demás inesperada, el exceso de hielo en el Ártico hace 180 años dio al traste con la expedición de Franklin, mientras que ahora, el poco hielo dificulta averiguar los detalles de cómo ocurrió el desastre.",
    "De manera paulatina hemos perdido como especie el lugar privilegiado que alguna vez creímos tener en el mundo. Así, cuando Nicolas Copérnico en el siglo XVI hizo público su modelo de sistema solar en el que los planetas giran en torno al Sol y no alrededor de la Tierra, perdimos el lugar que, como centro del universo, ostentamos por miles de años. De manera similar, cuando Charles Darwin en el siglo XIX enunció su teoría sobre el origen de las especies, la nuestra resultó ser una especie más, que evolucionó y cambió drásticamente a lo largo de millones de años al igual que los hicieron todas las demás; si bien manteniendo un lugar destacado como la más inteligente de todas.Para intranquilidad nuestra, sin embargo, con la irrupción de la inteligencia artificial podríamos estar en peligro de perder este último bastión. La posibilidad se ha vislumbrado ya por varias décadas. Por ejemplo, en la película “2001 Odisea del Espacio” del director Stanley Kubrick estrenada en 1968, la nave Discovery 1 realiza un viaje a Júpiter en una misión secreta con cinco tripulantes a bordo. A cargo de la nave se encuentra HAL 9000, una super computadora dotada de inteligencia artificial. En una secuencia de la película, HAL se rebela y da muerte a cuatro de los cinco tripulantes de la nave. Dadas las circunstancias, el tripulante sobreviviente se da a la tarea de desactivar uno a uno los circuitos de la computadora, que trata de convencerlo de que se detenga. No lo logra y HAL es finalmente desactivada, no sin antes alcanzar a entonar una canción.Sin bien las máquinas dotadas con inteligencia humana han sido motivo de especulaciones por parte de expertos, y autores de obras de ficción desde la segunda mitad del siglo XX, no fue sino hasta fechas recientes que empezaron a hacerse realidad como consecuencia del desarrollo de computadoras cada vez más poderosas. En este sentido, en la red Internet podemos acceder de manera libre a chats dotados de inteligencia artificial, como ChatGPT y Gemini, que nos proporcionan repuestas estructuradas a preguntas o solicitudes que se les formulan. ¿Hasta que punto chats como ChatGPT están dotados con inteligencia equivalente a la inteligencia humana? Un artículo aparecido en la revista “Scientific Reports” el pasado 10 de febrero aborda esta pregunta, particularmente, en lo que se refiere a las capacidades creativas del chat. Dicho artículo fue publicado por un grupo de investigadores encabezado por Kent Hubert de la Universidad de Arkansas.Los expertos distinguen entre pensamiento convergente y pensamiento divergente. El primero se centra en encontrar la solución óptima para un problema dado, mientras que el segundo busca de manera libre múltiples soluciones a dicho problema. Hubert y colaboradores enfocaron su investigación en la creatividad divergente del chat ChatGPT y su comparación con la correspondiente creatividad humana. Para este propósito, conjuntaron un grupo de 151 voluntarios y los sometieron a tres tipos de pruebas para medir su nivel de creatividad divergente: “usos alternativos”, “consecuencias” y “asociaciones divergentes”. Así, a los participantes se les pidió en un tiempo finito que proporcionaran tantos usos alternativos como pudieran para un “tenedor” y para una “cuerda”, y se midió tanto el número de usos propuestos, como la originalidad y la elaboración (número de palabras) de sus respuestas. También se les solicitó que elaboraran tantas respuestas como pudieran a las consecuencias de lo siguiente: “que no tuviéramos necesidad de dormir”, o “que camináramos de manos”. De manera adicional, para medir su creatividad divergente, se les pidió que proporcionaran diez nombres sustantivos con significados tan diferentes como pudieran. En este caso, se midieron las diferencias entre las palabras proporcionadas mediante una prueba estándar. Finalmente, con propósitos de comparación, se sometió al chat ChatGPT a las mismas pruebas.Encontraron los investigadores que: “En general, GPT-4 fue más original y elaborado que los humanos en cada una de las tareas de pensamiento divergente, incluso cuando se tomó en cuenta el número de las respuestas. En otras palabras, GPT-4 demostró un mayor potencial creativo en una batería completa de tareas de pensamiento divergente (es decir, tarea de usos alternativos, tarea de consecuencias y tarea de asociaciones divergentes)”. No obstante, hacen notar que, si bien el chat ChatGPT mostró superioridad en creatividad con respecto a nosotros, dicha creatividad no se activa por si sola, sino con el concurso de los humanos. A pesar de esta última observación, y dada la velocidad con que avanza la tecnología, cabe preguntarse: ¿cuánto tiempo podría transcurrir antes de la creatividad de las computadoras se active por si sola y perdamos nuestro último bastión?",
    "Según datos de CONAGUA, del 15 de febrero del presente año, el ochenta por ciento del territorio nacional está afectado por algún grado de sequía. En este sentido, CONAGUA considera cinco categorías, que en orden ascendente de afectación son: anormalmente seco, sequía moderada, sequía severa, sequía extrema y sequía excepcional. Las áreas de nuestro país con la máxima categoría de sequía se encuentran, sobre todo, en los estados de Chihuahua, Sonora, Durango y San Luis Potosí. En nuestro estado, toda la región huasteca sufre de sequía excepcional. El municipio de San Luis Potosí, por su parte, está afectado por sequía extrema.El agua es un insumo que nos es indispensable, de modo que la sequía nos obliga a sustituir el agua que no nos proporcionan las lluvias. En este respecto, sabemos que el agua que usamos se clasifica fundamentalmente de dos tipos: superficial y subterránea. Las fuentes de agua superficial las constituyen los ríos, lagos o presas que se alimentan de la lluvia, mientras que el agua subterránea está almacenada en roca porosa en el subsuelo. Dicha agua representa, aproximadamente el 30 por del total del agua utilizada.  La extracción del agua subterránea, sin embargo, puede resultar en un abatimiento del nivel de un acuífero si el agua se extrae a una mayor velocidad que la velocidad de recarga natural del acuífero, lo que sabemos ocurre en una infinidad de casos. En efecto, basta considerar que uno de los efectos de la sobre extracción del agua subterránea es el hundimiento del suelo y la formación de grietas, de lo cual tenemos numerosos ejemplos.Mas allá de casos anecdóticos, un artículo aparecido el pasado mes de enero en la revista “Nature” nos da cuenta de los resultados de un estudio llevado a cabo para documentar la tendencia de los niveles de agua en las últimas décadas de pozos de agua en 40 países. El artículo fue publicado por un grupo de investigadores encabezado por Scott Jasechko de la Universidad de California en Santa Barbara. Jasechko y colaboradores compilaron y analizaron datos de estadísticas nacionales para determinar cómo han cambiado los niveles de agua de 170,000 pozos en 1,693 acuíferos.  Encontraron que, a lo largo del presente siglo, un 71 por ciento de los pozos disminuyó su nivel de agua. En un 36 por ciento de los acuíferos dicho nivel disminuyó a una velocidad de más de 10 centímetros por año, mientras que en un 12 por ciento esta velocidad superó los 50 centímetros por año. En contraste, solamente un 6 por ciento de los acuíferos aumentaron su nivel de agua por más de 10 centímetros por año, mientras que el 1 por ciento lo hizo por más de 50 centímetros por año. De manera adicional, los investigadores compararon estas tendencias con las observadas a lo largo de las dos últimas décadas del siglo pasado y encontraron que en un 30 por ciento de pozos la velocidad con que disminuyó el nivel de agua se hizo más rápida en el siglo presente. Así, encuentran Jasechko y colaboradores que el problema ha crecido en la medida en que transcurrió el tiempo.  Por otro lado, los investigadores encuentran que dicho problema tiende a ser peor en las áreas de cultivo en climas árido o semi-árido. Asimismo, encontraron una correlación entre la disminución de las precipitaciones pluviales a lo largo del año y el abatimiento del nivel de los acuíferos.  En este sentido, podríamos quizá esperar que la recarga de un acuífero sea más lenta por la ausencia de lluvias, al mismo tiempo que su explotación se incremente para suplir al agua superficial faltante.A pesar de sus resultados pesimistas con respecto al abatimiento creciente de los niveles de los acuíferos, el estudio de Jasechko y colaboradores también llama al optimismo. Así, los investigadores encuentran casos en los que dicho abatimiento se ha desacelerado o incluso revertido mediante políticas adecuadas de explotación del acuífero. Advierten, sin embargo, que la velocidad con las que están creciendo la profundidad de los pozos, en los casos en que esto está ocurriendo, es mayor que la velocidad con la que dicha profundidad está disminuyendo en los casos de éxito. De uno u otro modo, habría que reconocer que las consideraciones sobre el abatimiento de los niveles de los acuíferos pasan en estos momentos a un segundo plano, pues lo urgente por ahora es reponer el agua que la naturaleza nos está escamoteando. Pero no habría que echar en saco roto el estudio de Jasechko y colaboradores en cuanto pase el apuro.",
    "Con el advenimiento de las computadoras y la inteligencia artificial, muchas profesiones están en peligro de desaparecer. Consideremos, por ejemplo, la de erudito con conocimientos enciclopédicos en un determinado tópico o tópicos. Por supuesto, no existe la profesión de erudito como tal. En cambio, sí existe la red Internet en la que están almacenados una cantidad inmensa de datos a los que virtualmente cualquiera puede acceder. Así, por ejemplo, sin no recordamos la fecha y circunstancias de tal o cual hecho histórico, podemos fácilmente averiguarlo en Internet.  Por lo demás, un erudito no es solamente un almacén de datos, sino que, además de serlo, tiene la capacidad de analizarlos y de llegar a conclusiones, capacidad en la que por ahora no nos superan las máquinas. La profesión de erudito no está entonces en peligro de desaparecer. Al menos por el momento, pero cualquier cosa podría pasar en el futuro en la medida que se sofistiquen los algoritmos de inteligencia artificial disponibles para el público en general. En estas circunstancias, debemos ser cuidadosos cuando consultamos información de Internet. En particular, dado que cualquier persona puede subir información a la red, no podemos tener seguridad de que ésta sea cierta. Debemos así tener una actitud crítica para evaluar la información que obtengamos. El pensamiento crítico es una de las habilidades que consideran esenciales en la era digital, en la que las computadoras y sus algoritmos están cada vez más presentes y provocan cambios acelerados. Y esto no solamente en el día a día, sino también en el campo profesional. El pensamiento crítico, por otro lado, no es la única habilidad esencial para el desempeño profesional. Otras son la creatividad y la innovación, la habilidad para resolver problemas, la flexibilidad para adaptarse a los cambios, la capacidad para trabajar en equipo, y la inteligencia emocional para relacionarse con otras personas.El fomento de las nuevas habilidades debe ser entonces parte de los programas académicos de formación profesional de nivel universitario, particularmente de aquellos programas clasificados dentro en el área STEM, un acrónimo en inglés para los términos Ciencia, Tecnología, Ingeniería y Matemáticas. La educación STEM contempla un entrenamiento multidisciplinario en áreas de ciencia e ingeniería, y enfatiza el desarrollo en el estudiante de las nuevas habilidades profesionales.  Esta educación, que se considera clave para el futuro, está recibiendo un gran impulso. Hoy en día, según cifras de “National Science Foundation”, una cuarta parte de los puestos de trabajo en los Estados Unidos corresponden a áreas STEM.  El pasado jueves 15 de febrero, con motivo la celebración del 30 aniversario de la licenciatura en Ingeniería Física de la UASLP, tuvimos ocasión de reflexionar sobre la educación STEM en nuestra Universidad.  Mencionaremos que dicha licenciatura se inició en el verano de 1993 como parte de la oferta académica de la Facultad de Ciencias. De acuerdo con la vocación académica de la licenciatura, que busca formar especialistas en campos científicos y tecnológicos, la totalidad de las actividades académicas se llevan a cabo en las instalaciones del Instituto de Investigación en Comunicación Óptica de la UASLP y están a cargo de sus investigadores.Desde su creación, el programa de la licenciatura de Ingeniería Física incluyó características de la educación STEM. A saber, una educación multidisciplinaria en las áreas de física, matemáticas, computación, electrónica y óptica, cubriendo tanto aspectos teóricos como prácticos. En cuanto a estos últimos, se enfatiza el aprendizaje mediante proyectos. Así, el estudiante aprende a plantear un proyecto y a trabajar en su solución. Lo hace, además, en conjunción con otros estudiantes. Se entrena de este modo en el trabajo en grupo y en el desarrollo de su creatividad. Los cursos teóricos, por otro lado, lo impulsan a desarrollar un pensamiento crítico.  Los egresados de la licenciatura se han incorporado tanto a la academia como profesores, como al sector productivo. La mayor parte de ellos laboran en México, pero los hay también quienes radican en el extranjero. Aproximadamente la mitad de los egresados han buscado proseguir con estudios de posgrado. De estos, un 22 por ciento ha alcanzado un grado doctoral, en instituciones tanto de México como del extranjero. La licenciatura en Ingeniería Física de la UASLP nació hace tres décadas, en la misma época en la que se empezó a delinear el concepto STEM en los Estados Unidos, pero sin que hubiera puntos de contacto. Aun así, podemos encontrar muchas de las características de la educación STEM en el programa de Ingeniería Física, incluyendo el fomento de las habilidades que se consideran clave para el siglo XXI. Siendo un proyecto nacido enteramente en la UASLP y dado su éxito, consideramos que la licenciatura en Ingeniería Física constituye un posible modelo para la educación STEM en México.",
    "Según la “Alzheimer’s Association”, aproximadamente el 11 por ciento de los norteamericanos mayores de 65 años sufre de la enfermedad de Alzheimer. En contraste, la prevalencia de esta enfermedad en la población Tsimané mayor de 60 años apenas sobrepasa el 1 por ciento. Los Tsimané son un pueblo que habita en las tierras bajas de Bolivia y practica un estilo de vida preindustrial con una gran actividad física y una dieta baja en grasas que los mantiene sin sobrepeso. Además de la baja prevalencia de Alzheimer, los Tsinamé son conocidos por gozar también de una baja incidencia de enfermedades cardiovasculares.Lo anterior sugiere que hay una correlación entre el estilo de vida y la incidencia de la enfermedad de Alzheimer y otras demencias. En este sentido, un artículo aparecido el pasado 25 de enero en la revista “Journal of Alzheimer Disease” concluye que hay pocas evidencias de la enfermedad de Alzheimer en sus fases avanzadas en registros históricos del mundo Greco-Romano. Dicho artículo fue publicado por Caleb Finch y Stanley Burnstein, de la Universidad del Sur de California y de la Universidad Estatal de California, de manera respectiva.Dado que dos mil años atrás no habia términos técnicos que describieran la enfermedad de Alzheimer, Finch y Burnstein buscaron pasajes en textos en los que se hiciera alusión a una pérdida de la capacidad mental con la edad y encuentran que Hipócrates en el siglo IV a.C. listó enfermedades de la vejez, incluyendo problemas digestivos, físicos, de sordera y de pérdida de la vista, y de insomnio, pero no encuentran alusiones a la pérdida de la capacidad mental. Así mismo, encuentran que Aristóteles, también en el siglo IV a.C., describió el deterioro cognitivo con la edad: “Los hombres viejos y aquellos que ha pasado ya de su mejor edad tienen en su mayoría caracteres que son opuestos a los de la juventud. Ellos viven en la memoria mas que en la esperanza…..hablan incesantemente del pasado porque encuentran placer en los recuerdos”. El texto de Aristóteles, sin embargo, no incluyó una descripción de una pérdida severa la capacidad cognitiva característica de la enfermedad de Alzheimer.Finch y Burnstein consultaron también textos de la Roma antigua en busca de alusiones a enfermedades mentales asociadas a la edad. Escribe Cicerón en el siglo I a.C.: “La tontería de los ancianos, que generalmente se llama delirio es característica de viejos irresponsables, pero no de todos los viejos”. Cicerón, sin embargo, no incluyó al deterioro mental como uno de los cuatro males de la vejez, a saber: el fin de una vida activa, la debilidad física, el cese de placeres y la cercanía a la muerte. Así, la pérdida de memoria con la edad era algo inusual en Roma como lo era en Grecia.No obstante, encuentran Finch y Burnstein un mayor número de incidencias de afectaciones mentales severas con la edad en Roma en comparación con Grecia. Así, Juvenal escribió en el siglo II d.C.: “Mayor que todos los daños a las extremidades es la demencia, que no conoce los nombres de los esclavos ni la cara del amigo con el que cenó la noche anterior o a aquellos a quienes engendró e hizo crecer\". Especulan Finch y Burnstein que el incremento en el número de menciones al deterioro mental con la edad está asociado con un incremento en la contaminación atmosférica y ambiental de Roma en comparación con Grecia.  Otra posible explicación, aventuran los investigadores, es la contaminación con plomo, pues se sabe que los romanos añadían como endulzante al vino un compuesto que contenía plomo. De la misma manera, las tuberías que conducían el agua que bebía la población contenían plomo.  Concluyen los investigadores que, si bien identificaron textos griegos y romanos en los que se describen casos de deterioro mental con la edad, los casos de un deterioro severo equivalente al que se observa en la actualidad con la enfermedad de Alzheimer u otras enfermedades mentales son escasos. Estos, a su vez, fueron aparentemente menos frecuentes en Grecia que en Roma, sugiriendo que están relacionados a factores ambientales. Por supuesto, a dos mil años de distancia es difícil llegar a conclusiones sólidas, pero la baja incidencia de la enfermedad de Alzheimer entre los Tsimané -que viven en una sociedad preindustrial, sujetos a condiciones similares a las de los antiguos griegos y romanos- sugiere que se lleven a cabo investigaciones históricas adicionales.",
    "La Luna no es un lugar acogedor para los visitantes. Entre otras cosas porque no tiene aire para respirar, además de que las temperaturas que ahí se experimentan pueden oscilar entre 120 grados centígrados durante el día y menos 150 grados centígrados durante la noche. Estas variaciones de temperatura, que son debidas a que la Luna no tiene atmósfera que regule su temperatura y a la larga la duración de los días y noches lunares que se extienden por dos semanas, hacen imposible sobrevivir en la Luna sin equipo especial. Si bien en la Tierra no llegamos a estos extremos, sabemos que hay lugares con variaciones de temperatura que pueden llegar a ser incómodas. En lugares secos, por ejemplo -como es el caso de nuestra ciudad- las noches despejadas producen mañanas frescas por la pérdida de calor debida a la ausencia de nubes que reflejen el calor emitido por la superficie de la Tierra. Al mismo tiempo, en días sin nubes el flujo de radiación solar incrementa la temperatura ambiental de manera considerable.  En la situación actual, en la que el planeta está experimentando un cambio climático, cabe preguntarse en qué medida dicho cambio está afectando las diferencias de temperatura entre el día y la noche. Esta pregunta es abordada por un artículo aparecido el pasado mes de noviembre en la revista “Nature Communications”. Dicho artículo fue publicado por un grupo internacional de investigadores encabezado por Ziquian Zhong de la Universidad Normal de Beijing, y en el mismo se concluye que, en los últimos 30 años, por efecto del cambio climático, el incremento en las temperaturas diurnas sobrepasa al incremento de las temperaturas nocturnas. Esto significa que la brecha entre ambas temperaturas está creciendo.  La diferencia de incrementos de temperatura entre el día y la noche, conocido como “incremento asimétrico”, ha sido observado desde la segunda mitad del siglo pasado. Entre 1961 y 1990. dicho incremento fue negativo, lo que significa que la diferencia entre las temperaturas diurna y nocturna estaba disminuyendo. En su artículo, Zhong y colaboradores encuentran que en las últimas tres décadas esta tendencia se ha revertido, de modo que las temperaturas diurnas están creciendo más rápidamente que la nocturnas y la brecha entre ambas temperaturas está aumentando. De manera precisa, encuentran los investigadores que entre 1961 y 1990 en el 81 por ciento de la superficie terrestre el incremento asimétrico era negativo, mientras que a partir de 1991 dicho incremento se tornó positivo en el 70 por ciento de dicha superficie.Zhong y colaboradores basaron su estudio en dos conjuntos de observaciones meteorológicas que cubren todo el planeta, si bien no lo hacen de manera suficientemente amplia para Sudamérica y África. En estas condiciones, encuentran los investigadores que ambos conjuntos de observaciones arrojan resultados similares para Norteamérica, Europa, Asia y Australia. No es el caso de Sudamérica y África, posiblemente por datos insuficientes, concluyen los investigadores. Las causas que llevaron a este cambio de tendencias son complejas, como lo es el clima de la Tierra. Sin embargo, consideran Zhong y colaboradores que dicho cambio está asociado a una disminución de nubosidad. En palabras de estos investigadores: “Descubrimos que el cambio en la tendencia del calentamiento asimétrico está estrechamente relacionado con cambios en la radiación solar asociados con la nubosidad total. Este hallazgo ofrece una nueva visión y una perspectiva diferente sobre el cambio climático en las últimas décadas. Dado que las nubes pueden continuar teniendo una retroalimentación positiva sobre el calentamiento global a través de flujos de radiación, el fenómeno de incremento en la temperatura diurna por encima del de la temperatura nocturna puede persistir y potencialmente intensificarse en el futuro.  Por lo tanto, es necesario prestar más atención a este fenómeno de calentamiento asimétrico, desde la perspectiva de abordar los desafíos que plantea el calentamiento global”.No esperaríamos, por supuesto, que la brecha entre las temperaturas diurnas y nocturnas alcanzara los niveles observados en la Luna -ni aún los de Marte, que también serían letales. Según los expertos, sin embargo, mayores brechas de temperatura podrían afectar a la producción de alimentos e incluso a nuestra salud. En este sentido, podrían llevar a un mayor ritmo cardiaco y presión sanguínea, y a un incremento en enfermedades cardiovasculares y respiratorias. Así, con el calentamiento asimétrico añadimos una raya más al tigre del calentamiento global.",
    "Para celebrar el juego de futbol americano del día de hoy entre los equipos Leones de Detroit y los 49ers de San Francisco por el campeonado de la Conferencia Nacional de Futbol, la fachada del edificio de la Estación Central de Michigan en Detroit se iluminó con la palabra “LIONS”, con letras de siete pisos de altura. En este sentido, habría que recordar que desde hace tres décadas el equipo no había tenido un éxito equivalente.La Estación Central de Michigan fue inaugurada en diciembre de 1913 y se mantuvo en servicio como estación de ferrocarril hasta el año de 1988, cuando fue cerrado definitivamente. A partir de ese momento, el edificio se deterioró de manera paulatina, convirtiéndose en un símbolo de la decadencia de la ciudad de Detroit. En estas condiciones fue adquirido por la compañía Ford que lo esta restaurando.Como sabemos, como parte de proceso de desindustrialización que ha sufrido el medio oeste y el nordeste norteamericanos generando la llamada franja de la herrumbe, Detroit ha perdido dos terceras partes de su población desde el año 1950 y sufrido un deterioro urbano que incluyó al edificio de la Estación Central de Michigan.Detroit es quizá el ejemplo más dramático del proceso de desindustrialización que se ha dado en áreas del norte y este de los Estados Unidos y que ha incluido a ciudades como Flint en Michigan y Gary en Indiana, el cual ha llevado a un proceso de despoblamiento urbano. Los especialistas consideran que este proceso continuará, y en este sentido un artículo aparecido este mes en la revista “Nature Cities” considera y analiza los desafíos que enfrenarán las ciudades norteamericanas en el resto del siglo por la pérdida de población. El artículo fue publicado por un grupo de investigadores encabezado por Uttara Sutradhar de la Universidad de Illinois en Chicago.En su artículo, Sutradhar y colaboradores consideran cerca de 30,000 ciudades -definidas como aglomerados de pobladores- que clasifican como urbanas, suburbanas, periurbanas y rurales, dependiendo de su localización, número de casas-habitación y número de habitantes. Distinguen entre áreas urbanas y periurbanas sobre la base del tiempo diario de traslado. Los investigadores consideran, además, cuatro áreas geográficas de los Estados Unidos: nordeste, medio-oeste, oeste y sur. Encuentran Sutradhar y colaboradores que el 43 por ciento de las ciudades estudiadas están disminuyendo su población, incluyendo a las mayores ciudades del medio oeste, mientras que el 40 por ciento la están incrementando y en el 17 por ciento restante dicha población está fluctuando. En cuanto a las tendencias futuras, los investigadores encuentran que, si bien todas las urbanas en las áreas geográficas oeste y sur habrán ganado población en el año 2100, en las áreas nordeste y del medio oeste cerca del 17 por ciento de las ciudades urbanas la perderán en cierto grado. De manera global, las 30,000 ciudades estudiadas perderían en el año 2100 hasta un 25 por ciento de su población.Las ciudades norteamericanas pueden perder pobladores porque estos se mueven a los suburbios o bien a otras ciudades. En cualquier caso, como señalan Sutradhar y colaboradores, la pérdida de pobladores deja infraestructura urbana subutilizada que tiene que ser desmantelada y presiona a las finanzas de la ciudad por la disminución del número de contribuyentes.   En México, en donde la disminución de población solo ha ocurrido en nuestra escala de tiempo en casos puntuales, el proceso que está ocurriendo en nuestro vecino del norte nos parece quizá algo lejano. De hecho, si hemos de juzgar por lo que vemos en nuestra ciudad, lejos de un encogimiento urbano, somos testigos de un crecimiento acelerado por el acuerdo comercial con los Estados Unidos y Canadá. Cruzamos los dedos, no obstante, para todo sigan sin novedad. Por lo demás, esperemos que Detroit se recupere. Y no solo en lo que respecta al futbol americano.",
    "El pasado 16 de enero el Instituto Tecnológico de California (Caltech) publicó un comunicado en el que daba cuenta de la terminación exitosa del proyecto “Space Solar Power Demostrator” (SSPD-1) que tuvo como objetivo evaluar tres tecnologías clave para el aprovechamiento de la energía solar desde el espacio. El sol es una fuente de energía prácticamente inagotable, pero al mismo tiempo intermitente, no disponible durante horas de la noche. Así, colectar energía solar en el espacio, en donde el Sol brilla de manera permanente, tiene ventajas claras y según algunos expertos podría contribuir a mitigar la crisis climática. Para este propósito, se han concebido proyectos como el de Caltech para colocar en una órbita terrestre espejos que reflejen la luz del Sol y la enfoquen en paneles solares para su conversión en energía eléctrica. La energía eléctrica así generada será a su vez convertida en radiación de microondas que son dirigidas hacia una estación receptora en la superficie terrestre que la transforma nuevamente en energía eléctrica para su distribución. El proyecto SSPD-1 tuvo como propósito llevar a cabo tres experimentos: DOLCE, ALBA y MAPLE. DOLCE, según el comunicado de Caltech, “es una estructura que mide 1.8x1.8 metros que demuestra la novedosa arquitectura, el esquema de empaquetado y los mecanismos de despliegue de una estructura modular escalable que eventualmente formará una constelación de escala kilométrica que servirá como central eléctrica”. Según el mismo comunicado, ALBA “es una colección de 32 tipos diferentes de celdas fotovoltaicas para permitir una evaluación de los tipos de celdas que pueden soportar entornos espaciales exigentes”, mientras que MAPLE “es un conjunto de transmisores de potencia de microondas livianos y flexibles basados ​​en circuitos integrados personalizados con control de sincronización preciso para enfocar la energía de manera selectiva en dos receptores diferentes para demostrar la transmisión de energía inalámbrica a distancia en el espacio”.De acuerdo con la comunicación de Caltech, los tres experimentos fueron llevados a cabo de manera exitosa y servirán como punto de partida para desarrollar la energía solar espacial. En particular, se pudo demostrar el envío de energía mediante microondas entre dos puntos en el espacio, así como desde el espacio hacia el laboratorio de los investigadores en Caltech.  Por otro lado, al margen de los resultados exitosos del proyecto SSPD-1, la energía solar espacial según algunos expertos está todavía lejos de ser una realidad. No solamente por las pérdidas de energía que de manera inevitable se producen por la conversión de la energía solar en eléctrica y de ahí en microondas y nuevamente en eléctrica, sino fundamentalmente por el alto costo que implica colocar en órbita una instalación solar con dimensiones de kilómetros.   Con relación a esto último, hay que señalar que de manera óptima es necesario colocar a la instalación solar como un satélite de la Tierra en un punto fijo en el firmamento, es decir, en una órbita geo-sincrónica con un periodo de rotación de 24 horas, de modo que el satélite gire a la misma rapidez que rota la Tierra. La altura de una órbita sincrónica es de unos 35,000 kilómetros, aproximadamente un décimo de la distancia Tierra-Luna. Una órbita más baja reduciría el costo de desplegar la instalación solar, pero al mismo tiempo ésta giraría a una mayor velocidad que la Tierra y serían necesarios varios satélites solares para proporcionar energía continua a un punto dado sobre la superficie terrestre. En la actualidad se están desarrollando cohetes más potentes capaces de poner en órbita grandes pesos -150 toneladas en el caso del cohete Starship de SpaceX- y con seguridad en algún momento será posible ensamblar en una órbita geo-sincrónica satélites solares para aplicaciones terrestres, algo que de momento suena todavía de ciencia-ficción.  Dichos satélites serían capaces de proporcionar energía a partir de una fuente limpia y prácticamente inagotable. Además, de manera continua, ya que una instalación solar espacial no está sujeta ni al ciclo día-noche ni a las vicisitudes del clima de la Tierra, pues la radiación de microondas llegaría a la superficie de la Tierra aun a través de una capa de nubes.Habría, no obstante, que hacer algunas precisiones. Primero, para evaluar a la energía solar espacial como una fuente limpia de energía habría que tomar en cuenta que poner en órbitas geo-sincrónicas estructuras kilométricas requiere de quemar enormes cantidades de combustible que generan residuos contaminantes. Y segundo, que un satélite en una órbita geo-sincrónica no está sujeto al clima de la Tierra, pero sí al clima del espacio con radiaciones de alta energía que degradan la eficiencia de los paneles solares. Así, como es lo usual, habría ganancias, pero no a costo cero.",
    "Hoy en día podemos adquirir con unos cuantos pesos un peine de plástico ordinario, de unos 20 centímetros de largo, dividido en dos secciones, una con 75 de dientes y otra con 40 dientes. Así, podemos peinarnos a un costo esencialmente igual a cero. Esto no era el caso en el pasado, cuando los peines se fabricaban de manera artesanal empleando materiales como la madera, el hueso o el marfil y tenían un precio equivalente muy superior al de un peine de plástico. En este sentido, habría que considerar el enorme esfuerzo que era necesario para fabricar artesanalmente un peine de hueso o marfil con decenas de dientes separados por poco más de un milímetro.El costo de un peine de plástico en la actualidad resulta tan bajo porque se fabrican en serie, empleando materiales de muy bajo costo. Como sabemos, la industria de los plásticos, que son materiales artificiales que no existen en la naturaleza, tuvo su origen en las primeras décadas del siglo pasado, y ha tenido tanto éxito que a los plásticos los podemos encontrar prácticamente en todas partes. En la actualidad, la producción anual de plásticos a nivel mundial está cerca de los 400 millones de toneladas. Desafortunadamente, una vez que los plásticos cumplen con su vida útil se acumulan como desechos contaminantes a lo largo de todo el planeta -anualmente más de 30 millones de toneladas-. Se sabe que estos desechos plásticos sufren con el tiempo un proceso de fragmentación, generando lo que se conoce como micro plásticos, definidos como partículas de plástico con dimensiones entre 5 milímetros y un micrómetro. Para apreciar esta última dimensión, habría que recordar que el diámetro de un cabello humano ronda los 100 micrómetros.Se sabe también que los micro plásticos se fragmentan aún más, dando lugar a los nano plásticos, definidos como partículas de plástico con dimensiones inferiores a un micrómetro. Los nano plásticos son motivo de preocupación entre los expertos, pues por su pequeño tamaño pueden traspasar las barreras biológicas y penetrar en el cuerpo humano, con una toxicidad que es aún desconocida.      Los nano plásticos están virtualmente en todas partes en el planeta Tierra, incluyendo en el agua que bebemos y en la comida que ingerimos. En particular, se sabe que están presentes en el agua potable embotellada en recipientes de plástico, y en este sentido, un artículo aparecido esta semana en la revista “Proceedings of the National Academy of Sciences” cuantifica la densidad de nano plásticos presente en algunas marcas de agua embotellada que se comercializa en los Estados Unidos.  El artículo fue publicado por un grupo de investigadores encabezado por Naixin Quian de la Columbia University en Nueva York.Para llevar a cabo su estudio, Quian y colaboradores desarrollaron una técnica que permite detectar partículas con dimensiones tan pequeñas como un décimo de micrómetro, al mismo tiempo que determina su composición química. Los investigadores detectaron un promedio de 240,000 partículas por litro de agua, un 90 por ciento de las cuales son nano plásticos. Esto representa de 100 a 1000 veces más partículas que las reportadas con anterioridad. Los investigadores pudieron detectar 7 diferentes tipos de plásticos en el agua analizada. Estos, sin embargo, representan apenas el 10 por ciento de todas las partículas detectadas. Es decir, la identidad química del 90 por ciento de estas partículas es desconocida.Basados en los tipos de nano plásticos identificados, Quian y colaboradores sugieren que algunas de las partículas contaminantes pudieran haber sido liberadas de la botella de plástico durante el transporte o el almacenaje. En otros casos, los contaminantes pudieron haberse generado durante la producción del agua, posiblemente por la membrana utilizada en el proceso de purificación del agua. Así, dicho proceso habría producido agua contaminada con nano plásticos.La Asociación Internacional del Agua Embotellada reaccionó de manera negativa al artículo de Quian y colaboradores y de alguna manera descalificó su método de determinación de nano plásticos. Así, en la página web de dicha asociación podemos leer: “Actualmente faltan métodos estandarizados y no hay consenso científico sobre los posibles impactos en la salud de las partículas de nano y micro plásticos. Por lo tanto, los informes de los medios sobre estas partículas en el agua potable no hacen más que asustar innecesariamente a los consumidores”.Es difícil descalificar los resultados de Quian y colaboradores cuyo artículo pasó por un proceso riguroso de revisión de pares. Por otro lado, de acuerdo con los expertos, sí faltarían estudios sobre la toxicidad de los nano plásticos. En conclusión, la emergencia de los plásticos no solamente nos permitió fabricar peines baratos para arreglarnos el cabello, lo mismo que un sinnúmero de otros objetos que nos cambiaron la vida, sino que también ha traído una contaminación invisible cuya toxicidad está por determinarse.",
    "¿En qué medida la irrupción de la inteligencia artificial cambiará el mercado de trabajo? ¿Cuáles son las profesiones que están en mayor peligro de desaparecer por la emergencia de esta tecnología? Para encontrar una respuesta acudimos a Bard, el chatbot de Google que, lo mismo que otros chatbots, proporciona respuestas estructuradas a preguntas específicas, a partir de informaciones almacenadas en Internet. Dado que Bard mismo es producto de la inteligencia artificial, confiamos en nos haya proporcionado una respuesta con conocimiento de causa. De manera específica, le hicimos al chatbot la siguiente pregunta: ¿Cuáles trabajos desaparecerán por Bard? Su respuesta se resume a continuación.“Es difícil predecir con exactitud cuáles trabajos desaparecerán por la llegada de Bard u otras inteligencias artificiales similares. Sin embargo, podemos analizar los tipos de tareas que estas tecnologías podrían automatizar, y por tanto poner en riesgo ciertos empleos”. Como trabajos que tienen una alta probabilidad de automatización, el chatbot considera aquellos que involucran tareas rutinarias y repetitivas, pues “Bard puede procesar información y realizar tareas repetitivas mucho más rápido que los humanos”. Esto, según Bard, podría afectar a trabajos de captura de datos, de servicio al cliente y de redacción de contenido básico. Con relación a esto último, comenta el chatbot: “Aunque Bard puede generar contenido básico como informes o resúmenes, aún carece de la creatividad y matices de los escritores humanos. Sin embargo, podría automatizar aspectos como la investigación de mercado o de análisis de datos para la creación de contenido”.En su respuesta, el chatbot considera también trabajos con menos riesgo de desaparecer, pero sí con probabilidades de transformación. Esto involucra trabajos que combinan tareas rutinarias y creativas, en cuyo caso, “Bard podría automatizar las tareas rutinarias dejando a los humanos la parte creativa y estratégica”. Esto podría beneficiar a trabajos como el periodismo, de modo tal que “Bard puede ayudar con la investigación, la verificación de datos y el análisis de datos, permitiendo a los periodistas centrarse en la narración de historias y la realización de reportajes en profundidad”.La inteligencia artificial puede transformar también el trabajo de los desarrolladores de software. Según el chatbot: “Bard puede generar código y automatizar tareas repetitivas, liberando a los desarrolladores para que se concentren en el diseño, la arquitectura y la resolución de problemas”. Puede, igualmente, transformar la enseñanza: “Bard puede personalizar los materiales de aprendizaje y proporcionar retroalimentación automatizada, permitiendo a los profesores centrarse en las necesidades individuales de los estudiantes y en la interacción en el aula”.  Así, si hemos de creerle a Bard, cuyas opiniones son compartidas por los expertos, la inteligencia artificial tendrá un gran impacto en el mercado de trabajo, eliminando o transformando puestos de trabajo, y al mismo tiempo creando otros con nuevas cualificaciones. En este sentido, el Foro Económico Mundial en su “Reporte 2020 sobre el Futuro del Empleo” lista los empleos con demandas crecientes y decrecientes. Entre los empleos a la baja, incluye los siguientes: capturista de datos, secretaria administrativa y ejecutiva, contador y tenedor de libros, mecánico y reparador de máquinas, analista financiero y cajero de banco, entre otros. Por otro lado, entre los empleos al alza se cuentan:  analista y científico de datos, especialista en inteligencia artificial y aprendizaje automático, especialista en “big data”, especialista en mercadeo digital, especialista en automatización, especialista en internet de las cosas, gerente de proyectos e ingeniero en robótica, entre otros muchos.El mercado de trabajo del futuro demandará de cambios en el sistema educativo para empatar con nuevas habilidades requeridas por los nuevos empleos. Según Bard, en el futuro serán clave las siguientes habilidades: pensamiento crítico y resolución de problemas, creatividad e innovación, e inteligencia social y emocional. Por los demás, al menos por ahora se concibe a las máquinas digitales como colaboradoras de los humanos, formando una dupla con una “inteligencia híbrida”. Cada integrante de la dupla se especializa en cierto tipo de actividades que se complementan para potenciar sus capacidades y realizar en conjunto tareas más allá de lo que podrían llevar a cabo de manera independiente. Cabe preguntarse, sin embargo, si dada la velocidad a la que están avanzando las tecnologías digitales, no llegará un momento en el que a las máquinas no les represente una ventaja su asociación con los humanos. Habría que señalar, no obstante, que por ahora Bard no contempla esta posibilidad.",
    "El año que termina el día de hoy es sin duda el año de la inteligencia artificial. Como recordamos, fue el 30 de noviembre de 2022 que la compañía OpenAI lanzó el chatbot ChatGPT, capaz de procesar grandes cantidades de información y proporcionar respuestas coherentes a preguntas y solicitudes hechas por el usuario. El éxito de ChatGTP fue instantáneo y en menos de un año alcanzó semanalmente 100 millones de usuarios. Siguiendo al lanzamiento de OpenAI, a lo largo de 2023 otras compañías, incluyendo a Google, Amazon, Microsoft e IBM, dieron también a conocer sus propias versiones de chatbot.  Por otro lado, 2023 no fue un año positivo para otra aplicación de la inteligencia artificial: los automóviles autónomos. De hecho, la revista “MIT Technology Review” incluye a los robotaxis, o automóviles de alquiler sin conductor, de la compañía Cruise en su lista de los peores fracasos tecnológicos del año. Esto, juntamente con el submarino Titan que implosionó cuando se sumergió 3,500 metros en el océano Atlántico tratando de llegar a los restos del Titanic, y el supuesto desarrollo de un superconductor a temperatura ambiente hecho público por investigadores coreanos y que resultó ser un fiasco.   Cruise es una empresa propiedad de General Motors, fabricante de automóviles, que mantenía un servicio de robotaxis en la ciudad de San Francisco, California. En un inicio a Cruise le fue solamente permitido operar su servicio de taxis en un horario restringido de menor tráfico, pero el 10 de agosto pasado obtuvo permiso para operar en un horario abierto. Poco le duró el gusto, sin embargo, pues por una serie de incidentes de tráfico fue obligada a reducir a la mitad el número de robotaxis en servicio. Peor aún, el 24 de octubre la licencia para operar taxis autónomos le fue revocada por un incidente en la que una persona sufrió heridas graves y en el que estuvo involucrado uno de sus robotaxis. En dicho incidente, ocurrido el 2 de octubre, una mujer fue atropellada por un vehículo conducido por un humano y arrojada en el camino del robotaxi que circulaba a su lado. Sin alcanzar a detenerse, el robotaxi golpeó a la mujer y le pasó por encima antes de detenerse brevemente. En seguida, sin notar que estaba encima de la mujer atropellada, el robotaxi se movió a baja velocidad para apartarse del tráfico, arrastrando a la mujer por aproximadamente seis metros.  Con la revocación de su permiso, Crusier solamente puede operar robotaxis si llevan a bordo un conductor humano de seguridad que pueda tomar control del vehículo en una emergencia, de modo tal que su autonomía estaría limitada. Habría que señalar, no obstante, que de entrada dicha autonomía era limitada y que la operación de los robotaxis de Cruise está vigilada de forma remota por conductores humanos de seguridad. En este sentido, un artículo publicado el pasado mes de noviembre en el periódico New York Times, apunta que la flota de robotaxis de Cruise está apoyada de manera remota por un equipo de conductores de seguridad -en promedio 1.5 conductores por cada automóvil en circulación- que intervienen en su operación cada 4-8 kilómetros de recorrido. Es decir, en el manejo de los taxis sin conductor interviene un número de conductores de seguridad mayor que el número de conductores humanos que serían necesarios para manejar un número equivalente de taxis no autónomos.Con respecto al affaire Cruise, lo expertos señalan que el proceso de aprendizaje de los automóviles autónomos empleando técnicas de inteligencia artificial no es todavía lo suficientemente completo para que puedan percibir correctamente la gran variedad de situaciones que podrían encontrar en su camino. Por ejemplo, que como resultado de un accidente haya terminado arriba de una persona, lo que lo debe llevar a permanecer estático. Por lo demás, de dicho affaire quizá podamos aprender algunas cosas. En primer término, que los automóviles realmente autónomos no están todavía a la vuelta de la esquina. Y en segundo término, que aún si se  resolvieran los problemas de entrenamiento de los automóviles autónomos para ciudades como San Francisco, con un tráfico ordenado, no necesariamente se habrán resuelto para ciudades con menor orden urbano, en donde lo automóviles tendrían que sortear toda clase de situaciones inesperadas, desde la presencia de un bache capaz de destrozar una llanta, hasta el manejo rápido y agresivo de los conductores (humanos) que a toda costa quieren adelantar al automóvil que tienen enfrente o cruzar con el semáforo casi en rojo.",
    "El día de mañana habrán transcurrido dos años desde que el 25 de diciembre de 2021 la NASA lanzó al espacio el telescopio James Webb, y lo colocó en una órbita alrededor del Sol a una distancia de 1.5 millones de kilómetros de nuestro planeta -aproximadamente cuatro veces la distancia Tierra-Luna-. Dicho telescopio está equipado con dos cámaras infrarrojas, que a lo largo de los dos últimos años nos han proporcionado imágenes espectaculares del Universo con una resolución sin precedente. Una de estas imágenes fue dada a conocer por la NASA el pasado 10 de diciembre y será parte de un calendario de adviento incluido en la decoración navideña de la Casa Blanca del presente año. La imagen en cuestión corresponde a Cassiopeia A, un objeto en el firmamento distante de la Tierra unos 11,000 años luz. Los astrónomos saben que este objeto corresponde a un remanente de supernova, es decir, de una estrella que colapsó al final de su vida liberando una enorme cantidad de energía. Dicha imagen fue tomada con luz infrarroja que es invisible al ojo humano. Empleando falso color, sin embargo, la NASA tradujo la luz infrarroja a luz visible y construyó una imagen que puede ser apreciada a simple vista. La imagen de Cassiopeia A, que puede observarse en el comunicado de la NASA, es ciertamente, espectacular y podría ser utilizada para decorar un árbol navideño como apunta la agencia espacial. Nos muestra un anillo brillante en expansión de 10 años luz de diámetro, con una estructura intrincada y colores naranja y rosa. Vista desde la Tierra, la explosión estelar habría ocurrido hace unos 350 años. Habría que recordar, sin embargo, que la imagen de Cassiopeia A corresponde a un acontecimiento ocurrido hace 11,000 años, que es el tiempo que le tomó a la luz en llegar hasta nosotros.Además de su espectacularidad, Cassiopeia A es atractiva porque encierra un misterio. En efecto, dada la enorme cantidad de energía que libera una supernova, es difícil no avistar su aparición en el firmamento. La supernova que originó Cassiopeia A tendría así que haber sido observada hace unos 350 años. No hay, sin embargo, una evidencia histórica incontrovertible de que así haya sido, si bien el asunto es controversial. En este sentido, se ha argüido que la supernova fue avistada por el Astrónomo Real británico John Flamsteed el 16 de agosto de 1680, quién reportó la observación de una estrella conocida como 3 Cassiopeia cercana A, en una posición en donde ninguna estrella se observa en la actualidad. No obstante, no hay un acuerdo entre los expertos de que Flamsteed haya realmente observado la supernova y el asunto sigue sin aclararse.En un comentario aparecido en noviembre de 2009 en la revista “Nature” comenta Geoff Brumfiel: “Los restos de la supernova conocida como Cassiopeia A, han constituido un misterio para los astrónomos. Las supernovas usualmente producen un objeto extremadamente denso tal como un agujero negro o una estrella de neutrones. Pero por décadas no se ha observado un objeto con estas características en el centro de Cassiopeia A”. Así las cosas, en 1999 el observatorio Chandra de rayos X identificó un objeto compacto en el lugar correcto. Este objeto, sin embargo, no corresponde a lo que esperaban los expertos, de acuerdo con Craig Heinke de la Universidad de Alberta en Canadá. No obstante, Heinke y su colega Wynn Ho de la Universidad de Southampton en el Reino Unido consideran que con este descubrimiento es posible explicar a Cassiopeia A, que contendría una estrella de neutrones en su centro.  Habría, sin embargo, explicaciones alternativas de modo tal que no existe una explicación ampliamente para la naturaleza de Cassiopeia A.¿Observó Flamsteed en el siglo XVII la supernova que generó a Cassiopeia A? Y en todo caso ¿Cuál es la naturaleza íntima de este objeto estelar? No lo sabemos con seguridad por el momento, y tendremos que esperar por más estudios futuros. No conocer las respuestas a estas preguntas, sin embargo, no nos impide admirar la belleza pictórica de las imágenes de Cassiopeia A, ni utilizarlas como adornos navideños de gran originalidad. Ni admirarnos de la maravilla tecnológica que produjo imágenes espectaculares a millones de kilómetros de la Tierra.",
    "En el año 1900, el 41 por ciento de los trabajadores norteamericanos eran agricultores, mientras que en el año 2000 este porcentaje se redujo al 2 por ciento. Esta drástica reducción fue debida a la mecanización de la agricultura y al incremento consecuente en la productividad agrícola, que redujo el número de trabajadores necesarios para mantener la producción de alimentos. Esta reducción es un ejemplo dramático de la pérdida de puestos de trabajo por un avance tecnológico; en este caso por el desarrollo de maquinaria agrícola que eficientizó la producción de los granjeros estadounidenses. El caso no es único, por supuesto, y la pérdida de trabajos es un suceso común cuando irrumpe una nueva tecnología.  Así, los profesionales con habilidades para la mecanografía encontraron que las mismas ya no les eran útiles para encontrar un trabajo. De la misma manera que los profesionales de la fotografía del siglo pasado se encontraron con que sus habilidades para revelar negativos fotográficos e imprimir fotografías eran obsoletos ante la aparición de las cámaras fotográficas digitales. Los trabajos secretariales o de fotografía profesional, por otro lado, no desaparecieron, sino que se transformaron, demandando nuevas habilidades: para manejar el procesador de palabras de una computadora o para imprimir fotografías digitales.  La destrucción de puestos de trabajo y la creación de otros nuevos con habilidades diferentes ocurre ante la irrupción de una nueva tecnología, proceso que ha ocurrido rápidamente en el caso de la tecnología de la computación.  Particularmente en lo que se refiere a la inteligencia artificial y la minería de datos, que son tecnologías disruptivas que se apuntan para hacer desaparecer y al mismo tiempo crear muchos puestos de trabajo en un sinnúmero de áreas. Consideremos, a manera de ejemplo, la profesión de abogado y en particular el concepto de “abogado computacional” acuñado por un grupo de investigadores encabezado por Dell Zhang de Thomson Reuters Lab., en el artículo intitulado “Haciendo un abogado computacional”. Dicho artículo apareció el pasado mes de abril en las memorias del congreso “SIAM International Conference on Data Mining”, y fue comentado por los autores el pasado 1 de diciembre en “SIAM News”. De acuerdo con Zhang y colaboradores, el concepto de abogado computacional se refiere a: “Un software inteligente capaz de ayudar a los abogados humanos con una amplia gama de tareas legales complejas de alto nivel”. Como apuntan los autores, el software iría más allá de simplemente llevar a cabo tareas mundanas de procesamiento de información legal, tal como la redacción de escritos legales para la acusación o la defensa en el tribunal que pueden ser realizadas por ayudantes, y auxiliarían a los abogados humanos con tareas complejas, incluyendo la elaboración de escritos para acusación o defensa ante el tribunal. El abogado computacional tendría acceso a una extensa base de datos legales y una vez que alcance un cierto grado de madurez, “podría empezar a exhibir habilidades emergentes, una de las cuales es el razonamiento legal”. Como comentan Zhang y colaboradores, un futuro abogado computacional debe cumplir ciertos requisitos. Primeramente, debe actualizarse y mantenerse al día con cambios en el ámbito legal. Para facilitar este proceso podrá emplear técnicas de aprendizaje continuo, desaprendizaje automático y minería de datos. Deberá también razonar dentro de los límites del ámbito legal de la jurisdicción relevante, y sus opiniones y sentencias jurídicas deben derivar de leyes y normas pertinentes. Los abogados computacionales deberán ser capaces de “captar detalles sutiles y matices en las instrucciones de otros abogados o clientes legales. También deberían ser enseñables, en el sentido de que sigan aprendiendo de las demostraciones y comentarios humanos”. Zhang y colaboradores no esperan que los abogados computacionales reemplacen a los abogados humanos, “sino que colaboren con ellos como socios competentes y confiables”. Sin embargo, si atendemos a las características que listan en su artículo, los futuros abogados computacionales aprenderían y se mantendrían al día como humanos, razonarían como humanos, redactarían sus escritos como humanos, y se comunicarían con otros abogados como humanos. Así, como abogados, les faltaría poco para ser indistinguibles de los humanos. Y dada la velocidad a la que está avanzando la tecnología de inteligencia artificial, cabe preguntarse si llegará el tiempo en el que, tal como sucedió con granjeros, mecanógrafos y fotógrafos, los abogados humanos tengan problemas para encontrar un trabajo. Esto estaría por verse, pero lo que sí es claro es que tal posibilidad debe ser tomada en cuenta en los planes de formación de nuevos abogados (humanos).",
    "El viernes último terminó el semestre académico en la UASLP, y en el Instituto de Investigación en Comunicación Óptica, como es ya tradicional, se llevó a cabo el evento de exposición de proyectos finales por parte de estudiantes de licenciatura y posgrado. En la exposición pudimos observar estudiantes exponiendo resultados de proyectos variados que involucraban técnicas avanzadas de cómputo para simular la síntesis de un material, o el desarrollo de juegos por computadora. Así mismo, otro grupo de estudiantes expuso un interesante proyecto para la fabricación de prismas de calcita -material abundante en el país- para aplicaciones especializadas y que alcanzan un alto valor de mercado, mientras que otro grupo nos explicó su trabajo sobre el desarrollo de técnicas de inteligencia artificial para clasificar imágenes. Todo esto, entre muchos otros trabajos que se presentaron.La experiencia nos hizo reflexionar sobre los grandes cambios que ha experimentado la UASLP en las últimas décadas. En efecto, si un estudiante de la Universidad en la década de los años setenta del siglo pasado se hubiera trasladado por medio de una “máquina del tiempo” a la universidad actual, con seguridad habría pensado que por alguna razón la máquina había cometido un error que lo llevó a un lugar equivocado. Entre otras cosas, habría notado una gran cantidad de programas de maestría y doctorado que no le eran familiares, así como grupos de investigación con competencia internacional que no existían en su tiempo.Todo lo anterior revela una trayectoria de la Universidad hacia la profesionalización de la enseñanza, impartida, en buena medida, por grupos de profesores de tiempo completo que, además de enseñar, son investigadores activos. De hecho, los estudios de doctorado requieren de realizar una tesis original que típicamente es asesorada por un investigador en activo.La investigación en la universidad, por otro lado, no es importante solamente para los estudios de posgrado, sino también para los de licenciatura. Esto es particularmente cierto para campos de estudio que evolucionan rápidamente y que requieren profesores que estén en constante actualización. Un ejemplo actual, dramático por lo demás, es la inteligencia artificial, que irrumpió y se hizo omnipresente en el curso de un año, después de décadas de latencia, y que amenaza con destruir muchas especialidades de trabajo, incluyendo la de programador de computadora que ahora sería tomado por un programa de inteligencia artificial.En estas circunstancias ¿cómo deben modificarse los planes de estudio de las universidades para adaptarlos a los cambios en el estado del conocimiento y evitar generar desempleados?  Estas modificaciones solo las pueden llevar a cabo profesores capaces de reaccionar con rapidez, lo que típicamente implica profesores-investigadores en activo.Así, la investigación debe ser un elemento fundamental de la UASLP, como lo es en las llamadas “universidades de investigación”. En este sentido, la “Asociación Americana de Universidades”, que agrupa a las más importantes universidades de investigación de los Estados Unidos, dice en su página de Internet:“La investigación básica crea los componentes básicos de futuros productos y procesos, algunos de los cuales pueden desarrollarse rápidamente, mientras que a otros les puede tomar décadas. Además de crear nuevos conocimientos, las universidades utilizan sus actividades de investigación para educar a los estudiantes que se convertirán en los científicos de la próxima generación, ingenieros, profesores y líderes del gobierno y la industria.”“Esta fusión de educación e investigación de vanguardia ha sido una característica única del sistema de investigación universitaria de los Estados Unidos… El éxito del modelo estadounidense ha convertido a las universidades del país en la envidia del mundo, y el modelo es ahora ampliamente emulado. Estados Unidos continúa atrayendo a un número significativo de los más talentosos estudiantes internacionales por la extraordinaria educación y oportunidades de capacitación que ofrecen las universidades de investigación intensiva del país.”Una vez que se reponga de la sorpresa, nuestro viajero en el tiempo entenderá que la máquina del tiempo no se equivocó y que se encuentra en su “alma mater”, solo que muy evolucionada. Habría de reconocer, no obstante, que todavía le queda mucho camino por recorrer, en particular, en lo que se refiere al fomento a la investigación y al crecimiento del cuerpo de profesores-investigadores. Solo así podremos contribuir a que algún día tengamos un país competitivo tecnológicamente y no solamente un país maquilador, que es atractivo solamente por su mano de obra barata y por su cercanía a los Estados Unidos. Y, por supuesto, evitar que nuestra universidad se convierta en una fábrica de desempleados.Por lo demás, la exposición de proyectos de fin de curso que tuvimos el pasado viernes nos pone optimistas al respecto.",
    "Días antes del inicio de la Conferencia de la Naciones Unidas sobre Cambio Climático que se desarrolla en Dubái del 30 de noviembre al 12 de diciembre del presente año, la Agencia Internacional de Energía (AIE) hizo público el documento: “Las industrias del petróleo y gas en transiciones netas cero”, en el que se analiza el papel de estas industrias en la mitigación del cambio climático. Como sabemos, los acuerdos de París de 2015 fijan como una meta limitar a 2 grados centígrados el incremento en la temperatura del planeta con respecto a sus valores preindustriales, e instan a hacer esfuerzos para que este incremento no sobrepase los 1.5 grados centígrados y el documento de la AEI plantea que las industrias productoras de gas y petróleo deben asumir un papel activo en la mitigación del calentamiento global.De acuerdo con la AIE, se mantiene abierta la posibilidad de limitar el calentamiento global a 1.5 grados centígrados. No obstante, esta agencia hace notar que, si bien la energía solar ha experimentado un auge y las compras de automóviles eléctricos se han disparado -uno de cada cinco automóviles eléctricos vendidos a nivel mundial en 2023 será eléctrico-, alcanzar una emisión cero neta en 2050 requiere de más acciones. En particular, se requiere la participación activa de todas las empresas del ramo energético. Señala la AIE que hay dos concepciones erróneas con respecto al papel de las empresas energéticas en la mitigación del calentamiento global. La primera de ellas es que “las transiciones energéticas solo pueden ser impulsadas por un cambio en la demanda de energéticos”, de modo tal que el papel de las empresas de energía sería simplemente esperar a que baje la demanda y disminuir su producción en consecuencia. Lejos de esto, según la AIE, “Nadie comprometido con el cambio debería esperar a que otro actúe primero. Las transiciones exitosas y ordenadas son aquellas en las que los proveedores trabajan con los consumidores y los gobiernos para expandir nuevos mercados para productos y servicios bajos en emisiones”. De acuerdo con una segunda concepción errónea, la captura y almacenamiento de carbono como un medio para limitar la contaminación atmosférica no es viable como estrategia, para pretender mantener el nivel actual de consumo de combustibles. Como explica la AIE: “Si el consumo de petróleo y gas natural evolucionaran como proyectan las políticas actuales, en 2050 se requeriría la captura inconcebible de 32 mil millones de toneladas de carbono para limitar el aumento de la temperatura a 1,5 grados centígrados. Esto requeriría una potencia eléctrica equivalente a la demanda mundial de electricidad en 2022, y más de 3.5 millones de millones de dólares en inversiones anuales desde hoy hasta mediados de siglo, que es una cantidad equivalente a los ingresos promedio anuales de toda la industria en los últimos años.Se espera que la demanda de combustibles fósiles alcance un máximo al final de la presente década. Mas allá de este punto, sin embargo, con la tendencia actual la demanda por dichos combustibles no disminuirá lo suficientemente rápido para limitar el calentamiento global a 1.5 grados centígrados. Hoy en día, por otro lado se invierten anualmente 800 mil millones de dólares en el sector del petróleo y el gas, que son el doble de lo que se necesitará en 2030 para limitar el calentamiento a 1,5 °C. De este modo, plantea la AEI, en un escenario en que cae la demanda de petróleo y gas - el uso de estos combustibles caería más de un 75% en 2050-, las compañías de energías no tendrán necesidades de llevar a cabo nuevos proyectos en combustibles fósiles y podrán dedicar el 50 por ciento de su inversión al desarrollo de proyectos de energías limpias. En esta dirección, sin bien algunas compañías podrán usar sus conocimientos tecnológicos para desarrollar nuevos proyectos de energías limpias, otras dirigirán sus inversiones hacia otros campos. El documento de la AIE hace énfasis en que exista un diálogo entre productores y consumidores de petróleo y gas: “El diálogo entre todas las partes de las cadenas de valor del petróleo y el gas sigue siendo esencial para lograr un abandono ordenado de los combustibles fósiles y garantizar que los productores de hoy tengan un interés significativo en la economía de la energía limpia… Las transiciones energéticas pueden ocurrir sin la participación de la industria del petróleo y el gas, pero el viaje hacia el cero neto será más costoso y difícil de recorrer si no están a bordo”.Por nuestro lado, podríamos quizá esperar que una industria que tiene ganancias anuales que alcanzan millones de millones de dólares, tenga disposición para participar en la solución de un problema que ella misma ayudó a crear.",
    "En una escena de la película “2001: Odisea del Espacio” del director Stanley Kubrick, podemos ver a una azafata a bordo de una nave espacial con una charola en sus manos, caminando en posición horizontal por las paredes del recinto y posteriormente de cabeza por el techo del mismo. Esto lo hace empleando zapatos que se adhieren a las superficies y en virtud de que en el espacio no hay gravedad, de modo tal que los conceptos arriba y abajo pierden su significado. De la misma manera, en otra escena de la misma película vemos a los astronautas en viaje a Júpiter que hacen ejercicio corriendo en una superficie curva en el interior de una nave que rota para simular la fuerza de gravedad. Para filmar estas escenas, Kubrick se asesoró con expertos y produjo una película que resulta verosímil desde el punto de vista de las leyes de la física. Por este motivo, y por muchos otros elementos producto del genio de Kubrick, “2001: Odisea del Espacio”, estrenada en 1968, estableció una referencia para las películas de ciencia ficción.Lo anterior viene a colación por el estreno esta semana de la película “Napoleón” del director Ridley Scott, que algunos especialistas han criticado por su falta de exactitud histórica. En particular, se señala que, durante su invasión a Egipto a finales del siglo XVIII, Napoleón nunca bombardeó la pirámide de Guiza tal como aparece en la película. De hecho, Napoleón tenía un gran aprecio por Egipto, como lo demuestra el hecho que haya llevado consigo como parte de la expedición militar, un batallón de más de 160 artistas y científicos de todas las disciplinas que estudiaron y documentaron el pasado egipcio, y con quienes Napoleón creó el “Instituto de Egipto” en El Cairo. Bombardear la pirámide de Guiza -una de las siete maravillas de la antigüedad- no pareciera entonces ser una acción que Napoleón hubiera considerado llevar a cabo.Igualmente, consideran los expertos que la escena de la película de Scott en donde Napoleón asiste a la ejecución de María Antonieta nunca ocurrió. En el momento de dicha ejecución, en octubre de 1793 en París, Napoleón se habría encontrado en la ciudad de Tolón en el sur de Francia combatiendo a los realistas sublevados contra la república. De hecho, el éxito militar que Napoleón tuvo en Tolón fue el inicio de la carrera que lo llevó a convertirse en emperador. En otra inexactitud, María Antonieta habría llevado el cabello corto en el momento de su ejecución y no largo como aparece en la película.  Por otro lado, sorprende que, como respuesta a las críticas a las inexactitudes de su película, el director Scott haya preguntado a sus críticos: ¿Estuvieron ahí? No, no estuvieron, entonces ¿cómo pueden saber?  En el mismo tenor, el director de una película de ciencia ficción en donde se oye un estruendo cuando algo explota en el espacio -de las que hay ejemplos- podría responder a quien mencionara que el suceso es imposible, pues en el espacio no hay aire y por tanto no hay sonido: ¿Ya estuvieron ahí para corroborarlo?Incidentalmente, en algún momento, Stanley Kubrick planeó hacer una película sobre Napoleón. Sin bien sus intenciones finalmente no se concretaron, la información que recopiló para este fin la empleó para filmar la película “Barry Lyndon” de 1975, en la que un joven irlandés participa como combatiente en la Guerra de los Siete Años, librada en Europa entre los años 1756 y 1763. En esos años, por supuesto, aun no existía la luz eléctrica, y para situar a la película en la época, Kubrick filmó las escenas interiores empleando exclusivamente luz de velas.El cine es una industria de entretenimiento y por tanto podríamos esperar que los directores de cine se tomaran algunas libertades históricas o de otro tipo para maximizar las probabilidades de que una película alcance el éxito comercial. Habría que preguntarse, sin embargo, si esto justifica que se falseen hechos históricos o se presenten escenas en donde se violan los principios de la física, sin que el director haga obvio que éste es el caso. Por lo demás, directores como Stanley Kubrick demuestran que el negocio del cine no está necesariamente en contra de reflejar al mundo, pasado y presente, tal cual es.",
    "“Ver para creer”, dice el refrán, aludiendo a que el sentido de la vista es indispensable y suficiente para percibir al mundo tal cual es. Así, si los ojos nos están indicando tal o cual situación, debemos asumirla como cierta.  Tal parece, sin embargo, que la inteligencia artificial está dando al traste con el refrán, al menos en lo que respecta a la percepción de la imagen de una persona en la pantalla de una computadora, misma que podríamos concluir corresponde a una persona real, pero que en realidad podría haber sido construida por medio de un algoritmo de inteligencia artificial. ¿Tenemos la habilidad para distinguir si la fotografía del rostro de una persona que aparece en la pantalla de la computadora corresponde a la de una persona física o bien se trata de una imagen sintetizada por un programa de cómputo? En un artículo aparecido esta semana en la revista “Psychological Science”, un grupo de investigadores encabezado por Elizabeth Miller de la Universidad Nacional de Australia, abordan esta pregunta.   En una etapa de su proyecto de investigación, Miller y colaboradores hicieron uso de un conjunto de 100 fotografías del rostro de personas reales y 100 imágenes faciales de personas ficticias generadas por un algoritmo de inteligencia artificial, todas ellas de raza blanca. Como parte del experimento, presentaron 100 imágenes de dicho conjunto a un grupo de 124 adultos de raza blanca y les pidieron que decidieran cuáles de estas imágenes eran reales y cuáles falsas. Encontraron que el 66 por ciento de las imágenes falsas fueron clasificadas como reales, mientras que el 51 por ciento de las fotografías reales fueron correctamente clasificadas como pertenecientes a personas reales. Es decir, una mayoría de los participantes dieron una respuesta incorrecta con respecto a las imágenes generadas por inteligencia artificial y concluyeron que pertenecían a personas reales. Fueron así, en su mayoría, engañadas por el algoritmo de inteligencia artificial que las construyó. Esto constituye un fenómeno que los investigadores llaman hiperrealismo de inteligencia artificial. En contraste, las respuestas con respecto a las fotografías de personas reales fueron tan frecuentemente correctas como incorrectas, lo que implica que, en promedio, los participantes no pudieron distinguir su origen.Miller y colaboradores señalan que la tendencia a clasificar incorrectamente como reales las imágenes de rostros de raza blanca generados por computadora no se observa con imágenes, reales o ficticias, de personar de color, que de manera equilibrada son clasificadas correcta o incorrectamente. Esto explicaría los resultados de su experimento, dado que el algoritmo de inteligencia artificial que fue empleado para generar las imágenes fue entrenado mayoritariamente con rostros de personas de raza blanca. Así, la computadora se especializó en generar imágenes de personas de raza blanca, más que de personas de color. Y se especializó a tal grado que es capaz de engañarnos. La generación preferencial de imágenes de raza blanca por computadora podría tener un impacto racial. En palabras de Amy Dawel, uno de los autores del artículo de referencia: “Si los rostros blancos de IA se perciben consistentemente como más realistas, esta tecnología podría tener serias implicaciones para las personas de color al reforzar en última instancia los prejuicios raciales en línea. Este problema ya es evidente en las tecnologías de inteligencia artificial actuales que se utilizan para crear fotografías de apariencia profesional. Cuando se usa con personas de color, la inteligencia artificial altera su piel y color de ojos a los de las personas blancas”. Por su parte, Elizabeth Miller comenta: “De manera preocupante, las personas que pensaron que los rostros generados por inteligencia artificial fueron reales, muy a menudo fueron paradójicamente los más convencidos de que sus juicios fueron correctos. Esto significa que las personas que confunden a los impostores de la inteligencia artificial con personas reales no saben que están sido engañados”.De todo lo anterior, pareciera ser que, efectivamente, el hiperrealismo de inteligencia artificial está poniendo en jaque al refrán “Ver para creer”, atribuido a Tomás Apóstol y que por tanto tendría dos milenios de antigüedad. Amenaza igualmente al refrán “Una imagen vale más que mil palabras”, pues ¿qué valor tendría una imagen que puede ser engañosa? Por lo demás, si bien la inteligencia artificial es el origen del problema, también proporciona su solución. Así, Miller y colaboradores reportan en su artículo el desarrollo de un algoritmo de inteligencia artificial que detecta rostros falsos con una exactitud del 94 por ciento.",
    "Sabemos que el cuerpo debe mantener una temperatura de alrededor de los 37 grados centígrados y para esto debe disipar el exceso de calor generado por su metabolismo a través de diferentes mecanismos. Uno de estos mecanismos involucra al sudor, que actúa como un refrigerante que absorbe calor del cuerpo al evaporarse en contacto con la piel. La eficiencia de la sudoración como mecanismo de refrigeración corporal, sin embargo, depende de las condiciones ambientales, específicamente de la humedad atmosférica.Podemos entender este hecho de la siguiente manera. Cuando el sudor en contacto con la piel se evapora, lo hace tomando una cierta cantidad de calor y contribuyendo así a disminuir la temperatura del cuerpo. Para que el sudor se evapore, sin embargo, la atmósfera debe estar en capacidad de recibirlo y esto depende de la cantidad de vapor de agua que ya contiene; es decir, de la humedad atmosférica. Así, si la atmosfera está saturada de humedad, el sudor no podrá evaporarse y perderá su capacidad como refrigerante. En caso de que la saturación del aire no sea completa, el sudor conservará esta capacidad, pero reducida en mayor o menor grado dependiendo del grado de saturación de humedad de la atmósfera.Sabemos por experiencia que, efectivamente, el sudor pierde su capacidad refrigerante en ambientes húmedos, de modo tal que la incomodidad que produce una temperatura de, por ejemplo, 40 grados centígrados, es mayor en un clima húmedo que en uno seco. Este hecho nos es familiar y no nos produce demasiada preocupación. Podría, sin embargo, convertirse en un dolor de cabeza por el cambio climático que está produciendo eventos de calor extremo. Con relación a esto último, consideran los expertos que la máxima temperatura de bulbo húmedo compatible con el funcionamiento del cuerpo es de 35 grados centígrados. Más allá de este valor, el cuerpo no podría regular su temperatura y podría sobrevenir la muerte.  Hay que señalar que una temperatura de bulbo húmedo -medida con un termómetro cuyo bulbo está envuelto en una tela húmeda- es una medida, no solamente de la temperatura del aire, sino también de la humedad atmosférica, y que de alguna manera refleja el grado de incomodidad que experimentamos. Si la humedad atmosférica es del 100 por ciento, la temperatura de bulbo húmedo coincide con la temperatura de bulbo seco, que es la que nos es familiar. Por otro lado, con una humedad atmosférica de 75 por ciento, una temperatura de bulbo húmedo de 35 grados centígrados corresponde a una temperatura del aire de 41 grados centígrados. Según los expertos, solamente por excepción han ocurrido, por breves periodos de tiempo, episodios que han alcanzado una temperatura de bulbo húmedo de 35 grados centígrados. Estos episodios, sin embargo, serán más frecuentes en la medida en que avance el cambio climático, según concluye un artículo aparecido el pasado 9 de octubre en la revista “Proceeding of the National Academy of Sciences” de los Estados Unidos. El artículo fue publicado por un grupo de investigadores encabezado por Daniel Vecellio de Pennsylvania State University. Vecellio y colaboradores consideran cuatro escenarios, con incrementos de temperatura atmosférica de 1.5, 2, 3 y 4 grados centígrados con respecto a sus valores preindustriales. Hacen notar que hemos sobrepasado ya 1 grado centígrado de incremento y que, de no mitigarse la emisión de gases de invernadero, se alcanzarían los 3 grados centígrados en el año 2100. En su análisis, los investigadores encuentran cuatro áreas particularmente vulnerables: el norte de la India, el este de China, el Medio Oriente y el África subsahariana. Por ejemplo, con un incremento de 2 grados centígrado en la temperatura atmosférica, la ciudad de Lahore, Pakistán, experimentaría 147 horas anuales con temperaturas de bulbo húmedo que excederían los 35 grados centígrados, mientras que un incremento de 3 grados centígrados elevaría este número de horas a 447. Números similares se obtienen para la ciudad de Bandar Abbas en Irán. Un caso especial es la ciudad de Al Hudaydah Ade en Yemen, que experimentaría 300 días anuales con exceso de temperatura, si se incrementara la temperatura atmósfera por 4 grados centígrados, lo que la convertiría en inhabitable. Además, habría que mencionar que las áreas más afectadas se encuentran entre las más pobladas del mundo. Con respecto a nuestro país, de manera afortunada la situación no luce tan sería y solamente habría consecuencias para la región noroeste y para la costa del golfo de México si la temperatura atmosférica tuviera un incremento de 4 grados centígrados. En palabras de Vecellio y colaboradores: “Estos resultados indican que una porción significativa de la población mundial experimentará, por primera vez en la historia de la humanidad, exposiciones prolongadas a un calor húmedo extremo no compensable”. Ciertamente, la situación en las próximas décadas no luce muy promisoria.",
    "El 4 de septiembre de 1882 entró en operación la planta “Pearl Street Station” para proporcionar energía eléctrica a la ciudad de Nueva York. Dicha planta, construida por la compañía Electric Illuminating Company de Thomas Alva Edison, fue la primera estación comercial a nivel central para generar y distribuir electricidad. El negocio de la electricidad en el siglo XIX generó intensas disputas comerciales. Una de las más conocidas es la llamada “guerra de las corrientes”, que enfrentó a Edison y la corriente continua, por un lado, y a George Westinghouse y la corriente alterna, por el otro. Recordamos que en las dos últimas décadas del siglo XIX convivían dos tecnologías eléctricas que se disputaron la supremacía: la corriente directa y la corriente alterna. En un sistema de corriente alterna, el sentido de la corriente cambia de dirección de manera alternada, mientras que en un sistema de corriente continua la corriente siempre tiene la misma dirección. Con la corriente alterna, empleada por Westinghouse, es posible incrementar o reducir el voltaje de manera relativamente simple. Esto tiene ventajas para trasmitir la potencia eléctrica generada en la planta central hasta los usuarios, la cual experimenta menos pérdidas si la línea opera a altos voltajes. Así, en el sistema de Westinghouse, el voltaje de la línea de trasmisión se elevaba a miles de volts y se reducía hasta un nivel seguro justo antes de entregar la energía al usuario. El sistema de corriente continua empleado por Edison, en contraste, no tenía la habilidad de incrementar y reducir el voltaje de la línea de trasmisión y esto limitaba la distancia a la que podría trasmitir la energía eléctrica.En esta situación de desventaja, Edison hizo uso de todo tipo de artimañas. Por ejemplo, buscó crear la impresión de que la corriente alterna es peligrosa y capaz de matar a una persona por accidente. Para ahondar en esta dirección, Edison hizo claro que la silla eléctrica, recién instrumentada como método de ejecución, empleaba corriente alterna, lo que probaba su letalidad. Edison mismo contribuyó al desarrollo de la silla eléctrica, como es relatado en el libro “Edison y la silla eléctrica” del autor Mark Essig. Sin embargo, al final, con todo y artimañas, la corriente alterna se impuso a la corriente directa. Así, prevaleció un modelo en el que la energía eléctrica se produce en grandes plantas centrales y se distribuye a grandes distancias por medio de líneas de trasmisión de alto voltaje.Un modelo alternativo, que está emergiendo rápidamente, es el de la generación distribuida, en el que la energía se produce y se consume en un mismo lugar. Esto ha sido posible por el desarrollo de tecnologías para aprovechar la energía del sol o del viento, que están distribuidas sobre toda la superficie del planeta, si bien no de manera uniforme. En particular, la progresiva reducción de precios que están experimentando los paneles solares hacen cada vez más atractiva su instalación en las casas habitación.¿Qué tan extendida será en el futuro esta tendencia y en qué medida los consumidores generarán su propia energía y no dependerán de una instalación central? Un artículo publicado el pasado 2 de noviembre en la revista “Joule” por un grupo de investigadores encabezado por Max Kleinebrahm, del Instituto de Tecnología de Karlsruhe, Alemania, buscan contestar a esta pregunta.Kleinebrahm y colaboradores llevaron a cabo un estudio para determinar el potencial para que 41 millones de casas habitación en países de la Unión Europea, más el Reino Unido y Noruega, alcancen una independencia energética con respecto a la red pública de electricidad. Evaluaron este potencial, tanto para la época actual como para el año 2050. Para este propósito, seleccionaron 4,000 casas habitación representativas. En este sentido, hay que considerar que las regiones incluidas en el estudio incluyen áreas climáticamente variadas, desde el sur hasta el norte de Europa. En base a su estudio, los investigadores encuentran que, en la actualidad, desde un punto de vista técnico, el 53 por ciento de los 41 millones de casas habitación consideradas podrían alcanzar una independencia total con respecto a la red pública de electricidad. Proyectan, además, que en el año 2050 este número alcance el 75 por ciento.Desde un punto de vista económico, sin embargo, para alcanzar una independencia energética total los consumidores tendrían que pagar más que lo que le pagarían a la red pública. Así, en el año 2050, hasta 2 millones de casas habitación serían totalmente independientes sí sus dueños están dispuestos a pagar 50 por ciento más de lo que pagarían de seguir conectados a la red pública.Podríamos así esperar que, en las próximas décadas prevalecerá, si bien algo debilitado, el modelo de generación de energía eléctrica en instalaciones centrales que emergió de la guerra de las corrientes. En contraste con la silla eléctrica, que, afortunadamente, casi despareció del mapa.",
    "En un reporte emitido a las 16:00 horas del pasado lunes 23 de octubre, el Centro Nacional de Huracanes de los Estados Unidos informaba lo siguiente sobre la probable trayectoria de la, en esos momentos, tormenta tropical Otis: “Aunque se espera que Otis permanezca en un ambiente de viento de cizalladura moderada, la humedad abundante y las temperaturas superficiales del mar cálidas deberían favorecer su gradual fortalecimiento hasta que toque tierra. Los modelos han tendido al alza y ahora parece probable que Otis esté cerca de alcanzar fuerza de huracán cuando llegue a la costa“. El Centro Nacional de Huracanes cerraba su boletín con dos mensajes clave: “1) Fuertes lluvias por Otis comenzarán a impactar zonas del suroeste México a principios de esta semana. Esta lluvia producirá tormentas repentinas e inundaciones urbanas, además de deslizamientos de tierra en zonas de terreno más alto. 2) Se esperan condiciones de tormenta tropical y es posible que ocurran condiciones de huracán a partir del martes por la noche a lo largo provocadas de partes de la costa sur de México, por lo cual se emite un aviso de tormenta tropical y una alerta de huracán”.Al día siguiente, sin embargo, en un reporte emitido a las diez de la noche del martes 24 de octubre, el Centro Nacional de Huracanes modificó drásticamente sus predicciones. De acuerdo con el segundo reporte: “Un escenario de pesadilla se desarrolla esta tarde en el sur de México al estar Otis fortaleciéndose rápidamente y acercándose a la costa. Las imágenes de satélite muestran que Otis se ha intensificado durante las últimas horas y se espera que alcance velocidades de viento de 260 kilómetros por hora, lo que lo convertirá en un huracán de categoría 5. Otis ha intensificado de manera explosiva su velocidad de viento en 175 kilómetros por hora durante las últimas 24 horas, marca solo superada por el huracán Patricia en 2015”. Como sabemos, en forma desafortunada se cumplió la predicción del Centro Nacional de Huracanes y Otis alcanzó Acapulco pocas horas después, pasada la medianoche, como un huracán de categoría 5, con vientos de más de 260 kilómetros por hora y dando poco tiempo para que los habitantes de la ciudad tomaran las precauciones debidas. Para los expertos, Otis es un ejemplo extremo del fenómeno de “intensificación rápida” de un huracán que se ha visto incrementado en los últimos años. En este sentido, mencionaremos que un artículo publicado en la revista “Scientic Reports” el pasado 19 de octubre por Andra Garnez de Rowand University en los Estados Unidos, encuentra que la velocidad de crecimiento de la intensidad de los huracanes en el océano Atlántico se ha elevado cerca de un 30 por ciento en el periodo 2001-2020 en comparación con el periodo 1971-1990.Para crecer, los huracanes toman energía del agua cálida en la superficie del océano, y en este sentido lo expertos consideran que el calentamiento global, al elevar la temperatura de los océanos, incrementa la posibilidad de que un huracán experimente el fenómeno de intensificación rápida. En el caso de Otis, la temperatura del océano cerca de Acapulco era de 31 grados centígrados, inusualmente alta, lo que habría dado las condiciones para que el huracán creciera a la velocidad a la que lo hizo.  Un artículo publicado en noviembre de 2022 en la revista “Nature Communicacions” por un grupo de investigadores encabezado por  Kieran Bhatia de la compañía Guy Carpentier, Nueva York, encuentra también un incremento en el número de huracanes que experimentaron una intensificación rápida en el periodo 1982-2017. Bhatia y colaboradores atribuyen este incremento al calentamiento global debido a la creciente emisión de gases de invernadero a la atmósfera.Por otro lado, de acuerdo con los expertos, a pesar de que el avance de la ciencia del clima es considerable y permite predecir eventos climáticos con anticipación, el fenómeno de intensificación rápida de los huracanes no está todavía lo suficientemente entendido. En estas circunstancias, en la medida en la que, inevitablemente, se incremente la concentración de gases de invernadero en la atmósfera en los años por venir, antes de llegar a una eventual estabilización, es crítico que la ciencia del clima avance lo suficiente para predecir, si este fuera el caso, la inminente intensificación rápida de un huracán en curso. Y los trágicos eventos de Acapulco del pasado miércoles son más que explícitos al respecto. Finalizaremos recordando que el huracán Patricia, que ostenta el segundo lugar en la lista de intensificación rápida de huracanes, también se desarrolló en el océano Pacífico y tocó tierra en la costa de Jalisco.",
    "El bien conocido escritor de ciencia ficción Isaac Asimov publicó en el año de 1940 el cuento o novela corta que lleva por título, ”Robbie”, en la que relata la historia de Gloria, una niña de ocho años, y su compañero de juegos Robbie, un robot humanoide que interactuaba con Gloria como lo haría otro niño o niña compañera de juegos. Era tal el apego de Gloria por Robbie, que sus papás estaban preocupados por la posibilidad que la relación de Gloria con el robot pudiera de alguna manera perturbar su futura adaptación social, tal como lo podrían estar ahora los papás por el apego de sus hijos a los teléfonos móviles y a las redes sociales.  Más allá de adentrarnos en la historia de Gloria y Robbie y los esfuerzos infructuosos de sus padres por separarlos, nos limitaríamos a señalar que Asimov fue un visionario que escribió su novela en un momento cuando no se había aun construido la primera computadora digital de uso general, lo que ocurrió en el año de 1946. Y que, por supuesto, sin computadoras no sería posible construir un robot con las características de Robbie. En sus novelas, Asimov se adelantó por décadas a su tiempo e imaginó un futuro en el que los robots llevan a cabo de manera más eficiente actividades que tradicionalmente son realizadas por los humanos. Va incluso más allá y plantea la posibilidad de interacciones entre humanos y robots en términos igualitarios, tal como la que describe entre Gloria y Robbie. En concordancia con la visión de Asimov, los robots juegan un papel cada vez más importante en el mundo. Sabemos, por ejemplo, que se emplean en las fábricas de automóviles y que su uso en la industria en general y en un gran número de aplicaciones está en aumento. Así, se nos plantea un escenario en el que los robots son usados, no solamente como herramientas para realizar funciones de manera más rápida y eficiente, sino en roles de igualdad con los humanos, como los imaginados por Asimov hace ochenta años, en los que una niña interactúa de manera estrecha con un robot. En este contexto, un artículo aparecido el pasado 18 de octubre en la revista “Frontiers in Robotics and AI”, se reportan los resultados de un experimento llevado a cabo para determinar la eficiencia de un grupo de trabajadores de una fábrica de tarjetas electrónicas que llevan a cabo sus tareas en colaboración, en términos de igualdad, con un robot. De manera específica, a los trabajadores se les encarga revisar visualmente las tarjetas electrónicas producidas por la fábrica en busca de fallas que afecten su funcionamiento. Las tarjetas son también revisadas por un robot para hacer una doble verificación.  El artículo fue publicado por un grupo de investigadores encabezado por Dietlind Helene Cymek, de la Universidad Técnica de Berlín.Cymek y colaboradores sabían que, si bien una doble revisión tiene como objetivo reducir el número de tarjetas defectuosa que no son detectadas, un trabajador puede poner menos empeño en su actividad cuando trabaja en equipo que cuando lo hace de manera individual. Las razones para este comportamiento son varias. Por ejemplo, al saber un trabajador que otro compañero revisó previamente la tarjeta, pondrá menos esfuerzo en su propia revisión. Adicionalmente, dado que la recompensa que recibiría por identificar todas las tarjetas defectuosas está compartida con otro compañero, tiene menos incentivos para dar su máximo esfuerzo, mismo que llevaría a cabo si fuera el único responsable del trabajo. La investigación llevada a cabo por Cymek y colaboradores, por otro lado, tiene un elemento original: la colaboración en el trabajo no se da con otros compañeros humanos, sino con un robot. En el estudio reportado en el artículo de referencia participaron 42 trabajadores, divididos en dos grupos. Los integrantes de uno de los grupos trabajaron individualmente, mientras que los del otro grupo lo hicieron en colaboración con un robot. En este último caso, los trabajadores recibieron circuitos que previamente habían sido revisados por el robot, lo que era de su conocimiento. El estudio mostró que los trabajadores que trabajaron solos encontraron un promedio de 4.2 defectos, mientras que aquellos que trabajar en equipo con el robot encontraron un promedio de solamente 3.3 defectos.Así, Cymek y colaboradores encuentran que el mismo efecto de “holgazanería social” que ocurre cuando una persona trabaja en equipo con otras personas, tiene lugar también cuando las personas trabajan en equipo con un robot, que sería percibido como altamente eficiente en virtud de su perfección tecnológica. Habría que ver si, en la visión de Asimov, esta percepción se extiende en el futuro a otras esferas de interacción con los robots. Por ejemplo, la que de manera ficticia se dio entre Gloria y Robbie.",
    "El pasado mes de agosto, la India se convirtió en el cuarto país que logra colocar una sonda en la superficie de la Luna. El primer país en lograrlo fue la Unión Soviética en el año 1966, seguida de los Estados Unidos ese mismo año. Posteriormente, en 2013, China se convirtió en el tercer país en alunizar una sonda de manera suave. En este año ha habido también intentos fallidos para colocar una sonda en la Luna, específicamente, de Rusia y de Japón, en este último caso conducido por una compañía privada. Así, después de una larga pausa desde la década de los años setenta del siglo pasado, existe un renovado interés en la exploración de la Luna, no solamente por parte de gobiernos de países avanzados tecnológicamente, sino también de compañías privadas. En particular, el programa más ambicioso es el Artemis de la NASA, el cual, en colaboración con las agencias espaciales de Japón, Canadá, Australia, Israel y de países europeos, busca reanudar las misiones tripuladas a la Luna en 2025, y establecer bases permanentes de exploración que lleven a la explotación de los recursos minerales de la Luna y que sirvan de base para una futura misión tripulada a Marte.En este contexto, para la exploración de nuestro satélite es necesario que los futuros exploradores lunares cuenten con caminos adecuados para su desplazamiento que pueden ser muy polvorientos. En este sentido, habría que recordar que la superficie de la Luna está cubierta por una gruesa capa de un polvo fino llamado regolito, que fácilmente se dispersaría por el paso de un vehículo y permanecería suspendido por un tiempo largo antes de asentarse, dado que en la Luna la fuerza de gravedad es pequeña comparada con la gravedad terrestre. Además, puesto que en la Luna no hay aire que erosione la superficie de las partículas de regolito y suavice sus contornos, éstas constituyen un polvo abrasivo que daña los instrumentos y los mecanismos de los vehículos exploradores. Dados estos inconvenientes, se plantea la necesidad de pavimentar los caminos que utilizaran los vehículos exploradores lunares. Esto no resulta una tarea fácil, sin embargo. Para empezar, no se podrían transportar desde nuestro planeta los materiales necesarios para esta pavimentación, lo que resultaría prohibitivamente caro. Así, se tendría que hacer uso de materiales lunares y desarrollar métodos para procesarlos y construir una superficie firme para el paso de los vehículos.Una publicación aparecida esta semana en la revista “Scientific Reports” describe experimentos llevados a cabo para investigar la posibilidad de usar el regolito lunar como materia prima para pavimentar caminos en la Luna. El artículo fue publicado por un grupo de investigadores encabezado por Juan Carlos Ginés-Palomares de la Universidad de Aalen, Alemania.   En su artículo, Ginés-Palomares y colaboradores reportan la fabricación de una especie de ladrillos o tejas empleando un polvo comercial cuya composición emula a la del regolito lunar, y un láser infrarrojo de alta potencia. Para esto, colocaron una capa de polvo en un crisol y lo fundieron localmente con el calor del láser, desplazándolo a lo largo de su superficie.  Construyeron así una teja de forma triangular y contornos ondulados, con un orificio en el centro. Las tejas pueden enlazarse unas con otras para cubrir una superficie y proporcionar un piso firme.  Para la fabricación de las tejas en la Luna, sin embargo, no se podrá disponer de un láser de alta potencia como el empleado por Ginés-Palomares y colaboradores en su laboratorio. Es su lugar, se emplearía luz solar enfocada con una lente de un metro y medio de diámetro, la cual sería transportada desde la Tierra.La solución planteada por Ginés-Palomares y colaboradores para pavimentar los caminos de la Luna es una de las propuestas que se han hecho para resolver el problema de las polvaredas de regolito que se formarían al paso de los vehículos. Para quienes hemos circulado en un automóvil por una brecha sin pavimentar en un clima seco, no es difícil entender que el polvo que levantaría el vehículo lunar constituye un problema mayor, dadas sus características abrasivas y el tiempo extendido que dura en suspensión. Habida cuenta, además, que el regolito está eléctricamente cargado y por tanto es muy pegajoso y se adheriría al traje de los exploradores, quienes lo llevarían al interior del refugio lunar en donde constituirían un problema de salud para quienes lo habiten. El regolito lunar será entonces un peligro, quizá no el mayor, para los mineros y exploradores que vivirán en la Luna explotando sus recursos minerales. Y en ese sentido, podríamos quizá pensar que, dadas las riquezas minerales de la Luna, el riesgo estará justificado. Otros en cambio se preguntarán, emulando a los clásicos: ¿pero qué necesidad?",
    "Se dice que los males nunca vienen solos, y esto es sin duda válido en cuanto a los males que aquejan al planeta Tierra. En efecto, como sabemos, nuestro planeta está sufriendo un problema climático por la alta y creciente concentración de gases de invernadero en la atmósfera, la cual se han incrementado 40 por ciento desde el inicio de la Revolución Industrial hace unos 250 años. Así mismo, hay pérdidas de biodiversidad, y problemas de contaminación del medio ambiente y de acidificación de los océanos, entre otras calamidades. Éstas no son independientes, sino que se encuentran ligadas entre sí de diversas formas.  Así, la contaminación por plásticos es producto de la actividad industrial, misma que es a su vez genera los gases de invernadero que son responsables de la acidificación de las aguas marinas.En un artículo publicado en el año 2009 en la revista “Nature” por un grupo de investigadores encabezados por Johan Rockstrom de la Universidad de Estocolmo, se identifican nueve “fronteras planetarias” que no debieran ser transgredidas si hemos de preservar el medio ambiente del planeta, tal como ha prevalecido a lo largo de los últimos 10,000 años desde el fin de la última glaciación. Estas fronteras de refieren a problemas ambientales asociados con, el cambio climático, la emisión de aerosoles a la atmósfera, la pérdida de la biodiversidad, la acidificación de los océanos, el adelgazamiento de la capa de ozono en la estratósfera, la alteración de los ciclos del nitrógeno y el fósforo por la producción de fertilizantes, la contaminación ambiental por desechos químicos, los cambios en el uso del agua, y la transformación de hábitats naturales en áreas urbanas o agrícolas. En el momento en que publicaron su artículo, Rockstrom y colaboradores concluyen que se habían ya transgredido las fronteras planetarias, asociadas a la concentración de gases de invernadero en la atmósfera, a la alteración del ciclo del nitrógeno, y a la pérdida de biodiversidad. En cuanto a la primera frontera, en el año 2009 la concentración de gases de invernadero había sobrepasado el límite propuesto: 25 por ciento por arriba de sus valores preindustriales. De manera similar, la remoción de nitrógeno de la atmósfera había sobrepasado por un factor de cuatro la frontera propuesta como segura. Según Rockstrom y colaboradores, el nitrógeno removido de la atmósfera termina en último término como un contaminante ambiental; por ejemplo, como óxidos de nitrógeno que constituyen gases de invernadero.   Con relación a la pérdida de biodiversidad, Rockstrom y colaboradores apuntan que ésta ha alcanzado un valor de 100 a 1000 veces más grande que el que podría esperarse de forma natural. Este valor no tiene precedente desde la última extinción masiva de especies ocurrida hace 65 millones de años. Según los autores del artículo, la principal causa de la pérdida de biodiversidad es la conversión de ecosistemas naturales en superficies urbanas o para uso agrícola.El pasado 13 de septiembre apareció en la revista “Science Advances” una actualización del estudio original de Rockstrom y colaboradores. El nuevo artículo, en el que Rockstrom es coautor, lleva como autor principal a Katherine Richardson, de la Universidad de Copenhague. De acuerdo con dicho artículo, a 14 años de distancia del estudio original, el número de fronteras planetarias transgredidas creció de tres a seis, de modo tal que las únicas fronteras que no han sido superadas son las referentes a la acidificación de los océanos, a la concentración de aerosoles en la atmósfera y al nivel de ozono en la estratósfera. La acidificación del mar, sin embargo, está a punto de alcanzar su frontera, mientras que la concentración de aerosoles atmosféricos excede regionalmente su frontera en algunos casos. Además, la pérdida de biodiversidad, el cambio climático y la contaminación con nitrógeno, incrementaron su nivel de transgresión con respecto al observado en 2009.En estas condiciones, Richardson y colaboradores concluyen que la transgresión de seis de nueve fronteras planetarias, “sugiere que la Tierra está ahora muy fuera del espacio operativo seguro para la humanidad”…. y que, “hay una necesidad urgente de herramientas científicas y políticas más poderosas para analizar todo el sistema terrestre integrado con confiabilidad y regularidad y orientar los procesos políticos para evitar alterar el estado del sistema terrestre más allá de niveles tolerables para las sociedades actuales”.Así, podríamos concluir diciendo que, efectivamente, los problemas no vienen solos y que el cambio climático está acompañado por una serie de calamidades que incluyen la pérdida de biodiversidad, la acidificación de los océanos y la contaminación del medio ambiente por diversos agentes, entre otras. Y que todas estas calamidades, además, están conectadas unas con otras.",
    "Cuando al final del siglo XIX e inicio del siglo XX nació la industria de los automóviles, los vehículos eléctricos competían de manera efectiva con los de combustión interna. Como sabemos, la competencia fue ganada por los automóviles de gasolina, a pesar de que su versión eléctrica era mecánicamente más simple y más atractiva por lo silencioso de su operación.  Los automóviles eléctricos tenían, sin embargo, una desventaja: su reducida autonomía de desplazamiento por la limitada capacidad de almacenamiento de energía de sus baterías eléctricas. Los automóviles con motores de combustión interna, en cambio, se beneficiaron con la alta densidad de energía contenida en la gasolina.  Así, a lo largo de siglo XX y lo que va del presente, el número de automóviles de gasolina se ha incrementado de manera incontenible. Y con esto se han constituido en una amenaza al medio ambiente, lo que ha motivado que los automóviles eléctricos, con baterías renovadas, hayan vuelto a la escena como una opción menos contaminante para sustituir a los automóviles de gasolina. Una segunda opción para este propósito son los automóviles de hidrógeno, en los que se quema hidrógeno y se genera calor y agua como residuo. En otra versión de este automóvil, el hidrógeno reacciona con el oxígeno en una pila de combustible, generando electricidad y también agua como residuo. Así, la operación de un automóvil de hidrógeno no genera residuos contaminantes.Más allá de su uso en la industria de los automóviles, el hidrógeno se concibe como el combustible de una economía de hidrógeno en la que este gas se convierte en un sustituto no contaminante de los combustibles fósiles en un rango amplio de aplicaciones.  Para esto, el hidrógeno debe obtenerse empleando medios que produzcan bajos niveles de gases de invernadero. En este sentido, el hidrógeno se obtendría descomponiendo el agua en hidrógeno y oxígeno por medio del proceso de electrólisis. La electricidad necesaria para llevar a cabo este proceso se obtendría de una fuente limpia de energía, ya sea solar o eólica.  Además de lo anterior, en una economía de hidrógeno se emplearía el hidrógeno obtenido por electrólisis para las diversas aplicaciones que actualmente se le da al hidrógeno, incluyendo la producción de amoniaco para fertilizantes y de otras sustancias químicas.  Para investigar la generación de hidrógeno en un contexto de economía de hidrógeno, un grupo de investigadores encabezado por Davide Tonelli de la Universidad Católica de Lovaina, Bélgica, se dio a la tarea de investigar los límites para la producción de hidrógeno electrolítico a nivel global. Los resultados de esta investigación aparecieron el pasado 8 de septiembre en la revista “Nature Communications”.De manera específica, Tonelli y colaboradores investigaron la disponibilidad de áreas territoriales, tanto a nivel global como en diferentes países, para establecer los generadores eólicos o paneles solares para la electrólisis del agua en una economía de hidrógeno.  Encuentran que, si bien sería necesario usar menos del uno por ciento del agua disponible a nivel global para generar el hidrógeno que se necesitaría en el año 2050, el agua no es un recurso que esté uniformemente distribuido entre todos los países. De este modo, algunos países encontrarían problemas de escasez de agua para satisfacer sus necesidades de hidrógeno. De la misma manera, mientras que algunos países cuentan con grandes extensiones territoriales, otros tendrían problemas para encontrar lugares para la instalación de los paneles solares o los aerogeneradores que serían necesarios, y se convertirían en importadores de hidrógeno. A partir de su análisis, Tonelli y colaboradores encuentran que países como Canadá y Australia, Argentina, Bolivia y Paraguay, así como países en el sur y centro de África, cuentan con suficientes recursos para convertirse en exportadores de hidrógeno. En contraste, otros países como Japón, Corea del Sur, República Dominicana, y países de Europa Occidental, tendrán problemas para satisfacer sus necesidades de hidrógeno. México, en ese sentido, está en una posición intermedia.Así, a cien años del surgimiento del petróleo como combustible, el mundo está forzado a buscar combustibles menos contaminantes. En particular, el tráfico de automóviles ha crecido a un grado tal que se ha convertido en una especie de plaga que atenta contra el medio ambiente -al igual que contra nuestra tranquilidad urbana-. ¿Será la economía de hidrógeno una solución al problema? Nos lo dirán las décadas por venir.",
    "El chip semiconductor o circuito integrado, que incorpora decenas de miles de millones de elementos electrónicos en obleas de silicio en un área de un centímetro cuadrado, es, sin duda alguna, el objeto más complejo jamás construido. Para fabricar un chip semiconductor se emplea como herramienta una máquina de una complejidad extrema, en la que se integran cientos de miles de componentes a un costo de cientos de millones de dólares. Esta máquina es esencial para la fabricación de los chips, y permite grabar sobre la oblea de silicio los patrones que darán origen a sus elementos electrónicos. En retrospectiva, ciertamente mucho tuvo que suceder a lo largo de millones de años de evolución para que nuestra especie desarrollara la capacidad necesaria para fabricar dispositivos y máquinas tan extraordinariamente complejas. En este sentido, nos indican los expertos que el género Homo, al que pertenecemos, se originó hace unos dos y medio millones de años y que a lo largo de este tiempo surgieron diversas especies Homo, de las cuales solamente la nuestra sobrevive. En la medida en que evolucionaron nuestros antecesores arcaicos, su cerebro creció en volumen, desarrollaron herramientas de piedra y eventualmente dominaron el uso del fuego. Todo esto antes de la aparición de hombre moderno, que habría ocurrido hace unos 300,000 años según los expertos.  La resistencia de la piedra al desgaste con el paso del tiempo ha facilitado que la evidencia de herramientas piedra fabricadas hace millones de años haya llegado hasta a nuestros días de manera relativamente eficiente. No ha sucedido lo mismo con las posibles herramientas de madera -material que se pudre fácilmente expuesta al medio ambiente- que hubieran sido fabricadas hace cientos de miles de años. Es por esto, que resultó sorprendente que se haya descubierto en las inmediaciones del lago Tanganica, cerca de la frontera entre Zambia y Tanzania, un trabajo de carpintería realizado hace casi medio millón de años. El artículo fue publicado en la revista “Nature” por un grupo de investigadores encabezado por Lawrence Barham, de la Universidad de Liverpool en el Reino Unido. En su artículo, Barham y colaboradores reportan el descubrimiento, enterrados en arena, de dos troncos unidos transversalmente por medio de muescas rebajadas a propósito. Este arreglo probablemente fue parte de una plataforma de pesca o de una casa habitación. Los troncos lograron llegar hasta nuestros días debido a que permanecieron sumergidos en arena húmeda, lo que retrasó su deterioro. La arena en la que fueron encontrados enterrados fue datada con una antigüedad de 476,000 años, lo que es anterior a la aparición del hombre moderno. Según comenta Barham: “Este hallazgo ha cambiado mi forma de pensar sobre nuestros primeros antepasados. Olvídese de la etiqueta “Edad de Piedra”, mire lo que estaban haciendo estas personas: hicieron algo nuevo y grande con madera. Usaron su inteligencia, imaginación y habilidades para crear algo que nunca antes habían visto, algo que nunca antes había existido. Transformaron su entorno para hacer la vida más fácil, aunque fuera solo haciendo una plataforma para sentarse junto al río a realizar sus tareas diarias. Estas personas se parecían más a nosotros de lo que pensábamos y eran menos nómadas de lo que se creía”.El descubrimiento de Braham y colaboradores muestran, que aun, antes de la aparición de nuestra especie, nuestros antecesores estaban en el camino para fabricar dispositivos y máquinas con una sofisticación creciente. Si bien en un inicio el avance fue lento, a la larga el progreso ha sido impresionante: pasamos de ensamblar estructuras de madera con algunos troncos de árbol para usarlas como plataformas de pesca, a ensamblar decenas de miles de millones de componentes electrónicos en una pastilla de silicio para fabricar circuitos integrados. Todo esto en el curso de medio millón de años.  Y a quien objetara que medio millón de años no es un tiempo precisamente corto, habría que recordarle que la sofisticación de los dispositivos y máquinas construidas se ha acelerado de manera vertiginosa en los últimos dos siglos por la aplicación sistemática del método científico al desarrollo de tecnología. Y que no tendríamos que esperar todo este tiempo para ser testigos de avances tecnológicos tan grandes como los que separan a los ensambles de troncos, de los ensambles de componentes electrónicas en los circuitos integrados.",
    "Como en su momento fue dado a conocer por los medios de comunicación, el vuelo 370 de Malaysia Airlines, que despego de Kuala Lumpur la madrugada del 8 de marzo de 2014 con rumbo a Beijing, nunca llegó a su destino y su suerte quedó envuelta en el más grande de los misterios. Se acepta que el avión se estrelló en algún lugar del océano Índico cerca de Australia, muriendo todas las personas a bordo -239, entre pasajeros y tripulantes- pero no se sabe a ciencia cierta que fue lo que realmente le sucedió. Lo que sí sabemos es que poco después de despegar de Kuala Lumpur, a las 1:21 de la madrugada del 8 de marzo, el piloto de Malaysia Airlines se comunicó por última vez con el control aéreo del aeropuerto al salir del espacio aéreo malayo. A partir de este punto, el vuelo debería haberse adentrado en el espacio aéreo de Vietnam, pero aparentemente nunca lo hizo. En lugar de eso, dio un giro brusco hacia el suroeste, cruzó hacia el estrecho de Malaca, y desapareció del radar. A partir de ese momento no hay certidumbre sobre la ruta que siguió el avión.  Presumiblemente se dirigió hace el sur y se adentró en el océano Índico, hasta que se quedó sin combustible y se estrelló en el mar. Sabemos, por otro lado, que el avión siguió en el aire cuando menos hasta las 8:19. Esto lo sabemos porque envió siete señales automáticas de rutina que fueron captadas por un satélite de la compañía de telecomunicaciones Inmarsat, la última a las 8:19.  A partir de las señales captadas por el satélite, fue posible determinar que el lugar probable en donde se habría estrellado el avión se localiza en algún punto alrededor de un arco de miles de kilómetros de longitud localizado enfrente de la costa oeste de Australia. Si bien contar con esta información es mejor que no tener información alguna sobre dónde en el océano Índico se habría estrellado el vuelo 370, buscar un avión sumergido varios kilómetros en un área tan poco específica no es una tarea sencilla. De hecho, los restos del avión malayo no fueron encontrados y su búsqueda se suspendió hace ya varios años.Recordemos, sin embargo, que en julio de 2015, más de un año después del accidente, un alerón del avión de Malaysia Airlines fue encontrado en la costa de Reunión, isla que se encuentra enfrente de la costa de Madagascar, a 6,000 kilómetros del sitio en que se supone se habría estrellado el vuelo. El alerón fue trasportado hasta ahí por las corrientes marinas.Basados en este hallazgo, algunos investigadores han tratado de reconstruir la trayectoria que siguió el alerón hasta Reunión y determinar el lugar en el que se encuentra el avión accidentado.  Un ejemplo de estos esfuerzos es un artículo aparecido el pasado 23 de agosto en la revista “AGU Advances”, en el que se plantea determinar dicha trayectoria a partir del estudio de los crustáceos que colonizaron el alerón desde el momento del accidente. El artículo fue publicado por un grupo de investigadores encabezados por Nasser Al-Kattan, de la Universidad del Sur de la Florida.De manera específica, Al-Kattan y colaboradores estudiaron en el laboratorio la misma clase de crustáceos que se encontraron adheridos al alerón, con el objeto de determinar cómo cambia el contenido de un cierto isótopo de oxígeno en su cubierta calcárea, según la temperatura a la que se encuentran. Con esta información, y a partir del contenido de dicho isótopo en la cubierta de los crustáceos encontrados en el alerón, determinaron las temperaturas del océano a las que dichos crustáceos habían estados expuestos en su travesía hasta Reunión. Esta información, juntamente con un conocimiento de las temperaturas del océano, les permitiría determinar su trayectoria desde el punto del accidente. Cabe mencionar que esto último es más fácil decirlo que llevarlo a cabo, pues existen múltiples caminos que podrían producir los mismos resultados en cuanto a la composición de la cubierta de los crustáceos. El conocimiento que proporcionan, sin embargo, sirve de guía para tratar de, finalmente, encontrar los restos de avión malogrado. En este sentido, esperan Al-Kattan y colaboradores que el gobierno francés, quien se encuentra en posición del alerón, ponga a disposición de los investigadores más especímenes de los crustáceos adheridos al mismo.De tener éxito las propuestas de Al-Kattan y el avión accidentado es encontrado, y con esto la caja negra, indispensable para determinar las causas del desastre, se develará finalmente el misterio del vuelo 370, alrededor del cual se han tejido múltiples hipótesis, ninguna con una base suficientemente firme. Y todo esto con la ayuda de un crustáceo -“Lepas anatifera- que, por lo demás, no luce nada extraordinario.",
    "Una interesante fotografía, que puede ser encontrada en Internet, muestra a Thomas Alva Edison en un primer plano durmiendo debajo de un árbol, y más atrás al presidente de los Estados Unidos, Warren Harding, sentado en una silla leyendo un periódico. La fotografía fue tomada en Maryland en julio de 1921 en un campamento de verano. Si bien la situación retratada es por demás curiosa, habría que recordar que Edison solía dormitar sosteniendo esferas con ambas manos, de modo tal que al quedarse dormido las esferas cayeran al suelo y lo despertaran. Edison llevaba a cabo esta extraña práctica como un medio para estimular su creatividad como inventor. Su idea era que, al entrar en la primera fase del sueño, su cerebro estaría libre para encontrar una solución al problema que pretendía resolver. Y una vez que la encontrara, tendría que despertar rápidamente para hacerla consciente y evitar que se perdiera.La técnica empleada por Edison habría funcionado, si hemos de juzgar por las más de mil patentes que le fueron otorgadas a lo largo de su carrera. Por lo demás, al margen de su éxito como inventor, las ideas de Edison son respaldadas en buena medida por investigaciones llevadas a cabo recientemente. Por ejemplo, en un artículo publicado en el número de diciembre de 2021 de la revista “Science Advances”, se reportan experimentos que demuestran que la zona intermedia entre la vigilia y el sueño es un punto de gran creatividad. El artículo fue publicado por un grupo de investigación encabezado por Celia Lacaux de la Sorbonne Université. Como sabemos, existen dos etapas durante el sueño, una etapa inicial, que a su vez está dividida en tres subetapas, N1, N2 y N3, y una segunda etapa caracterizada por un movimiento rápido de los ojos, denominada REM por sus siglas en inglés. Lacaux y colaboradores se propusieron investigar la subetapa, N1 al inicio del sueño. Para este propósito, conjuntaron a un grupo de 103 voluntarios a los que se les proporcionó un problema matemático que podría ser resuelto rápidamente siguiendo una regla oculta. El descubrimiento de esta regla sería una medida de creatividad.Para iniciar el experimento, los participantes llevaron a cabo 30 intentos de encontrar la solución, descartando a aquellos que pudieron encontrar la regla escondida. A todos los demás se les pidió descansar por 20 minutos con los ojos cerrados, sosteniendo un objeto con su mano derecha. Si alguno de los participantes dejaba caer el objeto, indicando que había entrado en la fase N1 del sueño, se le pedía que manifestara la cadena de pensamientos que tuvo antes de despertar por el ruido del objeto al golpear el suelo. En esta etapa los participantes fueron divididos en tres grupos, un primer grupo con aquellos que permanecieron despiertos durante los 20 minutos, un segundo grupo de aquellos que alcanzaron la fase N1 del sueño sin pasar a un sueño más profundo, y un tercer grupo que avanzó hasta la etapa N2 l sueño. Encuentran Lacaux y colaboradores que aquellos que permanecieron cuando menos 15 segundos en la etapa N1 del sueño durante el periodo de 20 minutos, triplicaron la probabilidad de encontrar la regla oculta en comparación con aquellos que permanecieron despiertos durante este periodo. Esta ventaja, sin embargo, desapareció para aquellos que avanzaron a una etapa más profunda del sueño.Los resultados de Lacaux y colaboradores son apoyados por un estudio publicado el pasado 15 de mayo en la revista “Scientific Reports” por un grupo de investigadores encabezado por Adam Har Horowitz, de Massachusetts Institute of Technology. Al igual que en la investigación previamente citada, Horowitz y colaboradores encuentran  que la primera etapa del sueño es un punto ideal de creatividad. Encuentran también que la creatividad puede ser estimulada en un determinado tópico, induciendo sueños relativos a ese tópico en la etapa N1.Aparte de las experiencias de Edison, hay otras evidencias anecdóticas sobre el papel del sueño en la creatividad. En este sentido, Salvador Dalí habría usado una técnica similar a la de Edison para encontrar inspiración.  También, es leyenda que el químico August Kekulé ideó la estructura del benceno, que tiene forma de anillo, después de soñar con una serpiente que se mordía la cola.Tal parece entonces que podríamos estimular nuestra creatividad y resolver el problema que traemos en la cabeza de manera relativamente simple siguiendo el procedimiento de Edison: sentémonos tranquilamente en un sillón reclinable con un objeto en la mano y procuremos conciliar el sueño. Una vez que nos durmamos y el objeto caiga al suelo despertándonos, con suerte tendremos la solución a nuestro problema. En caso contrario, poco habremos perdido con haber hecho el intento.",
    "El pasado jueves 24 de agosto, el Gobierno de Japón confirmó que había dado inicio a la liberación en el océano de agua contaminada con elementos radiactivos provenientes de la malograda planta nuclear de Fukushima. Como recordamos, el 11 de marzo de 2011 un terremoto ocurrido cerca de la costa noreste de Japón provocó un desastre nuclear de grandes proporciones. En el momento del terremoto, los reactores de la central de Fukushima se apagaron automáticamente, como se esperaba, haciendo que las plantas eléctricas de emergencia entraran en funcionamiento para mantener operando los sistemas de enfriamiento de los reactores. El terremoto, sin embargo, provocó un tsunami con olas de 14 metros de altura que rebasaron la barrera de protección de los reactores frente al mar, inundándolos e inutilizado las plantas de emergencia. En estas circunstancias, los núcleos de los reactores se sobrecalentaron y se fundieron, liberando grandes cantidades de elementos radioactivos a la atmósfera.A partir de entonces se ha inyectado de manera permanente agua de mar a los reactores con el fin de mantener frío el combustible nuclear –que genera calor aún apagado el reactor- y evitar más fugas de radiación. El agua de enfriamiento, sin embargo, se contamina por el contacto con dicho combustible de modo que no es regresada al mar, sino que se almacena en una instalación con más de 1,000 tanques. A la fecha, sin embargo, dichos tanques han acumulado más de un millón de toneladas de agua y han llegado al límite de su capacidad, por lo que ahora se busca regresarla al mar. El regreso no se haría de golpe, sino de manera paulatina; de hecho, muy paulatina, pues tomaría varias décadas en completarlo, habida cuenta que el volumen de agua acumulada sería suficiente para llenar más de 500 albercas olímpicas.  Para evitar contaminar al océano con desechos radiactivos, el agua es sometida a un proceso de filtrado especialmente diseñado, el cual, según el Gobierno japonés y la compañía Tokyo Electric Power, propietaria de la planta nuclear, elimina los contaminantes de manera efectiva. Los eliminaría a un grado tal que el agua descontaminada sería tan pura que resultaría incluso potable. Si bien el Organismo Internacional de Energía Atómica aprueba el plan del Gobierno japonés, no todo mundo está de acuerdo. Por ejemplo, la revista “Science” cita en su número del 24 de enero pasado a Ken Buesseler, de Woods Hole Oceanographic Institution, quien declara que no “hay datos con la cantidad y calidad suficientes que apoyen lo que afirma la compañía propietaria de la planta, y que se necesita más información”. Igualmente, Feres Dalnoki Veress, de Middlebury Institute of International Studies, en California, afirma que en realidad no sabemos que contaminantes contiene el agua, pues solamente se han analizado pequeños volúmenes de un cuarto de los tanques de almacenamiento. El Gobierno de China, por su parte, se mostró contrario a la decisión del Gobierno japonés y anunció que la prohibición que mantiene para la importación de productos del mar provenientes de la región de Fukushima, se extenderá a todos los productos de mar que vinieran de Japón.  Corea del Sur, de manera más moderada, considera que lo importante es que Japón siga estándares científicos y proporcione información de manera transparente.El accidente de Chernóbil de 1986, el de la Isla de las Tres Millas de 1979, y el evento de Fukushima, integran el “Top three” de los accidentes asociados a la energía nuclear. Este tipo de energía se califica algunas veces como “limpia” y como una de las vías para combatir el cambio climático. Los tres mayores accidentes nucleares, sin embargo, demuestran que dichos accidentes pueden ocurrir y que, de ocurrir, pueden tener consecuencias catastróficas. El de Fukushima, en particular - independientemente de puntos de vista controvertidos sobre la decisión del Gobierno japonés- requerirá de décadas de esfuerzo y de inversiones multimillonarias que podrían ascender a más de 100,000 millones de dólares para remediar el desastre.Esperemos que en el futuro el “Top three” de los accidentes nucleares no se convierta en un “Top four” y que en su lugar apostemos por energías tales como la solar, la cual, además de inagotable, es sin duda considerablemente más amigable.",
    "Sabemos que la cantidad de energía solar que alcanza a nuestro planeta es miles de veces mayor que la que necesitamos para satisfacer nuestras necesidades a nivel global. Sabemos también que el concurso de esta energía es esencial para combatir el cambio climático y que contamos con la tecnología necesaria para capturarla y convertirla en energía eléctrica. Así, la energía del Sol resulta abundante, inagotable, barata, relativamente no contaminante y aprovechable. El Sol, sin embargo, tiene también un defecto que le ha impedido convertirse en nuestra principal fuente de energía: brilla solamente durante el día. Dada esta circunstancia, se han dado iniciativas exóticas -por decir lo menos- que proponen colocar tendidos de paneles solares en el espacio para capturar y enviar energía a la Tierra por medio de un haz de microondas. Los paneles solares se colocarían en un lugar en el que nunca se oculte el Sol, de manera tal que dispondríamos de energía sin interrupciones.  Volviendo a la Tierra, para obtener energía eléctrica de manera continua podríamos usar paneles solares en combinación con algún sistema de almacenamiento de energía. Los paneles capturarían la energía del Sol durante el día, y almacenarían la porción no usada para las horas sin luz solar. Desafortunadamente, esto es más fácil decirlo que hacerlo, de modo tal que la energía solar no ha tenido un uso tan amplio como hubiera sido deseable.¿Cómo podría almacenarse la energía capturada por los paneles solares? La primera opción que quizá nos viene a la cabeza es un banco de baterías, tan grande como fuera necesario. Una batería es sin duda la opción adecuada para, por ejemplo, impulsar un automóvil eléctrico. Para un almacenamiento masivo de energía, sin embargo, podríamos considerar otra opción, a saber, el uso de la fuerza de gravitación de la Tierra.  En este sentido, imaginemos dos estanques de agua colocados a diferentes alturas y una bomba accionada por la electricidad proporcionada por paneles solares, que eleva el agua desde el estanque inferior al superior. El agua en su posición elevada adquiere energía en forma de energía gravitatoria, actuando como una batería gigante. Para recuperar la energía almacenada, el agua es enviada de regreso al estanque inferior a través de una turbina, que a su vez acciona un generador de electricidad, tal como sucede en una planta hidroeléctrica convencional. La técnica para almacenar energía empleando la fuerza de gravitación de la Tierra no es nueva, y de hecho, fue empleada por primera vez en Europa hace ya más de un siglo. En la actualidad, sin embargo, ha crecido el interés en usarla como un medio para almacenar la energía del Sol o del viento. Así, la capacidad instalada a nivel global de plantas hidroeléctrica que emplean bombeo de agua es equivalente a más de cien veces la capacidad de la planta nucleoeléctrica de Laguna Verde, en el estado de Veracruz.En el contexto del combate al cambio climático es importante preguntarse sobre el impacto de las plantas hidroeléctricas con bombeo de agua tienen en cuanto a la generación de gases de invernadero, y habría que considerar que la construcción de la planta misma con sus dos estanques de agua y su posterior mantenimiento implica la emisión de una cierta cantidad de contaminantes atmosféricos. En este sentido, un artículo aparecido esta semana en la revista “Enviromental Science and Technology” hace una evaluación de dichas emisiones y las compara con las emisiones características de otros medios de almacenamiento de energía, incluyendo el aire comprimido y tres clases de baterías. El artículo fue publicado por un grupo de investigadores encabezados por Timothy Simon del Laboratorio Nacional de Energías Renovables, en Golden, Colorado.Como resultado de su trabajo, Simon y colaboradores concluyen que, por un margen considerable, el almacenamiento de energía empleando la fuerza gravitatoria de la Tierra es el medio que emite menos contaminantes atmosféricos. Para hacer su evaluación, lo investigadores consideraron todos los gases de invernadero que se generan durante la construcción de los dos estanques y los sistemas de bombeo y generación de electricidad. Asumen que la planta tendría una vida útil de 80 años, y que tendría que darse un mantenimiento a los 40 años. En contraste, no consideran los gastos de desmantelamiento al final de su vida útil.A nivel global, se estima que existen unos 600,000 sitios potenciales para instalar una planta hidroeléctrica con bombeo de agua. En estas circunstancias, las necesidades de electricidad del mundo podrían ser cubiertas -sin interrupciones- enteramente por medio de grandes extensiones de paneles solares y de baterías gigantes que funcionan con la fuerza gravitacional de la Tierra.  Esquema que, además, contribuirá a mitigar el cambio climático.",
    "La madrugada del 6 de agosto de 1945, hoy hace 78 años, un bombardero B-29 de los Estados Unidos, apodado Enola Gay, partió de la isla de Tinian con rumbo a Hiroshima en la costa japonesa, con una bomba de uranio a bordo. Al arribar a Hiroshima, a las 8:15 de la mañana el Enola Gay hizo explotar la bomba sobre la ciudad. Para la ocasión, el bombardero se hizo acompañar de otros dos aviones; uno de ellos, apodado Great Artist, estaba equipado con instrumentos de medición para documentar los efectos físicos de la bomba; el otro, llevaba a bordo equipo para fotografiar la explosión.Como sabemos, el bombardeo nuclear de Hiroshima es la primera de las dos únicas ocasiones en la que se ha empleado una bomba nuclear en contra de la población. La segunda ocurrió tres días después, cuando otro bombardero B-29 lanzó una bomba nuclear de plutonio sobre la ciudad de Nagasaki en el sur de Japón. En conjunto, los bombardeos de Hiroshima y Nagasaki produjeron más de 200,000 muertos, tanto en el momento de la explosión, como posteriormente por enfermedades producidas por la radiación.Sabemos también que las bombas nucleares que destruyeron Hiroshima y Nagasaki fueron parte del esfuerzo de guerra de los Estados Unidos durante la Segunda Guerra Mundial, en un contexto de competencia con Alemania, que se pensaba pudiera estar desarrollando su propia bomba. Alemania, sin embargo, se rindió en mayo de 1945, de modo tal que cuando los Estados Unidos decidieron lanzar la bomba sobre Japón meses después de esta rendición, no existía ya el peligro nazi. No habría habido así, según algunos, necesidad de usar la bomba nuclear en contra de la población japonesa.Expresando opiniones en este sentido, un grupo de científicos hizo llegar en junio de 1945 al Secretario de Guerra de los Estados Unidos un documento en el que analizaron las consecuencias políticas y sociales que podría tener el uso de la bomba nuclear en contra de la población japonesa. Dicho grupo estaba formado por tres físicos, tres químicos y un biólogo, y  era presidido por James Franck de la Universidad de Chicago, premio Nobel de Física 1925. El documento, que puede ser consultado en la página de Internet del “Bulletin of the Atomic Scientists”, se envió antes de que se llevara a cabo la primera explosión nuclear de la historia en el desierto de Nuevo México en julio de 1945.En su documento, Franck y colaboradores expresan preocupación por la posibilidad de que, con el uso de la bomba nuclear en contra de Japón, se desatase un carrera armamentista -como efectivamente sucedió- en la que varios países buscaran desarrollar armas nucleares. Para prevenir esto, consideran la posibilidad de mantener en secreto la tecnología nuclear que poseían los Estados Unidos. Ellos mismos, sin embargo, argumentan que esto solamente podría retrasar el problema algunos años, pues existían varios países con los conocimientos suficientes para desarrollar armas nucleares en el corto plazo. Y en este sentido tuvieron razón, pues la Unión Soviética detonó su primera bomba nuclear en 1949.  Consideran también la posibilidad de controlar la producción de uranio y de otros materiales fisionables, lo cual, igualmente, concluyen era impráctico. En particular, y dado que la Unión Soviética era su principal preocupación, no creían posible que un país que cubre la quinta parte de la superficie del planeta no cuente con yacimientos de uranio.Además de lo anterior, argüían que, en el evento de una guerra nuclear, los Estados Unidos estarían en desventaja con respecto a la Unión Soviética. Esto, debido a que, por su extenso territorio, la Unión Soviética podría dispersar sus núcleos poblacionales de una manera más efectiva de lo que lo podrían hacer los Estados Unidos con el fin de mitigar el impacto de un ataque nuclear.Franck y colaboradores concluyen que el único camino disponible para disipar el peligro nuclear es lograr un acuerdo de control de armas, y en ese sentido no era recomendable iniciar la era nuclear con un ataque en contra de la población japonesa que dificultaría que los Estados Unidos tomasen el liderazgo para lograr un acuerdo de control de armas.A toro pasado, sabemos que los Estados Unidos no atendieron a las recomendaciones de los científicos y detonaron dos bombas nucleares sobre la población civil japonesa. Esto, ciertamente aceleró la rendición japonesa, pero al mismo tiempo no ayudó a evitar la carrera armamentista que dio origen a la Guerra Fría entre los Estados Unidos y la Unión Soviética.",
    "Cuando los grupos de rock ingleses, con Los Beatles a la cabeza, llegaron a los Estados Unidos en la década de los años sesenta, la llamada “invasión británica”, dominaron la escena musical norteamericana por un buen número de años. Esto, a pesar de que dichos grupos tocaban una versión modificada del rock and roll desarrollado por músicos norteamericanos como Elvis Presley, Buddy Holly y Chuck Berry, entre muchos otros. Hoy en día, de alguna manera la historia se repite. Específicamente, en relación a los microcircuitos o chips semiconductores, inventados en los Estados Unidos, pero cuya fabricación se lleva a cabo hoy en día mayormente en Corea del Sur y Taiwan.  La historia de los chips semiconductores se remonta al final de la década de los años cuarenta, específicamente al mes de diciembre de 1947, cuando Walter Brattain y John Bardeen demostraron la operación del primer transistor de la historia en los laboratorios Bell de la compañía ATT en Nueva Jersey. A partir de esta demostración, la tecnología de los transistores progresó rápidamente y en septiembre de 1958, Jack Kilby, de la compañía norteamericana Texas Instruments, demostró la operación de un circuito en el que integró varias componentes electrónicas en una sola pastilla. Por sus descubrimientos, tanto a Brattain y Bardeen, como a Kilby, les fue otorgado el premio Nobel de Física.Medio año después de la demostración de Kilby, Robert Noyce, de la compañía norteamericana Fairchild Semiconductor, demostró la operación del microcircuito o chip, básicamente tal como lo conocemos ahora. A partir de ese momento, la tecnología y la complejidad de los microcircuitos avanzaron a una enorme velocidad, al grado tal que hoy en día los microcircuitos pueden incorporar decenas de miles de millones de transistores en una pastilla con un área de un centímetro cuadrado. No en balde, se ha dicho que el microcircuito de silicio es el objeto artificial más sofisticado que se ha construido. El chip semiconductor es también uno de los inventos que más trascendencia han tenido en la historia de la civilización. Así, el microcircuito habilitó una gran cantidad de tecnologías, incluyendo la computación, el Internet, los automóviles autónomos, los teléfonos celulares y los dispositivos de geolocalización, por mencionar solo algunas. En el futuro, en la medida en la que se desarrollen las aplicaciones de la inteligencia artificial, el impacto de los chips semiconductores se incrementará concurrentemente. Por otro lado, si bien el microcircuito de silicio es un invento norteamericano, apenas el 12 por ciento de los chips semiconductores que se emplean a nivel global son producidos en los Estados Unidos, siendo Taiwan y Corea del Sur los mayores fabricantes de estos dispositivos. De acuerdo con la revista MIT Technology Review, esto es debido a que en los Estados Unidos los costos de producción de chips son un 40 por ciento más elevados que en países de Asia, debido a diferencias salariales y de costos de construcción, así como de incentivos gubernamentales.  Dadas estas circunstancias, y con el objeto de incentivar la producción de semiconductores en el territorio de los Estados Unidos, el congreso norteamericano aprobó el pasado año la “Ley de CHIPS y Ciencia”, mediante la cual pretende hacer financieramente atractivo construir fábricas de semiconductores dentro de sus fronteras.En este contexto, la ciudad de Siracusa en el Estado de Nueva York ha visto una oportunidad para superar la crisis en la que se encuentra. Como sabemos, Siracusa está situada en el llamado Cinturón del Óxido y, al igual que otras ciudades de dicho cinturón, notablemente Detroit, ha sufrido por varias décadas de un proceso de desindustrialización por la crisis de las industrias tradicionales del automóvil, del carbón y del acero. Aprovechando la Ley de CHIPS y Ciencia, la compañía Micron, fabricante de chips semiconductores, ha desarrollado un proyecto para instalar hasta cuatro fábricas de semiconductores al norte de Siracusa con una inversión total que podría alcanzar los 100,000 millones de dólares. Con esta inversión, en el área de Siracusa se sustituirá a la industria pesada tradicional, propia de la industrialización de los Estados Unidos de la primera mitad del siglo XX, por la industria de los semiconductores del siglo XXI, que transformará de manera profunda nuestra civilización.El chip semiconductor, el objeto más complejo jamás construido, se ha convertido en un elemento de primera importancia estratégica. Si en la década de los años sesenta los norteamericanos toleraron -y hasta disfrutaron- la invasión británica, en la actualidad no se pueden dar el lujo de tolerar una falta de control en la fabricación de los chips semiconductores. A un grado tal que invertirán cientos de miles de millones de dólares para conjurarla.",
    "A las cinco y media de la mañana del 16 de julio de 1945, un grupo de adolescentes que se encontraban acampando cerca de Ruidoso, Nuevo México, fueron despertadas por una explosión. En una entrevista concedida a la revista National Geographics, Barbara Kent, una de las adolescentes del grupo, recuerda el incidente: \"Todas estábamos conmocionadas... y luego, de repente, había una gran nube sobre nuestras cabezas y luces en el cielo. Incluso nos lastimaron los ojos cuando miramos hacia arriba. Todo el cielo se volvió extraño. Era como si saliera un sol tremendo”. Horas después de la explosión comenzaron a caer copos blancos del cielo. Pensando que eran copos de nieve, las niñas empezaron a jugar con ellos, frotándoselos en la cara. Algo extraño había, sin embargo, pues los copos no eran fríos sino calientes. A pesar de este hecho inusual, las niñas, de solo 13 años, pensaron que esto era por el calor que hacía.  Desafortunadamente, los copos no eran de nieve, sino producto de la primera explosión nuclear de la historia y por tanto estuvieron expuestas a residuos radiactivos. Esta explosión se llevó a cabo en el desierto de Nuevo México, cerca de la ciudad de Alamogordo, a unos 60 kilómetros del campamento en el que se encontraban las adolescentes.  La bomba nuclear de Alamogordo, lo mismo que las bombas que destruyeron posteriormente a la ciudades japonesas de Hiroshima y Nagasaki al terminar la Segunda Guerra Mundial, fueron resultado del proyecto Manhattan, llevado a cabo de manera ultrasecreta por el Gobierno de los Estados Unidos como parte del esfuerzo de guerra. El líder científico del proyecto fue Robert Oppenheimer, quien con seguridad se hará popular, pues es el protagonista de la película que lleva su nombre y que fue estrenada en las salas de cine esta semana. El desierto de Nuevo México fue escogido para llevar a cabo la primera explosión nuclear de la historia- bautizada como prueba Trinity- por su baja densidad de población. No obstante, por la naturaleza ultrasecreta de la operación, miles de personas que se encontraban a decenas de kilómetros del sitio de la explosión -como sucedió con las adolescentes de campamento de Ruidoso- no fueron alertadas de su inminencia y fueron expuestas a altas dosis de radiación. Así, en los años que siguieron a la prueba Trinity, las adolescentes que jugaron con copos radiactivos en el campamento de Ruidoso enfermaron, a tal grado que, al alcanzar la edad de 30 años, Barbara Kent era la única sobreviviente del grupo, si bien habiendo sufrido varios tipos de cáncer. ¿En qué medida y con qué alcance se dispersaron los contaminantes radiactivos de la prueba Trinity? Un artículo aparecido esta semana en el sitio “arXiv” intenta responder a esta pregunta. El artículo, que todavía debe pasar por una revisión formal, fue escrito por un grupo de investigadores encabezado por Sébastien Philippe de Princeton University. En su trabajo, Philippe y colaboradores conjuntaron datos gubernamentales y datos climáticos, con cálculos sofisticados del movimiento de las capas atmosféricas para determinar la dispersión de contaminantes radiactivos durante los diez días siguientes a la prueba Trinity.Los cálculos demuestran que los contaminantes radiactivos se dispersaron en dirección noreste, llegando eventualmente a todos los estados de la unión americana, con la excepción de Washington y Oregon. Las mayores concentraciones se dieron a lo largo de una franja que cubre una buena parte del medio oeste norteamericano. Dichos contaminantes cruzaron también hacia Canadá y México. En este último caso a largo de toda la frontera, desde California hasta Texas. Philippe y colaboradores encuentran que la nube radioactiva llegó al lago Crawford, en Ontario, Canadá, el 20 de julio de 1945, con un pico de concentración dos días después. Dado que la prueba Trinity se llevó a cabo con plutonio, el 20 de julio de 1945 marca la fecha en la que se depositó plutonio en el fondo del lago Crawford por vez primera. De esta manera, los expertos proponen que este lago sea el sitio que marque el inicio del Antropoceno, una nueva época geológica que seguiría al holoceno -vigente desde el fin de la última glaciación-. El Antropoceno está caracterizado por un estrato geológico con una huella que marca la presencia humana en la Tierra, en este caso, una capa con plutonio en el fondo del lago Crawford.Así, tenemos fe que, en un futuro lejano, un geólogo -humano o no humano- al explorar el fondo del lago Crawford -o lo que quede de él- encuentre una capa de plutonio que le indique el momento geológico cuando los humanos empezaron a modificar su planeta -o lo que quede de él.",
    "Sin pretender crear un sentimiento de culpa en aquellos aficionados a un buen corte de carne, habremos de mencionar que, según los expertos, la producción de un kilogramo de carne de vacuno genera un volumen de gases de invernadero equivalente a 100 kilogramos de dióxido de carbono. Esto incluye el dióxido de carbono producido por la alimentación de las vacas, así como el metano generado por su digestión, que es un gas de invernadero más potente que el dióxido de carbono. A nivel global, se estima que la producción de carne comestible constituye un 15% del total de emisión de gases de invernadero. Dados los números anteriores, como una estrategia para disminuir la emisión de gases de invernadero, se ha planteado sustituir la carne comestible producida por los métodos tradicionales, por carne cultivada. En esta dirección, en el año 2020 Singapur fue el primer país en aprobar la carne cultivada para consumo humano.  Los Estados Unidos, por su parte, acaban de hacer lo propio y más de 150 empresas en ese país están preparándose para introducir la carne cultivada en los anaqueles de los supermercados.La carne cultivada se fabrica en biorreactores a partir de tejidos extraídos de animales jóvenes, empleando básicamente los mismos ingredientes de la carne tradicional y sin la crianza y matanza de ganado. Así, además de potencialmente contribuir a paliar la crisis climática, la carne cultivada se alinea con las ideas sobre la protección de los animales actualmente en boga.Ciertamente, si bien la carne cultivada tendría indudablemente un impacto positivo sobre vacas, puercos y ganado en general -que sin duda respirarán aliviados- no es claro que el clima del planeta se verá igualmente beneficiado. Un artículo aparecido el pasado mes de abril en el sitio “bioRxiv” no es muy positivo en este respecto. Dicho artículo, que no ha pasado todavía por un proceso de revisión por otros colegas, fue elaborado por un grupo de investigadores encabezados por Derrick Risner, de la Universidad de California en Davis. En su artículo, Risner y colaboradores consideran dos casos extremos, En un primer caso, asumen que la industria de carne cultivada empleará los métodos empleados por la industria farmacéutica, que requiere la purificación de los materiales empleados en la fabricación de los medicamentos para eliminar contaminantes. Por lo contrario, en un segundo caso los investigadores asumen que la fabricación de la carne cultivada empleará ingredientes con la pureza empleada por la actual industria de fabricación de alimentos. En uno u otro escenario, Risner y colaboradores encuentran impactos ambientales drásticamente diferentes.En efecto, asumiendo que la carne cultivada se fabricará sin emplear ingredientes ultrapuros, el impacto ambiental será positivo y se reducirá el volumen de gases de invernadero emitidos por kilogramo de carne producida. En contraste, empleando ingredientes ultrapuros se incrementaría dicho volumen hasta por un factor de diez. Si este fuera el caso, desde el punto de vista del clima del planeta, el remedio resultaría peor que la enfermedad.De manera entendible, las conclusiones de Risnar y colaboradores no han sido del agrado de las empresas que están trabajando en la comercialización de la carne cultivada. Cuestionan, por ejemplo, que haya necesidad de emplear materiales ultrapuros en su fabricación.  Y sin el empleo de materiales ultrapuros, aparentemente la carne cultivada sí resultaría un remedio para paliar la crisis climática. ¿Llegará la carne cultivada a los estantes de los supermercados? Habría quizá que esperar algunos pocos años para averiguarlo. Por lo demás, si fuera el caso, habría que comparar su sabor y textura con el sabor y textura de la carne real.  Después de todo, si somos fanáticos de, por ejemplo, las chuletas de puerco y en un restaurant nos sirven chuletas de puerco cultivadas, no será suficiente que nos las anuncien como tales. A menos que seamos ambientalistas convencidos y/o partidarios de la protección de los animales, y estemos dispuestos, en cualquier situación, a hacer un sacrificio.",
    "Como sabemos, el agua cubre aproximadamente el 70 por ciento de la superficie de nuestro planeta. En volumen, el agua terrestre, incluyendo océanos, ríos, lagos, depósitos subterráneos y el vapor de agua en la atmósfera, supera los 1,400 millones de kilómetros cúbicos. Este número es demasiado grande para que podamos concebirlo, pero si lo dividimos entre la población del mundo resulta que, a cada uno de nosotros nos tocan alrededor de doscientos mil millones de litros de agua dulce. Si bien este número está todavía más allá de nuestra comprensión, es claro que no llegaríamos a consumir un volumen tal de agua aun si viviéramos mil años.  Por lo demás, habría que señalar que, del total de agua en la superficie de la Tierra, apenas el 2.5 por ciento es agua dulce y que, de ésta, el 98 por ciento está congelada. Sin embargo, haciendo los cálculos correspondientes, podemos convencernos de que, ni aun así, deberíamos tener problemas con el suministro de agua dulce para nuestro uso personal.Sabemos, no obstante, que usamos el agua de distintas maneras y que tenemos problemas con este suministro por un número de factores, entre los que se encuentra el cambio climático. Con relación a esto, en la página de Internet de ONU-Agua podemos leer lo siguiente: “El agua y el cambio climático están íntimamente relacionados. Los eventos climáticos extremos están haciendo que el agua sea más escasa, más impredecible, más contaminada o las tres cosas. Estos impactos a lo largo del ciclo del agua amenazan el desarrollo sostenible, la biodiversidad y el acceso de las personas al agua y al saneamiento”.Una opción que manejan los expertos para combatir la crisis de agua por la que traviesa el planeta es la desalación del agua del océano. Se puede desalar el agua por destilación; es decir, hirviéndola y condensando el vapor que se produce en una superficie fría. Otra manera es por medio del proceso conocido como ósmosis inversa. En este proceso, el agua con sal es forzada a pasar mediante presión a través de una membrana semipermeable, la cual permite el paso del agua al mismo tiempo que impide el paso de la sal disuelta en ella. Un tercer esquema para desalar agua usa la técnica de electrodiálisis. Esta técnica emplea membranas semipermeables que, de manera selectiva, permiten el paso de uno de los componentes de la sal y bloquean el paso del otro. En este sentido, para remover el cloruro de sodio -sal común- el agua es forzada a pasar a través de un conducto bordeado por dos membranas, una que permite el paso del sodio y bloquea el paso del cloro, y otra que hace lo contrario. Todo esto ayudado por la electricidad.  Los tres procesos de desalación anteriores requieren de grandes cantidades de energía que se traducen en altos costos para el agua producida. Así, las plantas de desalación de agua de mar han avanzado de manera lenta. En la medida en que avance la tecnología de desalación de agua, sin embargo, estas plantas se harán económicamente más competitivas. En este respecto se puede mencionar un artículo aparecido el pasado mes de abril en la revista ACS Energy Letters, publicado por un grupo de investigadores encabezado por Nayeong Kim, de la Universidad de Illinois en Urbana-Champaign, en el que se reporta el desarrollo de una celda de electrodiálisis en la que se ha sustituido una de las membranas semipermeables, al mismo tiempo que se ha reducido de manera sustancial la energía demandada por su operación. Todo esto, según Kim y colaboradores, contribuirá a reducir el costo de desalación del agua.  Podemos así esperar que en el futuro el mundo obtenga de mar el agua dulce que necesita, del mismo modo que obtendrá del Sol la energía necesaria para operar las plantas desaladoras. Después de todo, los océanos constituyen la mayor fuente de agua que tenemos a nuestra disposición, de la misma manera que el Sol lo es con respecto a la energía.  Nada es gratis, sin embargo, y la desalación de grandes cantidades de agua marina generará igualmente grandes cantidades de agua de desecho con grandes concentraciones de sal que, vertidas de regreso al mar, podrían generar localmente grandes impactos ambientales de no hacerse de forma juiciosa. La contaminación salina se uniría así a la contaminación atmosférica y al cambio climático con su cauda de olas de calor y de frío, lo mismo que de inundaciones y sequías. Como reflejo de que, efectivamente, el planeta nos ha quedado chiquito.",
    "Una fotografía antigua de San Luis Potosí constituye un testimonio fidedigno de la imagen de la ciudad en el momento en que fue tomada. De la misma manera, una serie de fotografías tomadas a lo largo de un periodo de tiempo nos ilustra sobre la evolución temporal de dicha imagen. Así, fotografías de nuestra ciudad impresas en el libro “El San Luis que se fue”, publicado por Pro San Luis Monumental, nos muestran que al iniciar el siglo XX la catedral solamente contaba con una torre, al igual que el templo de San Miguelito. De la misma manera, una fotografía del año 1920 nos muestra que, desafortunadamente, han desaparecido los edificios que ocupaban las dos esquinas de la calle de Zaragoza al desembocar en la Plaza de Armas.La fotografía, por supuesto, es una proyección en dos dimensiones de la imagen urbana y nos proporciona solamente una pálida idea de cómo habría lucido la ciudad en el pasado, sobre todo si la fotografía es en blanco y negro. Imagine por un momento que pudiéramos superar esta limitación y que, de alguna manera, pudiéramos sumergirnos, en tres dimensiones y a todo color, en un ambiente urbano ya desaparecido; que pudiéramos, por ejemplo, caminar por la antigua calle de Juárez, hoy Álvaro Obregón, cuando aún no demolían el templo de San Nicolás para construir el Palacio de Cristal.¿Qué posibilidades hay de que esto último pudiera llegar a ocurrir? ¿Sería posible, por ejemplo, usar un casco de realidad virtual para sumergirnos en una proyección creada por una computadora, de un ambiente urbano ya desaparecido?  Ciertamente, dicho ambiente podría recrearse en un set cinematográfico, como es común cuando se filma una película. Desafortunadamente, muy posiblemente no contemos con suficiente información para asegurar que dicha recreación fuera fidedigna.No es éste el caso en otras latitudes, como lo muestra un artículo aparecido esta semana en la revista Plos One, en el que se muestra la reconstrucción de dos vecindarios de la ciudad de Columbus, Ohio, tal como lucían hace más de medio siglo. Dicho artículo fue publicado por un grupo de investigadores encabezado por Yue Lin, de la Universidad Estatal de Ohio. Lin y colaboradores basaron su trabajo en los mapas Sanborn, empleados por las compañías de seguros para evaluar los riesgos de incendio de una propiedad. Estos mapas, que eran continuamente actualizados, acumulan datos de edificios de 12,000 ciudades y pueblos en los Estados Unidos en los siglos XIX y XX. Los mapas Sanborn contienen una gran cantidad de información acerca de cada uno de los edificios en un vecindario, incluyendo los materiales de construcción de los que estaban hechos, sus dimensiones y su contorno, y la ubicación de ventanas y puertas. Se incluía también información acerca del departamento local de bomberos, de la localización de las tomas de agua y gas disponibles en caso de emergencia, lo mismo que información de los edificios públicos, iglesias y empresas en el vecindario.   Los mapas Sanborn ya no son usados por las compañías de seguros, pero la gran cantidad de datos que contienen los convierten en materiales de consulta para un gran número de investigaciones, incluyendo, el estudio de la evolución del entorno urbano de las ciudades en los Estados Unidos, por los programas de renovación urbana, y por la construcción de vías rápidas para la circulación de automóviles, que constituyeron líneas divisorias de áreas urbanas.  Dada la gran cantidad de información contenida en los mapas Sanborn, Lin y colaboradores utilizaron técnicas de inteligencia artificial para extraerla de manera rápida y eficiente. La información adquirida les permitió desarrollar mapas tridimensionales en computadora de dos vecindarios de la ciudad de Columbus que fueron divididos por una vía rápida en la década de los años sesenta. En su artículo, Lin y colaboradores incluyen una visualización interactiva que nos permite adentrarnos en estos dos vecindarios, pudiendo ver a los edificios con gran detalle (https://bit.ly/3Dj3IgN). En la visualización se muestran también los edificios que fueron demolidos por la construcción de la vía rápida. Lin y colaboradores nos demuestran que es posible “revivir” un vecindario tal como lucía hace más de medio por medio de la información contenida en los mapas Sanborn. Dado que este tipo de mapas existen desde la segunda mitad del siglo XIX, tal pareciera que el proceso de resurrección podría extenderse todavía más hacia el pasado. En contraste, en nuestra ciudad, y dado que solo contamos con poco más que fotografías antiguas en blanco y negro, tal parece que tendríamos que conformarnos con bastante menos.",
    "En 1854, Antonio Meucci, inventor italiano emigrado a los Estados Unidos, puso en funcionamiento un dispositivo que bautizó como “teletrófono”, para mantenerse en comunicación con su esposa que padecía de reumatismo. El teletrófono conectaba la oficina del inventor en la planta baja de edificio, con el dormitorio de su esposa en el primer piso, y se considera que es el precursor de los teléfonos modernos. Desafortunadamente, por problemas económicos Meucci no pudo patentar su invención y solo la protegió de manera parcial. Quien sí pudo hacerlo fue Alejandro Graham Bell, lo que motivó una demanda por parte de Meucci. Sin éxito, pues murió sin que le hubiera sido reconocida su invención y así, Graham Bell fue reconocido como el inventor del teléfono. Esto se mantuvo hasta 2002, año en el que el congreso de los Estados Unidos aprobó un documento que reconoce a Meucci como el verdadero inventor del teléfono.De haber podido viajar 150 años hacia el futuro, a Meucci le hubiera complacido enormemente saber que al fin se le había hecho justicia. Al mismo tiempo, con seguridad se habría sorprendido al atestiguar hasta qué punto ha evolucionado su invención hasta el punto de hacerla irreconocible. No solamente porque los teléfonos actuales pueden comunicarse por medios inalámbricos, sino porque, a partir de su función original, los teléfonos han adquirido un gran número de funciones adicionales. Así, hoy en día, aparte de usarlos para hablar por teléfono, usamos a los teléfonos inteligentes para trasmitir textos e imágenes, para ver películas y videos, para buscar direcciones, para guiarnos hacia ellas, para tomar fotografías y como dispositivos de geolocalización, entre otras muchas aplicaciones.   De este modo, no sorprende que un grupo de investigadores encabezados por Joseph Breda, de la Universidad de Washington hayan propuesto una nueva función para los teléfonos inteligentes: su uso como termómetros para medir la temperatura del cuerpo. La propuesta de Breda y colaboradores fue publicada el pasado mes de marzo en la revista “Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies”.Para cumplir esta nueva función, Breda y colaboradores hacen uso de los termistores, que son dispositivos que se encuentran en el interior de los teléfonos móviles y que sirven para medir la temperatura en ciertos puntos clave. Por ejemplo, miden la temperatura de la batería como un medio para diagnosticar su estado de salud. Para implementar esta aplicación, los investigadores desarrollaron la aplicación “FeverPhone”, que estima la temperatura corporal a partir de los cambios de temperatura que miden los termistores cuando la pantalla del teléfono es presionada contra la frente del paciente por 90 segundos. Al estar en contacto frente y pantalla se establece un flujo de calor que eleva la temperatura del teléfono hasta que se alcanza un equilibrio. FeverPhone captura los cambios de temperatura, los interpreta y calcula la temperatura corporal. Los investigadores probaron su dispositivo con 37 voluntarios, 16 de los cuales había tenido cuando menos una fiebre leve. FeverPhone fue capaz de determinar la temperatura corporal de los pacientes con un error promedio de 0.23 grados centígrados, error que es comparable con lo que se obtiene con termómetros comerciales.Breda y colaboradores señalan que no pretenden sustituir los termómetros corporales, que están diseñados específicamente para medir temperaturas, con un dispositivo diseñado para otros propósitos. Hacen notar, sin embargo, que a menudo no se cuenta con un termómetro a la mano en caso de necesidad. Citan, por ejemplo, un estudio llevado a cabo con 141 padres de niños con fiebres convulsivas que requerían de un seguimiento continuo de su temperatura corporal. Sin embargo, según el estudio citado, solamente en un 15 por ciento de casos los padres tuvieron acceso a un termómetro.  En circunstancias tales, FeverPhone sería de gran utilidad. Es decir, mientras que en el mundo son mayoría quienes cuentan con un teléfono móvil, no podemos decir lo mismo con respecto de aquellos con un termómetro corporal.Ciertamente, el dispositivo inventado por Meucci hace 170 años ha cambiado tanto que le resultaría irreconocible. Hasta tal punto ha cambiado, que seguir llamándolo “teléfono” resulta fuera de lugar. ¿Cuál podría ser un nombre adecuado para un dispositivo que sirve para hablar por teléfono, pero también para enviar un texto escrito? ¿O para tomar una fotografía y subirla a una red social? ¿O para buscar y encontrar una dirección o una información en la Wikipedia? O si FeverPhone finalmente se consolida, ¿para medir la temperatura corporal?",
    "De acuerdo con cifras de la Oficina Nacional de Administración Oceánica y Atmosférica de los Estados Unidos, el nivel promedio de mar a nivel global se ha incrementado aproximadamente 20 centímetros desde 1880; es decir, se ha elevado a una velocidad promedio de 1.5 milímetros por año en el último siglo y medio. Según la misma fuente, en el período 2006-2015, dicha velocidad promedio se ha más que duplicado, alcanzando 3.6 milímetros por año. Nos explican los expertos que, entre los factores que provocan la elevación del nivel del mar, se encuentran la fusión de los hielos polares en Groenlandia y en la Antártida, y la expansión del agua del océano en la medida que se eleva su temperatura por el cambio climático.No son estos factores, sin embargo, los únicos que contribuyen al crecimiento del nivel del mar, y en este sentido, un artículo aparecido esta semana en la revista “Geophysical Research Letters”, reporta los resultados de un estudio llevado a cabo para determinar la influencia que la extracción del agua subterránea tiene sobre el incremento del nivel de los océanos. El estudio fue publicado por un grupo de investigadores encabezado por Ki-Weon Seo, de la Universidad Nacional de Seúl, en Corea del Sur. Como sabemos, la sobreexplotación de los mantos acuíferos es uno de los problemas que enfrenta el mundo y lleva al agotamiento de los depósitos de agua subterránea. Traer agua subterránea a la superficie, además, tiene el efecto de incrementar el nivel del agua superficial. Así, Seo y colaboradores señalan que, por medio de un modelo climático, se estima que entre 1993 y 2010 se extrajo del subsuelo suficiente agua para incrementar el nivel de los océanos en 6.24 milímetros. Antes de iniciar su investigación, sin embargo, no existían evidencias directas que corroboraran esta cifra. En estas condiciones, los investigadores decidieron determinar el efecto que, sobre el nivel del mar, ha tenido el agua subterránea traída a la superficie. El método que siguieron para cumplir con sus objetivos resulta, quizá, sorprendente, pues los investigadores se basaron en la desviación sufrida por el eje de rotación de la Tierra entre 1993 y 2010. ¿Por qué la extracción de agua subterránea influye en la orientación de este eje? Para entenderlo, pensemos en la Tierra como un trompo gigante que gira alrededor de un eje inclinado, completando un giro en 24 horas. Dicho eje, a su vez, gira con un movimiento de precesión, completando un ciclo en alrededor de 26,000 años, y lleva a cabo otros movimientos que dependen de las fuerzas a las que está sujeto el planeta por otros cuerpos celestes. Adicionalmente, la orientación del eje de rotación de la Tierra es sensible a una redistribución de masa en el planeta, como lo que puede ocurrir durante un terremoto de gran magnitud. Lo mismo que por la extracción a la superficie de grandes cantidades de agua subterránea, que es el caso que nos ocupa. Como resultado de su estudio, Seo y colaboradores encuentran que para explicar la desviación del eje de rotación de la Tierra observada entre 1993 y 2010, es necesario considerar, aparte de otros factores, el volumen de agua subterránea extraída que arroja el modelo climático. Solamente por esta extracción, el eje de rotación se habría movido cerca de 80 centímetros.Si bien los expertos descartan que la desviación del eje de rotación de la Tierra por la extracción de agua subterránea pueda afectar el clima del planeta, las conclusiones del trabajo de Seo y colaboradores resultan apabullantes: sin importar el enorme tamaño de la Tierra, hemos logrado modificar su eje de rotación en un tiempo récord. Se confirma así que el planeta ya nos quedó chico. El problema, sin embargo, es que no tenemos otro de que echar mano.",
    "Desde su invención hace más de un siglo, el cine ha sido una forma de entretenimiento enormemente atractiva. Por razones, además, variadas, dado que el cine aborda prácticamente cualquier tema y en circunstancias diversas. La película “Casablanca” de 1942, por ejemplo, está considerada por muchos como una de las mejores de todos los tiempos, a pesar de que cuando se filmó no se pensaba que sería una película excepcional. Como sabemos, la acción de dicha película se sitúa en Casablanca, en el Marruecos francés, durante la Segunda Guerra Mundial, cuando Francia había sido invadida por los alemanes y los Estados Unidos estaban a punto de entrar a la guerra. Se considera que Casablanca fue parte de la propaganda antinazi estadounidense. Al mismo tiempo, la película entrelaza historias de intriga, corrupción, altruismo y amor, entre Humphrey Bogart e Ingrid Bergman, que la hicieron grandemente atractiva. Esto último, a pesar de que Bergman superaba en altura a Bogart por cinco centímetros, lo que en ocasiones lo obligó a subirse a un banquito cuando apareció a su lado en la película. Las películas tienen la habilidad de sumergirnos en situaciones que mucho o poco tienen que ver con la realidad.  Así, una película nos permite viajar a la Luna o a Marte, o a las profundidades del océano, lo mismo que a versiones precisas o distorsionadas del pasado, reciente o remoto. Según los expertos, esta habilidad se potenciará con las nuevas tecnologías de inteligencia artificial para generar imágenes. En este sentido, en su último número, la revista de divulgación MIT Technology Review trae a colación la película corta “The Frost” –“La Escarcha”-, que es la primera compuesta por escenas -13 minutos en total- elaboradas totalmente por medio de un programa de inteligencia artificial a partir de un libreto.Una versión gratis de “The Frost” puede ser encontrada en Internet. El argumento es simple: una mala manipulación de geoingeniería climática llevó a que la temperatura de la Tierra descendiera drásticamente, a un grado tal que solamente quedó habitable una pequeña región de la Antártida. En las escenas iniciales, en un paisaje congelado, aparece un grupo de personas en un campamento improvisado calentándose alrededor de una fogata y quejándose del frío extremo. Las escenas son extrañas, sin el movimiento fluido al que estamos acostumbrados. El movimiento de los labios al hablar tampoco nos parece natural.  Repentinamente, el grupo de personas recibe lo que interpretan como “la señal” que viene de lo alto de una montaña y se sienten obligados a ir a buscarla. En su ascenso a la montaña helada, uno de los miembros de grupo resbala y queda colgando de una cuerda, teniendo que ser sacrificado cortando la cuerda para evitar que arrastrara en su caída a otros compañeros. Enseguida, el grupo sufre el embate de una avalancha de nieve que sepulta a algunos. Escenas posteriores muestra a los sobrevivientes sentados en la nieve reponiéndose del desastre y finalmente arribando al lugar en el que se originó “la señal”. En ese punto, la película retrocede cinco años, con la imagen de un científico percatándose que la intervención programada para modificar el clima de la Tierra tendrá efectos desastrosos. La película cambia enseguida al recinto de la ONU, mostrándonos un orador que habla del inminente lanzamiento de la misión para cambiar el clima del planeta. El lanzamiento se llevó a cabo sin importar las protestas del científico que alertaba sobre los efectos desastrosos que tendría. En este momento la película nos traslada a la montaña con el grupo de personas que ha alcanzado el lugar en el que se emite “la señal”, finalizando sin explicitar que fue lo que ocurrió después.El argumento de “The Frost” es muy simple y la película está integrada por grupos de escenas que progresan por saltos. Claramente, la película no nos da la sensación de verosimilitud a la que estamos acostumbrados en el cine. Las escenas de “The Frost”, sin embargo, están elaboradas totalmente empleando una tecnología de inteligencia artificial. Así, se esperaría que en el futuro cercano la sensación de verosimilitud de películas similares mejore sustancialmente.Por lo demás, los expertos van más allá y consideran las posibilidades de interacción del espectador con la película. Por ejemplo, la posibilidad de que el espectador pudiera manipular el fin de la película según sus preferencias. Así, de haberse filmado Casablanca 80 años después, el espectador podría, quizá, haber tenido la posibilidad de cambiar el fin de la película, de modo tal que Rick (Humphrey Bogart) y Ilsa (Ingrid Bergman) hubieran abordado juntos el avión que los sacaría de Casablanca. O incluso más allá, con la posibilidad de que el espectador mismo fuera protagonista de la película, en sustitución de Rick o de Ilsa, según fuera el caso.",
    "Como sabemos, la máquina de vapor fue uno de los elementos que impulsó la revolución industrial de los siglos XVIII y XIX. Sabemos también que fue el carbón mineral el combustible empleado por dichas máquinas y que con esto se produjo una transición energética en la que el carbón sustituyó a la madera y a otros biocombustibles tradicionales. Posteriormente, con el advenimiento de la industria eléctrica y la industria de los automóviles, se incluyó al petróleo en el conjunto de fuentes de energía, con lo que se amplió la transición energética. Habría que añadir que el petróleo, además de conveniente, era una fuente de energía barata.En efecto, a quienes vivimos la década de los años sesenta del siglo pasado nos consta que la mayor de las preocupaciones de los diseñadores de automóviles no era precisamente el ahorro de energía. Así, veíamos circular por las calles automóviles de ocho cilindros que recorrían apenas cinco kilómetros por litro de gasolina. Por lo demás, dado que en México un litro de gasolina costaba alrededor de un peso, los dueños de los automóviles tampoco estaban particularmente preocupados.  La situación cambió drásticamente en octubre de 1973, cuando, con el llamado embargo petrolero, los precios del petróleo se elevaron drásticamente para no volver a sus precios previos. Dado que las reservas de petróleo se localizaban en el Medio Oriente, fuera de los países industrializados, éstos se abocaron a desarrollar sustitutos para el petróleo y reducir así su dependencia con respecto a los conflictos políticos en el Medio Oriente. La fuente de energía alternativa más obvia fue quizá la energía solar, que es abundante e ilimitada. Una manera de aprovechar la energía solar es mediante su conversión en energía eléctrica empleando celdas solares. La primera celda solar de silicio fue desarrollada en 1954 en los laboratorios Bell de la compañía American Telephone and Telegraph y en 1958 el satélite norteamericano Vanguard fue equipado con un panel de silicio de 10 por 10 centímetros cuadrados para proveerlo de energía. En 1973, sin embargo, apenas a 20 años de su invención, las celdas solares estaban en su infancia y no constituían una opción económicamente viable para suplir al petróleo. Es decir, podían generar energía eléctrica, pero a un precio prohibitivamente alto.Hoy en día, a cuarenta años de la crisis del petróleo, la situación se ha revertido, y estamos ante una nueva transición energética. Así, según el último reporte de la Agencia Internacional de Energía, 2023 será el primer año en el que las nuevas inversiones en energía solar superarán a las nuevas inversiones en producción de petróleo. Como señala el World Economic Forum, la energía solar está creciendo rápidamente, a un grado tal que en 2022 el crecimiento de la capacidad instalada de generación de energía solar superó al crecimiento de la capacidad combinada de todas las demás fuentes de energía, incluyendo la energía eólica, el gas natural, el carbón, la energía hidroeléctrica y la energía nuclear, entre otras. Habría que señalar, sin embargo, que el cambio energético ocurre en forma más marcada entre los países industrializados que entre aquellos en desarrollo, a pesar de que algunos de éstos últimos, incluido el nuestro, cuenten con grandes recursos solares. Una característica intrínseca de la energía solar es que se encuentra disponible solamente durante el día. El problema que representa esta intermitencia puede ser atenuada por una red de distribución eléctrica que transfiera energía desde una región iluminada hacia otra región que en esos momentos sufra una baja en la intensidad solar.  Por lo demás, ciertamente es necesario desarrollar sistemas de almacenamiento masivo de energía. Un ejemplo en este sentido es el proyecto Snowy 2.0 en el estado de Nueva Gales del Sur en Australia, que pretende almacenar energía en forma de energía gravitacional. mediante un sistema de dos represas colocadas con una diferencia de alturas de 700 metros. Las dos represas están conectadas por un túnel de 27 kilómetros. Para almacenar energía, el agua es bombeada a través del túnel desde la represa inferior hacia la represa superior por medio de energía solar. La energía así almacenada es recuperada regresando el agua hacia la represa inferior haciéndola pasar por una turbina y un generador de electricidad.  En vista de lo anterior, estamos ante una nueva transición energética, en la que los combustibles fósiles serán reemplazados por fuentes alternas de energía. Entre estas fuentes, la energía solar será la dominante.",
    "Como uno de los mayores desastres ecológicos de los que nuestro tiempo ha sido testigo, el Mar de Aral, una vez el cuarto mayor lago de agua salada del planeta, se ha ido desecando paulatinamente hasta convertirse en dos cuerpos separados de agua y un desierto de arena tóxica. El colapso del Mar de Aral, situado entre Kazajistán y Uzbekistán, fue producido por el desvío de los ríos Sir Darya y Amu Daryia que lo alimentaban, para irrigar plantaciones de algodón. Estos ríos fueron desviados cuando Kazajistán y Uzbekistán formaban parte de la Unión Soviética. En la actualidad, la superficie del Mar de Aral es apenas un 10 por ciento de la superficie original, la cual ascendía a 65,000 kilómetros cuadrados, aproximadamente la superficie del estado de San Luis Potosí.El Mar de Aral es, ciertamente, un caso extremo de desastre ambiental producido por la intervención humana que esperaríamos no se repitiese; al menos, no en una escala de tales dimensiones. Un artículo aparecido esta semana en la revista “Science”, sin embargo, concluye que el volumen de agua almacenada en más de la mitad de los lagos y represas a nivel global ha disminuido a lo largo de las últimas décadas. El artículo fue publicado por un grupo de investigación encabezado por Fangfang Yao, de la Universidad de Colorado en Boulder. Como apuntan Yao y colaboradores, el volumen de agua almacenada en lagos naturales y embalses fluctúa siguiendo cambios en precipitaciones pluviales y descargas de los ríos, así como también por factores humanos como la construcción de represas y el consumo de agua. Para determinar cómo todos estos factores han influido en el volumen de agua almacenada en cuerpos de agua naturales y artificiales, Yao y colaboradores llevaron a cabo un estudio con imágenes satelitales de 1972 cuerpos de agua, que representan el 95 por ciento del volumen de agua almacenada a nivel global. De estos cuerpos de agua, 1052 son lagos naturales con áreas superficiales entre 100 y 377,000 kilómetros cuadrados, mientras que 921 son lagos artificiales con superficies entre 4 y 67,000 kilómetros cuadrados.   Con su estudio, Yao y colaboradores no solamente buscaban determinar cómo el volumen de agua almacenada a nivel global ha evolucionado en las últimas décadas, sino también identificar los factores, naturales o humanos, que han determinado dicha evolución. Haciendo uso de 250,000 fotografías satelitales recolectadas entre 1992 y 2020, los investigadores determinaron que en el 53 por ciento de los depósitos de agua a nivel global está disminuyendo el volumen del agua almacenada. En el caso de los lagos naturales, los principales factores responsables de esta disminución son el cambio climático y el consumo humano, mientras que la sedimentación es el factor principal de la degradación del volumen de agua almacenada en represas. Con base a su estudio, los investigadores concluyen que 2,000 millones de personas viven en áreas en la que se observa una tendencia significativa de disminución en el volumen de agua almacenada y que estas personas serán las más afectadas por la falta de agua en el futuro.Yao y colaboradores también comentan que su estudio puso al descubierto una tendencia decreciente del volumen de agua almacenado en lagos tales como el Lago Salton en el sur de California y el lago Mar Chiquita en Argentina, que es el mayor lago salado en Sudamérica. Con respecto a este último, sin embargo, el diario digital Infobae recoge de declaraciones de Eduardo Pioviano, investigador del Conicet en Argentina, quien niega que el Mar Chiquita se esté desecando por causas humanas y que vaya a desaparecer. Afirma que Yao y colaboradores fundan sus conclusiones en mediciones del nivel del lago en un periodo de tiempo demasiado corto. De hecho, Pioviano espera que este nivel se incremente en cuanto cambien las condiciones climáticas. De un modo u otro, las conclusiones de Yao y colaboradores no son demasiado alentadoras. Ciertamente, tenemos la esperanza que el futuro no nos depare desastres como el del Mar de Aral, con sus extrañas e impactantes imágenes de barcos pesqueros descansando en medio de un inmenso mar de arena contaminada por los desechos de los pesticidas empleados en los campos de algodón. Confiaríamos que las consecuencias de la escasez de agua en el futuro no alcancen niveles tan dramáticos. Pero igual, no debemos dejar de preocuparnos.",
    "Cuando estén disponibles, los viajes de turismo espacial a la Luna y más allá serán, sin duda alguna, experiencias fascinantes. No todo será así, sin embargo, al menos con el mismo grado. Es posible, por ejemplo, que la comida que nos sea servida en un restaurante en la Luna no tenga la misma calidad que la comida que podemos disfrutar en un restaurante ordinario, aquí en la Tierra.  Una idea de la comida que podríamos esperar ingerir durante un viaje de placer interplanetario nos la dan los proyectos ganadores del concurso “Deep Space Food Challenge”, organizado por la NASA. De acuerdo con la agencia espacial norteamericana: “El objetivo de este concurso fue generar nuevas tecnologías o sistemas de producción de alimentos que requieran recursos mínimos y produzcan residuos mínimos, al tiempo que proporcionan alimentos seguros, nutritivos y sabrosos para misiones de exploración humana de larga duración”. En este sentido, dado que las misiones espaciales tienen que embarcarse con todos los alimentos necesarios para el viaje y que éstos caducan en un año y medio, para el caso de misiones de larga duración, es necesario desarrollar técnicas para producirlos a bordo de la nave.La convocatoria para el concurso “Deep Space Food Challenge”, que consta de tres fases, fue publicada por la NASA en enero de 2021. Se inscribieron cerca de 200 participantes, y de estos, cinco compañías norteamericanas y tres compañías extranjeras sobrevivieron a la fase 2 y se encuentran compitiendo en la fase 3. Esto último fue anunciado por la NASA el pasado 19 de mayo. De acuerdo por declaraciones de la NASA recogidas por un artículo publicado esta semana en la revista “MIT Technology Review”: “La fase 2 fue una especie de demostración a nivel de cocina, mientras que la fase 3 busca desafiar a los participantes para que escalen sus tecnologías”. Entre los participantes en la fase 3 encuentra la compañía “Air Company”, basada en Nueva York. De acuerdo con su página en Internet, esta compañía se enfoca en el uso del dióxido de carbono en la atmósfera para producir alcohol y combustibles. Entre los productos desarrollados por Air Company se incluyen combustibles para aviación, perfumes y Vodka. Con relación al concurso de la NASA, Air Company propone usar el dióxido de carbono expelido por los astronautas al respirar para generar alcohol, y a partir de éste producir alimentos. Según declaraciones de Air Company aparecidas en “MIT Technology Review”: “El proceso produce alcohol que luego se puede alimentar a la levadura, produciendo algo que es comestible. Esencialmente, un batido de proteínas similar a un sustituto vegano de la carne”.Un proyecto similar es propuesto por la compañía finlandesa “Solar Foods”, que produce “Solein”, una especie de harina obtenida partir del dióxido de carbono. El proyecto espacial de “Solar Foods” involucra la producción de “Solein” a partir del dióxido de carbono exhalado por los astronautas y del hidrógeno producido durante la generación de oxígeno por la descomposición del agua.  Otra de las compañías finalistas en la fase 3 del concurso es “Interstellar Lab”, basado en Florida, que propone el uso de “NUCLEUS” para producción de alimentos en el espacio. NUCLEUS consiste en nueve módulos, cada uno de ellos con su propio sistema de control de temperatura, humedad e irrigación. Los módulos pueden se usados para crecer vegetales, hongos e insectos al mismo tiempo. Como explica “Interstellar Lab” en un video en YouTube: “En el espacio, los astronautas usarán hongos, insectos y vegetales, para crear una variedad de platillos y recetas. Por ejemplo, podrían usar hongos para crear una sabrosa sopa. Podrían moler insectos y obtener un polvo rico en proteínas que se mejorará el sabor de cualquier comida. Usando, plantas, insectos y hongos, los astronautas crearán un sabroso y nutritivo menú”. La compañía sueca Mycorena es otro de los finalistas. Para producir alimentos en el espacio, esta compañía propone el uso de una proteína producida a partir de la fermentación de un hongo. En declaraciones de la jefe de investigación de Mycorena recogidas por “MIT Technology Review”: “Esta proteína por sí misma no tiene mucho sabor, pero añadiéndole saborizantes es posible obtener diferentes alimentos, incluyendo hamburguesas y nuggets”. Además, empleando una impresora 3D, “desde una pantalla se podría escoger e imprimir un filete de pollo”.    Habríamos de coincidir que, basados en las propuestas para producir comida en el espacio profundo, las opciones que tendríamos como turistas espaciales no resultarían muy halagadoras para la mayoría. Aun así, el impactante ambiente espacial en el que estaríamos inmersos, con seguridad nos haría olvidar cualquier mala experiencia que hubiéramos tenido con un filete de pollo artificial impreso en 3D, bañado con una salsa de moscas negras.",
    "Como recordamos, en septiembre de 1991 dos alpinistas descubrieron el cuerpo congelado de Otzi. El descubrimiento ocurrió en un glaciar de los Alpes, cerca de la frontera entre Austria e Italia. Lo extraordinario del caso es que Otzi murió hace unos 5,300 años y que su cuerpo momificado fue preservado por el frío extremo al que estuvo expuesto desde su muerte. El estado de preservación en el que fue encontrado nos permite saber, según la Wikipedia, que la indumentaria de Otzi al morir, consistía en una capa de fibra vegetal, un gorro de piel de oso, un chaleco de piel de cabra, taparrabos de cuero y zapatos tejidos de cuero.El descubrimiento de los restos de un hombre en buenas condiciones de preservación es, sin duda, fascinante. En particular, nos ilustra sobre el tipo de vestimenta que se usaba hace 5,000 años, basada en pieles de animales y otros materiales de origen vegetal con, relativamente, poca manipulación. Con el tiempo, en la medida en la que avanzó la civilización, las técnicas de fabricación de la ropa se hicieron más sofisticadas con la invención de las telas o textiles. Esto involucró el desarrollo de técnicas para la obtención de hilos a partir de materiales naturales como el algodón o la lana, y la manipulación de dichos hilos en un telar para integrarlos en un textil. La función original de la ropa fue la de proporcionar protección al cuerpo en contra de las inclemencias climáticas. En la actualidad, a miles de años de su invención, los textiles han adquirido una nueva función con la invención de los textiles inteligentes. Estos textiles incluyen elementos que les permiten ir más allá de su función tradicional. Una prenda confeccionada con un textil inteligente, por ejemplo, tendría la capacidad de medir la temperatura ambiente y de reaccionar en consecuencia para proporcionar confort al usuario. Podrían también vigilar las funciones vitales del cuerpo y enviar una señal de alarma al médico en caso de que algo vaya mal.  Un artículo aparecido esta semana en la revista “Science Advances” reporta la fabricación de textiles inteligentes empleando un procedimiento similar al empleado para fabricar un textil común y corriente. El artículo fue publicado por un grupo internacional de investigadores que lleva como primer autor Sanghyo Lee de la Universidad de Cambridge en el Reino Unido.En su artículo, Lee y colaboradores describen la fabricación de un textil en el que se mezclan fibras ordinarias con fibras en las que se les han construido dispositivos que desarrollan una de cuatro funciones, a saber, detección de luz, almacenamiento de energía, procesamiento electrónico y emisión de luz. De este modo, el textil constituye un sistema electrónico capaz de almacenar energía, captar una señal, procesarla electrónicamente y enviar una señal de respuesta. Una fibra funcional se fabrica construyendo sobre la misma el dispositivo correspondiente. Así, hay fibras especializadas en almacenar energía, mientras que a otras se les fabrican detectores de luz o LEDs emisores de luz. Asimismo, hay fibras encargadas de procesar la información recibida por los detectores y proporcionarla a los emisores para su retransmisión. Por otro lado, las fibras y los elementos que las hacen funcionales no deben dañarse con el manejo rudo a la que está normalmente sometido un textil, y en este sentido, Lee y colaboradores manifiestan que sus fibras pasan la prueba.  Los textiles inteligentes descritos por Lee y colaboradores se fabricarían con fibras funcionales siguiendo el procedimiento normalmente utilizado para fabricar un textil ordinario. Sus funciones, sin embargo, serán ampliadas de manera radical, y no solamente cubrirían la función original de proteger al usuario de las inclemencias del tiempo, de manera mucho más efectiva que los textiles ordinarios, sino que incluirían funciones completamente fuera del alcance de una prenda de ropa ordinaria. Una ampliación de funciones similar a la que ocurrió con los teléfonos inteligentes, que ahora nos sirven para comunicarnos por medio de las redes sociales, para leer las noticias, para guiarnos por medio del GPS, para escuchar música, y para tomar fotografías y videos, entre otras muchas aplicaciones; incluyendo la de hablar por teléfono.   Retomando la historia de Otzi, un estudio de sus restos momificados encontró una punta de flecha alojada en un pulmón, lo que indicaría que probablemente murió asesinado. Sin embargo, probablemente no lo sabremos nunca con seguridad. Otra sería la situación si en lugar de vestirse con pieles, Otzi hubiera usado un chaleco inteligente que hubiera trasmitido los momentos de su muerte. Desafortunadamente, Otzi vivió y murió a 5,300 años de esta posibilidad.",
    "Como nos enseñan en la escuela primaria, la Tierra gira alrededor del Sol, al igual que los otros siete planetas del sistema solar. Esto no siempre fue así. Es decir, si bien los planetas siempre han girado alrededor del Sol, hubo tiempos en los que se pensaba que el Sol giraba alrededor de la Tierra. De hecho, esto es lo que nos parece en primera instancia cuando observamos que en las primeras horas de la mañana el Sol emerge del horizonte, se eleva a una máxima altura al mediodía y termina por desaparecer por el oeste al atardecer. Una explicación alternativa es que, independientemente de quien gira alrededor de quien, la Tierra gira alrededor de su eje de modo tal que la cara que expone al Sol está continuamente cambiando. Como sabemos -otra vez como nos lo enseñan en la escuela- esta es la explicación correcta. De manera natural, esta segunda explicación no fue en su momento la primera opción porque nos sacaba de una posición privilegiada como el centro del Universo. Esta situación empezó a cambiar en el siglo XVI con el advenimiento de la ciencia moderna que nos ayudó a deshacernos de prejuicios y nos convenció que el mundo es más fácil de entender si renunciábamos ser el centro del Universo. Una vez que lo aceptamos, nos quedó claro que los planetas, incluyendo a la Tierra, giran alrededor del Sol, y los satélites alrededor de sus respectivos planetas. Y con lo que aprendimos fuimos capaces de colocar satélites artificiales alrededor de la Tierra, e incluso alrededor de otros planetas en el sistema solar.  Giordano Bruno fue un personaje que vivió el proceso que nos colocó fuera del centro del Universo. Con muy poca fortuna, pues fue ejecutado en la hoguera en el año 1600 por sostener opiniones que la Iglesia Católica consideró heréticas. Entre estas opiniones, Bruno creía en la multiplicidad de los mundos, según la cual las estrellas que vemos en el cielo son mundos con planetas similares al nuestro, habitados por seres semejantes a nosotros. No estaríamos así solos en el Universo, de modo tal que, no solamente perderíamos nuestra posición como centro del Universo, sino también nuestra exclusividad como seres inteligentes.La multiplicidad de los mundos no era sino una hipótesis que Bruno no tenía manera de comprobar. Sin embargo, dada el inmenso número de estrellas en nuestra galaxia y asumiendo que la aparición de la vida es un proceso que ocurre al azar, podemos esperar que Bruno haya tenido razón y que la vida, incluso inteligente, haya, efectivamente, aparecido en una multiplicidad de mundos. Con esto en mente, a lo largo de las últimas cinco décadas, el proyecto SETI ha escudriñado el cielo en busca de señales de radio que pudieran interpretarse como provenientes de una civilización extraterrestre, sin un resultado conclusivo hasta la fecha. Algunos expertos han considerado también la posibilidad de que una civilización extraterrestre pudiera haber ya detectado señales artificiales que hayan escapado de nuestro planeta y que le hubieran revelado nuestra presencia en el Universo. Esto es analizado en un artículo aceptado para su publicación en la revista Monthly Notices of the Royal Astronomical Society, publicado por un grupo de investigadores encabezado por Ramiro Saide de la Universidad de Mauricio, en la isla Mauricio, localizada enfrente de la costa de Madagascar.En su investigación, Saide y colaboradores calcularon la intensidad de las ondas de radio emitidas por las antenas de comunicación de las redes de teléfonos celulares. En este sentido, hay que considerar que estas redes han crecido rápidamente en las últimas décadas, al punto tal que en la actualidad el número de teléfonos celulares supera al número de habitantes del planeta. Hay que considerar también que, si bien dichas antenas están extendidas por toda la superficie de la Tierra, su distribución no es homogénea, de tal manera que la radiación que escapa hacia el espacio en dirección a un punto en el cielo, varía de manera periódica con la rotación de la Tierra. Al respecto, las regiones del mundo que más contribuyen a las ondas de radio emitidas son Europa Occidental, la costa este de Asia y la costa oeste de los Estados Unidos. Concluyen, Saide y colaboradores que, a pesar del crecimiento vertiginoso que han tenido los teléfonos celulares, las ondas de radio que han escapado de nuestro planeta no habrían sido detectadas por una civilización que se encuentre dentro de un radio de diez años luz de nuestro planeta y que cuente con un nivel tecnológico equivalente al nuestro. Podría no ser el caso, sin embargo, si cuenta con un nivel tecnológico superior.En todo caso, podremos ser detectados por nuestros teléfonos celulares en la medida que crezca, en el futuro inmediato, la potencia de ondas de radio emitidas. Algo en lo que, con seguridad, Giordano Bruno hubiera mostrado mucho interés.",
    "Como recordamos, en el verano de 2021 la costa oeste de los Estados Unidos y Canadá fue golpeada por una ola de calor sin precedente. Los efectos que produjo este evento climático extremo están descritos en un artículo publicado el pasado 9 de febrero en la revista Nature Communications: “Muchos lugares rompieron récords de temperatura máxima de todos los tiempos en más de 5 °C, y el récord nacional de temperatura canadiense se rompió en 4.6 °C, con una nueva temperatura récord de 49.6 °C. Los impactos de este evento fueron catastróficos, incluidos cientos de muertes en el noroeste del Pacífico, mortalidad masiva de vida marina, rendimientos reducidos de cultivos y frutas, inundaciones de ríos por el rápido derretimiento de la nieve y los glaciares, y un aumento sustancial de los incendios forestales, lo que provocó deslizamientos de tierra en los meses siguientes. Estos impactos brindan ejemplos de lo que podemos aprender y nos dan una descripción vívida de cómo el cambio climático puede ser tan devastador.”.En el futuro, en la medida que progrese el cambio climático, se espera que ocurran más eventos extremos con consecuencias catastróficas similares; incluso con peores consecuencias, de tener lugar en un país en desarrollo que dispondría de menos recursos para enfrentarlo. ¿Puede predecirse la ocurrencia de un evento climático extremo y de esa manera estar preparados para enfrentarlo? Desafortunadamente, el clima del planeta es un sistema tan complejo que no es posible predecir los eventos extremos. Algo se puede hacer, no obstante, y en este sentido un artículo aparecido esta semana en la revista Nature Communications describe los resultados de una investigación llevada a cabo para determinar en qué lugares de nuestro planeta existe un mayor riesgo de sufrir un evento climático extremo. Dicho artículo fue publicado por un grupo de investigadores encabezado por Vikki Thomson de la Universidad de Bristol, en el Reino Unido. Thomson y colaboradores centraron su investigación en un indicador climático: la máxima temperatura alcanzada en un determinado lugar a lo largo de un año. Con esto en mente, consultaron estadísticas de mediciones de temperaturas máximas acumuladas a lo largo de las últimas seis décadas. Esto les permitió, empleando una técnica matemática especializada, calcular el tiempo probable que deberá transcurrir para que, en un determinado lugar, se rompiese el récord de temperatura alta ahí prevaleciente. Basados en sus cálculos, Thomson y colaboradores determinaron los lugares en nuestro planeta que enfrentan los riegos más altos de sufrir un evento extremo de temperatura alta. Entre estos lugares se encuentran Afganistán y la región de Centroamérica, que se esperaría rompan su récord de temperatura alta en menos de 100 años. El caso de Afganistán es, particularmente grave debido a su alta tasa de crecimiento poblacional. Igualmente, las probabilidades apuntan a que, en menos de 100 años, romperán sus récords de temperatura alta Papúa Nueva Guinea, el noroeste de Argentina, partes de Europa Central, lo mismo que partes de China y de Australia.Como comentan Thomson y colaboradores, la ocurrencia de un evento climático extremo promueve en el país que lo sufre la implantación de medidas para paliar un evento similar futuro. De este modo, un país que no ha sufrido en el pasado reciente uno de tales eventos no tendría incentivos para poner en marcha medidas preventivas, lo que implica un riesgo adicional.Además de lo anterior, Thomson y colaboradores encuentran que han ocurrido ondas de calor extremo que se desvían de las estadísticas y que, por lo mismo, de no haber tenido lugar se habrían considerado altamente improbables.  Éste es el caso de la onda de calor que afectó a los Estados Unidos y Canadá en el verano de 2021.  Más grave aún, los investigadores encuentran que, en el 31 por ciento de las regiones examinadas, las temperaturas máximas observadas son excepcionales y se desvían grandemente de las estadísticas, por lo que no es posible estimar el tiempo que tendría que transcurrir para el rompimiento del valor récord de temperatura alta. De acuerdo con los datos publicados por Thomson y colaboradores, San Luis Potosí se encuentra en este caso, de modo que no es posible establecer un horizonte de tiempo para la ocurrencia del siguiente evento climático extremo que rompa el récord de temperatura alta. Estaríamos de este modo sin información al respecto; así fuera en términos de probabilidades. En tal situación, quizá fuera prudente que cruzáramos los dedos. O bien, que tocáramos madera.",
    "Siendo el satélite más grande de Júpiter y del sistema solar -mayor incluso que el planeta Mercurio- Ganímedes tiene presencia en la literatura. Por ejemplo, el cuento corto “Navidad en Ganímedes” del escritor de ciencia ficción Isaac Asimov, se centra en la empresa Productos Ganimedinos, que opera en Ganímedes explotando sus recursos minerales y su mano de obra nativa. Los nativos ganimedinos fueron bautizados “astruces”, porque de alguna manera recuerdan a los avestruces, pero “con el cuello más corto, la cabeza más grande y un plumaje que parece que de un momento a otro vaya a desprenderse de raíz”. También, están provistos de “un par de brazos flacos y huesudos con tres dedos rechonchos”. Los nativos de Ganímedes son medianamente inteligentes y hablan inglés, “pero, cuando uno los oye, preferiría que no lo hicieran”.   Basados en la descripción anterior, tal parecería que los ganimedinos constituyen una mano de obra ideal para explotar. Y, sin embargo, como relata Asimov, la compañía minera tuvo problemas cuando los nativos se enteraron de la existencia de Santa Claus y las visitas que anualmente hace en la Tierra para entregar regalos: se negaron a trabajar a menos que también a ellos los visitara.  Con muchas dificultades, la compañía logró simular una visita de Santa Claus y con esto sus directivos pensaron que habían resuelto el problema, pues, dada la poca inteligencia de los ganimedinos, confiaban que para el siguiente año el recuerdo de Santa Claus se hubiera desvanecido. No habían tomado en cuenta, sin embargo, que en Ganímedes un año dura apenas un poco más que una semana.Como sabemos Ganímedes es parte del grupo de cuatro satélites galileanos -los otros son Calisto, Io y Europa- que pueden ser observados como puntos luminosos orbitando a Júpiter; como los pudo ver Galileo en 1610 cuando apuntó su telescopio hacia ese planeta. Los satélites galileanos fueron bautizados por el astrónomo alemán Simon Marius, contemporáneo de Galileo, y en este sentido habría que recordar que Ganímedes, Calisto, Io y Europa, fueron amantes de Zeus, el equivalente griego del romano Júpiter.   Tres de los satélites galileanos, Ganímedes, Calisto y Europa, serán estudiados por la misión europea Juice, lanzada hacia Júpiter del pasado 14 de abril desde la Guyana francesa. El viaje será largo, pues no será sino hasta julio de 2031 cuando la misión arribará a la vecindad de Júpiter. La trayectoria que seguirá será compleja e involucrará cuatro órbitas alrededor del Sol, durante las cuales será asistida por las fuerzas de gravitación de la Tierra y de Venus para alcanzar la velocidad que lo impulse hasta Júpiter. Una vez ahí, la sonda sobrevolará Ganímedes, Calisto y Europa en 12, 21 y 2 ocasiones, respectivamente. Al final de la misión en el año 2035, Juice se insertará en una órbita alrededor de Ganímedes, en lo que constituirá la primera ocasión que una sonda se coloque en órbita alrededor de un satélite natural, aparte de la Luna. Al final de su vida, Juice perderá velocidad y se estrellará contra la superficie de Ganímedes.Ganímedes, Calisto y Europa resultan interesantes porque hay indicios de que los tres albergan océanos de agua líquida por debajo de su superficie helada. Ganímedes es un objeto de estudio particularmente interesante. De acuerdo con la sociedad Planetaria, este satélite podría albergar un océano de 100 kilómetros de profundidad, enterrado por una capa de hielo de 150 kilómetros de espesor. ¿Podría un océano de agua subterráneo albergar alguna forma de vida? En nuestro planeta sabemos que la vida se puede dar en el fondo del océano alrededor de respiraderos hidrotermales que proporcionan los nutrientes necesarios. ¿Existen respiraderos hidrotermales en el fondo de los océanos subterráneos de los satélites galileanos, alrededor de los cual hubiera prosperado la vida? Según los expertos, si bien la sonda Juice no podrá detectar vida en Ganímedes o Europa, sí nos proporcionará un mayor conocimiento sobre las condiciones de habitabilidad de esos mundos helados.        Por lo demás, si existiera vida en Ganímedes presumiblemente ésta sería primitiva, muy lejos de los astruces imaginados por Asimov, con todo y lo estúpido que pudieran haber sido.",
    "Como informaron los medios de comunicación, Space X logró en días pasados la aprobación gubernamental para el vuelo de prueba de su cohete Starship, el cohete más poderoso jamás construido, capaz de colocar en órbita una carga de 150 toneladas.  De acuerdo con Elon Musk de Space X, la nave Starship será el vehículo para llevar misiones tripuladas a Marte en fecha tan cercana como el año 2026. Si bien los expertos consideran poco factible llegar hasta Marte con una misión tripulada en 2026, con seguridad esto ocurrirá en un futuro cercano. Sin embargo, llegar hasta Marte con una tripulación de unos pocos astronautas es una cosa, y otra mucho más difícil es establecer ahí una colonia permanente como algunas veces se propone, pues el medio ambiente en Marte es mucho más hostil de lo que a primera vista sugieren las fotografías que desde ese planeta nos han hecho llegar las sondas automáticas colocadas en su superficie. En este sentido, no podemos extrapolar las dificultades que encontraron, por ejemplo, los europeos para colonizar el Nuevo Mundo, con las que enfrentarán los exploradores en a Marte. Así, si Magallanes y Elcano se enfrentaron con un inmenso océano desconocido al dar la vuelta al extremo sur del continente americano, lo mismo que con el escorbuto por la dieta deficiente que fueron forzados a llevar, esto será nada comparado con lo que enfrentarán los futuros exploradores de Marte, pues, entre otras cosas, no tendrán aire para respirar. Colonizar Marte u otras regiones del espacio es extremadamente difícil por una razón muy simple: los colonos enfrentarían un medio ambiente marcadamente diferente del medio terrestre en el que evolucionaron como especie. Así, por ejemplo, en la Tierra evolucionamos bajo la influencia de una fuerza fija de gravedad, mientras que en el espacio esta fuerza no existe. Los colonizadores europeos, enfrentaron condiciones ambientales difíciles, pero siempre dentro del mundo en el que habían evolucionado.  En un artículo aparecido esta semana en la revista “Frontiers in Astronomy and Space Sciences”, se analizan las dificultades a vencer para reproducir en el espacio las condiciones ambientales de la Tierra, como un requisito indispensable para la colonización del espacio. El artículo fue publicado por Lee Irons y Morgan Irons, de Norfolk Institute y Cornell University, respectivamente.Los investigadores consideran en primer lugar a la gravedad, que crea diferencias en la presión de los fluidos corporales a lo largo del cuerpo y en función de la altura -de manera similar a como la presión en una alberca se incrementa con la profundidad. En la ausencia de gravedad, la diferencia de presiones desaparece y con esto resulta afectada la fisiología del cuerpo que está sintonizada con dicha diferencia. En el espacio, es posible generar una fuerza de gravedad mediante una estructura rotante de grandes dimensiones -como la que aparece en la película “2001 Odisea del Espacio”. Como mencionan los autores, sin embargo, si bien esto simularía la fuerza de gravedad, no podría producir una diferencia de presiones en los fluidos corporales. Para esto, el aire en el interior de la estructura tendría también que rotar, y para esto se necesitaría una provisión continua de energía. Un segundo factor es la atmósfera de oxígeno que en la Tierra es mantenida de manera natural y autosustentable. En el espacio, dicha atmósfera tendría que ser mantenida artificialmente, lo que implica un gasto de energía. Además, si ocurriera una falla no habría un mecanismo natural que la restaurara y la colonia colapsaría. Por lo demás, el tamaño y la complejidad de la colonia dependería de la cantidad de energía que estuviese a su disposición, y en la medida en que dicha colonia estuviese más alejada del Sol la energía sería más escasa. En este respecto, como comentan los autores, si la civilización actual fuera relocalizada en un lugar en el espacio en donde hubiera un menor acceso a la energía, la civilización humana tendrá un declive hasta llegar a un estado compatible con la energía disponible. Así, de manera incontrolable, las cadenas de aprovisionamiento desaparecerán, los recursos se agotarán, los sistemas sociales y de gobierno desfallecerá o colapsarán, la población disminuirá, lo mismo que la diversidad genética, y el conocimiento humano será olvidado.Así, enviar una misión tripulada a Marte con un puñado de viajeros será sin duda una realidad en un futuro cercano, si bien no previsible. En la visión de los autores, por lo contrario, establecer una colonia permanente autosustentable será bastante más difícil de lograr.",
    "Nos enteramos por los medios de comunicación que el pasado 24 de marzo murió Gordon Moore a los 94 años. A pesar de que para una mayoría de personas su nombre no les resulte familiar, Gordon Moore tuvo, y sigue teniendo, una enorme influencia sobre la vida de todos nosotros. En efecto, Moore es reconocido como uno de los principales actores en la historia de los chips semiconductores, los cuales han posibilitado el desarrollo de las computadoras tal como las conocemos y de una miríada de artilugios, incluyendo los teléfonos móviles y la red Internet, que tanto han cambiado nuestro estilo de vida. La historia del desarrollo de los chips semiconductores es fascinante y en sus primeras etapas tiene muchos episodios coloridos, particularmente aquellos en los que participó William Schockley, uno de los inventores del transistor a finales de la década de los años cuarenta. Schockley en algún momento fue jefe de Gordon Moore cuando este último iniciaba su carrera, y si bien ambos fueron actores de primera línea en el desarrollo de los chips, en muchos aspectos diferían marcadamente uno de otro. Pero vayamos al principio de la historia.El origen de los chips semiconductores se remonta al mes de diciembre de 1947, cuando Walter Brattain y John Bardeen demostraron la operación del primer transistor en los laboratorios Bell de la compañía ATT en Nueva Jersey. Shockley, que en esos momentos era el jefe de Brattain y Bardeen, no participó directamente en el descubrimiento, lo que lo molestó profundamente. En su enojo trató de escamotear el mérito de Bardeen y Brattain, lo que era innecesario, pues Schockley era un físico extremadamente brillante, con grandes méritos que fueron clave para el desarrollo posterior del transistor.  A la vez que brillante Schockley era una persona muy compleja y con su actitud provocó la salida de Bardeen y Brattain de los Laboratorios Bell. Con el tiempo, Schockley mismo dejó la compañía y enfiló hacia la costa oeste de los Estados Unidos para fundar la empresa Shockley Semiconductor Laboratory al sur de San Francisco, con el propósito de fabricar transistores de silicio. Con esta acción, Schockley inauguró lo que posteriormente sería conocido como el Valle del Silicio. Una vez establecido en California, Schockley se dio a la tarea de reclutar a investigadores de primer orden para llevar a cabo su proyecto. Uno de éstos fue Gordon Moore, a quien le fue encargado el desarrollo de silicio ultrapuro para la fabricación de transistores. Pronto, sin embargo, la compleja personalidad de Schockley hizo crisis y llevó a la salida de ocho de sus investigadores más brillantes, Gordon Moore incluido. Schockley consideró esta salida como una traición y bautizó a los investigadores que renunciaron como los “ocho traidores”.Los “ocho traidores” dejaron la compañía de Schockley con un propósito bien definido: fundar la compañía Fairchild Semiconductors, dedicada a la fabricación de transistores y de chips semiconductores, en lo que se integraban un cierto número de transistores en una sola pastilla de silicio. Pronto, esta compañía floreció y ganó dinero vendiendo chips, mientras que la compañía de Schockley terminó por desaparecer. Años más tarde, Gordon Moore y Rober Noyce -otro de los “traidores”- dejaron Fairchild para fundar la compañía Intel Corporation, que se dedicó a fabricar chips semiconductores para computadoras y otras aplicaciones. Moore primeramente fungió como vicepresidente de Intel y posteriormente como presidente, director ejecutivo y, finalmente, presidente del consejo de directores. Moore, no obstante, es sobre todo conocido por la llamada ley de Moore enunciada por Moore en 1965 y reformulada en 1975. la cual predice que el poder de cómputo de los chips semiconductores se duplicará cada dos años. Si bien la ley de Moore no es una ley física, sirvió a lo largo de los años para guiar el desarrollo de las compañías electrónicas. En particular, llevó a Intel a convertirse en una de las mayores compañías fabricantes de chips semiconductores del mundo.Moore, con su visión empresarial, se convirtió en un multimillonario que donó 600 millones de dólares al Instituto Tecnológico de California y estableció una fundación altruista dotada con 5,000 millones de dólares para impulsar proyectos científicos y de conservación del medio ambiente. Shockley, por su parte, recibió el premio Nobel de Física, juntamente con Bardeen y Brattain, por su descubrimiento del transistor. Pero nunca se convirtió en el millonario que pretendía ser.",
    "Un día como hoy, hace 196 años, murió en Viena Ludwig van Beethoven a la edad de 56 años. Como mito cultural que es, su muerte ha sido dramatizada desde un inicio. Así, según el testimonio del compositor austriaco Anselm Huttenbrenner, amigo de Beethoven y quien lo acompañó en sus últimos momentos: “Después de que Beethoven yaciera inconsciente en los estertores de la muerte desde las tres de la tarde hasta pasadas las cinco del 25 de marzo de 1827, vino un relámpago acompañado por un violento trueno, que iluminó llamativamente la cámara mortuoria (la nieve estaba frente a la vivienda de Beethoven). Después de este inesperado fenómeno de la naturaleza, que me sobresaltó mucho, Beethoven abrió los ojos, levantó la mano derecha y miró hacia arriba durante varios segundos con el puño cerrado y una expresión muy seria y amenazadora como si quisiera decir: “¡Potencias enemigas, os desafío! ¡Lejos contigo! ¡Dios está conmigo!\". Momentos después Beethoven había muerto: “¡Ni un respiro más, ni un latido más! ¡El genio del gran maestro de los tonos huyó de este mundo de engaño al reino de la verdad!”, remata Huttenbrenner.Igualmente, por su condición de mito cultural, mucho se ha especulado sobre las causas de la muerte de Beethoven. Sabemos que no gozaba de buena salud y que padeció de una sordera progresiva que se inició en su juventud, lo mismo que de problemas digestivos recurrentes y de una enfermedad hepática. También, se piensa que pudo haber sufrido de envenenamiento con plomo por beber el vino del Rin al que era adicto, más allá de lo que hoy se consideraría prudente. En este sentido, habría que recordar que se añadía plomo al vino para darle un sabor dulce. Si bien doscientos años después no es fácil establecer con precisión las causas de la muerte de una persona, un artículo aparecido esta semana en la revista Current Biology intenta arrojar luz sobre la muerte de Beethoven a partir del estudio genético de restos de cabello que le habrían pertenecido. El artículo fue publicado por un grupo de investigadores encabezado por Tristan Begg de la Universidad de Cambridge en el Reino Unido.En su investigación, Begg y colaboradores tuvieron a su disposición ocho muestras de cabello que son atribuidas a Beethoven. Como primer paso, los investigadores llevaron a cabo un estudio genético para determinar la autenticidad de dichas muestras. Concluyeron que, efectivamente, cinco de ellas son auténticas, mientras que dos definitivamente no lo son. Una última muestra no arrojó suficiente evidencia para concluir en un sentido o en otro. Así, los investigadores tuvieron a su disposición cinco muestras de cabello de Beethoven para determinar si sus problemas de salud tuvieron un origen genético.Desafortunadamente, los investigadores no pudieron determinar si su sordera y sus problemas digestivos tuvieron un origen genético. Concluyeron, en cambio, que su muerte probablemente ocurrió por una enfermedad hepática debida a una predisposición genética, agravada por una combinación de una hepatitis B que sufrió en los últimos meses antes de su muerte, y de un consumo excesivo de alcohol.Begg y colaboradores llevaron también a cabo una investigación genética con personas vivas con el mismo apellido de Beethoven y que, basados en los registros genealógicos, tienen un ancestro común. En este caso, los investigadores enfocaron su estudio en característics genéticas que se heredan solamente por la vía paterna, de padre a hijo, como sucede con el apellido paterno. Para su sorpresa, encontraron que no había las coincidencias genéticas esperadas entre Beethoven y sus descendientes vivos con el mismo apellido. Al mismo tiempo, éstos últimos coinciden genéticamente con un Aert van Beethoven del siglo XVI, del que sería también descendiente Ludwig van Beethoven según los registros genealógicos. Para explicar estas contradicciones, Begg y colaboradores arguyen que en algún momento entre la generación de Aert van Beethoven y la de Ludwig van Beethoven, se dio un episodio extramarital en la que el padre biológico del hijo no correspondió al padre que aparece en el registro genealógico.La investigación de Begg y colaboradores no solamente demuestra el poder de los métodos de la genética para averiguar las posibles enfermedades que llevaron a la muerte a una persona hace doscientos años y cuyos restos nos han llegado a través del tiempo. También muestra sus poderes detectivescos para poner al descubierto hechos que en su momento los involucrados hubieran preferido mantener en secreto. Por lo demás, todo esto es de interés solo en el caso de personas realmente excepcionales.",
    "Cuando escuchamos la palabra tecnología es posible que nos vengan a la mente compañías como Google o aplicaciones como WhatsApp. Menos frecuentemente pensaríamos en chips semiconductores o en compañías como Intel o TSMC, a pesar de que éstas son dos de los mayores fabricantes de los chips semiconductores sin los cuales no podrían existir ni Google ni las redes sociales. Habría que señalar, no obstante, que a raíz de la pandemia se ha dado una escasez de chips semiconductores -referidos en la prensa simplemente como semiconductores- que los ha sacado de su oscuridad relativa y ha hecho patente el papel fundamental que juegan en la actualidad. En efecto, como lo informaron los medios de comunicación, la industria automotriz sufrió en años pasados una escasez de semiconductores debido a que, durante la pandemia, los fabricantes de chips para la industria automotriz redirigieron su producción hacia chips semiconductores para computadoras y tabletas, cuya demanda creció con el aislamiento. Esto incluso obligó a paros en la producción de automóviles.    Los chips semiconductores o microcircuitos consisten en pastillas planas de silicio en las que se integran monolíticamente miles de millones de transistores en un área del tamaño de la uña de un dedo. El punto de partida para la fabricación de los chips es una oblea de silicio del tamaño de una pizza, de la cual se obtendrán un gran número de microcircuitos, con miles de millones de elementos cada uno de ellos. Para la fabricación de los chips se graban todos sus elementos electrónicos, incluyendo sus interconexiones eléctricas, empleando luz ultravioleta, lo que implica una dificultad extrema, dada la pequeñez de los elementos a grabar. Hecho esto, se corta la oblea en pastillas, cada una de ellas conteniendo un microcircuito. Como paso final se encapsulan los microcircuitos y se prueba su funcionamiento.   Como es fácil entender, para fabricar un microcircuito se emplean tecnologías que se cuentan entre las más complejas que existen, y que incluyen tanto la fabricación del chip propiamente dicha, como la fabricación del equipo necesario para llevarla a cabo. Con relación a esto último, habría que señalar que solamente hay un fabricante en el mundo que provee las máquinas de luz ultravioleta para grabar los microcircuitos. De la misma manera, son muy pocas las fábricas a nivel mundial para los chips más avanzados, entre las que se encuentran la norteamericana Intel, la coreana Samsung, y la que es con mucho la más grande de todas, la taiwanesa TSMC.En estas circunstancias, dadas las crecientes aplicaciones de los chips, la capacidad para fabricar semiconductores se ha convertido en un asunto estratégico. Así, los Estados Unidos están preocupados porque China pueda desarrollar la capacidad para fabricar los chips semiconductores más avanzados. En estas circunstancias, en julio de 2022 el congreso de los Estados Unidos aprobó la “Chips and Science Act”, dotada con 280,000 millones de dólares para fortalecer la innovación en ciencia y tecnología de semiconductores en ese país.De la misma manera, esta semana nos enteramos de los planes de Samsung para establecer en Corea del Sur el centro de fabricación de chips semiconductores más grande del mundo, mediante la inversión en los próximos 20 años de 230,000 millones de dólares. Esto consolidará la posición de Samsung y Corea del Sur como uno de los grandes fabricantes de chips en el mundo.A los chips semiconductores los encontramos en computadoras, teléfonos celulares, consolas de juegos, televisiones, automóviles, equipos médicos, sistemas de comunicación, cajeros bancarios, hornos de microondas, refrigeradores y en general en accesorios eléctricos domésticos. Y en lo que es uno de sus mayores impactos, al permitir el desarrollo de la red Internet, los chips semiconductores han cambiado drásticamente nuestra forma de vida.Al mismo tiempo, los chips semiconductores, al igual que otras tecnologías de sofisticación creciente, están aumentando la brecha tecnológica entre los pocos países que poseen su tecnología y aquellos que somos simplemente sus usuarios. Y con las inversiones de cientos de miles de millones de dólares que esos pocos países están haciendo para aumentar todavía más la complejidad de los microcircuitos, no parecería que la situación se vaya a revertir en el futuro.",
    "El 27 de agosto de 1859, Edwing Drake terminó la perforación de lo que sería el primer pozo de petróleo explotado comercialmente en los Estados Unidos, cerca del pueblo de Titusville en el estado de Pensilvania. Drake había sido enviado ahí, una zona en donde el petróleo brotaba a flor de tierra, por la compañía Seneca Oil Company para explorar el potencial de dicha zona para la producción de petróleo en gran escala. El éxito de Drake marcó el inicio de la expansión del uso del petróleo, inicialmente como combustible para lámparas de iluminación y después en las industrias eléctrica y de los automóviles.En la actualidad, a un siglo y medio de la revolución en el campo de los energéticos iniciada por Edwing Drake, algunos especialistas en el tema apuntan a la posibilidad de que pudiera ocurrir una revolución similar, esta vez con el hidrógeno como protagonista. Como sabemos, el hidrógeno es un gas que puede ser usado como combustible en sustitución del petróleo y el gas natural,  con la ventaja de que la combustión del hidrógeno produce solamente agua y no contribuye a la emisión de gases de invernadero a la atmósfera. Así, la revolución energética que nos traería el hidrógeno contribuiría a mitigar el cambio climático que está experimentando el planeta.El hidrógeno puede ser fabricado descomponiendo agua en hidrógeno y oxígeno empleando energía eléctrica. Si esta energía se obtiene a partir de una fuente limpia -por ejemplo, el Sol- al hidrógeno obtenido se denomina “verde”. Si, por el contrario, para producir la energía eléctrica necesaria para descomponer al agua se emplean combustibles fósiles, el hidrógeno obtenido se adjetiva con colores que pueden llegar hasta el negro, dependiendo del grado de contaminación atmosférica que genera su producción. Claramente, el hidrógeno verde es el que tendría un mayor impacto en la mitigación del cambio climático. El hidrógeno verde, sin embargo, es caro de fabricar y, de hecho, la producción mundial de hidrógeno se lleva a cabo casi en su totalidad empleando combustibles fósiles. Esto último podría ser superado si existieran depósitos naturales de hidrógeno de modo tal que no tuviéramos que fabricarlo sino solamente tomarlo de donde se encuentre, y en este sentido son relevantes las opiniones de Viacheslav Zgonnik de la compañía Natural Hydrogen Energy LLC expresadas en un artículo publicado en febrero de 2020 en la revista Earth-Science Reviews.De acuerdo con Zgonnik, existen depósitos de hidrógeno natural en el subsuelo, al igual que existen depósitos de combustibles fósiles. Pero, si este es el caso ¿por qué han pasado desapercibidas hasta ahora? Después de todo, la tierra ha sido perforada en incontables ocasiones en busca de petróleo desde hace 150 años. Zgonnik nos ofrece algunas respuestas. Señala que el hidrógeno tiende a situarse en estratos geológicos diferentes a los estratos en los que se encuentra el petróleo. Además, siendo el hidrógeno el gas más ligero que existe se disipa rápidamente en la atmósfera al alcanzar la superficie dificultando su detección. Igualmente, por su pequeño tamaño, el hidrógeno se filtra a través de las rocas y es difícil confinarlo en una trampa geológica por tiempos largos.Pero, sobre todo, Zgonnik sostiene que la invisibilidad del hidrógeno ha sido debida a prejuicios de los especialistas que no esperan encontrar hidrógeno y por tanto no lo buscan de manera explícita.De un modo u otro, la presencia de hidrógeno ha sido confirmado en un yacimiento en Mali y con esto el hidrógeno natural ha cobrado interés en la comunidad científica que busca  evaluar su potencial para generar hidrógeno verde. En este sentido, una presentación de Geoffrey Ellis y Sarah Gelman del US Geological Survey en un congreso de The Geological Society of America el pasado mes de octubre concluye que el hidrógeno natural muy probablemente podrá cubrir cuando menos el 50 por ciento de la producción de hidrógeno verde en el año 2100. De confirmarse el potencial del hidrógeno natural para satisfacer nuestras necesidades de energía se contaría con un elemento para contrarrestar el cambio climático, 150 años después que una revolución energética contribuyó sensiblemente a dispararlo. Tendríamos así dos revoluciones energéticas, una contra la otra.",
    "El terremoto de magnitud 7.8 que golpeó a Turquía de pasado 6 de febrero, y que se ha convertido en el más mortífero en la historia de ese país, nos hace preguntarnos sobre la posibilidad de predecir un terremoto con la suficiente antelación para que la población que vive en zonas sísmicas pueda ponerse a salvo. Desafortunadamente, como lo expresa un editorial de la revista Nature Computational Science del pasado 17 de febrero, las fuerzas que desencadenan los terremotos son muy complejas y no se cuenta todavía con el suficiente conocimiento científico para llevar a cabo esta predicción.Sabemos que los terremotos se producen en la frontera entre dos placas tectónicas que se deslizan una contra la otra. El sismo del 19 de septiembre de 1985 en México, por ejemplo, fue debido al movimiento de la placa tectónica de Cocos en el océano Pacífico que se desliza por debajo de la placa continental de Norteamérica. De la misma manera, el terremoto que devastó la ciudad de San Francisco, California, en 1906 fue debido al deslizamiento horizontal de las placas de Norteamérica y del Pacífico, que mueven una contra la otra a lo largo de la falla de San Andrés.El movimiento relativo de dos placas tectónicas en contacto, por otro lado, es resistido por las fuerzas de fricción entre dichas placas que intentan impedir su deslizamiento, de la misma manera que al empujar una caja pesada para intentar deslizarla a lo largo del piso, las fuerzas de fricción entre el piso y la superficie de la caja en contacto con el piso se oponen a nuestros esfuerzos. Una vez que la fuerza aplicada alcance un valor mínimo, sin embargo, la caja se pondrá en movimiento. Al igual que lo harán las placas tectónicas una vez que se supere la fuerza de fricción y se llegue a un punto de ruptura. Se liberará así la energía acumulada, generando ondas sísmicas cuya potencia dependerá de la magnitud de las fuerzas de fricción involucradas.Se tiene así que la posibilidad de que se genere un terremoto de gran magnitud depende de las fuerzas de fricción entre las placas. Si estas son grandes, se acumulará energía a lo largo de un periodo más o menos largo, misma que podrá liberarse generando un terremoto de grandes proporciones. Sí, por otro lado, las fuerzas de fricción fueran pequeñas, las rupturas se producirán en periodos de tiempo más cortos, sin acumular demasiada energía y sin producir terremotos devastadores. Esto último es el tópico de un artículo aparecido el pasado 17 de febrero en la revista Science, el cual fue publicado por un grupo de investigadores encabezado por Srisharan Shreedharan de la Universidad de Texas en Austin. En su artículo, Srisharan y colaboradores reportan los resultados de un estudio que los llevó a descubrir que después de ocurrir un terremoto las placas se unen nuevamente después de un cierto tiempo recuperándose las fuerzas de fricción -como se recuperan, en cierto grado, las fuerzas de fricción entre el piso y la caja de nuestro ejemplo una vez que se pone en movimiento. El tiempo de recuperación de las fuerzas de fricción entre placas tectónicas es crucial, pues si este  es largo, los esfuerzos que se acumulen por el posterior deslizamiento de placas tenderán a aliviarse en periodos más cortos sin generación de grandes terremotos, los cuales tendrán más probabilidades de ocurrir si las fuerzas de fricción se recuperan rápidamente después  de la ruptura. Como parte de sus investigaciones, Srisharan y colaboradores llevaron a cabo un estudio de laboratorio con materiales extraídos de la falla geológica situada 800 metros por debajo del nivel del mar en la costa de Nueva Zelanda, encontrando tiempos muy largos para la recuperación de la fricción. A partir de sus resultados pudieron predecir por medio de un modelo de computadora que las tensiones acumuladas en la falla geológica se aliviaban en tiempos cortos, sin generación de terremotos. Esto coincide con la historia de la falla neozelandesa, que se sabe produce movimientos lentos de placas sin generación de ondas sísmicas de magnitud apreciable. Concluyen Srisharan y colaboradores que, si bien sus resultados no llevarán a un método para predecir terremotos, si proporcionan nueva información científica acerca de los mecanismos que generan los terremotos.Por lo demás, en tanto desarrollamos capacidad para predecir terremotos, podemos reducir el número de personas que fallecen en uno de estos eventos tomado las medidas pertinentes. Y en este sentido, los más de 50,000 muertos en el terremoto de Turquía resultan ciertamente demasiados.",
    "Al desembocar al océano Pacífico después de dar la vuelta al extremo sur del continente americano, la expedición de Fernando de Magallanes se encontró con un océano cuya inmensidad no esperaba. Así, sin prever la enorme distancia que habría de recorrer antes de tomar tierra, Magallanes no se hizo de las suficientes provisiones de comida y sufrió las consecuencias. En palabras del italiano Antonio Pigafetta, quién sirvió como cronista de la expedición, “Navegamos por espacio de tres meses y veinte días sin probar ni un alimento fresco. El bizcocho que comíamos ya no era pan, sino un polvo mezclado de gusanos que habían devorado toda su sustancia, y que además tenía un hedor insoportable por hallarse impregnado de orines de rata. El agua que nos veíamos obligados a beber estaba igualmente podrida y hedionda. Para no morirnos de hambre nos vimos aun obligados a comer pedazos de cuero de vaca y a menudo estábamos reducidos a alimentarnos de aserrín y hasta de ratas”. Prosigue Pigafetta: “Sin embargo, esto no era todo. Nuestra mayor desgracia era vernos atacados de una especie de enfermedad que hacía hincharse las encías hasta el extremo de sobrepasar los dientes en ambas mandíbulas, haciendo que los enfermos no pudiesen tomar ningún alimento. De éstos murieron diecinueve. Además de los muertos, teníamos veinticinco marineros enfermos que sufrían dolores en los brazos, en las piernas y en algunas otras partes del cuerpo, pero que al fin sanaron”. Hoy sabemos que los marineros de la expedición sufrían de escorbuto, que es una enfermedad debida a una deficiencia en la ingesta de vitamina C, y que sus desventuras se debieron en último término a una alimentación que no incluía frutas frescas y verduras. Los expedicionarios, sin embargo, no pudieron saberlo, pues fue solo hasta el siglo XVIII que el médico escocés James Lind descubrió que los enfermos de escorbuto mejoraban con la ingesta de naranjas y limones. Así, la expedición de Magallanes, por ignorancia y circunstancias del viaje, estuvo sujeta a una alimentación a la que nuestro cuerpo no está adaptado.   Este último comentario vale con respecto a la exploración espacial, que ha tomado nuevos bríos y que tiene en la mira la realización de misiones tripuladas que expondrán a los astronautas a las condiciones del espacio por tiempos prolongados a las que nuestro cuerpo tampoco está adaptado. En particular, los exploradores espaciales estarán sujetos a condiciones de microgravedad, es decir, de ausencia de peso, que son extrañas a los habitantes de la Tierra que han evolucionado en un medio ambiente con gravedad.¿Cómo afecta al cuerpo la exposición prolongada a la microgravedad? Un artículo aparecido esta semana en la revista Small intenta arrojar luz al respecto, en particular, sobre cómo la microgravedad afecta las conexiones de cerebro. El artículo fue publicado por un grupo de investigadores encabezado por Steven Jillings de la Universidad de Amberes, en Bélgica. Jillings y colaboradores llevaron a cabo una investigación con 14 astronautas, que fueron sujetos a estudios para determinar si sus conexiones cerebrales fueron modificadas después de una estancia de seis meses en la estación espacial internacional. Para este propósito, los astronautas fueron sujetos a tres estudios de resonancia magnética, antes de la misión, inmediatamente después de finalizada, y ocho meses después de esta finalización. Encuentran los investigadores que hay algunos cambios en la organización del cerebro inmediatamente después del término de la misión, y que después de ocho meses algunos de ellos permanecen, mientras que otros desaparecen, regresando el cerebro a su estado original. Interpretan los investigadores que los cambios no permanentes corresponden a adaptaciones del cerebro a las condiciones de microgravedad -por ejemplo, a la pérdida de la sensación arriba-abajo- que se revierten una vez de regreso a la Tierra. Los cambios permanentes, por otro lado, corresponderían a un proceso de aprendizaje del cerebro durante el tiempo que estuvo sometido a nuevas condiciones ambientales. Quinientos años atrás, Magallanes y su tripulación pudieron haber superado el escorbuto consumiendo frutas frescas y verduras. Para esto, sin embargo, tendrían que haber sabido algo que solo se supo más de dos siglos después, además de haber contado con refrigeradores, todavía más lejanos en el tiempo. Por su lado, los exploradores espaciales enfrentarán condiciones mucho más severas que aquellas que enfrentaron los exploradores del pasado. Ahora, no obstante, sabemos mucho más. Por ejemplo, que el cerebro se modifica en el espacio. Además de contar con la capacidad de construir máquinas mucho más sofisticadas.",
    "Sabemos que el cambio climático que está sufriendo el planeta es causado por la quema de combustibles fósiles. Así, la vía obvia para resolver el problema es dejar paulatinamente de consumir dichos combustibles. Esto, desafortunadamente, ha sido más fácil decirlo que llevarlo a cabo y el abandono de los combustibles fósiles no se está dando a la velocidad necesaria. En estas circunstancias, se han hecho varias propuestas para disminuir la cantidad de energía solar que es absorbida por la Tierra y mitigar así el incremento en su temperatura. Según sus proponentes, esto por sí solo no resolvería el problema, pero contribuiría a lograrlo juntamente con otras medidas.La idea es muy simple: colocar una especie de sombrilla que atenúe la radiación solar que recibe el planeta. Sobre cómo llevar esto a cabo las propuestas han sido variadas, desde dispersar gases en la estratósfera que reflejen la radiación solar, hasta colocar una pantalla entre la Tierra y el Sol que obstaculice el paso de los rayos solares. En un artículo aparecido esta semana en la revista Plos Climate se ofrece una propuesta más. El artículo fue publicado por un equipo de investigadores encabezado por Benjamin Bromley de la Universidad de Utah en los Estados Unidos.La propuesta de Bromley y colaboradores consiste en colocar una nube de polvo en el llamado punto L1 de Lagrange, localizado a aproximadamente 1.5 millones de kilómetros de la Tierra a lo largo de la línea que une al Sol y a la Tierra. En este punto, las fuerzas de atracción de la Tierra y el Sol sobre un objeto se combinan para hacer que la velocidad de translación de dicho objeto alrededor del Sol sea igual a la velocidad de translación de nuestro planeta. De este modo, la nube de polvo estaría colocada permanentemente entre la Tierra y el Sol.  Como discuten Bromley y colaboradores, para atenuar un 1.8 por ciento de la radiación solar la nube deberá contener más de diez millones de toneladas de polvo. Anticipan, además, que la nube tenderá a esparcirse -entre otras cosas, por el choque con los rayos del Sol- por lo que tendrá que ser continuamente recompuesta. ¿De dónde se tomaría el polvo necesario para esto? Los autores del artículo proponen que sea de la Luna. Argumentan que llevarlo desde la Luna implicaría un menor gasto de energía en comparación con llevarlo desde la Tierra, dada su menor fuerza de gravedad. Además, Bromley y colaboradores encuentran que los granos de polvo lunar tienen el tamaño adecuado para dispersar la radiación del Sol. Así, para mantener en forma el escudo solar, el polvo lunar sería periódicamente lanzado desde la superficie de la Luna hasta el punto de Lagrange La propuesta de Bromley y colaboradores, sin embargo, se clasifica dentro del campo de la geoingeniería solar que tiene muchos críticos. En este sentido, una carta abierta lanzada en enero de 2020 por 60 expertos del clima de todo el mundo y actualmente respaldada por más de 570 científicos, hace un llamado para la firma de un acuerdo para no usar la geoingeniería solar. En el encabezado de dicha carta podemos leer: “Hacemos un llamado a Gobiernos, Naciones Unidas y otros actores a una acción política inmediata para prevenir la normalización de la geoingeniería solar como opción en política climática. Los gobiernos y las Naciones Unidas deben velar por un control político eficaz, restringiendo el desarrollo de las tecnologías de geoingeniería solar a escala global. Específicamente, llamamos a un Acuerdo Internacional de No Uso de la Geoingeniería Solar”.Los firmantes aducen en su carta abierta que los intentos de cambiar la temperatura del planeta provocarían cambios en el clima global que los científicos con el conocimiento actual no pueden anticipar. Los impactos “variarán según las regiones, y existen incertidumbres sobre los efectos en los patrones climáticos, la agricultura y la provisión de las necesidades básicas de alimentos y agua”. Apuntan también que la posibilidad de mitigar el cambio climático por medio de la geoingeniería solar podría ser usado como un argumento para retrasar las políticas de reducción de los combustibles fósiles.Así, el intento por cambiar artificialmente el clima del planeta podría resultar en un remedio peor que la enfermedad. Al margen de que las propuestas para llevar esto a cabo -involucrando incluso mundos fuera del nuestro- estén en plena floración.",
    "¿Cuál es la relación que existe entre el ajedrez y la contaminación del aire? A primera vista no pareciera que hubiera alguna. A menos que, por ejemplo, consideráramos que quizá no es recomendable jugar una partida de ajedrez en algún jardín cuando haya un episodio severo de contaminación atmosférica. Un artículo publicado en línea el pasado 26 de enero en la revista “Management Science”, sin embargo, encuentra que sí hay una relación, más permanente que contingente, entre el ajedrez y la contaminación. Dicho artículo fue publicado por un grupo de investigadores encabezado por Steffen Khunn de la Universidad de Maastrichst en los Países Bajos. En su artículo, Khunn y colaboradores reportan los resultados de un proyecto de investigación llevado a cabo para averiguar en qué medida la contaminación del aire en un espacio interior afecta el desempeño de un jugador profesional de ajedrez. Se sabe que dicha contaminación afecta negativamente las habilidades cognitivas de una persona y que el ajedrez es un ejercicio en el que los jugadores despliegan precisamente dichas habilidades. Así, los investigadores esperaban encontrar una relación entre la contaminación del aire y la decisión estratégica que un jugador de ajedrez tomara al realizar un movimiento para ganar la partida jugadas más adelante.En su investigación, Khunn y colaboradores analizaron más de 30,000 movimientos que realizaron 121 jugadores en 607 juegos, llevados a cabo en tres torneos oficiales de ajedrez en Alemania en los años 2017-2019. Los torneos tuvieron una duración de 8 semanas, lo que dio suficiente tiempo para evaluar el desempeño de los jugadores en condiciones cambiantes de contaminación del aire.Los investigadores centraron su estudio en la contaminación por partículas con tamaños menores a 2.5 micrómetros. Estas partículas, que se consideran son el mayor riego medioambiental para la salud humana, se generan por fuentes múltiples, incluyendo los motores de combustión interna, los fuegos forestales y de desechos agrícolas, y la quema ineficiente de combustibles. Durante su estudio Khunn y colaboradores instalaron un detector de partículas contaminantes para medir su concentración en el recinto en el que se llevó a cabo el torneo y evaluaron el desempeño de los jugadores bajo diferentes concentraciones de contaminantes. Los movimientos de ajedrez de los jugadores fueron analizados con un algoritmo de inteligencia artificial que determinó que tan cercanos o lejanos estaban de un movimiento óptimo. Como resultado, los investigadores reportan: “En general, nuestros hallazgos muestran que las concentraciones en espacios interiores de partículas contaminantes empeoran significativamente la capacidad de los sujetos para seleccionar el movimiento óptimo”. Así, un incremento relativamente pequeño en la concentración de partículas incrementa en más de 25 por ciento la probabilidad de que un jugador cometa un error. Por otro lado, habría que recordar que los jugadores en un torneo juegan con un límite de tiempo, que los obliga a hacer los primeros 40 movimientos en un tiempo fijo. De no hacerlo pierden la partida, lo que los pone bajo presión en la medida en que se acerca el límite de tiempo. En su estudio, los investigadores encuentran que la medida en que se acerca este límite, el efecto de la contaminación sobre el desempeño de los jugadores se amplifica. Así, la contaminación tiene una influencia negativa sobre la toma de decisiones de un jugador de ajedrez, sobre todo si está bajo presión.     Dicho lo anterior, habría que señalar que el interés de Kuhnn y colaboradores no es por el ajedrez por sí mismo. Lejos de esto, a través del estudio de las decisiones que tomaron los jugadores de ajedrez bajo condiciones cambiantes de contaminación del aire, buscaron determinar en qué medida dicha contaminación afecta las habilidades cognitivas de un trabajador en una empresa, y por tanto su proceso de toma de decisiones estratégicas, que es de importancia central para la misma. Según Khunn y colaboradores, sus resultados “Resaltan los beneficios de invertir en infraestructura para proteger a los trabajadores contra los peligros externos y para mejorar la calidad del aire el interior de los edificios”.Resultaría así que la contaminación atmosférica, más allá impulsar un cambio climático y de afectar nuestra salud, podría también aturdirnos y entorpecer nuestra capacidad para tomar decisiones, sobre todo si estamos bajo presión.",
    "La pandemia de Covid nos ha recordado algo que en cierto modo nuestra generación había olvidado: que existen patógenos letales que de cuando en vez producen epidemias de grandes proporciones que nos pueden poner en peligro de muerte. Ciertamente, sabíamos de epidemias y de enfermedades mortales como el ébola. Éstas, sin embargo, no eran percibidas como un peligro generalizado, como sí lo ha sido el Covid.  Afortunadamente, el Covid hizo su aparición hasta el siglo XXI, cuando el conocimiento científico sobre el origen de la enfermedad ha hecho posible la generación de vacunas en un tiempo récord y ha atenuado drásticamente su impacto en la salud de la población.En el pasado nuestros ancestros no tuvieron la misma suerte, y en este sentido dos de las tres epidemias de peste bubónica que se han dado en los últimos dos mil años constituyen el ejemplo más mortífero. La primera de estas epidemias ocurrió en los siglos VI-VIII d.C. en el Imperio Romano de Oriente y es conocida como la plaga de Justiniano, y que podría haber matado hasta a un 25 por ciento de la población. La segunda epidemia se desarrolló en los siglos XIV-XIX y se inició con la llamada muerte negra, durante la cual habría fallecido la mitad de la población de Europa. Una tercera epidemia de peste bubónica, actualmente en curso, se ha extendido lo largo de los últimos tres siglos. En su novela histórica “Diario del año de la peste”, el escritor Daniel Defoe relata el clima que vivió Londres en el año 1665 durante la epidemia de peste bubónica que azotó a la ciudad y que habría matado a un cuarto de su población. Defoe no vivió estos acontecimientos personalmente, pero se asume que supo de ellos por un pariente, que sí los presenció. La novela de Defoe constituye así un recuento creíble de los infortunios de los londinenses.   La plaga de Londres, por otro lado, jugó un papel inesperado y crucial en el desarrollo de la física y de la ciencia en general. En efecto, como parte del confinamiento social decretado por el rey inglés para mitigar la propagación de la enfermedad, la Universidad de Cambridge tuvo que cerrar sus instalaciones. Esto obligó a Isaac Newton -el físico más importante de la historia y a la sazón en la Universidad de Cambridge- a mudarse por dos años a Woolsthorpe, su lugar de nacimiento. Ahí, en aislamiento forzado, Newton descubrió la ley de gravitación universal, que tuvo un impacto profundo en el desarrollo del método científico.Siendo la bacteria “Yersinia pestis”, el patógeno causante de la peste bubónica, la responsable de las dos pandemias más mortíferas de la historia, se han llevado a cabo muchos esfuerzos para entender su evolución desde su primera aparición hace 5,000 años. Desafortunadamente, como se documenta en un artículo aparecido la pasada semana en la revista “Communications Biology”, la empresa no ha sido fácil. El artículo fue publicado por un grupo de investigadores encabezado por Katherine Eaton de la Universidad McMaster, en Canadá.En su investigación, Eaton y colaboradores llevaron a cabo un análisis de 600 secuencias genómicas de Yersinia pestis recogidas a lo largo de todo el mundo. Las muestras estudiadas corresponden, tanto a las primeras apariciones del patógeno hace 5,000 años, como a todas las epidemias posteriores. Pudieron distinguir los investigadores cinco poblaciones de bacterias para las cuales fueron capaces de obtener información acerca de su evolución. Llegan a la conclusión que las cepas del patógeno que dieron origen al inicio de una epidemia, tal como la muerte negra, no fueron recientes en su momento, sino que se gestaron con una anticipación de decenas o cientos de años antes del brote.Eaton y colaboradores concluyen que la evolución de la Yersinia pestis a lo largo de la historia es en extremo compleja y que para descifrarla es necesario no solamente tener en cuenta la evidencia genética, sino que también hay que considerar los contextos histórico, ecológico, social y cultural.No es de sorprender que resulte difícil esclarecer el rumbo que siguió la peste bubónica a lo largo de cinco milenios. Lo sorprendente es que, a pesar de la distancia en el tiempo, podamos tener información acerca de esta evolución, así resulte incompleta. Por lo demás, habríamos de sentirnos afortunados que el conocimiento científico y la disponibilidad de antibióticos nos haya blindado, en buena medida, de los horrores de las epidemias del pasado. Y que la pandemia de peste bubónica haya tenido participación en el desarrollo del método científico que la ha combatido.",
    "En un comunicado conjunto fechado el 13 del presente mes, la Semarnat y el Conacyt expresan su rechazo a la realización en territorio mexicano de experimentos que buscan alterar el clima del planeta. Esto, como respuesta a los experimentos llevados a cabo en Baja California, sin el conocimiento del gobierno de México, por la compañía Make Sunsets, la cual lanzó globos con dióxido de azufre para ser dispersado en la estratósfera. De acuerdo con este comunicado “…la Semarnat implementará una estrategia que prohíba estas prácticas dentro del territorio nacional, que sirvan para robustecer las primeras referencias a nivel mundial”. Como sabemos, el incremento paulatino en la temperatura de nuestro planeta por la emisión de gases de invernadero está alcanzando límites alarmantes.  Para mitigar este incremento, se han propuesto una serie de estrategias. La más obvia es reducir el consumo de combustibles fósiles. Otra es remover los gases de invernadero que ya se han acumulado en la atmósfera, o bien evitar que lleguen hasta ahí, capturándolos en cuanto son generados. Una tercera posibilidad es disminuir la cantidad de radiación solar que es absorbida por nuestro planeta.¿Cómo podríamos atenuar la radiación que nos llega de Sol? Una manera de hacerlo es colocando una sombrilla entre la Tierra y el Sol, que tendría que ser enorme para provocar un efecto apreciable. Una propuesta más “aterrizada”, clasificada dentro del campo de la geoingeniería -es decir, la ingeniería de la Tierra- es la de dispersar dióxido de azufre a lo largo de grandes áreas en la estratósfera. Sabemos que el dióxido de azufre refleja la radiación solar y tiene así el efecto de una sombrilla.  Dispersar dióxido de azufre a una altura de 20 kilómetros es lo que intentó realizar la empresa Make Sunsets en México, elevando dos globos conteniendo helio y dióxido de azufre para que estallaran y dispersaran su contenido. No sabemos cuál fue el resultado del experimento, pero de haber sido exitoso no habría tenido un efecto apreciable sobre el clima de la Tierra, pues apenas se habría dispersado veinte gramos de dióxido de azufre, suficiente, según el sitio de la compañía, para compensar por un año la emisión de veinte toneladas de dióxido de carbono a la atmósfera. Por otro lado, lo que sí sabemos es que Make Sunsets provocó el rechazo de México, lo que llevó a la compañía a suspender sus experimentaos hasta que “encuentren la manera de colaborar con el gobierno mexicano”.Habría que recordar que la geoingeniería es una disciplina con un desarrollo incipiente y por tanto su aplicación es motivo de controversia. Así, por ejemplo, si bien dispersar dióxido de azufre en la estratósfera es relativamente sencillo y esto bien pudiera tener el efecto de enfriamiento buscado, los expertos están preocupados por una posible afectación de la capa de ozono y una reducción en el volumen de lluvias, que serían, además, más ácidas. Se anticipa también que los cambios de clima no serán los mismos en toda la superficie de la Tierra, lo que generaría conflictos geopolíticos. Igualmente, se teme que la reducción de insolación solar lleve a un falso sentido de seguridad que aminore los esfuerzos para reducir la emisión de gases de invernadero. En el contexto anterior, Make Sunsets vende “créditos de enfriamiento” a diez dólares cada uno. Cada crédito implica la liberación en la estratósfera de un gramo de dióxido de azufre, suficiente para compensar a lo largo de un año la emisión de una tonelada de dióxido de carbono, según la compañía. Sin embargo, como señala la revista MIT Technology Review en su último número, los expertos critican los rudimentarios experimentos de Make Sunsets, su intento para comercializar la geoingeniería, y su irresponsable comportamiento al llevar a cabo sus experimentos en México sin el conocimiento del gobierno mexicano. Todo esto, opinan, puede minar intentos serios en el futuro para desarrollar una tecnología que podría contribuir a mitigar el cambio climático.Por lo demás, quien tenga interés en comprar un crédito de enfriamiento tendría que esperar. En el sitio de la compañía podemos leer lo siguiente: “Tenga en cuenta que actualmente no tenemos un lanzamiento futuro, por lo que pasará un tiempo antes de que podamos entregar su crédito de enfriamiento. Su compra es reembolsable en todo momento”.",
    "Según los antiguos griegos, la gran pirámide de Guiza y los jardines colgantes de Babilonia formaban parte del grupo de las siete maravillas del mundo.  No es sorprendente que los griegos se hayan maravillado con la pirámide de Guiza, una inmensa mole de piedra de 140 metros de alto y 230 metros de lado, cuya existencia podía ser constatada por cualquiera que tuviera la oportunidad de viajar a Egipto. En contraste, la existencia de los jardines colgantes de Babilonia era más incierta. De hecho, aun hoy los arqueólogos no han logrado encontrar restos de dichos jardines que prueben que existieron en algún momento de la historia. Así, pudieran ser solamente una leyenda. O quizá la historia sea injusta con los jardines de Babilonia, que bien pudieran haber existido y su rastro haberse en el tiempo. Esto, a diferencia de la pirámide de Guiza, que ha sobrevivido miles de años, tanto por su enorme volumen, como por los grandes bloques piedra con los que fue construida. Los jardines de Babilonia, en contraste, habrían sido construidos con ladrillos y habrían resistido menos el paso de los años. Sirva lo anterior para introducir el tema a tratar hoy: el notable desarrollo por los romanos del concreto como material de construcción, con el cual construyeron carreteras, acueductos y edificios que, sin tener el volumen de una pirámide egipcia, han perdurado por dos mil años. Es el caso del Panteón de Roma inaugurado en el año 128 d.C., el cual cuenta con la cúpula de concreto no reforzado -43 metros de diámetro- más grande del mundo.La durabilidad del concreto desarrollado por los romanos se ha atribuido al uso en su fabricación de cenizas volcánicas del área de la bahía de Nápoles. Un artículo aparecido la semana pasada en la revista “Science Advances”, sin embargo, apunta en otra dirección. Dicho artículo fue publicado por un grupo de investigadores encabezado por Linda Seymour del Instituto Tecnológico de Massachusetts.Como apuntan Seymour y colaboradores en su artículo, en contraste con sus contrapartes modernas, el concreto desarrollado por los romanos en la antigüedad se ha mantenido estable en una variedad de climas, en zonas sísmicas, e incluso en contacto con el agua de mar. Para investigar el origen de la longevidad del concreto romano, los investigadores se enfocaron en pequeños depósitos blancos de tamaño milimétrico que son ubicuos en los concretos romanos y que no se encuentran en sus contrapartes modernas. Dichos depósitos se han atribuido a la fabricación descuidada del concreto por los romanos. Seymour y colaboradores, sin embargo, encuentran que están formados de varias formas de carbonato de calcio y que, lejos de resultar de una manipulación poco cuidadosa, son producto del proceso de fabricación del concreto empleado por los romanos, en el que se mezcla cal viva en lugar de cal apagada.      Por otro lado, y más importante, Seymour y colaboradores concluyen que en las inclusiones de carbonato de calcio se encuentra el secreto de la longevidad de los concretos romanos. Es decir, cuando el concreto envejece y se forman grietas que lo debilitan, el carbonato de calcio reacciona con el agua y llena dichas grietas impidiendo su propagación. Se obtiene así un concreto longevo con propiedades de autosanación. Además de que nuestros descendientes en un futuro lejano encuentren pruebas irrefutables de que en nuestro tiempo fuimos capaces de construir edificios capaces de permanecer en pie por miles de años -conviviendo con la pirámide de Guiza, pero sin alcanzar sus dimensiones- ¿qué otro beneficio podría derivarse de un concreto con propiedades de autosanación? Se sabe que la industria de fabricación de concreto es altamente contaminante y que contribuye con un 8 por ciento de las emisiones de gases de invernadero a la atmósfera, y en este sentido, Seymour y colaboradores sostienen que en la medida en que se fabriquen concretos más duraderos se incrementaría la longevidad de las construcciones y se reduciría el ritmo de emisión de gases de invernadero a la atmósfera. Lo que es indispensable si queremos que en el futuro alguien se maraville de nuestras construcciones.",
    "El pasado martes, el Instituto Tecnológico de California -conocido como Caltech- colocó en el espacio por medio de un cohete de Space X una plataforma de 50 kilogramos para probar tres prototipos de componentes clave para una futura estación espacial de generación de energía solar.  La construcción de una instalación solar en el espacio ha sido una propuesta que ha estado presente a lo largo de varias décadas. El concepto es esencialmente el de una instalación situada en una órbita terrestre que captura la energía del Sol, la transforma en energía eléctrica y la envía a la Tierra mediante un haz de microondas. La energía es recibida en la Tierra por una estación receptora que la transforma nuevamente en energía eléctrica y la distribuye para su consumo.  Una instalación solar en el espacio tiene varias ventajas. Una ventaja obvia es que no estaría sujeta a los ciclos de noche y día que limitan el periodo de tiempo a lo largo del cual una instalación sobre la superficie de la Tierra puede generar energía. En contraste, en una instalación solar espacial el tiempo improductivo puede ser drásticamente minimizado escogiendo cuidadosamente su órbita alrededor de la Tierra. Además, la intensidad solar en el espacio es más alta que aquella en la superficie de la Tierra, debido que no está sujeta a la reflexión y absorción en la atmósfera, ni a la variabilidad de las condiciones atmosféricas, incluyendo la nubosidad. Igualmente, el haz de microondas que transporta la energía podría ser dirigido a diferentes estaciones receptoras atendiendo a la demanda de energía en ese momento.  La energía solar espacial, por otro lado, enfrenta numerosos obstáculos. El principal es el alto costo para poner en órbita una instalación con dimensiones de kilómetros. Una órbita conveniente para este propósito es la geoestacionaria, la cual tiene un periodo orbital de 24 horas. Visto desde la superficie de la Tierra, un satélite en una órbita geoestacionaria aparece siempre en un mismo punto en el firmamento, al mismo tiempo que, desde la estación solar, el punto hacia el cual habría que dirigir el haz de microondas aparece igualmente fijo. Desafortunadamente, un satélite en una órbita de este tipo está a más de 35,000 kilómetros sobre la superficie de la Tierra -cerca de un décimo de la distancia entre la Tierra y la Luna-, lo que eleva los costos de construcción de la estación solar, al mismo tiempo que dificulta su mantenimiento.No obstante, y a pesar de los enormes obstáculos que enfrenta, algunos expertos consideran que la energía solar espacial es una opción viable para mitigar el cambio climático que está sufriendo el planeta. En este contexto, Caltech puso en marcha un proyecto para desarrollar una estación solar espacial, que se ensamblaría añadiendo ciertos módulos básicos. La estructura cambiaría su orientación en el espacio a medida que gira en torno a la Tierra para maximizar la cantidad de energía solar capturada por sus paneles fotovoltaicos. Como parte de un proyecto general para desarrollar una estación solar espacial, la plataforma colocada en el espacio por Caltech el pasado martes evaluará tres tecnologías clave por medio de los proyectos DOLCE, ALBA y MAPLE. DOLCE probará el despliegue en el espacio de la estructura que alojará a los paneles solares y los emisores de microondas, hasta alcanzar una dimensión de 2 por 2 metros. El experimento ALBA evaluará un total de 32 diferentes celdas solares para determinar la que tiene el mejor comportamiento en el espacio. En este sentido, habría que mencionar que el espacio constituye un medio muy agresivo, con radiaciones de alta energía que degradan el desempeño de las celdas. Finalmente, el experimento MAPLE demostrará la transmisión de energía en el espacio por medio de microondas. La construcción de instalaciones en el espacio con dimensiones de kilómetros para aprovechar la energía solar se antoja, sin duda, de ciencia ficción. Y quizá lo sea por el momento, si bien no tanto porque no se cuente con las tecnologías necesarias, sino por el enorme costo que implicaría su construcción. No obstante, en el contexto actual de crisis climática, y dado el interés renovado en el espacio, hay quienes apuestan por las instalaciones solares en el espacio. De hecho, el proyecto de Caltech está siendo financiado con fondos privados. Con 100 millones de dólares, según información proporcionada por esta universidad.",
    "Al acercarse el fin del año es costumbre hacer un recuento de los avances científicos y tecnológicos desarrollados a lo largo del mismo, al igual que hacer predicciones sobre los que se darán en el nuevo año.Una de las noticias científicas del año que ayer terminó fue, sin duda, la puesta en marcha del telescopio espacial James Webb de 10,000 millones de dólares. A este costo, podemos dar por descontado que el telescopio incorpora instrumentos científicos y de captura de luz y formación de imágenes con la más avanzada tecnología. En particular, cuenta con un espejo de 6.5 metros de diámetro formado por 18 segmentos hexagonales, cada uno de los cuales puede ser orientado de manera independiente para mantener al telescopio con un enfoque preciso. Más allá de esta sofisticación, sin embargo, el espejo del telescopio está basado en los mismos principios físicos que empleaban los telescopios desarrollados en el siglo XVII. Así, el telescopio James Webb constituye una conjunción de tecnologías propias del último medio siglo, con una tecnología para capturar luz y formar imágenes que tiene 400 años de antigüedad. Y lo que sucede con el telescopio espacial también sucede con otros dispositivos más terrenales. Por ejemplo, con los teléfonos inteligentes que, aparte de sus funciones de comunicación, incluyen la de tomar y desplegar fotografías. Esta última función la llevan a cabo con una asombrosa y creciente habilidad, integrando tecnologías de microelectrónica, de computación, de fabricación de pantallas y de miniaturización de cámaras fotográficas. Habría que señalar, no obstante, que para esto último se combinan tecnologías de diferentes siglos:  los modernos sensores microelectrónicos que convierten la luz en una señal eléctrica, y el lente objetivo que captura la luz del sujeto fotografiado y la enfoca sobre el sensor para formar la imagen, y que fue ideada en primera instancia en el siglo XVII.Por otro lado, si lo que se busca es desarrollar teléfonos cada vez más del delgados -al nivel de una tarjeta de crédito-, los factores que limitan los avances en esa dirección son la batería del teléfono y la lente de la cámara fotográfica, formada por el apilamiento de lentes necesario para disminuir las distorsiones de la imagen.  Esto último estaría por cambiar en 2023, al menos de acuerdo con un artículo publicado en el periódico The Washington Post el pasado 30 de diciembre firmado por Aaron Brown. Dicho artículo hace referencia a las llamadas metalentes y a la compañía Metalenz que las comercializará el año que viene empleando tecnología desarrollada en el laboratorio de Federico Capasso en la Universidad Harvard.Para entender que es lo que son las metalentes, habría que considerar que una lente normal es un objeto hecho de vidrio o de algún material transparente con superficies curvas, que cambia de manera controlada la dirección de la luz que pasa a través de ella según la curvatura de sus superficies. De este modo, por necesidad, una lente normal tiene un cierto espesor dictado por dicha curvatura. Las superficies de una metalente, en contraste, son planas y por tanto su espesor se reduce a un mínimo. En un teléfono inteligente el apilamiento de lentes de la cámara fotográfica será sustituido por un sola metalente y no será el factor que limite su espesor.  Por lo demás, el impacto que tendrían las metalentes no se limitaría a reducir el espesor de los teléfonos inteligentes. Lejos de esto, habría que señalar que las metalentes trabajan con un principio físico diferente al de las lentes normales y por tanto proporcionan información adicional. Así, las cámaras fotográficas equipadas con metalentes constituirían una expansión de nuestros ojos y nos darán una visión del mundo diferente a la que estamos acostumbrados. En este sentido, mencionaremos que la polarización es una propiedad de la luz a la que nuestros ojos son insensibles. En contraste, como explica Capasso, las cámaras equipadas con metalentes pueden ser sensibles a dicha propiedad y ver un mundo que nos es invisible. Una cámara fotográfica normal puede ser sensible a la polarización, pero sería voluminosa y tendría que estar equipada con aditamentos especiales. Una cámara con metalentes, por el contrario, tendría esta capacidad, un costo reducido y un tamaño compacto.  Si como pronostican algunos, 2023 será el año de los metalentes, se habría transcendido una tecnología originada hace 400 años, al mismo tiempo que tendremos, no solamente teléfonos más delgados, sino nuevos ojos para ver al mundo.",
    "Habiéndose ya iniciado el llamado puente Guadalupe-Reyes, lleno de fiestas y celebraciones y en general de momentos de ocio, es ilustrativo considerar algunos descubrimientos científicos que se llevaron a cabo justamente durante el periodo de fiestas decembrinas y que por su enorme impacto han dado forma a la sociedad actual. Dichos descubrimientos fueron realizados por científicos que decidieron no participar en dichas fiestas; o bien que decidieron hacerlo de manera moderada.Empezaremos por considerar el avistamiento el 25 de diciembre de 1758 del cometa Halley por el astrónomo aficionado Georg Palitzsch, el cual había sido predicho en 1705 por Edmund Halley. Halley sospechaba que los cometas avistados en 1531, en 1607, y por él mismo en 1682, eran en realidad un mismo cuerpo celeste que visitaba nuestra vecindad de manera periódica. Para poyar su hipótesis, Halley utilizó las entonces nuevas leyes de gravitación y del movimiento de Isaac Newton para calcular la órbita que tendría dicho cuerpo en su viaje alrededor del Sol. Basado en sus cálculos, Halley predijo que volveríamos a ver al cometa en 1758, como efectivamente ocurrió.  En su tiempo, esto constituyó una espectacular confirmación de las teorías de Newton sobre la mecánica del movimiento y demostró el poder del método científico.Un segundo ejemplo es el descubrimiento de la fisión nuclear llevado a cabo durante las vacaciones de diciembre de 1938 por Lise Meitner y Otto Frisch. Meitner fue una investigadora de ascendencia judía que, por esta circunstancia, tuvo que abandonar su puesto de trabajo en el Instituto Kaiser Wilhem en Berlín -de manera precipitada y con solo dos pequeñas maletas como equipaje- y refugiarse en Estocolmo. En Berlín, Meitner, juntamente con Otto Hahn y Fritz Strassmann, llevaron a cabo experimentos en los que bombardearon átomos de uranio con neutrones. Con su huida a Suecia, Meitner interrumpió sus experimentos en Berlín, pero siguió en contacto con Hahn, quien, en una carta fechada el 19 de diciembre de 1938, le hizo saber que al bombardear uranio con neutrones se producían átomos de bario que es un elemento con un peso que es aproximadamente la mitad del peso del uranio. Hahn no tenía una idea clara de cómo se generaba el bario y Meitner se dio a la tarea de encontrar una explicación sólida. Así, juntamente con su sobrino Otto Frisch que la visitaba durante las fiestas de diciembre, descubrió el misterio: el bombardeo de neutrones parte al átomo de uranio en dos pedazos, desprendiendo una enorme cantidad de energía. Este descubrimiento, en primera instancia, dio origen a las bombas nucleares que destruyeron Hiroshima y Nagasaki al final de la Segunda Guerra Mundial, y posteriormente a la bomba atómica de fusión, todavía más poderosa, que tuvo en vilo al mundo a lo largo de la llamada guerra fría. El descubrimiento de Meitner y Frisch condujo igualmente al desarrollo de los reactores nucleares para la generación de energía eléctrica.Un tercer ejemplo es la demostración del transistor llevada a cabo el 23 de diciembre de 1947 por John Bardeen y Walter Brattain en los Laboratorios Bell en Nueva Jersey, Estados Unidos. Inmediatamente después de esta demostración, como lo relatan Michel Riordan y Lillian Hodeson en su libro Crystal Fire, William Shockley, también de los Laboratorios Bell, se encerró en un cuarto de hotel para desarrollar la teoría de operación de los transistores que todavía está vigente. Como sabemos, la invención del transistor ha producido un cambio radical en nuestra civilización y con el tiempo ha llevado al desarrollo de las computadoras y de la red Internet, así como a toda una gama de aparatos electrónicos. Los anteriores son solo algunos ejemplos de descubrimientos científicos que han sido de enorme trascendencia y que han sido llevados a cabo durante las fiestas del mes de diciembre. No podríamos, sin embargo, pensar que diciembre es un mes especial en este sentido, de modo que, asumiendo que los científicos estén más interesados en su trabajo que en las fiestas decembrinas, los descubrimientos importantes deben repartirse por igual a lo largo de los doce meses del año.Por lo demás, al margen de llevar a cabo una investigación para determinar la veracidad de esta última afirmación, nos aventuramos a sugerir que el puente Guadalupe-Reyes no es precisamente un promotor del avance científico del país.",
    "Un artículo publicado el pasado viernes en la revista MIT Technology Review confirma que el proyecto de Arabia Saudita conocido como The Line está en plena construcción. Dicho proyecto, anunciado por el príncipe heredero de Arabia Saudita en 2021, contempla la construcción de dos rascacielos de 500 metros de altura en medio del desierto. Dado que en el mundo hay cuando menos diez edificios que superan esta altura, a bote pronto dicho proyecto no parecería cosa del otro mundo. Habría que señalar, no obstante, que The Line tendrá características que lo sitúan más allá de cualquier otro proyecto construido hasta ahora. \nEn efecto, se pretende que The Line se constituya en una megaciudad en el desierto saudí para 9 millones de habitantes con un diseño muy singular: dos edificios de 500 metros de altura separados por 200 metros, los cuales se extienden a lo largo de 170 kilómetros. La estructura forma una especie de cañón angosto de 500 metros de altura y 170 kilómetros de largo. Además, con las paredes de los edificios que ven hacia el desierto cubiertas de espejos. Los habitantes de la ciudad, que carecerá de calles y automóviles, se moverán en el espacio entre los dos edificios y podrán alcanzar cualquier servicio caminando en cinco minutos. Para distancias más largas dispondrán de taxis aéreos y de un tren subterráneo que les permitirá recorrer la longitud total de la ciudad en 20 minutos, a una velocidad de más de 500 kilómetros por hora.\nThe Line constituirá así una ciudad lineal y vertical en la que sus habitantes vivirán con todas las comodidades en una franja de terreno de 200 metros de ancho y 170 kilómetros de largo, aislados del desierto por dos edificios de 500 metros de altura. Alojaría a 9 millones de habitantes en un área de 34 kilómetros cuadrados -una superficie cuadrada de aproximadamente 6 kilómetros por lado.\nPara su operación, The Line necesitará apoyarse fuertemente en tecnologías tales como la robótica y la inteligencia artificial, lo mismo que en tecnologías para desalinizar agua, producir lluvia y generar energías renovables, que se pretende cubran totalmente las necesidades energéticas de la ciudad. La inversión en la megaciudad alcanzaría los 500,000 millones de dólares. \nDada la magnitud del proyecto y las tecnologías necesarias para su realización, se dudaba que pudiera algún día llevarse a cabo. La construcción de The Line, no obstante, se habría iniciado en abril del presente año y el artículo de MIT Technology Review citado con anterioridad lo confirma de manera plena. Esto, a partir del análisis de fotografías satelitales de alta resolución llevado a cabo por la compañía Soar Earth, el cual muestra que “se han excavado alrededor de 26 millones de metros cúbicos de tierra y rocas -78 veces el volumen de edificio más alto del mundo, el Burj Khalifa”.\nDe su análisis fotográfico, Soar Earth también encuentra 425 vehículos en el sitio en el que se localizarán los edificios y 650 vehículos en una base de construcción cercana al sitio, la cual sirve de alojamiento para los trabajadores del proyecto. Dicha base, de cinco kilómetros cuadrados de superficie, cuenta con su propia instalación de celdas solares, lo mismo que con varias albercas para nado y varios campos de futbol y de cricket. Se observó actividad de construcción en alrededor de la mitad de la longitud total del proyecto. \nThe Line ha sido presentado por sus creadores como una “Una revolución en civilización” que trascenderá a las “ciudades disfuncionales y contaminadas, que ignoran la naturaleza” y en las que “la humanidad ha vivido por mucho tiempo”.  El proyecto, sin embargo, ha sido motivo de críticas por parte de los expertos. La revista en línea de arquitectura “dezeen” menciona algunas de ellas. Se aduce, por ejemplo, que hay una gran cantidad de fenómenos físicos y ambientales que será necesario controlar para la operación de la ciudad, lo cual será difícil de llevar a cabo a lo largo de un periodo de tiempo. De la misma manera, la operación de la ciudad dependerá de tecnologías que en algunos casos no están completamente desarrolladas.  Por otro lado ¿Sería atractivo vivir en un espacio de 200 metros de ancho que algunos encuentran claustrofóbico? Sin duda habrá opiniones diversas al respecto.\nEstas y otras críticas han expresado los escépticos del proyecto The Line, el cual, sin embargo, avanza a todo vapor en el desierto saudí de acuerdo con MIT Technology Review. Ciertamente, impulsado por la abundancia de recursos.",
    "Hoy en día, algunos materiales como el litio o las llamadas tierras raras son altamente apreciados por sus numerosas aplicaciones, entre las que se encuentran la generación de energía renovable, las telecomunicaciones y las memorias de computadora. De manera similar, hace 3,500 años el estaño constituía un material de gran importancia para civilizaciones establecidas, desde Asia oriental hasta el mar Mediterráneo. En efecto, sabemos que hace unos 5,000 años se descubrió que añadiendo un 10 por ciento de estaño al cobre se obtiene una aleación que es más dura que sus dos constituyentes. Esto llevó a la fabricación de armas para la guerra y utensilios y herramientas para la vida diaria, y al inicio de la llamada Edad del Bronce que se prolongó hasta el fin del segundo milenio a.C. Por otro lado, sabemos que los yacimientos de litio y de tierras raras no están distribuidos de manera uniforme a lo largo de la superficie del planeta y por tanto constituyen materiales de importancia estratégica. Lo mismo sucedía con el estaño durante la Edad del Bronce, cuyos depósitos minerales estaban lejos de los centros urbanos. Así, al igual que hoy en día los países industrializados se preocupan por asegurar el flujo de los materiales estratégicos que necesitan para el funcionamiento de su economía, las civilizaciones del mundo antiguo tenían que asegurar el flujo del estaño que requerían. El descubrimiento de un naufragio de la Edad del Bronce en la costa sur de Turquía con un cargamento de 10 toneladas de cobre y una tonelada de estaño proporcionó una oportunidad para averiguar los orígenes, y las rutas que seguía el estaño desde el lugar de extracción hasta el de su consumo. Los expertos, sin embargo, no habían logrado ponerse de acuerdo sobre estos puntos, hasta que un estudio llevado a cabo por un grupo de investigadores encabezado por Wayne Powell, de Brooklyn College en Nueva York, desveló la procedencia y la intrincada ruta que habría seguido el estaño. Los resultados de dicho estudio fueron publicados esta semana en la revista Science Advances.Para desentrañar el misterio, Powell y colaboradores llevaron a cabo un análisis de las composiciones de isótopos de estaño y plomo de 108 lingotes de estaño del naufragio y las compararon con las composiciones correspondientes de yacimientos de estaño conocidos -como sabemos, los elementos químicos pueden existir como diferentes isótopos que difieren entre sí en cuanto a su peso-. De este modo, la coincidencia de la composición isotópica de un lingote con la de un yacimiento indicaría la procedencia de dicho lingote. Siguiendo este método, encontraron los investigadores que un tercio de los lingotes procedía de minas en Uzbekistán, mientras que los dos tercios restantes tenían su origen en minas de Turquía.  Con los resultados de su investigación, Powell y colaboradores ponen al descubierto una red de comercio con una antigüedad de 3,500 años, mediante la cual el estaño extraído en minas en Asia Central por pequeñas comunidades de mineros, fue enviado, a través de más 3,000 kilómetros, hasta el mar Mediterráneo, en donde fue conjuntado con estaño extraído de minas en Turquía bajo el control de un gobierno central. De acuerdo con los investigadores, el cargamento de cobre y estaño del barco malhadado habría sido suficiente para fabricar espadas para un ejército de 5,000 soldados. Así, la red de comercio de estaño, además de sofisticada, estaba organizada con elementos dispares.   Por otro lado, mencionan los investigadores, que aún quedan misterios por desvelar. En particular, no sabemos en dónde se llevó a cabo el proceso de fundición del mineral de estaño extraído de las minas y la fabricación de los lingotes que tienen diferentes formas. Al margen de la respuesta que eventualmente pudiera darse a estas preguntas, dos cosas nos resultan sorprendentes. Primeramente, que en la Edad del Bronce hubieran sido capaces de organizar una ruta comercial con la complejidad demostrada, y segundo, que se hayan podido desentrañar secretos después de 3,500 años. Aunque, pensándolo nuevamente, quizá no deberíamos sorprendernos de que nuestros antecesores hayan organizado una compleja ruta comercial hace miles de años. Después de todo, para entonces ya habían inventado la metalurgia del bronce, que se antoja más complicada. Como quizá tampoco deberíamos sorprendernos de la habilidad del método científico para escudriñar el pasado, de la cual tantas pruebas nos ha dado.",
    "Después de 1,500 años de permanecer en la oscuridad, la existencia de un emperador romano de nombre Esponsiano habría sido revelada hace dos siglos cuando se descubrieron monedas romanas de oro con su efigie. Si bien hemos de señalar que la autenticidad de dichas monedas ha sido puesta en duda y por tanto también lo ha sido la existencia de Esponsiano, un artículo aparecido esta semana en la revista PLOS ONE proporciona pruebas de que dichas monedas sí son auténticas. El artículo fue publicado por un grupo de investigadores encabezados por Paul Pearson del University College London.      De acuerdo con Pearson y colaboradores, una nota escrita a mano en 1713 por Carl Gustav Heraeus, quien era Inspector de Medallas de la Colección Imperial en Viena, documenta la adquisición de ocho monedas de oro de cinco diseños diferentes, una de la cuales mostraba la efigie de Esponsiano. Estas monedas eran parte de un lote de monedas que habría sido descubierto en 1713 en Transilvania, en la actual Rumania. Heraeus adquirió solamente parte del lote; el resto se dispersó entre diferentes compradores.  En su artículo, Pearson y colaboradores reportan los resultados de un estudio llevado a cabo con cuatro monedas que habrían formado parte del lote descubierto en Transilvania y que son actualmente parte de la colección del museo The Hunterian de la Universidad de Glasgow. Una de estas monedas muestra la efigie de Esponsiano, mientras que las otras tres muestran a emperadores romanos bien conocidos, una de ellas a Gordiano III y las otras dos a Felipe I o a Felipe II. De acuerdo con los investigadores, el diseño de cuatro monedas comparten muchas características que hacen pensar que fueron fabricadas por una misma persona. En particular, las monedas retratan a los emperadores con mentones prominentes y ojos saltones. La similitud entre las monedas puede ser apreciada en el artículo citado, de consulta libre por Internet.En un primer momento, se pensó que las monedas descubiertas en Transilvania eran auténticas, si bien imitaciones burdas de monedas romanas fabricadas “más allá de los límites del imperio”. Igualmente, se consideró que Esponsiano fue un usurpador local, que habría aprovechado el desorden imperante durante la llamada crisis del siglo III del Imperio romano. En 1868, sin embargo, como apuntan Pearson y colaboradores, el experto más prominente en el tema declaró que las monedas eran “falsificaciones mal hechas y ridículamente concebidas”. A la fecha el asunto no había sido esclarecido, y en estas circunstancias, los investigadores se propusieron estudiar las cuatro monedas de la colección de The Hunterian con técnicas modernas de análisis para tener más elementos de juicio. Durante su investigación, Pearson y colaboradores tomaron imágenes de las monedas empleando luces visible, infrarroja y ultravioleta. Observaron también las monedas con un microscopio electrónico que les permitió hacer un análisis químico de su superficie. Llegaron a la conclusión que las monedas muestran rayones que evidencian un degaste natural durante el tiempo que estuvieron en circulación, y no un desgaste artificial que habría sido ocasionado por un intento de falsificación. Igualmente, encuentran depósitos minerales en la superficie de las monedas que evidencian que estuvieron enterradas por un tiempo largo. En base a sus resultados, Pearson y colaboradores llegan a la conclusión que muy probablemente las monedas encontradas en Transilvania son auténticas y que Esponsiano debe ser históricamente reevaluado. Especulan los investigadores que Esponsiano habría sido un comandante militar en Dacia -actual Transilvania-, que fue una provincia en los extremos del Imperio romano, y que habría tomado el poder durante la crisis de siglo III, a lo largo de la cual hubo un cambio constante de emperadores que llegaban al poder por la vía militar.  Tendríamos así que Esponsiano es un emperador que ha llegado a la historia por la puerta de atrás, por decirlo de alguna manera, y que sin las técnicas modernas de análisis no habría quizá pasado de ser un personaje ficticio inventado por un falsificador de monedas. Por otro lado, hay también que señalar que, en la medida que se desarrollen técnicas científicas para averiguar el pasado, la puerta de atrás podría convertirse en la puerta del frente.",
    "Como lo han informado los medios de difusión, esta semana dio inicio en Egipto la cumbre climática de la Organización de la Naciones Unidas COP27, durante la cual más de 120 lideres mundiales acordarán medidas para enfrentar el cambio climático. Dicho cambio está alcanzando niveles alarmantes; en palabras del secretario general de la ONU, “el mundo se está dirigiendo hacia un infierno climático con el pie en el acelerador”. Por su parte, en su discurso en la COP27, el presidente de los Estados Unidos, el segundo mayor emisor de gases de invernadero, afirmó que el planeta se “encuentra en un peligro inminente” por el cambio climático. En efecto, un reporte publicado en la revista Earth System Science Data, elaborado por un numeroso grupo internacional de especialistas, proyecta que las emisiones de gases de invernadero en 2022 se incrementarán 1 por ciento con respecto a 2021, alcanzado un nivel récord, ligeramente superior al nivel prepandemia de 2019. De este modo, se espera que la concentración de dichos gases en la atmósfera alcance 417 partes por millón, valor que es más de 50 por ciento superior al que prevalecía al inicio de la revolución industrial. De seguir la emisión de gases de invernadero creciendo al ritmo actual, en nueve años se alcanzará la concentración máxima estimada para limitar a 1.5 grados centígrados el incremento de la temperatura global con respecto a su valor preindustrial. Sabemos que los principales gases de invernadero son el dióxido de carbono y el metano, formado este último por átomos de hidrógeno y carbono. El carbono juega entonces un papel central en los problemas climáticos por lo que estamos pasando y podríamos quizá considerarlo como el villano de la película. En este sentido, habría que señalar que en el planeta contamos con átomos de carbono distribuidos en diferentes formas químicas en la atmósfera, en la vegetación, en el mar y en el suelo. El número total de átomos es fijo, pero a través del ciclo del carbono pueden pasar de una forma a otra. Así, a través del proceso de fotosíntesis, las plantas generan y almacenan carbono en la forma de carbohidratos a partir del dióxido de carbono en la atmósfera. El carbono almacenado puede posteriormente regresar a la atmósfera como dióxido de carbono a través del proceso de respiración de las plantas o a través de un proceso de combustión. De manera similar, el carbono puede ser intercambiado entre la atmósfera y el mar o entre la atmósfera y el suelo.Todos los procesos del ciclo del carbono llegaron a un equilibrio a lo largo de un tiempo muy largo, equilibrio que ha sido alterado por las actividades humanas en los últimos doscientos cincuenta años. En este sentido, el proceso más importante ha sido la quema de combustibles fósiles, que implica la liberación en la atmósfera, como dióxido de carbono, del carbono almacenado por cientos de millones de años en el subsuelo. Otro proceso que contribuye de manera importante, aunque en menor medida, al cambio climático es el cambio en el uso del suelo. Por ejemplo, la deforestación para abrir tierras al cultivo, la cual implica la transferencia a la atmósfera del carbono almacenado en las plantas y la eliminación del sumidero de carbono que constituye las áreas deforestadas. Los principales países que contribuyen a la emisión de gases de invernadero por el cambio de uso del suelo son Indonesia, Brasil y la República Democrática del Congo.Varias decenas de miles de delegados de 200 países asisten a la COP27, entre los que, sin embargo, no se encuentra ni el presidente de China, el mayor emisor de gases de invernadero, ni el primer ministro de la India, el tercer mayor emisor de estos gases. Tampoco asiste el presidente de Rusia, el cuarto mayor emisor de gases de invernadero. En contraste, según el diario El País, a la cumbre climática asisten 636 delegados que son empleados, o están ligados, a compañías de combustibles fósiles. En estas circunstancias, habría que reconsiderar llamar al carbono el villano de la película, con la seguridad de que habrá abundancia de candidatos para dicho título. Además de que, pensándolo bien, el carbono está lejos de ser un villano. De hecho, el carbono es la base de la vida. En particular, estamos todos hechos de carbono y sin su concurso ninguno de nosotros habría visto la luz en este planeta.",
    "El pasado 9 de octubre se celebró en la ciudad de Puebla el partido de repechaje entre los equipos Guadalajara y Puebla para acceder a los cuartos de final del torneo de futbol de primera división. Durante la ejecución de la ronda de penaltis al final del partido -que fue necesaria dado que el partido terminó empatado en el tiempo reglamentario- el portero del equipo Guadalajara se quejó de que un aficionado en la tribuna le apuntaba a los ojos con un láser verde en el momento de enfrentar al tirador, con el objeto de obstruirle la visión y hacerlo fallar. El incidente no ha sido único y se ha dado en diferentes circunstancias, posibilitado por la fácil y libre adquisición de láseres con potencias apreciables.   Los láseres son fuentes luminosas con características únicas; entre éstas, la de producir un haz de luz que viaja en línea recta y que es lo que en primera instancia permite ataques como el que se dio en el partido Guadalajara-Puebla. Por lo demás, cegar al portero contrario no es el único uso malicioso que podemos concebir para un láser. En este sentido, un artículo escrito por un grupo de investigadores encabezado por Yulong Cao de la Universidad de Michigan nos proporciona una posibilidad, por demás inesperada. Dicho artículo está todavía en la fase de revisión por otros expertos para su publicación formal, pero es posible consultarlo en el sitio arXiv mantenido por la Universidad Cornell. En su artículo, Cao y colaboradores evaluaron el uso de un láser de luz infrarroja para engañar al sistema Lidar que guía a algunos automóviles sin conductor. Dicho sistema está basado en un láser infrarrojo y en detectores de radiación infrarroja colocados en el techo del automóvil. El láser gira rápidamente enviando pulsos de luz en todas direcciones, de modo tal que, si uno de estos pulsos encuentra un obstáculo, se reflejará produciendo un eco que será captado por los detectores del automóvil. A partir del eco, el sistema Lidar podrá determinar la posición del obstáculo, así como la distancia a la que se encuentra. Esto último midiendo el tiempo que le tarda al pulso del láser en su viaje de ida y vuelta hasta el obstáculo. Así, el sistema Lidar tendrá información sobre las condiciones del camino; es decir, si está libre para avanzar o si hay obstáculos que hay que evitar. Y en función de esta información, dará las instrucciones pertinentes al automóvil.  Desafortunadamente, lo anterior es más fácil decirlo que hacerlo realidad, pues los detectores del Lidar captan ecos múltiples producidos por los objetos interceptados por el haz del láser en su trayectoria, los cuales tienen que ser analizados por el sistema para su correcta interpretación.  Para esto, el Lidar prioriza las señales más intensas descartando otras más débiles. Aprovechando esta característica, Cao y colaboradores lograron confundir al sistema Lidar dirigiendo al automóvil un láser infrarrojo que simulaba ser una señal de eco intensa, la cual eclipsó a la señal verdadera proveniente de un peatón, haciendo que fuera ignorada. Es decir, por medio de su señal falsa, los investigadores generaron un punto ciego en el sistema de visión del Lidar, que ignoró la presencia de un peatón en la trayectoria del automóvil.De este modo, al igual que una persona armada con un láser podría bloquear la visión de un portero que intenta detener un tiro penal, otra persona con peores intenciones podría generar un punto ciego en el sistema Lidar de un automóvil autónomo que no detectaría la presencia de un peatón en su camino. Las consecuencias en los dos casos, por supuesto, serían muy diferentes. En un caso la ceguera podría resultar cuando mucho en la derrota injusta de un equipo, que se podría recuperar en el siguiente año, mientras que en el otro podría no haber recuperación posible. Por los demás, habría que señalar que el Lidar es solamente uno de los dos sistemas por los que un automóvil autónomo puede “ver” el camino y que hay un segundo sistema, basado en cámaras de video, que le hace competencia. Ambos sistemas tienen ventajas y desventajas y algunos expertos opinan que los automóviles autónomos del futuro podrían contar, tanto con un Lidar como con cámaras de video, para tener lo mejor de ambos mundos. En cuyo caso, la posibilidad de que un actor malicioso pudiera cegar a un automóvil autónomo resultaría menos preocupante.",
    "Una crónica europea de la Alta Edad Media habla de la aparición en el año 774 de un “crucifijo rojo” en el cielo después del atardecer. Una crónica china contemporánea consigna la ocurrencia de un fenómeno similar en el año 775. Si bien no es claro qué fue lo que originó estos fenómenos, algunos expertos aventuran que pudieron tratarse de resplandores ocasionados por el ingreso de partículas de alta energía provenientes del espacio a la atmósfera terrestre. Esto último está apoyado por estudios que midieron las concentraciones de carbono-14 en árboles milenarios que indican que en el año 775 hubo un aumento súbito en la intensidad de las radiaciones de alta energía que ingresaron a la atmósfera terrestre. Con respecto a esto último, habría que recordar que nuestro planeta está continuamente bombardeado por radiaciones de alta energía provenientes del Sol y de fuentes fuera del sistema solar. Dichas radiaciones se absorben en las partes altas de la atmósfera y, entre otros productos secundarios, generan isótopos de carbono-14 que eventualmente se incorporan a la biosfera y son absorbidos por las plantas. De este modo, la concentración de dichos isótopos en los anillos de crecimiento de un árbol -que se sabe crecen anualmente y cuyo estudio es el objetivo de la dendrocronología-, nos indicará la intensidad de las radiaciones de alta energía que arribaron a nuestro planeta desde el espacio a lo largo de la vida del árbol. Así, mediante la medición de la concentración de carbono-14 en árboles de cedro japonés fue posible determinar que en los años 774-775 ocurrió un incremento súbito de 1.2 por ciento en los niveles de radiación de alta energía que arribaron a nuestro planeta, que es el más grande que se ha dado en los últimos 11,000 años. De manera adicional, ha habido en este periodo otros eventos similares, aunque de menor magnitud. Todos estos eventos son conocidos como eventos Miyake, por el investigador que los descubrió.Dado que un evento como el ocurrido en 774-775 tendría un efecto catastrófico sobre los sistemas de comunicación de nuestro planeta, es de gran interés tratar de entender las causas de los incrementos súbitos en los niveles de radiación provenientes del espacio para tratar de anticiparlos. En este sentido, la hipótesis más extendida es que han sido debidos a incrementos súbitos en la actividad solar. No obstante, se ha considerado también que fueron debidos a incrementos en los niveles de rayos cósmicos generados más allá del sistema solar. Un artículo publicado en la revista Proceedings of the Royal Society A busca arrojar luz al respecto. Dicho artículo fue publicado por un grupo de investigadores y lleva como primer autor a Quingyuan Zhan, de la Universidad de Queensland, en Australia.En su investigación Quingyuan e investigadores modelizaron el ciclo del carbono en un periodo de 10,000 años e incorporaron los datos dendrocronológicos publicados acerca de seis eventos Miyake ocurridos en los últimos 9,000 años, incluyendo el evento de los años 774-775. No encuentran una correlación entre tales eventos y los ciclos de la actividad solar que apoye la hipótesis más aceptada y dejan abierta la posibilidad de que estén relacionados con incrementos súbitos en la intensidad de los rayos cósmicos.  En circunstancia, consideran que son necesarias más investigaciones para llegar a una conclusión sólida.Cuando los europeos de la Alta Edad Media observaron en el cielo lo que les parecía era un crucifijo rojo, estaban muy lejos de entender la naturaleza del fenómeno que estaban atestiguando. Habría que recordar que en 774 -apenas cuatro años después de que Carlomagno, el llamado Padre de Europa medieval, se convirtiera en rey de los francos-, Europa estaba todavía lejos de la revolución científica de los siglos XVI y XVII encabezada por Galileo Galilei e Isaac Newton. Y todavía más lejos de los siglos XIX y XX que vieron la luz de los modernos sistemas de comunicaciones que serían devastados por otro evento Miyake.Así, si en 774 el incremento en las radiaciones cósmicas no tuvo en apariencia mayores consecuencias para el mundo -Dios habría protegido la inocencia-, hoy en día lo pondría, de manera paradójica, cabeza abajo. por nuestra absoluta dependencia con respecto a redes de Internet, satélites de comunicaciones y líneas eléctricas de transmisión de potencia. Producto de nuestra pérdida de inocencia.",
    "Si por alguna razón o razones no se controla la emisión de gases de invernadero a la atmósfera y se incrementan, en número e intensidad, los eventos climáticos extremos, tales como sequías, huracanes, y olas de calor y de frío, al punto de hacer al planeta cada vez más inhabitable, tendríamos que buscar opciones para mudarnos a otro planeta. ¿Qué opciones tendríamos? Nuestra primera elección quizá fuera la Luna, que está, digamos, a tiro de piedra. La Luna, sin embargo, es un lugar inhóspito, seco y sin atmósfera, que haría imposible el establecimiento de colonos en números apreciables. Así, tendríamos que pensar en algunos otros lugares más lejanos y en este sentido, podríamos considerar a Venus y a Marte, que son los planetas más cercanos a la Tierra. Descartamos, no obstante, a Venus que cuenta con una densa atmósfera de gases de invernadero que provocan que la temperatura en su superficie supere los 400 grados centígrados. En cuanto a Marte, si bien sus condiciones ambientales son menos inhóspitas, cuenta con una atmósfera tenue de dióxido de carbono que no nos permitiría respirar. Mas allá de Marte, los planetas y sus satélites son muy fríos. Es, por ejemplo, el caso de Titán, el mayor satélite de Saturno, cuya temperatura superficial es de menos 180 grados centígrados. De este modo, tendríamos que buscar opciones de mudanza a algún planeta fuera de nuestro sistema solar. Por el momento no tenemos conocimiento de alguno que reúna las condiciones de habitabilidad que se requieren, pero los expertos se encuentran en su búsqueda. Para esto, cuentan con la ayuda del telescopio espacial James Webb, recientemente puesto en operación. Un planeta que está en la mira del James Webb es el TRAPPIST-1e, que orbita a la estrella TRAPPIST-1 localizada a 40 años luz de la Tierra -un año luz es la distancia que recorre la luz en un año-. TRAPPIST-1 es una estrella relativamente fría que cuenta con siete planetas rocosos orbitando a su alrededor. TRAPPIST-1e es el cuarto planeta más cercano a la estrella y se encuentra dentro de la zona en la cual puede existir agua líquida, por lo que es un candidato para albergar vida. Dada esta expectativa, dicho planeta será objeto de un estudio por parte del telescopio James Webb de la NASA con la esperanza de determinar sus condiciones reales para sostener vida.En tanto la NASA lleva a cabo sus estudios para caracterizar las condiciones ambientales de TRAPPIST-1e, un grupo internacional de expertos encabezados por Assaf Hochman de la Universidad Hebrea de Jerusalén se dio a la tarea de determinar la variabilidad climática que podría prevalecer en dicho planeta, que es clave para sostener la vida. Si bien por el momento no se tiene un conocimiento detallado de las condiciones ambientales imperantes en TRAPPIST-1e -conocimiento que se espera sea ampliado por las observaciones del telescopio de la NASA-, Hochman y colaboradores decidieron adelantarse y predecir la posible variabilidad climática de dicho planeta.  Los resultados de su investigación están reportados en un artículo aparecido esta semana en la revista The Astrophysical Journal. En su estudio, los investigadores se interesaron en determinar la variabilidad climática de TRAPPIST-1e ante cambios en la concentración de gases de invernadero en la atmósfera y encuentran que, en comparación con la Tierra, el clima de TRAPPIST-1e es más vulnerable a dichos cambios.   Si bien Hochman y colaboradores se han adelantado a las observaciones del telescopio James Webb, consideran que su estudio establece una plataforma para determinar las condiciones de habitabilidad de un planeta y su estabilidad climática ante cambios en la concentración de gases de invernadero en la atmósfera. Así, de confirmarse sus resultados, tendríamos que eliminar a TRAPPIST-1e de la lista de planetas candidatos para una mudanza en caso de que se agravaran las condiciones climáticas de nuestro planeta. Por lo demás, más nos vale que cuidemos el clima de nuestro planeta sin buscarle tres pies al gato, y limitemos la emisión de gases de invernadero. Pues, aparte de lo lejano que se encuentran los posibles planetas sustitutos, podría resultar que una vez ahí, encontremos un lugar con una mayor variabilidad climática. Con lo que habríamos salido de guatemala para entrar a guatepeor.",
    "Como podemos ver en videos disponibles en Internet, filmados en días pasados en la Galería Nacional de Londres, dos jóvenes activistas del grupo “Detengan el petróleo” se pararon frente al cuadro “Los girasoles”, del pintor holandés Vincent van Gogh, con sendas latas de sopa de tomate en la mano cuyo contenido arrojaron sobre la pintura. Hecho esto, las activistas se cubrieron la palma de la mano izquierda con pegamento y la presionaron contra la pared. Así, pegadas al muro, lanzaron un discurso ecologista pidiendo que se suspendiera el uso de los combustibles fósiles, antes de que el guardia de seguridad del museo las retirara del lugar.Si van Gogh, por medio de una máquina del tiempo, se hubiera trasladado hasta nuestros días y hubiera presenciado la escena, probablemente le hubiera extrañado sobremanera. Con un poco más de tiempo de permanencia en el futuro, sin embargo, se hubiera posiblemente enterado de que el planeta está pasando por un proceso de cambio climático que amenaza con desbordarse y llegar a un punto de no retorno. Y se habría enterado también que, en paralelo, se está dando un proceso de pérdida de biodiversidad a la que el cambio climático no es ajeno, al menos según el Informe Planeta Vivo 2022, dado a conocer esta semana por la sociedad conservacionista Fondo Mundial para la Naturaleza. En efecto, de acuerdo con dicho informe, si bien en la actualidad el cambio en el uso del suelo es la principal amenaza para la naturaleza, si no logramos limitar a 1.5 grados centígrados el aumento en la temperatura del planeta con respecto a su valor preindustrial, el cambio climático será probablemente la primera causa de la pérdida de la biodiversidad en las próximas décadas.Al margen de los acontecimientos futuros, aun ahora la velocidad con que se está perdiendo la biodiversidad es preocupante. De acuerdo con el Informe Planeta Vivo 2022, entre 1970 y 2018 se observa una reducción promedio del 69 por ciento en las poblaciones de vida silvestre monitoreadas. Además, dicha reducción, en la que se consideran solamente especies de animales vertebrados, no es uniforme a lo largo del planeta, y en este sentido, América Latina ocupa el primer lugar con un 94 por ciento de reducción, en comparación con 55 por ciento en la región Asia-Pacífico y 20 por ciento en Norteamérica. Estas cifras, sobre todo las que se refieren a América Latina suenan, ciertamente, escandalosas. Habría, no obstante, que aclarar que no se refieren a una disminución del número absoluto de animales silvestres. En este sentido, el sitio de Internet Our World in Data nos proporciona un ejemplo simple para entenderlo, a partir de la variación de poblaciones de rinocerontes en Tanzania y en Bostwana entre los años 1980 y 2017. De acuerdo con dicho sitio, en 1980, la población de rinocerontes en Tanzania y Bostwana era de 3795 y 30, en forma respectiva. En 2017, los números correspondientes fueron 160 y 50.  Así, en Tanzania se observó una reducción de 96 por ciento en la población de rinocerontes, mientras que en Bostwana la dicha población aumentó en 67 por ciento. Así, sumando ambas poblaciones, hubo una reducción de 95 por ciento en el total de rinocerontes. El Informe Planeta Vivo 2022, en contraste, toma el promedio de la reducción observada en Tanzania con el crecimiento en Bostwana y reporta una reducción de solo un 15 por ciento. Esto es engañoso pues no nos dice que en Tanzania los rinocerontes están en vías de extinción, mientras que en Bostwana han logrado incrementar el número de rinocerontes. De este modo, hay que tomar con cuidado las cifras sobre la reducción de las poblaciones de animales silvestres reportadas en el Informe Planeta Vivo 2022. No obstante, el hecho de que los números reportados resulten negativos hace encender las señales de alarma de los expertos. En particular, resulta impactante la diferencia de cifras entre América Latina y otras partes del mundo.En conclusión, habría sin duda que pedir disculpas a van Gogh por el atentado de las activistas. Y argüir que, en todo caso, quizás hubiera sido más apropiado, por lo simbólico, atentar contra alguna pintura de Paul Gauguin -amigo de van Gogh- realizada durante su permanencia en el paraíso de Tahití en el que buscó refugio, y que ahora está amenazado de inundación por el cambio climático.",
    "Como sabemos, durante los siglos VIII y IX de la era cristiana, la civilización maya de las tierras bajas del sur de la península de Yucatán entró en declive y eventualmente colapsó. Las causas que llevaron a este colapso no han sido completamente clarificadas. Se ha especulado, sin embargo, que podría haber sido originado por un cambio climático que habría ocasionado una mega sequía y un desastre agrícola. Se ha especulado también que un cambio en el clima habría sido un factor para el colapso del imperio romano de occidente, y al respecto se sabe que una serie de erupciones volcánicas provocaron un descenso en la temperatura ambiental en el siglo VI después de Cristo que se prolongó por 150 años.Dadas las experiencias pasadas ¿podría el cambio climático por el que está actualmente atravesando el planeta tener alguna consecuencia para la civilización contemporánea? Un artículo de opinión publicado esta semana en la revista Proceedings of the National Academy of Sciences expresa algunos puntos de vista al respecto. Dicho artículo fue publicado por un grupo de investigadores encabezado por Daniel Steel de la British Columbia University en Canadá. En su artículo, Steel y colaboradores definen el colapso de una civilización como la pérdida de la capacidad social para mantener las funciones esenciales de gobierno, especialmente las de brindar seguridad, garantizar el imperio de la ley y proveer los elementos básicos para la subsistencia, tales como la comida y el agua. Los investigadores consideran tres posibles escenarios. En un primer escenario, el colapso social debido al cambio climático se da en localidades aisladas y vulnerables, mientras que otros lugares han sido capaces de adaptarse al impacto climático. En este escenario, el colapso social se da a un nivel local. En un segundo escenario, el colapso se da en áreas más extendidas, incluso a nivel nacional, pero habrá áreas urbanas de gran extensión y gobiernos nacionales que sufrirán los efectos del cambio climático pero que sobrevivirán al mismo. Los autores se refieren a este escenario como “mundo roto”. Finalmente, en un tercer escenario, habrá un colapso global de la civilización. Como mencionan Steel y colaboradores, el impacto del cambio climático sobre la civilización puede ser directo, dando lugar a sequías, inundaciones, calores extremos y otros eventos climáticos que ponen en peligro a la agricultura y a las reservas de agua, al igual que a otros elementos esenciales para la civilización. El impacto puede ser también indirecto, en cuyo caso el cambio climático genera efectos sociales negativos. Por ejemplo, una sequia podría afectar severamente a la producción de alimentos y con esto generar una hambruna e inestabilidad social. Un impacto indirecto también ocurre cuando los impactos climáticos internos, directos o indirectos, generan vulnerabilidad ante amenazas externas; amenazas de guerra o de epidemia, por ejemplo.¿Qué tan real es que se produzca un colapso de la civilización como consecuencia del cambio climático? Como arguyen Steel y colaboradores, si bien algunos expertos se muestran escépticos al respecto, no es posible descartarlo. Así, por ejemplo, la fusión del permafrost por el calentamiento global exacerbaría el cambio climático e incrementarían las probabilidades de un colapso social. Hay que señalar que el permafrost es una capa del subsuelo, fundamentalmente en las regiones frías de hemisferio norte, que se encuentra congelada de manera permanente y que su posible fusión alarma a los expertos, pues liberaría grandes cantidades de gases de invernadero.En todo caso, consideran Steel y colaboradores que la probabilidad de un colapso de la civilización por el cambio climático no ha sido evaluada con la suficiente extensión y profundidad haciendo uso de métodos científicos. En su opinión, esta posibilidad ha sido abordada mayormente por novelistas, filósofos y cineastas, y no por los científicos, y esta situación debería cambiar. Y si así fuera el caso, una vez que se entiendan las causas que han llevado a las civilizaciones al colapso, podríamos hacer uso del conocimiento adquirido para evitar el colapso de la nuestra. Y como bono extra, quizá podamos entender qué fue lo que llevó al colapso de los mayas del sur de Yucatán.",
    "Por ser bien conocido, no nos extrañaría ver a un gorila pararse en dos patas y golpearse repetidamente el pecho con ambos puños. Como lo hace, por ejemplo, King Kong en lo alto del edificio Empire State en Nueva York, o como lo hacen los gorilas salvajes que podemos ver en videos en la Internet. En contraste, posiblemente nos extrañaría ver a un chimpancé hacer lo mismo. Y, sin embargo, se han observado chimpancés golpeándose el pecho imitando a un gorila.  Esto lo menciona un artículo publicado en el último número de la revista iScience por un grupo de investigadores encabezado por Crikette Sans de la Washington University in Saint Louis, en los Estados Unidos. Pero vayamos por partes. En su artículo, Sans y colaboradores reportan los resultados de una investigación llevada a cabo para estudiar a fondo la conocida interacción social que se da entre grupos de gorilas y chimpancés salvajes en áreas compartidas por ambas especies en África central. ¿Qué es lo que pretenden gorilas y chimpancés con esta interacción? Pudiera ser que busquen protección en contra de los depredadores integrando grupos más numerosos con miembros de ambas especies. O bien, mediante la colaboración entre especies, contar con mayores oportunidades para localizar nuevas fuentes de comida.  En cuanto a la primera posibilidad, tanto gorilas como chimpancés responderían a las señales de alarma dadas por un miembro de cualquiera de las dos especies a la vista de un depredador. Igualmente, grupos pequeños de gorilas o chimpancés relativamente inermes se asociarían con grupos de la otra especie en busca de protección.Encuentran Sans y colaboradores que, efectivamente, ambas especies responden a las señales de alarma emitidas por un miembro de la otra especie. Al mismo tiempo, sin embargo, encuentran también que los grupos que incluyen chimpancés y gorilas son más grandes que los grupos formados solamente por chimpancés, lo que contradice que la motivación para la asociación de los chimpancés con los gorilas sea la de buscar protección contra depredadores.En contraste, los investigadores encontraron evidencias que apoyan que la asociación entre gorilas y chimpancés está en gran medida motivada por la mayor oportunidad que esta representa de localizar nuevas fuentes de alimentos. Así, encuentran que 34 por ciento de los contactos documentados entre especies se dieron en un contexto en el que ambas especies se alimentaban de los mismos árboles, mientras que en un 18 por ciento de los casos de alimentaban de fuentes de comida diferentes, pero localizadas en la misma área. Además de lo anterior, la interacción social entre chimpancés y gorilas, que puede ser tanto afectiva como agresiva, proporciona oportunidades para el desarrollo de habilidades sociales, físicas y cognitivas. En este sentido, Sans y colaboradores fueron testigos de juegos entre chimpancés y gorilas que involucran luchas cuerpo a cuerpo, mordidas y golpes.  La interacción, además, crea oportunidades para la transmisión de conocimiento entre especies. La observación de un chimpancé dándose golpes en el pecho al estilo de los gorilas mencionada al inicio de este artículo, es un ejemplo al respecto. Escriben Sans y colaboradores: “También observamos gestos entre especies para iniciar interacciones sociales. Curiosamente, los chimpancés se golpeaban el pecho, un comportamiento característico de los gorilas”.   Como apuntan Sans y colaboradores, una interacción entre especies como la que se da entre chimpancés y gorilas podría haber estado presente entre especies humanas en el pasado remoto. En la actualidad, desde hace unos 40,000 años cuando desaparecieron los neandertales, estamos solos como especie humana sobre la faz del planeta. De haber subsistido los neandertales hasta nuestros días es interesante preguntarnos -de manera ociosa- ¿conviviríamos con ellos como lo hacen hoy chimpancés y gorilas? ¿Podría un niño, por ejemplo, jugar a las “luchas” con su amigo neandertal? No hay manera de saberlo, pero en tal caso el nuestro sería sin duda un mundo raro.",
    "El pasado 19 de septiembre ocurrió un evento inusual en México: un temblor de 7.1 en la escala de Richter con epicentro frente a la costa de Michoacán. Ciertamente, dicho temblor no sería particularmente notable de no haber sido el tercero que ha ocurrido en México con una magnitud superior a 7 en la escala de Richter en un día 19 de septiembre. No hay, por supuesto, ninguna razón para suponer que el 19 de septiembre sea una fecha especial para los temblores, de modo tal que tenemos que concluir que la ocurrencia de tres sismos de gran magnitud en un mismo día del mes de septiembre fue pura coincidencia. Sin ir más allá por lo pronto, pues desafortunadamente todavía no se tienen los suficientes conocimientos científicos para explicar cómo ocurren los terremotos. En relación a esto último, un artículo aparecido el pasado 5 de septiembre en la revista “Geology” describe resultados de un proyecto llevado a cabo frente a las costas de Japón por un grupo internacional de científicos encabezados por Harold Tobin de la Universidad de Washington. El proyecto fue llevado a cabo en el área de la falla de Nankai, frente a la costa japonesa del océano Pacífico. Dicha falla ha originado en el pasado terremotos y tsunamis de gran magnitud. Uno de estos fue el terremoto ocurrido en el año 1498 que generó un tsunami que destruyó el edificio que albergaba al gran buda de Kamakura de más de trece metros de altura, y que desde entonces ha quedado expuesto a la intemperie. Más recientemente, en 1946, ocurrió un terremoto con una magnitud superior a 8 en la escala de Richter que generó un tsunami con olas de cinco metros de altura que destruyó decenas de miles de casas y provocó miles de muertos.El objetivo del proyecto de Tobin y colaboradores fue investigar el estado de tensión en el que se encuentra la roca por debajo del fondo del mar en el área de la falla de Nankai, debida a la interacción entre las placas tectónicas euroasiática y filipina. Como sabemos, el movimiento de una placa tectónica con respecto a la otra genera tensiones y acumula energía que eventualmente se libera generando un terremoto. Los investigadores esperaban encontrar evidencia de un próximo terremoto de gran magnitud, dado que se sabe que la falla de Nankai produce un terremoto de gran magnitud aproximadamente cada cien años y el último generado por dicha falla ocurrió ya hace más de medio siglo.Para llevar a cabo su estudio, los investigadores perforaron el lecho marino hasta una profundidad de tres kilómetros haciendo uso del barco japonés de investigación “Chikyu”, y determinaron la tensión a la que estaba sujeta la roca a partir de la deformación de las paredes del pozo. No lograron, sin embargo, alcanzar la falla de Nankai que se encuentra dos kilómetros más profunda. Para su sorpresa, encontraron un estado de tensión casi cero, sin una indicación de la inminencia de un terremoto, para lo cual no tienen una explicación. Una posibilidad, especulan, es que la tensión esté acumulada a mayor profundidad, cerca de la falla tectónica a la que no llegaron; o bien que la falla necesite acumular una menor energía a la esperada antes de generar un terremoto. Otra posibilidad es que la tensión se generará rápidamente en el futuro antes de su liberación. Confían los investigadores, sin embargo, que estudios como el que han dado a conocer llevará a un mayor conocimiento de los procesos geológicos que dan origen a los terremotos y eventualmente a la capacidad de predecirlos con suficiente antelación.Al margen de la evolución futura del conocimiento que los expertos alcancen sobre la generación de terremotos, no podemos dejar de sorprendernos sobre las capacidades técnicas desplegadas por el grupo de investigación que perforó un pozo de tres kilómetros de profundidad y logró determinar el estado de tensión de la roca ahí abajo, midiendo las deformaciones de las paredes del pozo. Y en ese estado de ánimo, no podemos dejar de confiar en que en un futuro cercano lo expertos alcancen un nivel de conocimiento tal que les permita predecir terremotos con suficiente antelación para que todos podamos ponernos a buen resguardo. Y así podríamos quizá también entender la increíble coincidencia de que se produjeran tres terremotos mayores en México en un mismo día de septiembre.",
    "Un comentario publicado en octubre del año pasado en la revista Pan African Medical Journal, firmado por Sylvain Diop, hizo resurgir una historia que, sin duda, resulta fascinante. Dicha historia fue relatada en la revista “Edinburg Medical Journal” en 1884 por Robert Felkin, y tuvo lugar en 1879 al oeste de la actual Uganda. Felkin, quien era en esos momentos estudiante de medicina, se encontraba en el reino Bunyoro, en donde fue testigo de una operación cesárea practicada a una joven Bunyoro. Relata Felkin: “La paciente era una joven saludable y de buen aspecto, de unos veinte años de edad. Era su primer embarazo….Se le había suministrado abundante vino de plátano y estaba en un estado de semi-intoxicación…El operador se puso de pie en cuanto entré a la cabaña, sosteniendo su cuchillo en alto con su mano derecha y murmurando un conjuro. Después de esto se lavó las manos y lavó el abdomen de la paciente con alcohol de plátano y después con agua”…..”Seguido de esto, procedió a realizar un corte en el abdomen de la paciente y a extraer rápidamente al bebé, cauterizando los puntos de sangrado con un metal al rojo blanco y cerrando la herida con agujas de hierro”. Todo lo anterior fue realizado con una habilidad tal que una hora después de terminada la operación la paciente se veía sin molestia aparente. La operación, además, resultó todo un éxito, con la supervivencia del bebé y con las heridas de la madre completamente cerradas en once días. De acuerdo con la descripción de Felkin, los cirujanos de Bunyoro tenían un conocimiento sofisticado que los llevó superar los problemas que presentan las cirugías: el dolor, las hemorragias, y las infecciones. Así, los cirujanos Bunyoro emborrachaban al paciente para mitigar el dolor, cauterizaban las heridas con un metal caliente, y combatían las infecciones por medio de alcohol.Todo lo anterior es sorprendente si tomamos en cuenta que la cirugía de la época en Europa tenía poco tiempo de hacer uso de los anestésicos y de las prácticas de asepsia. De hecho, fue hasta mediados del siglo XIX cuando se demostró que, mediante el simple procedimiento de lavarse las manos antes de operar, el cirujano disminuía sensiblemente la mortalidad posterior. Antes de esto, los cirujanos ciertamente se lavaban las manos después de la operación, pero no necesariamente antes de la misma.          Si bien sin duda alguna los conocimientos de los cirujanos del reino Bunyoro, desarrollados de manera aislada, son sorprendentes, posiblemente más sorprendentes sean las conclusiones a las que llega un artículo publicado el pasado 7 de septiembre en la revista “Nature” por un grupo de investigadores encabezados por Tim Ryan Maloney de la Universidad Griffith en Australia. En dicho artículo se reporta la evidencia de la amputación quirúrgica de un pie hace 31,000 años, mucho antes de lo que se habría creído posible.Maloney y colaboradores basan sus conclusiones en el descubrimiento, realizado en una cueva en Borneo, de los restos óseos de un niño al que le fue amputado el pie izquierdo. Dicha amputación no fue debida a un accidente sino a una operación cuidadosamente realizada, como lo demuestra el corte limpio y oblicuo de los huesos de la pierna para separar el pie. El niño amputado habría sobrevivido de seis a nueve años después de la amputación.Antes del descubrimiento de Maloney y colaboradores, la amputación quirúrgica más antigua de la que se tenía noticia habría ocurrido hace 7,000 años. Ahora sabemos que, hace 31,000 años existían ya los suficientes conocimientos sobre anatomía, fisiología y prácticas quirúrgicas que permitieron amputar, reparar y desinfectar el pie de un niño, con un éxito tal que logró sobrevivir a la operación por varios años antes de ser inhumado.  A la luz de este descubrimiento, el caso de los cirujanos Bunyoro resulta sin duda más entendible. Al mismo tiempo que nos hace preguntarnos por qué la cirugía en Europa avanzó en un inicio a un ritmo relativamente lento. Si bien ahora ha alcanzado un nivel impresionante.",
    "Como relata Bernal Díaz del Castillo en su “Historia verdadera de la conquista de la Nueva España”, durante la marcha de los conquistadores españoles hacia Tenochtitlán, Diego de Ordás habría obtenido el permiso de Cortés para escalar el volcán Popocatépetl, aparentemente solo para averiguar qué era lo que había allá arriba. Se puso Diego de Ordás en marcha hacia la cima de la montaña acompañado por dos soldados y, según Díaz del Castillo, “…al subir comenzó el volcán a echar grandes llamaradas de fuego y piedras medio quemadas y livianas y mucha ceniza, y que temblaba toda aquella sierra y montaña adonde está el volcán, y que estuvieron quedos sin dar paso adelante hasta de ahí a una hora, que sintieron que había pasado aquella llamarada y no echaba tanta ceniza y humo, y que subieron hasta la boca, que era muy redonda y ancha, y que habría en el ancho un cuarto de legua”.Tiempo después, Cortés habría organizado un segundo ascenso al Popocatépetl, esta vez con propósitos más prácticos.  Ocurrió durante el sitio de Tenochtitlan, cuando los españoles se vieron faltos de pólvora para sus armas de fuego, particularmente de azufre que es un ingrediente indispensable. En estas circunstancias, Cortés habría enviado una expedición a la cima del Popocatépetl para recolectar azufre, que sabía existía en el volcán. La expedición habría sido exitosa dado que, ya después de la caída de Tenochtitlan, Cortés comisionó una segunda expedición al mismo volcán para traer más azufre, según relata Francisco Cervantes de Salazar en su “Crónica de la Nueva España”.Las expediciones de Cortés al Popocatépetl para conseguir azufre constituyen un ejemplo terrestre de lo que, en el contexto de la exploración espacial, se conoce como “Utilización de recursos in situ”, en referencia al uso de materiales extraterrestres que de otro modo tendrían que ser transportados desde la Tierra. La posibilidad de colectar y procesar materiales extraterrestres en lugar de transportarlos desde nuestro planeta a otro planeta o satélite es un punto esencial para la exploración del espacio, dados los altos costos del combustible necesario para llevar materiales al espacio venciendo la gravedad de nuestro planeta.   En este sentido, el 31 de agosto pasado apareció un artículo en la revista “Science Advances” en el que se describen los resultados del proyecto “MOXIE” que busca establecer la posibilidad de producir oxígeno en Marte empleando el dióxido de carbono de la atmósfera marciana. De acuerdo con dicho artículo, MOXIE es parte del explorador “Perseverance” de la NASA que está sobre la superficie de Marte desde febrero de 2021 y que ha logrado producir oxígeno en siete ocasiones, desde febrero de 2021 hasta el final de ese año, bajo condiciones atmosféricas variadas. En cada ocasión MOXIE logró producir seis gramos de oxígeno cada hora, equivalente a la que produce un árbol en la Tierra. Esta cantidad es modesta, pero demuestra la factibilidad de generar oxígeno en Marte empleando materiales marcianos y abre la posibilidad de fabricar combustible para cohetes, necesario para despegar desde la superficie marciana de regreso a la Tierra. Un segundo artículo, publicado el pasado mes de julio en la revista “International Journal of Applied Ceramic Technology” por un grupo de investigadores de la Universidad Estatal de Washington, reporta el desarrollo de materiales a partir de mezclas de aleaciones de titanio con materiales marcianos. Ya que no contamos con materiales marcianos para experimentar, los investigadores emplearon sustitutos terrestres que se les asemejan. En su estudio, los investigadores encuentran que añadiendo a las aleaciones de titanio un cinco por ciento de materiales marcianos obtienen materiales con una mayor dureza, lo que apunta, por ejemplo, a la posibilidad de fabricar en Marte partes de cohetes empleando materiales locales.Podemos concluir que, tal como Hernán Cortés se ayudó del azufre americano para derrotar a los mexicas, el mundo será testigo en un futuro cercano de expediciones espaciales que se ayudaran de materiales extraterrestres. Sin consecuencias funestas -esperemos- equivalentes a las que sufrieron los nativos americanos. Y sin los peligros -también esperemos- de asomarse a un volcán en activo.",
    "El cohete “Saturn V” que llevó a Neil Armstrong y a Buzz Aldrin a la superficie de la Luna tenía un peso cercano a las 3,000 toneladas. Sabemos que dicho cohete no llegó completo hasta la Luna. De hecho, llegaron solamente los módulos, lunar, de servicio y de comando -en el que viajaban los astronautas-, que sumaban una mínima parte de la masa total del cohete. Esto es lo esperado en una misión espacial, dado que, para impulsar a un satélite o a una nave espacial hasta una órbita terrestre y más allá, es necesario emplear enormes cantidades de combustible para vencer la fuerza de gravitación de la Tierra. Dicho combustible es almacenado en cada una las etapas que forman el cuerpo del cohete, las cuales deben desprenderse y desecharse una vez que agotan su combustible. Así, en el caso de la misión “Apollo XI”, las 3,000 toneladas de peso inicial se redujeron a menos de 50 toneladas una vez que la misión entró en una órbita lunar. El costo de poner un satélite en órbita refleja entonces, tanto el combustible empleado para el ascenso, como la inversión en los cohetes de desecho. Esta inversión puede ser astronómica. Por ejemplo, según un artículo aparecido el pasado mes de agosto en la revista “Science”, el costo para poner en órbita terrestre una masa de un kilogramo por medio del transbordador espacial de la NASA era de 65,000 dólares, mientras que los costos correspondientes del “Saturn V” y del nuevo cohete de la NASA son de 5,400 dólares y de 58,000 dólares, de manera respectiva. A diferencia de la exploración espacial de los primeros tiempos que fue impulsada con dinero público, en la actualidad existe una considerable actividad espacial por parte de empresas privadas, entre las que destaca “Space X” de Elon Musk. Esta compañía ha buscado reducir los costos para poner satélites en órbita. Para esto, ha desarrollado la tecnología necesaria para recuperar y reusar los cohetes empleados para este propósito. En este sentido, es posible encontrar en Internet filmaciones espectaculares en las que vemos a un cohete aterrizando suavemente en posición vertical, en tierra o en una plataforma marina, sostenido por sus motores.Por medio del cohete gigante “Starship” -de 120 metros de altura- y empleando la tecnología de recuperación de cohetes, “Space X” espera reducir el costo de colocar a un satélite en órbita a 10 dólares por kilogramo, lo que, según los expertos, cambiaría radicalmente a la industria espacial. En particular, incrementaría sustancialmente el número de satélites en órbita y con esto y todas las aplicaciones que de ellos derivan, incluyendo una observación más detallada de la superficie de la Tierra. De hecho, para lograr una reducción tan drástica en el costo para colocar una carga en órbita, sería necesario un incremento sustancial en el número de lanzamientos al espacio. Así, según Elon Musk, un solo cohete “Starship” podría realizar tres viajes semanales al espacio y con esto poner en órbita 15,000 toneladas, lo que sería casi equivalente a toda la carga que ha sido puesta en órbita en toda la historia espacial.  No todos los expertos, sin embargo, están de acuerdo en que es posible una reducción tan drástica en los costos para colocar satélites en órbita. En este sentido, se ha hecho notar que, a pesar de que “Space X” cuenta con la tecnología para recuperar cohetes, esto no ha impactado mayormente lo que dicha compañía cobra a sus clientes, que es de 2,600 dólares por kilogramo empleando su cohete “Falcon 9”. No es claro, sin embargo, si esto refleja una estrategia comercial de la compañía para mantener el costo de sus servicios, que son los más bajos del mercado.Por lo demás, independientemente del factor por el que “Space X” logre reducir sus tarifas, el ingreso de compañías privadas a la industria espacial la han colocado en una nueva dinámica que pronto nos hará percibir los espectaculares logros de la década de los años sesenta del siglo pasado -que nos llevaron a la Luna- como algo lejano y, ahora sí, del siglo pasado.",
    "Durante el verano del año 1816, la escritora Mary Shelly y su esposo coincidieron con el poeta británico Lord Byron y con su médico personal John Polidori, en una villa cercana al lago Ginebra en Suiza. Debido al mal tiempo imperante -1816 fue un año particularmente frío, conocido incluso como el “Año sin verano”-, los ocupantes de la villa se vieron obligados a mantenerse por tres días sin poder salir a pasear. Para matar el aburrimiento, según cuenta la anécdota, Lord Byron retó a los Shelly y a Polidori, a escribir una historia de terror.  Fue una buena idea la de Lord Bayron, pues si bien el único que terminó la historia fue Polidori, Mary Shelly concibió durante su encierro la novela “Frankenstein o el moderno Prometeo” que la haría famosa.    Como sabemos, en su novela Shelly relata la historia de Víctor Frankenstein, estudiante de medicina de la Universidad de Ingolstadt, quien se interesó en la química y por la fisiología del cuerpo humano y que se obsesionó por descubrir el secreto de la vida. Para este propósito, Frankenstein hizo experimentos uniendo partes de cadáveres hasta fabricar un “engendro” al que logró darle vida. No proporciona Shelly detalles de la fabricación de dicho “engendro”. En particular, no sabemos si utilizó la electricidad para darle vida como se muestra en la película de Boris Karloff de 1931, pero intuimos que así haya sido dado que se trataba de algo altamente novedoso en esos momentos.De un modo u otro, al margen de los recursos que habría imaginado Shelly para crear a su “engendro”, la reanimación de la materia muerta estaba muy lejos de las posibilidades de la tecnología hace 200 años. De hecho, todavía lo está ahora, aunque pareciera ser que los especialistas han dado pasos importantes en esa dirección como lo demuestra un artículo publicado al inicio del presente mes de agosto en la revista “Nature” por un grupo de investigadores encabezados por David Andrijevich de la Escuela de Medicina de Yale, en los Estados Unidos.En efecto, en dicho artículo se reportan resultados de experimentos durante los cuales se indujo un paro cardiaco a un grupo de cerdos, para posteriormente buscar revivirlos. Como explican Andrijevich y colaboradores: “Después de que cesa el flujo de sangre o de exposiciones isquémicas equivalentes, comienza en las células de los mamíferos una dañina cascada de procesos moleculares que eventualmente las llevan a su muerte”. No obstante, en el caso que nos ocupa y en contra del deterioro natural, después de una hora de que cesaron los latidos del corazón, los investigadores pudieron reactivar en seis horas el funcionamiento de algunos de los órganos de los animales, como el cerebro y el corazón, que incluso mostró cierta actividad eléctrica y de contracción. Para lograr esto, emplearon la tecnología “OrganEx”, patentada por el grupo de Yale, por medio de la cual, les fue administrado a los cerdos un coctel formado por su propia sangre y 13 sustancias más, incluyendo anticoagulantes y supresores del sistema inmunitario. Los resultados de Andrijevich y colaboradores no implican que los cerdos volvieron a la vida después de estar muertos por una hora. De hecho, no se pudo detectar una actividad cerebral que indicara que los animales habían recobrado la consciencia. No obstante, dichos resultados podrían apuntar hacia otro tipo de consecuencias. Por ejemplo, se podría anticipar un incremento en el número de órganos disponibles para trasplante. Al mismo tiempo, por otro lado, si resulta que el deterioro de los tejidos orgánicos puede ser revertido aun después de la muerte tal como ahora la entendemos, la distinción entre la vida y la muerte se vuelve difusa, lo que introduce también otro tipo de consideraciones. Así, por ejemplo, se tendría que redefinir que es lo que significa un estado de muerte cerebral irreversible y los criterios para declarar muerta a una persona y si, en dado caso, es candidato para donar sus órganos.En conclusión, a dos siglos de que Mary Shelly en un frio día de verano concibiera un “engendro” de más de dos metros de altura -que posteriormente fuera bautizado con el nombre de su creador- no tenemos aun la tecnología necesaria para hacerlo realidad y no es más que un personaje de películas y obras de ficción. Así, “Frankenstein o el moderno Prometeo” seguiría siendo por lo pronto la primera novela de ciencia ficción, como algunas veces se le califica. ¿Por cuánto tiempo lo seguiría siendo? La fecha es incierta pero posiblemente no demasiado lejana.",
    "Suponga usted que está tratando de conciliar el sueño, cuando repentinamente un mosquito empieza a zumbar alrededor de su oído con intenciones de alimentarse a costa suya. En esos momentos difíciles -a menos que sea usted fan de las maravillas del reino animal-, posiblemente no tendría disposición para reflexionar y agradecer a la naturaleza que hubiese fabricado un mosquito chupa sangre con tantas habilidades, incluyendo la capacidad de anticipar un manotazo y volar rápidamente para ponerse a salvo, lo mismo que la capacidad de detectar a la distancia a una presa recluida en su habitación.En momentos menos incomodos, esperaríamos, quizá pudiéramos estar de mejor humor para admirar las habilidades de los mosquitos. Particularmente su agudo sentido del olfato que es el que los guía hasta nuestra habitación, atraídos por el dióxido de carbono que exhalamos al respirar y por los olores que despedimos por la piel. Por el lado negativo, los mosquitos, además de molestos, son trasmisores de un número de enfermedades, incluyendo Dengue, Zika y Chikungunya, y en este sentido es importante entender detalladamente sus mecanismos olfatorios para desarrollar estrategias de defensa. Por ejemplo, para elaborar sustancias que atraigan a los mosquitos hacia una trampa, o bien para desarrollar repelentes más efectivos. Nuestro sentido del olfato está basado en neuronas con receptores olfativos colocados en la mucosa de la cavidad nasal. Cada neurona responde a la presencia de una molécula de olor y transmite la información a un centro de recepción localizado en el bulbo olfativo mediante una prolongación que atraviesa el hueso, a través de pequeñas perforaciones, localizado en la parte superior de la cavidad nasal. Un aspecto relevante es que cada receptor responde a un tipo particular de olor, de tal manera que si se deshabilitaran los receptores de un tipo se perdería la sensibilidad al olor correspondiente.¿Podría desarrollarse una estrategia de defensa contra los mosquitos enfocando nuestras baterías en contra de sus receptores sensibles a los olores corporales humanos? Desafortunadamente, tal parece que habría ciertas dificultades. Al menos de acuerdo con un artículo aparecido esta semana en la revista “Cell”, publicado por un grupo internacional de investigadores encabezado por Margaret Herre de la Universidad Rockefeller en Nueva York.Según los resultados del estudio de Herre y colaboradores, al contrario de la mayor parte de los animales, los receptores olfatorios de los mosquitos pueden responder a varios olores y no solamente a uno específico. Así, aun si se suprimen familias de receptores sensibles a los olores humanos, los mosquitos son todavía capaces de encontrar su camino hacia su fuente de alimento. Para sorpresa de los investigadores, el sistema olfatorio de los mosquitos es más maleable de lo esperado, y esto estaría relacionado con la dificultad que encontramos para librarnos de los mosquitos. Es decir, éstos tendrían varios caminos para llegar a Roma, de modo tal que, si uno o varios fallan, siempre les quedarán otros practicables.Así, a pesar de su tamaño, tal pareciera que los mosquitos son rivales más sofisticados de lo que hubiéramos pensado en primera instancia, que han desarrollado una estrategia de receptores redundantes que les asegura encontrar el camino hacia su alimento. Al mismo tiempo, sin embargo, no esperaríamos que a la larga salgan indemnes de los embates de los investigadores, los cuales ya han logrado averiguar algunos de sus detalles íntimos, particularmente acerca de su sistema olfatorio. Esto no es alentador para los mosquitos, pues facilitará el desarrollo de técnicas sofisticadas para combatirlos.Por lo pronto, en tanto dichas técnicas hacen su aparición, más nos conviene emplear procedimientos más tradicionales en contra de los mosquitos. Por ejemplo, la de cerrar puertas y ventanas cuando anticipemos que nos pueden invadir. So pena de pasar la noche en vela.",
    "Excepto en ciertos casos, por ejemplo, cuando nos aplicamos un ungüento para curar alguna quemadura o nos aplicamos gotas en los ojos irritados, por citar algunos, las medicinas o drogas que empleamos para curar enfermedades no se aplican directamente en el lugar afectado, sino que son introducidas al cuerpo por alguna vía, con la intención de que se difundan y eventualmente lleguen a su destino.  Visto de este modo, el procedimiento parece ineficiente y con la desventaja de que no solamente la parte enferma está expuesta a la acción de la droga, sino que también lo están otras partes sanas del cuerpo, con potenciales efectos secundarios.Para atender este problema, los investigadores están desarrollando robots minúsculos que puedan ser introducidos al cuerpo cargados con cantidades microscópicas de medicamentos altamente concentrados. Una vez en el interior, los robots avanzarían hasta la zona enferma en donde depositarían su carga de drogas. La enfermedad podría ser entonces atacada en forma local y más eficiente, minimizando la afectación a otras partes del cuerpo.Lo anterior parece, ciertamente, de ciencia ficción y en cierto modo lo es. No obstante, numerosos grupos de investigación alrededor del mundo están avanzando en la construcción de micro y nano robots para este tipo de aplicaciones. En este sentido y a manera de ejemplo, en el número del 15 de junio pasado de la revista de divulgación científica “Scientific American” se describe un micro robot del tamaño de un chícharo, que constituye un avance de robot para administrar medicamentos en el interior del cuerpo. Dicho robot fue creado por investigadores de la Universidad Stanford en California. Está inspirado en el arte origami de papel y puede deformarse al igual que lo hacen las figuras de origami. El robot puede ser guiado en su camino por un campo magnético y para esto se le añade un imán minúsculo en uno de sus extremos. La interacción de este imán con el campo magnético proporciona al robot la fuerza necesaria para desplazarse. Un segundo imán colocado en el extremo opuesto del robot, le permite comprimirse o extenderse como un acordeón. Así, mediante una expansión el robot se cargaría con el medicamento, mismo que, después de ser transportado hasta el lugar de la aplicación, sería liberado mediante una compresión.Los expertos, sin embargo, además de pretender construir minúsculos robots capaces de moverse en el interior del cuerpo guiados por una fuerza magnética o por otro medio, son aún más ambiciosos y buscan que éstos se muevan de manera autónoma, imitando el movimiento de una bacteria. Para esto, el robot tiene que estar dotado con un cierto grado de inteligencia que le permitiera optar por el mejor camino para llegar a la meta preestablecida. Deberá sortear, además, obstáculos inesperados en un medio ambiente cambiante.  En esta dirección, un artículo publicado en línea el pasado mes de junio en la revista “Communications Physics” reporta el desarrollo de un modelo de micro robot inteligente con la capacidad de desplazarse de manera autónoma en el interior del cuerpo a través de fluidos corporales. El artículo fue publicado por un grupo de investigadores encabezados por Zonghao Zou, de la Universidad de Santa Clara en California.Habría que señalar que Zou y colaboradores no construyeron físicamente un micro robot, sino los algoritmos de inteligencia artificial que lo dotaron de capacidad para aprender, por sí solo, a moverse hacia un punto determinado, en un fluido real y siguiendo las leyes de la física. Para esto, los investigadores idealizaron al micro robot como tres esferas del mismo radio unidas por dos barras. El robot podía deformase, cambiando o rotando la longitud de las barras que unían a las esferas, y con esto avanzar, detenerse o rotar. Podía, además, evaluar la situación en la que se encontraba y, de ser necesario, cambiar de rumbo para alcanzar la meta, aun en situaciones cambiantes que implicaban obstáculos no programados. El robot, además, se adaptó al medio ambiente y sus obstáculos inesperados sin ayuda externa; es decir, aprendió a nadar por sí mismo.Si bien los micro robots dispensadores de medicinas no están, ciertamente, a la vuelta de la esquina, la intensa actividad que se está llevando a cabo alrededor del mundo para desarrollarlos nos indican que no están tampoco demasiado lejos. Y pudieran ser no solamente robots “tontos” que tengan que ser llevados de la mano para hacer su tarea, sino robots inteligentes que se las arreglen sin ayuda externa.",
    "¿Qué tuvo que ver el rey Enrique VIII de Inglaterra con el cambio climático? Dado que el reinado de Enrique VIII - conocido en no poca medida por haber decapitado a dos de sus seis esposas- y el cambio climático están separados en el tiempo por casi quinientos años, es difícil encontrar una conexión directa entre ambos acontecimientos. No obstante, existe una cierta conexión como se discute a continuación.Como sabemos, a raíz de su divorcio de Catalina de Aragón para casarse con Ana Bolena, Enrique VIII rompió con la Iglesia Católica y que este rompimiento lo llevó a confiscar los bienes de los monasterios. Esto liberó tierras con depósitos de carbón que pudieron así explotarse en forma más eficiente. Como consecuencia, la disponibilidad de carbón, aunada a la escasez de madera como combustible, llevó a la sustitución paulatina de la leña por el primero. Dos siglos después, como también sabemos, se desarrolló en Inglaterra una revolución industrial que estuvo basada en el carbón como fuente de energía. Así, en forma retardada, Enrique VIII contribuyó al auge de la quema de carbón, que aun hoy subiste, y que ha contribuido de manera sustancial a la contaminación atmosférica y al cambio climático.Habría que reconocer, no obstante, que el carbón combustible ha tenido también efectos positivos, dado que el incremento en la generación de energía -a la que se han sumado muchas otras fuentes, aparte del carbón- y el progreso de la civilización están íntimamente relacionados. Y es en este contexto, en el que se están llevando a cabo grandes esfuerzos para desarrollar fuentes de energía menos contaminantes que aquellas basadas en combustibles fósiles.Por otro lado, habría que señalar que, hasta fechas muy recientes, hemos dependido en gran medida del Sol como fuente de energía. No solamente la madera que se usó como combustible desde tiempos inmemoriales y que es producto de la actividad solar -a través de la fotosíntesis-, sino también los combustibles fósiles, producto de vegetales sepultados hace cientos de millones de años. El viento, que ha sido usado en el pasado y sobre todo en el presente como una fuente de energía, es también, en último término, producto de la actividad solar. Igualmente lo son la energía gravitacional contenida en una represa que es usada para generar electricidad en una planta hidroeléctrica y la energía que produce un panel solar.No toda la energía que consumimos, sin embargo, proviene del Sol. Es el caso de la energía nuclear, que se obtiene mediante la desintegración controlada de ciertos elementos químicos. También es el caso de la energía geotérmica que aprovecha el calor del interior de la Tierra para generar energía eléctrica o energía en forma de calor. En este sentido, habría que recordar que la temperatura del subsuelo aumenta con la profundidad.  Una planta de electricidad geotérmica puede aprovechar fluidos a alta temperatura que de manera natural emergen de las profundidades de la Tierra. A falta de esto, los ingenieros y científicos están desarrollando técnicas para inyectar agua al subsuelo por un pozo hasta alcanzar un nivel de roca caliente, para después extraerla como agua a alta temperatura por un segundo pozo. Si la temperatura del agua extraída es lo suficientemente elevada -mayor a los 100 grados centígrados-, el vapor generado puede ser usado para mover una turbina y un generador de electricidad. Todo esto, por supuesto, asumiendo que el agua inyectada por el primer pozo es canalizada por el segundo pozo hasta la superficie, lo cual no está garantizado pues podría fugarse por grietas no detectadas.  En un artículo aparecido este mes de julio en la sección de noticias de la revista “Science”, firmado por Warren Cornwall, se describe un proyecto financiado con 218 millones de dólares por el Departamento de Energía de los Estados Unidos, el cual pretende desarrollar técnicas para inyectar agua al subsuelo y extraerla con una temperatura lo suficientemente elevada para generar electricidad. El proyecto se está llevando a cabo el desierto de Utah, y de tener éxito, se daría un gran paso para el desarrollo de una fuente renovable de energía, prácticamente inagotable, no contaminante, y de gran magnitud. En este respecto, los investigadores calculan que bajo el subsuelo de los Estados Unidos hay la suficiente roca caliente para generar cinco veces la energía eléctrica consumida por este país. Además, a diferencia de las energías solar y eólica, la fuente geotérmica sería independiente del Sol, de las intermitencias características de dichas energías, y sin la potencial contaminación de una planta nuclear.Dicho todo lo anterior, habría que ser indulgente con Enrique VIII por su contribución a contaminar el planeta, que tuvo, después de todo, un aspecto positivo, y que confiamos esté en vías de mitigación. Todo lo contrario, sin embargo, en lo que respecta a su manía de decapitar esposas.",
    "En enero de 2019, en medio del vórtice polar que azotó al medio oeste norteamericano, el entonces presidente Trump recomendaba a los ciudadanos que fueran cuidadosos y trataran de permanecer de ser posible dentro de sus casas. Según Trump, “Gran parte del país está sufriendo con una tremenda cantidad de nieve y de un frío casi récord. Es increíble lo grande que es este sistema.” A estas afirmaciones, difícilmente rebatibles, añadió el expresidente: “No estaría mal tener un poco de ese buen calentamiento global pasado de moda”.  Con respecto a esto último, habría que mencionar que, si bien a quienes sufrían los rigores del invierno de 2019 con seguridad no les hubiera caído nada mal un poco de calor, lo cierto es que, desafortunadamente, el calentamiento global no está pasado de moda, y de hecho, está muy presente y no hay necesidad de invocar su presencia. Por ejemplo, según los expertos, está presente en la actual ola de calor que está afectando a Europa y a otras partes del mundo, la cual fue muy probablemente disparada por el cambio climático. Y lo mismo podemos decir de la ola de calor que sufrió el mundo el pasado verano, y de otros eventos climáticos extremos -olas de calor, huracanes, sequías, etc.- que se han hecho cada vez más frecuentes. Dudemos por un momento de los especialistas -como lo hace Trump- y afirmemos que el cambio climático no es real y que los expertos mienten por alguna razón. Lo podríamos hacer, pero recordemos que, por lo general, tendemos a confiar en los expertos aun en contra de nuestras propias percepciones. Aceptamos, por ejemplo, que la Tierra es redonda, a pesar de las evidencias obtenidas con nuestros ojos que nos llevarían a concluir que es plana. Confiamos, no obstante, en los expertos que, de una manera u otra, han llegado a la conclusión que es redonda desde tiempos lejanos. Hace más de dos mil años, por ejemplo, Eratóstenes incluso logró determinar el radio de la Tierra midiendo la longitud de la sombra que proyectaba una estaca de la misma longitud colocada en Asuán y en Alejandría, en Egipto, el mismo día de solsticio de verano. Hoy, más allá de Eratóstenes, tenemos evidencias abrumadoras de que la Tierra es redonda, lo que admitimos como una verdad, a pesar de que dichas evidencias por lo general nos llegan solo de oídas y no por experiencia directa.Dicho lo anterior, habría que preguntarnos si los expertos tienen suficiente evidencia de que el cambio climático es real y, en dado caso, si dicho cambio está asociado a nuestras actividades. Primeramente, habría que recordar que los científicos del clima relacionan al calentamiento global con el incremento de la concentración de gases de invernadero en la atmósfera, particularmente de dióxido de carbono. Dicha concentración ha sido monitoreada de manera continua desde 1958 en el observatorio de Mauna Loa en Hawai. Las mediciones muestran un aumento anual continuo en el contenido de dióxido de carbono en la atmósfera, el cual ha crecido en 33 por ciento entre 1958 y 2021. Los expertos encuentran también que dicho aumento está ligado a la emisión de gases de invernadero, fundamentalmente por la quema de combustibles fósiles.Se sabe, por otro lado, que la temperatura de la superficie de la Tierra depende de un equilibrio entre la energía que recibe del Sol y el calor que es radiado por dicha superficie de regreso hacia el espacio. El dióxido de carbono en la atmósfera absorbe este calor y lo regresa parcialmente de regreso a la superficie de la Tierra, que de esta manera ve incrementada su temperatura; en mayor medida en cuanto más alta sea la concentración de este gas en la atmósfera.Así, la prueba definitiva de que la quema de combustibles fósiles es uno de los factores clave que está impulsando el cambio climático son las mediciones de la temperatura de la superficie terrestre que, si bien tiene variaciones positivas o negativas de un año a otro, en promedio tiene una tendencia creciente. Así, en los últimos dos siglos dicha temperatura ha aumentado en aproximadamente un grado centígrado.Según los expertos, si bien no se entiende de manera detallada cómo el calentamiento global dispara los eventos extremos, no se tiene ninguna duda de que existe una relación entre ambos fenómenos, que se esclarecerá en la medida en que los modelos del clima se hagan más sofisticados.Por lo demás, existen suficientes evidencias para creer en que el calentamiento global es real, y si bien muchas de estas evidencias son de naturaleza técnica y difícil de entender para los no especialistas -como en su momento fueron los experimentos de Eratóstenes-, con seguridad el calentamiento global será de aceptación universal en el futuro cercano. Después de todo, si el animal grazna como pato, camina como pato y se comporta como pato, entonces posiblemente sea un pato.",
    "Para muchos de quienes vivimos en la ciudad, ir al campo en una noche estrellada resulta toda una experiencia por la enorme cantidad de estrellas que brillan en el cielo y que en la ciudad son apantalladas por las luces artificiales. Ciertamente, en la ciudad podemos observar claramente estrellas, lo mismo que planetas como Marte o Júpiter. La interferencia de las luces nocturnas, sin embargo, nos hace difícil apreciar el cielo en todo su esplendor y complejidad. En el pasado las noches eran sensiblemente más oscuras -la luz eléctrica se inventó recientemente, hace poco más de un siglo- y teníamos un mayor contacto con estrellas y planetas. De hecho, el cielo ha sido observado con detalle por miles de años y sus características visibles han sido determinadas con precisión. Por ejemplo, por mucho tiempo hemos sabido que las estrellas tienen un movimiento de rotación alrededor nuestro, mientras que el movimiento que distingue a los planetas es sensiblemente más complicado.No obstante, aun en una noche estrellada en completa oscuridad, las limitaciones de nuestros ojos como detectores de luz nos imponen barreras para apreciar el firmamento en toda su complejidad. En este sentido, y de manera afortunada, los telescopios han venido en nuestro auxilio y nos han revelado un firmamento cada vez más complejo en la medida en que ha crecido su poder de resolución.Sabemos, por ejemplo, que cuando Galileo Galilei apuntó su telescopio hacia Júpiter descubrió cuatro pequeños puntos luminosos a su alrededor que cambian diariamente de posición. Galileo interpretó correctamente que se trataba de satélites que orbitaban alrededor de Júpiter -ahora conocidos como satélites galileanos- de manera similar a como los planetas orbitan alrededor del Sol. Esto apoyaba la posición del Sol como centro alrededor del cual giraban los planetas, lo que simplificaba considerablemente nuestra visión del mundo.Todo lo anterior viene a colación por la publicación esta semana de las primeras fotografías, a todo color, del nuevo telescopio James Webb de la NASA que son espectaculares. En este sentido, y para ser justos, habría que recordar que dicho telescopio fue diseñado para trabajar con radiación infrarroja y que, con la excepción del color rojo, la cámara del telescopio es ciega a colores como el verde y el azul. Las fotografías presentadas, en consecuencia, son en falso color. Dicho esto, habría que reiterar que las imágenes resultan, sin duda alguna, espectaculares, con una resolución que en mucho supera a la de otros telescopios.En particular, podemos constatar la superioridad el nuevo telescopio con respecto al telescopio Hubble de la NASA -que es su antecesor- en publicaciones de Internet en donde se comparan imágenes obtenidas con uno y otro telescopio de la misma región del espacio. En efecto, es claro que las imágenes del telescopio James Webb son más nítidas y muestran detalles que son difícilmente distinguibles en las imágenes del Hubble. Uno de los factores que determinan la superioridad de nuevo telescopio con respecto a su antecesor es su tamaño. Así, mientras que el espejo primario del telescopio Hubble tiene un diámetro de 2.4 metros, el correspondiente espejo del James Webb está formado por un conjunto de 18 segmentos hexagonales que hacen un diámetro de 6.5 metros. En estas condiciones, el telescopio James Webb intercepta seis veces más radiación que el Hubble y por tanto es capaz de detectar objetos más lejanos. De manera adicional, el telescopio Hubble está especializado para operar con radiación visible y ultravioleta, mientras que el James Webb fue diseñado para operar fundamentalmente con radiación infrarroja, la cual puede penetrar nubes de polvo cósmico que, de otro modo, oscurecen lo que está detrás de las mismas. Así, el nuevo telescopio de la NASA promete avances sustanciales en el conocimiento del Universo, incluyendo detalles de su nacimiento hace más de 13,000 millones de años, y las posibilidades de algunos planetas extrasolares de albergar vida. Todo esto en paralelo con un esfuerzo de relaciones públicas de la NASA para publicitar los logros de un proyecto que costó la friolera de 10,000 millones de dólares. Y con respecto a esto último, habría que reconocer que las imágenes del nuevo telescopio dadas a la publicidad no podrían haber sido más afortunadas.Por lo demás, las imágenes del nuevo telescopio, con todo y sus colores falsos, nos proporcionan visiones del universo, que hubieran sido imposibles de concebir mirando al cielo con los ojos desnudos sin la ayuda de instrumentos que amplíen sus capacidades. Sin mencionar la regresión que hemos tenido en el último siglo de luz eléctrica, que nos ha hecho perder contacto con el firmamento.",
    "Como es sabido, durante su viaje de cinco años alrededor del mundo en el bergantín HMS Beagle, Charles Darwin llevó a cabo una serie de observaciones de la fauna y la flora de los lugares visitados, las cuales le sirvieron de base para escribir y publicar en 1859 el Origen de la Especies, una obra controversial y muy influyente. El viaje se inició en Plymouth, Inglaterra, en diciembre de 1831, tocando varios puntos de Sudamérica, penetrando tanto desde la costa este como desde el litoral oeste, después de cruzar por el estrecho de Magallanes.  En particular, en las islas Galápagos de Ecuador, Darwin observó pinzones, claramente emparentados con pinzones de continente americano, con picos de diferentes formas que se adaptaron mediante mutaciones al tipo específico de alimento que consumían. Para explicar el mecanismo de adaptación de una especie al medio ambiente en el que habita, Darwin ideó la teoría de la selección natural. Según esta teoría, un organismo al reproducirse hereda sus rasgos a sus descendientes. Para que esto suceda, sin embargo, debe sobrevivir hasta la edad reproductiva, lo cual es contingente a lo bien o mal adaptado que esté al medio en el que vive, dado que los recursos naturales disponibles son limitados. Si tiene una buena adaptación logrará reproducirse y perpetuar sus rasgos; de otro modo, dichos rasgos se perderán. De este modo, con el transcurrir de las generaciones los rasgos de los individuos mejor adaptados prevalecerán en detrimento de aquellos con menor adaptación.  Así, sí la fuente de comida para los pinzones de las islas Galápagos fueran semillas duras, la selección natural desarrollará individuos con picos cortos y fuertes, en perjuicio de pinzones con picos largos y delgados, más adecuados, quizá, para una alimentación a base de insectos.  En estas condiciones, la selección natural suprime mutaciones peligrosas para la supervivencia y desarrolla individuos con rasgos adaptados a su medio ambiente que les permiten llegar a la edad reproductiva. En contraste, para la selección natural no sería relevante la suerte que dichos individuos pudieran sufrir una vez que sobrepasen esta edad. Así, pudiéramos esperar que el fin de la vida de un individuo coincida con el fin de la edad reproductiva, a partir de la cual sería víctima de mutaciones peligrosas para las cuales no tuvo defensa.Con relación a nuestra propia especie, a bote pronto esto último no deja de ser deprimente; es decir, desde el punto de vista de la evolución de nuestra especie, más allá de la edad reproductiva no tendríamos más tareas que realizar en este mundo. Pensándolo dos veces, sin embargo, notamos que, en contraste con otras especies -incluso con los chimpancés, la más cercana a nosotros- la vida de las mujeres se extiende mucho más allá de su edad reproductiva. Y dado que esto no es solamente por los avances médicos que la han extendido de manera considerable, cabe preguntarse por qué vivimos tanto tiempo “extra” sin una ventaja evolutiva aparente. Encontramos una respuesta en un artículo aparecido esta semana en la revista “Proceedings of the National Academy of Sciences” de los Estados Unidos, publicado por Raziel Davison y Michael Gruven de la Universidad de California en Santa Barbara. En dicho artículo, Davison y Gruven consideran la influencia intergeneracional que tienen los abuelos sobre la reproducción de las hijas y por consiguiente sobre la transferencia de sus propios rasgos a los nietos. Consideraron los investigadores sociedades de cazadores recolectores en las cuales los adultos mayores contribuyen a la generación de recursos para el sostenimiento y cuidado de los infantes, y de esta manera aligeran la carga que estos representan para los padres, que así pueden producir algunos hijos más. En este sentido, hay que considerar que, en los humanos, los hijos son dependientes por un periodo inusualmente largo antes de que empiecen a generar sus propios recursos.Concluyen Davison y Gruven, por otro lado, que los adultos mayores a partir de una edad de aproximadamente 70 años disminuyen su productividad y consumen más recursos que los que generan. Aun en esta situación, sin embargo, los adultos mayores pueden ejercer una fuerza evolutiva intergeneracional, pedagógica o de transferencia de información, enseñando a los nietos habilidades para que alcancen una mayor productividad.  De acuerdo con Davison y Gruven, los abuelos y adultos mayores tienen una influencia en la evolución y adaptación de nuestra especie al medio ambiente, no directa, como es el caso de los padres, pero sí indirecta a través de la transferencia intergeneracional de recursos y conocimientos. Y como tal, cumplen con una tarea en el mundo. Aun después de la edad reproductiva.",
    "Suponga que, a bordo de una máquina del tiempo, un viajero retrocede un millón de años hacia el pasado aterrizando en medio de un bosque con un frío que cala los huesos. A la distancia alcanza a ver a un individuo de la especie “Homo erectus” dado a la tarea de encender una fogata para calentarse -y posiblemente también para prepararse la comida-. El individuo en cuestión toma dos pedazos de madera que frota rápidamente uno contra el otro, hasta que, con el calor generado, logra prender fuego a un montón de paja y con éste a una pira de leña. Más allá de la imposibilidad de viajar hacia el pasado, ¿qué tan factible es que el viajero del tiempo pudiera ser testigo de una escena, hace un millón de años, en la que un humano primitivo enciende una fogata? Cualquiera que haya intentado iniciar un fuego sin cerillos o sin un encendedor estará posiblemente de acuerdo en que no resulta una tarea sencilla, por más que en Internet encontramos grabaciones en video que nos quieren convencen de que, con la suficiente práctica y con un experto que nos enseñe, podríamos lograrlo de manera relativamente rápida. El hecho es que dichas grabaciones nos muestran -por si hiciera falta- que, para los humanos primitivos, no habría sido simple aprender a encender un fuego desde cero, y que al hacerlo demostraron una gran capacidad imaginativa y considerables esfuerzos de prueba y error -sin videos demostrativos y sin nadie que los guiara.¿Cuándo se aprendió a manejar el fuego? Según los expertos, existen amplias evidencias del uso del fuego desde hace unos 200,000 años. Más allá de los 500,000 años antes del presente, en contraste, las evidencias son escasas. Esto podría significar que el uso del fuego se desarrolló hasta hace unos pocos cientos de miles de años, o bien que las técnicas analíticas para estudiar épocas tan remotas no son lo suficientemente sensibles.Con relación a esto último, un artículo publicado esta semana en la revista “Proceedings of the National Academy of Sciences” de los Estados Unidos, en el que se reporta el desarrollo de una técnica para determinar si una determinada roca estuvo sujeta a un calentamiento de cientos de grados centígrados por exposición al fuego. El artículo fue publicado por un grupo internacional de investigadores encabezados por Zane Stepka del Instituto Weizmann de Ciencias de Israel.La técnica reportada por Stepka y colaboradores hace uso de luz ultravioleta y de un algoritmo de inteligencia artificial para detectar los cambios a nivel molecular que ocurren en el interior de un material expuesto a altas temperaturas. La técnica es sensible aun si el material bajo estudio no muestra rasgos visibles de la acción del fuego que pudieran haberse desvanecido por el trascurrir del tiempo. Es decir, revela lo que está oculto a simple vista.Los investigadores aplicaron su técnica a materiales provenientes del sitio arqueológico Evron Quarry en Israel, que está datado entre 800,000 años y un millón de años antes del presente. Dicho sitio incluye fósiles de diferentes animales y objetos de piedra. Stepka y colaboradores aplicaron su técnica a 26 herramientas de piedra que no muestran signos visuales de haber sufrido una exposición al fuego, encontrando que habían sido expuestos a una temperatura superior a 600 grados centígrados. De manera adicional, un análisis de 87 restos animales empleando luz infrarroja, reveló que un colmillo de elefante también presentaba evidencia de haber sido sujeto a una alta temperatura.  Reconocen Stepka y colaboradores que, sobre la base de sus descubrimientos, no pueden concluir de manera categórica que los humanos primitivos hacían uso del fuego hace un millón de años, pues la exposición a altas temperaturas pudiera deberse a causas naturales, dado que Evron Quarry es un sitio arqueológico al aire libre. No obstante, el hecho que en un mismo sitio se hayan encontrado herramientas de piedra y restos animales que fueron sujetos a altas temperaturas, constituye una indicación, no conclusiva, del uso del fuego en épocas muy tempranas, comprendidas dentro del rango temporal del “Homo erectus”. En todo caso, Stepka y colaboradores, señalan que la técnica por ellos desarrollada podría ayudar a datar con evidencias sólidas el inicio del uso del fuego, acontecimiento que algunos consideran fue clave para la aparición de nuestra especie.Así, hasta nuevo aviso, habríamos de ser cautos antes de asegurar que, de retroceder un millón de años en el tiempo, nos pudiéramos encontrar con un miembro de la especie “Homo erectus” en el acto de encender una fogata. De lo que sí podríamos estar seguros es de la capacidad cognitiva de los humanos primitivos. Hace cientos de miles y posiblemente un millón de años.",
    "Una vez que los europeos se convencieron de que América no era Asia y de que para llegar de Europa a Catay, navegando hacia el suroeste, había que rodear al nuevo continente por el estrecho de Magallanes, españoles e ingleses buscaron afanosamente un paso por el norte, el famoso Paso del Noroeste, que constituyera un ruta más corta. Durante los siglos XVI y XVII se creía en la existencia del estrecho de Anían, un canal en el norte del continente americano que conectaría a los océanos Atlántico y Pacífico. Exploradores como Juan de Fuca a finales del siglo XVI, por ejemplo, afirmaron haberlo visto. Hoy en día sabemos que el estrecho de Anían reportado por de Fuca es solamente un canal que rodea la isla de Vancouver. Pronto quedó claro, sin embargo, que no había un Paso del Noroeste a través del continente americano y que este posiblemente se encontraba al norte del mismo, a través de los hielos árticos. Así, en el siglo XIX se organizaron expediciones para encontrarlo, algunas de la cuales resultaron trágicas. Es el caso de la expedición de 1845 comandada por John Franklin que quedó atrapada en el hielo, resultando muertos Franklin y los 128 miembros de la tripulación. Mejor suerte tuvo el noruego Roald Amundsen, quien en 1906 condujo al primer velero que logró cruzar desde el océano Atlántico al océano Pacífico a través del Ártico canadiense.Hoy en día transitar por el Paso del Noroeste se ha simplificado gracias a la tecnología naval de que disponemos, muy superior a la que contaban los pioneros exploradores del océano Ártico. Pero no solamente por eso, sino también por una circunstancia que es a la vez desafortunada y afortunada: el cambio climático que está produciendo una disminución del volumen de los hielos árticos. Así, el Paso del Noroeste es cada vez más practicable, lo mismo que la ruta marítima a través del ártico ruso que conecta igualmente los océanos Atlántico y Pacífico. La disminución de los hielos árticos está abriendo nuevas posibilidades para navegación marítima. En este sentido, habría que notar que la ruta entre Yokohama y Rotterdam a través del Paso del Noroeste es 37 por ciento más corta que la correspondiente ruta a través del canal de Suez, y los mismo sucede con otras rutas entre Asia y Europa. Una ruta que pase cerca del polo norte, que se sería practicable en algún momento, representaría ahorros de distancias todavía mayoresSe espera así un incremento sustancial en la actividad en el océano Ártico. Y en este contexto, un artículo aparecido esta semana en la revista “Proceedings of the National Academy of Sciences” de los Estados Unidos, alerta sobre las implicaciones que tendrá dicho incremento. El artículo fue publicado por un grupo de investigadores encabezado por Amanda Lynch de la Universidad Brown, en Providence, Rhode Island.Como hacen notar Lynch y colaboradores las rutas árticas no son tan activas como cabría esperar. Esto es debido, entre otros factores, a que son aún riesgosas por la variabilidad de las condiciones climáticas, y a que tienen una limitada cobertura satelital y capacidad de pronóstico de hielo. De manera adicional, las rutas árticas resultan caras debido a las reglamentaciones impuestas por Rusia para la navegación por sus rutas árticas. Con relación a esto, Lynch y colaboradores hacen notar que Rusia impone fuertes restricciones para la navegación marítima a lo largo de su costa ártica, lo que dificulta grandemente el tráfico marítimo. El control impuesto por Rusia está basado en una resolución de la Convención de las Naciones Unidas sobre el Derecho del Mar de 1982, según el cual los países con costas cerca de las rutas marinas tienen la capacidad de regular el tráfico marítimo de la ruta, siempre que el área permanezca cubierta de hielo la mayor parte del año, lo cual será cada vez más infrecuente.De este modo, la disminución del volumen de hielo en el Ártico llevará a un incremento en el tráfico marítimo, tanto por el desbloqueo de rutas como por la pérdida de control de dicho tráfico que experimentará Rusia en la medida en que disminuya el volumen de hielo del Ártico y las rutas marinas se alejen de su costa ártica.  En estas condiciones, Lynch y colaboradores alertan sobre las implicaciones legales, medioambientales y geopolíticas que tendrá la desaparición de los hielos árticos, y en la necesidad de reglamentar el incremento en la actividad en el Ártico que será inevitable en los años por venir. Y no solamente por el mayor volumen de mercancías transportadas, sino también por un incremento en el turismo y en la explotación de los recursos mineros y de combustibles fósiles de la región.Así, unos 500 años después, el Paso de Noroeste estará en plena actividad.",
    "La única arpa construida por Antonio Stradivari data de 1681, y es actualmente parte de la colección del museo del Conservatorio San Pietro a Majella de Nápoles. Se trata de un arpa pequeña, con una caja de sonido de 75 centímetros de longitud y una columna de 93 centímetros que lleva la inscripción: “Ant: Stradivarius Cremonen. F. 1681”. En su parte superior, el arpa está adornada con dos figuras esculpidas de sirenas, una de ellas con un cupido montado a sus espaldas, y con una máscara en la base de la columna. Si bien dicha arpa como instrumento musical no es particularmente notable, de acuerdo con un artículo aparecido el pasado mes de mayo en la revista “Dendrochronologia”, proporciona evidencia que indica que Antonio Stradivari aprendió la técnica de construcción de instrumentos de cuerda en el taller de Nicolo Amati. El artículo fue publicado por un grupo de investigadores encabezado por Bernabei Mauro, del CNR-IBE National Research Council, en Trento Italia.Como sabemos, Nicolo Amati fue miembro de una famosa familia -tercera generación- de constructores de violines en los siglos XVI y XVII establecida en la ciudad de Cremona, en el norte de Italia. Amati era unos 40 años mayor que Stradivari, y dado que este último era también originario de Cremona, resulta natural concluir que fue su aprendiz. No hay, sin embargo, pruebas documentales confiables en este sentido y la única evidencia al respecto es la inscripción: “Antonius Stradivarius Cremonensis Alumnus Nicolaij Amati, Faciebat Año 1666”, grabada en uno de los primeros violines fabricados por Stradivari. ¿Fue Stradivari aprendiz de Amati? Con el objeto de responder a esta pregunta, Mauro y colaboradores llevaron a cabo un análisis dendrocronológico del arpa de referencia. En referencia a este tipo de análisis, habría que recordar que el corte transversal de un árbol puede mostrar anillos concéntricos, con colores claros y oscuros alternados, y que en estos anillos está escrita la historia del crecimiento del árbol. Es decir, la variación del color y del grosor de los anillos refleja los cambios climáticos a lo largo del año, de modo que cada par de anillos corresponden a un año en la vida del árbol. Así, la edad de éste puede ser estimada contando el número de anillos de su tronco.  Para llevar a cabo su análisis, Mauro y colaboradores fotografiaron la placa sonora del arpa, hecha de madera de abeto, con una cámara de alta resolución y determinaron su patrón de anillos a lo largo de 150 años. Una primera conclusión a la que llegaron fue que, excepto por una pequeña área en una esquina inferior, la placa fue fabricada a partir de una sola pieza de madera.Haciendo una búsqueda en una base de datos dendrocronológicos de instrumentos de cuerda, Mauro y colaboradores encontraron que el patrón de anillos del arpa de Stradivari, fabricada en 1681, coincide con el correspondiente patrón de un violonchelo fabricado por Nicolo Amati dos años antes. La conclusión es que ambos instrumentos fueron fabricados a partir de una misma pieza de madera.A partir de este resultado se presentan dos posibilidades. Una de ellas es que, tanto el arpa como el violonchelo hubieran sido fabricados en el taller de Amati, lo cual implicaría que Stradivari fue efectivamente su aprendiz, como frecuentemente se asume. Cabría, no obstante, una segunda posibilidad: que los dos instrumentos hubieran sido fabricados por Amati y Stradivari de manera independiente a partir de piezas de madera provenientes de un mismo tronco de árbol. En este sentido, dado que tanto Amati como Stradivari vivían en la misma ciudad, no resulta descabellado pensar que compraban sus materiales a un mismo comerciante de madera.  Así, con su investigación Mauro y colaboradores aportan datos que apoyan la idea que Stradivari fue aprendiz de Amati. Los resultados, sin embargo, no son conclusivos y se requiere de trabajo adicional para corroborarlo. ¿Qué tan importante es conocer los detalles del entrenamiento de Stradivari en el arte de fabricar violines? Como sabemos, Antonio Stradivari es el constructor de violines más famoso que haya existido y algunos de sus instrumentos pueden alcanzar hoy en día precios de muchos millones de dólares. Sabemos también que los violines Stradivarius guardan secretos de fabricación que no han podido ser descifrados. En estas circunstancias, resulta importante -e interesante- averiguar cómo Antonio Stradivari alcanzó tal nivel de perfección en su arte.  Después de todo, los Antonio Stradivari no se dan en maceta.",
    "El desarrollo de los teléfonos móviles, que entre sus múltiples funciones están la de tomar fotografías y grabar videos con sonido, han hecho públicos -a través de la red Internet- eventos que se asumían eran privados, o que habían ocurrido en la vía pública, pero en la presencia de un reducido número de testigos. Los teléfonos móviles, aunados a otros dispositivos de grabación de imagen y sonido son así una especie de ojos y oídos con la capacidad de dar a conocer todo lo que oyen y ven a millones de personas.  Lo anterior viene a colación por un artículo aparecido esta semana en la revista “Enviromental Science and Technology Letters”, en el que se reporta la detección de una fuga gigante de metano en la plataforma petrolera Zaap-C de Pemex estacionada en la sonda de Campeche. Dicho artículo fue publicado por un grupo de investigadores encabezado por Itziar Irakulis-Loitxate de la Universidad Politécnica de Valencia, España.Pero vayamos por partes. Habría primeramente que recordar que, después del dióxido de carbono, el metano es el gas de invernadero más importante. En efecto, si bien tiene un tiempo de residencia en la atmósfera sensiblemente menor que el correspondiente tiempo del dióxido de carbono, se estima que el metano es un gas de invernadero 25 veces más potente que el dióxido de carbono en un periodo de 20 años. Se estima, además, que el metano es el responsable del 20 por ciento del calentamiento global que sufre el planeta.Por otro lado, habría también que recordar que en los campos petrolíferos se da la práctica de quemar el gas natural -mayormente metano- que se libera durante la extracción de petróleo. Esta práctica es muy criticada por llevar a una pérdida de recursos energéticos, además de que genera dióxido de carbono que se emite a la atmósfera. Se mantiene, no obstante, por razones económicas.Por lo demás, se estima que la industria del petróleo y el gas es responsable de alrededor del 30 por ciento de las emisiones de metano a la atmósfera y en este sentido preocupan a los especialistas los llamados eventos de ultra emisión, durante los cuales se liberan grandes cantidades de este gas. Estos eventos ocurren de manera esporádica, por accidente o de manera intencional por razones de mantenimiento de las instalaciones. En este escenario, los expertos consideran que es indispensable llevar un monitoreo de los campos petrolíferos para detectar ultra emisiones de metano, para lo que existen varias posibilidades. Una de éstas hace uso de fotografías satelitales.¿Cómo puede una imagen de satélite indicar la presencia de emisiones de metano? Para entenderlo, habría primeramente que considerar que las imágenes a emplear no son tomadas utilizando luz visible, sino con “luz” infrarroja, que es invisible para nosotros. Habría también que tomar en cuenta que el metano absorbe mayormente luz infrarroja con ciertos “colores que le son particulares, mismos que constituyen su huella distintiva. Tomando todo esto en cuenta, por medio de fotografías tomadas empleando luz infrarroja con los “colores” para los cuales el metano es absorbente, es posible deducir la presencia de una columna de metano por su silueta impresa en la fotografía.    Irakulis-Loitxate y colaboradores emplearon fotografías infrarrojas tomadas por dos satélites empleando la luz solar infrarroja reflejada por la superficie del mar, a partir de las cuales pudieron detectar que hubo tres ultra emisiones gigantes de  metano en la plataforma Zaap-C los días 8, 18 y 24 de diciembre 2021. Averiguaron también que en el periodo entre el 8 y el 27 de diciembre la quema del gas natural asociado al petróleo, que es una práctica corriente en dicha plataforma, se suspendió la mayor parte del tiempo. Encontraron así una coincidencia temporal entre la suspensión de la quema de gas y la ocurrencia de ultra emisiones de metano, lo que los lleva a concluir que, ambos eventos están relacionados y que “….este evento de ultra emisión -que está probablemente relacionado con condiciones anormales del proceso en el sitio, por ejemplo,  fallas en el funcionamiento o problemas con el equipo, y que ha resultado que en una cantidad sustancial de gas ventilado a través de la columna-  es un incidente único y con la duración más larga desde la actividad de la quema de gas comenzó en esta plataforma”.Por nuestra parte, podemos quizá concluir que la existencia de cámaras fotográficas no solamente nos ha robado privacidad -en un sentido amplio del término- sino que también es capaz de dar una difusión amplia a eventos que de otro modo hubieran sido menos conocidos. Claro que las cámaras fotográficas con que cuentan nuestros teléfonos móviles son considerablemente más simples que las cámaras infrarrojas satelitales. Fuera de esto, no obstante, juegan un papel similar en cuanto a la pérdida de privacidad.",
    "Gregorio Samsa, protagonista de la novela “La metamorfosis” de Franz Kafka, se despertó una mañana, tras un sueño intranquilo, “convertido en un monstruoso insecto”. No especifica Kafka qué clase de insecto, pero se especula que pudo haber sido un escarabajo, o bien una cucaracha como segunda posibilidad. Aunque, pensándolo bien, podría haber sido ninguno de los dos insectos si hemos de atenernos a las leyes de la física que harían altamente improbable la existencia de un insecto de un metro de altura, capaz de hacer girar con las mandíbulas el picaporte de una puerta como lo describe Kafka, que además disfrute de caminar por paredes y techos. Las leyes de la física, por supuesto, pasan a segundo plano en la impresionante novela de Kafka en la que el protagonista sufre una transformación que causa repulsión. En este sentido, Gregorio de transforma en algo así como un escarabajo o una cucaracha, y no, por ejemplo, en un conejo o un perro, animales que es menos probable que resulten repugnantes.Posiblemente estemos de acuerdo que las cucarachas son particularmente repulsivas y por esta razón nos esforzamos por acabar con ellas. Por más que esto sea particularmente difícil, pues ellas y sus antecesores han estado presentes en el planeta desde hace 350 millones de años.  Sirva lo anterior como introducción al tema que quisiéramos tratar hoy: la inusual y rápida evolución que están teniendo las cucarachas caseras por el uso de los insecticidas empleados para combatirlas. Esta evolución está reportada en un artículo publicado el pasado mes de mayo en la revista “Communications Biology” por un grupo de investigadores encabezado por Ayako Wasa-Katsumata de la Universidad Estatal de Carolina del Norte, en Raleigh. Según reportan Wasa-Katsumata y colaboradores, las sustancias empleadas para el control de plagas de cucarachas contienen insecticidas mezcladas con glucosa lo que ha desarrollado cucarachas con una aversión a la glucosa como un medio de defensa. Hacen notar, además, que recientemente se ha observado que las cucarachas macho tienen menos éxito cuando intentan aparearse con hembras con aversión a la glucosa que con hembras sin esta aversión. En estas condiciones, aventuraron una hipótesis en el sentido que existe una relación entre estos dos hechos y se propusieron llevar a cabo un estudio para confirmarla.  Para este propósito, llevaron a cabo una serie de experimentos de apareamiento de cucarachas, tanto con aversión a la glucosa como sin ella. Habría que señalar que, durante el cortejo previo al apareamiento, el macho ofrece a las hembras secreciones ricas en diferentes azúcares y las mantiene ocupadas disfrutando del manjar, lo que le da tiempo para vencer su resistencia y llevar a cabo el apareamiento. Los investigadores, sin embargo, encontraron que, en contraste con las hembras sin aversión a la glucosa, las hembras con dicha aversión no encontraron placentero el consumo de los azúcares contenidos en las secreciones del macho, pues su saliva los descompuso produciendo glucosa. En estas condiciones, la hembra tendió a reaccionar negativamente y a separarse del macho sin darle tiempo de completar el apareamiento. Las estrategias de apareamiento de las cucarachas macho, desarrolladas para hembras sin aversión a la glucosa, son entonces menos eficientes con hembras que han desarrollado dicha aversión. Adicionalmente, los investigadores encontraron que las cucarachas macho que han desarrollado aversión a la glucosa producen secreciones nupciales con azúcares que más difícilmente producen glucosa al mezclarse con la saliva de las hembras, en comparación con aquellos machos que no han desarrollado dicha aversión. Así, tienen más éxito en sus apareamientos con hembras igualmente con aversión a la glucosa. Según Wasa-Katsumata y colaboradores, sus resultados muestran “….como un rasgo gustativo adaptativo puede emerger rápidamente bajo una “selección natural” impuesta por humanos, y que este rasgo emergente crea desajustes en la comunicación sexual”. Podemos concluir que combatir a las cucarachas, que han habitado el planeta por cientos de millones de años, es ciertamente una tarea difícil, y que su capacidad de adaptación se ilustra por los resultados de Wasa-Katsumata y colaboradores. Por otro lado. si bien las cucarachas nos resultan repulsivas, no es del todo claro la razón para que así sea, pues son relativamente inofensivas en comparación con otros insectos. Y visto así, el que Gregorio Samsa se haya convertido en un monstruoso insecto, que bien pudiera haber sido una cucaracha gigante, es quizá punto menos que injusto para la especie.",
    "El pasado jueves, un grupo de investigadores encabezado por Gabriele Scorrano de la Universidad de Roma, publicó un artículo en la revista “Scientific Reports” en el que se reporta, por vez primera, la secuencia genómica de una víctima de la erupción del volcán Vesubio. Como sabemos, en el año 79 d.C. el Vesubio hizo erupción sepultando a las ciudades de Pompeya y Herculano, situadas a orillas del golfo de Nápoles, y matando a unos dos mil de sus habitantes.  Desde el siglo XVIII, cuando se iniciaron las excavaciones de Pompeya y Herculano, ambas ciudades han resultado fascinantes, pues al quedar repentinamente sepultadas por metros de cenizas y rocas volcánicas se congelaron en el tiempo, proporcionando una estampa de primera mano de la vida en esa parte del mundo en un día hace casi dos mil años.   Un testimonio escrito de lo sucedido ese día nos lo da Plinio el Joven, sobrino e hijo adoptivo de Plinio el Viejo. En el momento de la erupción del Vesubio, Plinio el Viejo comandaba la flota romana en Miseno, localidad situada en un extremo del golfo de Nápoles. Plinio el Joven, quien igualmente se encontraba en Miseno, escribe en referencia a su tío, un sabio con conocimientos enciclopédicos sobre un gran número de temas: “El 24 de agosto, cerca de la una de la tarde, mi madre le pidió que observara una nube que apareció de un tamaño y forma muy inusual (que luego se descubrió provenía del Vesubio) …. Este fenómeno le pareció a un hombre de tanto conocimiento e investigación como mi tío, extraordinario y digno de profundizar. Mandó preparar una embarcación ligera y me dio licencia, si quería, para acompañarlo. Dije que prefería seguir con mi trabajo”.A la postre resultó que la decisión de Plinio el Joven habría sido más sabía que la de su tío, pues éste, en su afán de profundizar sus conocimientos sobre la nube, terminó muerto, envenenado por la misma. Según Plinio el Joven, su tío: “Instantáneamente cayó muerto; sofocado, según conjeturo, por algún vapor grueso y nocivo, habiendo tenido siempre una garganta débil, que a menudo estaba inflamada”.Al margen del testimonio de Plinio el Joven, sabemos que una buena parte de los habitantes de Pompeya y Herculano murieron asfixiados o quemados por los flujos de gases y materiales volcánicos que avanzaron a gran velocidad y con temperaturas de cientos de grados centígrados, barriendo a las ciudades de manera súbita.Dada la exposición de los cuerpos a altas temperaturas, que lo mismo ocurrió al aire libre que en el interior de los edificios, los científicos dudaban que pudiera recuperarse material genético útil de los restos corporales. Scorrano y colaboradores, sin embargo, demostraron que sí es posible recuperar material genético que proporcione información valiosa.   La investigación se basó en dos esqueletos encontrados en lo que habría sido el comedor de la llamada “Casa del Artesano” en Pompeya. Uno de los esqueletos se encontró recostado sobre los restos de un triclinio, una especie de diván que los romanos usaban en el comedor -tenían la costumbre de comer acostados-, con el brazo y la pierna izquierda descansando en el suelo, y el brazo y pierna derechas sobre el diván. El segundo esqueleto se encontró sentado en el suelo, con los brazos recogidos enfrente de la cabeza, con ambos pies ladeados hacia la derecha, y con la espalda apoyada en otro triclinio. La posición en que se los encontró indica que ambos fueron sorprendidos por el flujo de gases volcánicos y que no intentaron huir. Los investigadores determinaron que el esqueleto encontrado sobre el triclinio perteneció a una persona del sexo masculino, con una edad entre 35 y 40 años, mientras que aquel encontrado sobre el suelo corresponde a una mujer con una edad superior a los 50 años. Si bien los restos de la mujer no proporcionaron suficiente información para un análisis genético completo, sí pudo determinarse que el hombre tenía un perfil genético consistente con la población de Italia central en la época romana, y que sus ancestros probablemente llegaron de Asia menor durante la edad neolítica. De manera adicional, un análisis del esqueleto masculino arrojó evidencia de que padecía tuberculosis espinal, lo que es apoyado por el hecho de que los investigadores encontraron ADN del patógeno de la tuberculosis en dicho esqueleto.  Scorrano y colaboradores concluyen que, si bien las altas temperaturas a las que fueron sometidos los cuerpos encontrados en Pompeya pudieron haber destruido su material genético, al mismo tiempo, el hecho de que hayan sido sepultados en cenizas mitigó el deterioro de dicho material por el contacto con el oxígeno del aire.  Así, existen esperanzas de que Pompeya nos proporcione una fotografía cada vez más precisa de una ciudad hace dos milenios y de la catástrofe que acabó con ella. Más allá de la información limitada que nos proporciona el único relato del que disponemos.",
    "En la medida en la que nos hacemos cada vez más dependientes de las energías solar y eólica para combatir la crisis climática se hace más notoria su naturaleza intermitente. Y con esto, la necesidad de desarrollar métodos para almacenarla en grandes cantidades para las horas sin sol o viento. Existen diferentes posibilidades para almacenar energía. Podemos, por ejemplo, usar la energía del sol para elevar un volumen de agua hasta una cierta altura y almacenar energía aprovechando la fuerza de gravitación. Durante la noche podremos usar la energía almacenada para accionar una turbina y con esto un generador de electricidad, tal como se hace en una planta hidroeléctrica. Podemos también almacenar energía en aire comprimido, es decir, en aire que ha sido sometido a presión por medio de un compresor accionado con energía solar o eólica. También lo podemos hacer en volantes puestos a rotar a grandes velocidades. O bien por métodos no mecánicos, en un fluido calentado a una alta temperatura por medio de la radiación solar, por ejemplo.Ciertamente, existen muchos métodos y dispositivos que pueden emplearse para almacenar energía. Entre estos, no obstante, destacan las baterías recargables, que almacenan energía en la forma de energía química en una celda electrolítica. Un ejemplo particularmente notable son las baterías de litio empleadas en los teléfonos móviles, las computadoras y tabletas, y los automóviles eléctricos. Habría que señalar que las baterías de litio son también empleadas para el almacenamiento masivo de energía. Es el caso del banco de baterías de litio Moss Landing de San Francisco, California, el más grande del mundo, que es suficiente para proveer de energía a 50,000-100,000 casas habitación por cuatro horas. Por otro lado, según los especialistas, si bien las baterías de litio son la mejor opción para instalaciones relativamente pequeñas, para el almacenamiento masivo de energía hay mejores posibilidades. De manera específica, las baterías de flujo serían más adecuadas para esta aplicación. Para entender la diferencia entre las baterías de litio y las de flujo, habría que recordar que las primeras generan la electricidad a partir de una reacción química que ocurre en una celda electroquímica, y que todos los elementos y sustancias químicas que necesita para su operación están alojados dentro de un solo contenedor sellado. En las baterías de flujo, en contraste, si bien también dependen de reacciones químicas que ocurren en una celda electrolítica, la energía se almacena en líquidos que se alojan en contenedores externos, y que se hacen circular hacia la celda electrolítica durante los procesos de carga y descarga de la batería.Es claro que con estas características una celda de flujo no es adecuada para, por ejemplo, un teléfono móvil, que requiere de una batería compacta y sellada. Por otro lado, para aplicaciones en las que se busca almacenar grandes cantidades de energía, el espacio ocupado por las baterías de almacenamiento juega un papel menor. De hecho, para estas aplicaciones las baterías de flujo tienen algunas ventajas sobre las de litio. En efecto, mientras que para aumentar por un cierto factor la potencia de un banco de baterías de litio es necesario incrementar el número de baterías por el mismo factor, para tener un incremento similar de potencia en un banco de baterías de flujo, solamente se necesita aumentar por dicho factor el volumen de los líquidos en los que se almacena la energía, sin modificar el tamaño de la celda electrolítica.    Las baterías de flujo tendrían además una ventaja adicional sobre las de litio, en el sentido de que en su construcción podrían emplearse materiales más abundantes que el litio y con una baja contaminación ambiental. La compañía norteamericana ESS, por ejemplo, fabrica baterías de flujo en las que emplea hierro, sal y agua para el líquido que almacena la energía. Según ESS, su tecnología puede producir baterías con bajo impacto ambiental, con elementos no tóxicos y sin peligro de explosión, como es el caso de las baterías de litio. Igualmente, la compañía afirma que sus baterías tienen un tiempo de vida de 25 años y que pueden operar más allá de las cuatro horas continuas características de las baterías de litio.De todo lo anterior, tal pareciera que las baterías de flujo, particularmente las que emplean hierro sal y agua, serán la solución para almacenar la energía del Sol y del viento. Habría, por supuesto, que esperar para ver si se cumple esta predicción. Por lo pronto, baterías que trabajen con hierro, sal y agua, ciertamente resultan una opción vistosa.",
    "Las drogas alucinógenas y la carrera espacial, dos elementos característicos de la década de los años sesenta del siglo pasado, están volviendo por sus fueros. Como recordamos, la carrera espacial se llevó a cabo en el marco del conflicto conocido como Guerra Fría, que se dio entre los Estados Unidos y la Unión Soviética al término de la Segunda Guerra Mundial. En un inicio, la Unión Soviética adelantó a los Estados Unidos al convertirse en el primer país en colocar en 1957 un satélite artificial en órbita terrestre, lo mismo que en llevar a cabo el primer vuelo orbital tripulado en 1961. Los Estados Unidos, no obstante, al final superaron a la Unión Soviética al convertirse en el primer país -único hasta el momento- en llevar en 1969 a un astronauta a la superficie de la Luna. Recordamos, por otro lado, que la década de los años sesenta vio un auge en el consumo recreativo de drogas alucinógenas, naturales y sintéticas, como parte del llamado movimiento contracultural. Una de estas drogas es el LSD, que fue sintetizado por el químico suizo Albert Hofmann en 1938. Las drogas alucinógenas, por ejemplo, están presentes en la música de los Beatles a partir de su álbum “Revolver” de 1966. Igualmente, se ha dicho -aunque Lennon lo negó- que las letras iniciales del título de la canción “Lucy in the Sky with Diamons” corresponden al acrónimo LSD. Timothy Leary es una las de figuras destacadas en la década de los años sesenta en torno al consumo del LSD. Leary fue catedrático de la Universidad de Harvard en donde desarrollaba su interés en el uso del LSD para tratar problemas psiquiátricos. Leary, sin embargo, no era un académico convencional y sus intereses iban más allá de los meramente propios de la academia. En este sentido, un artículo aparecido el pasado 6 de mayo en el diario “The New York Times” bajo la firma de Nina Burleigh hace remembranzas de Leary y sus veranos psicodélicos, llevados a cabo en Zihuatanejo en 1962 y 1963. Leary, creó la Federación Internacional de la Libertad Interior que organizó retiros de verano en el Hotel Catalina de Zihuatanejo. En dichos retiros, grupos de unos 50 norteamericanos, experimentaron el efecto de drogas alucinógenas. Según Burleigh, en la información entregada a los participantes se podía leer: “El objetivo de la comunidad transpersonativa es liberar a los miembros de sus redes para que puedan volar, a voluntad, a través del espacio infinito de su conciencia o a través del tiempo/espacio infinito de los campos de energía que los rodean”.  No es de sorprender que en estas circunstancias Harvard hubiese prescindido de los servicios de Leary, quien, para hacer peor las cosas, no asistía a impartir sus cursos. Por lo demás, el centro de entrenamiento psicodélico de Zihuatanejo no duró más allá del verano de 1963 por restricciones del gobierno mexicano. Posteriormente, en 1965, Leary fue arrestado en los Estados Unidos por posesión de marihuana recibiendo una sentencia de 30 años de cárcel. No fue a la cárcel, pues salió con libertad condicional. En 1970, sin embargo, fue nuevamente condenado por posesión de marihuana y está vez enviado a prisión. Estando encarcelado, Leary escribió el libro “Starseed” en el que hace conexión con otro de los elementos característicos de la época: el espacio extraterrestre. En dicho libro, Leary trata sobre la construcción de una nave espacial capaz de transportar a 5,000 personas para viajar a través del espacio para hacer contacto con civilizaciones extraterrestres y de esta manera avanzar en nuestra evolución como especie. Así, tenía Leary interés en dos tipos de viajes, hacia el interior de la mente, por medio de las drogas alucinógenas, y hacia el espacio exterior por medio de naves espaciales.  Hoy en día nos es claro que la carrera espacial y el uso de drogas alucinógenas que florecieron hace medio siglo, menguaron con el transcurso del tiempo. Ciertamente, la exploración del espacio no se ha suspendido en todos estos años, pero sin duda perdió el impulso de la década de los años sesenta. El LSD, por su lado, fue declarada una sustancia ilegal. La situación está cambiando, sin embargo. Así, tenemos que la carrera espacial ha adquirido un nuevo impulso con la entrada al negocio espacial, tanto de compañías privadas, como de países como China que mantiene robots exploradores en Marte y en la cara oscura de la Luna. De la misma manera, se ha reactivado el interés en el uso de las drogas alucinógenas para el tratamiento de problemas psiquiátricos. Así, están tomando nuevamente impulso dos elementos que caracterizaron a la década de los años sesenta. Hace 50 años Leary concibió un descocado proyecto para viajar al espacio en conjunción con viajes al interior con sustancias alucinógenas. Hoy en día, podríamos preguntarnos si dichas sustancias podrían ser útiles de una manera diferente a la imaginada por Leary: para aliviar los problemas psicológicos que aquejarán a los viajeros espaciales del futuro, quienes permanecerá aislados por largo tiempo.",
    "El pasado 15 de abril, la BBC estrenó en el Reino Unido su documental “Dinosaurs: The final day”, el cual trata sobre la extinción de los dinosaurios por el impacto de un asteroide. El documental, que será transmitido por la televisión pública estadounidense el próximo 11 de mayo con el título “Dinosaur Apocalypse”, es excelente y presenta al tema de manera muy atractiva, si bien algunos puntos de vista expresados en el mismo han sido motivo de controversia científica. Para entender esta controversia, habría que recordar que hace unos 66 millones de años un asteroide del tamaño del monte Everest, con una velocidad posiblemente superior a los 30,000 kilómetros por hora, impactó a nuestro planeta cerca de la actual costa de Yucatán. Como resultado, se liberó una cantidad de energía de tal magnitud que cambió a la Tierra para siempre. Así, la caída del asteroide marcó el fin del periodo Cretácico y el inicio del periodo Paleógeno, la llamada frontera KPg. Esta frontera significó la extinción de los dinosaurios y el despegue de los mamíferos en la Tierra. El impacto del asteroide está bien documentado lo mismo que los efectos que habría tenido en nuestro planeta, incluyendo la destrucción de todas las especies vivientes en un radio de 1,500 kilómetros alrededor del sitio de impacto, así como la emisión de gases y partículas a la atmósfera que se habrían dispersado por todo el planeta, y que habrían bloqueado por años la luz solar interrumpiendo el proceso de fotosíntesis. Como resultado, se habría extinguido un 75 por ciento de las especies en la Tierra.  Habría que señalar que el documental de la BBC se centra en fósiles datados en la frontera KPg, descubiertos por el paleontólogo norteamericano Robert DePalma en un sitio bautizado como Tanis, localizado en el estado norteamericano de Dakota del Norte. De acuerdo con DePalma, dichos fósiles corresponden a animales que fueron víctimas directas de la caída del asteroide y en ese sentido la conclusión es extraordinaria. De manera específica, en el documental de la BBC se muestra la pata fosilizada de un tescelosauro -un dinosaurio con un tamaño de 2.4-4 metros, según la Wikipedia-, con la piel claramente visible, un pterosaurio bebé a punto de salir del huevo, una tortuga atravesada por una estaca, y peces con tectitas -pequeñas esferas de vidrio- alojadas en las branquias y muertos en posiciones poco usuales.  Habría que señalar que en la frontera KPg, el centro del territorio norteamericano estaba atravesado por un mar interior que se comunicaba con el golfo de México, y que los fósiles desenterrados en Tanis correspondían tanto a animales terrestres como acuáticos.Según DePalma, los fósiles de Tanis muestran evidencia de animales que fueron muertos de manera súbita, el mismo día que impactó el asteroide. Para explicar cómo un evento ocurrido a 3,000 kilómetros de distancia pudo actuar tan rápidamente, DePalma arguye que al caer el meteorito generó un sismo de gran magnitud que a su vez generó una elevación súbita del nivel del mar cerca de Tanis, lo que ocasionó que el agua penetrara a gran velocidad en tierra arrastrando animales marinos y sepultándolos en lodo juntamente con los animales terrestres que encontró a su paso. Al mismo tiempo, el material eyectado hacia arriba a gran velocidad por el impacto del asteroide, o bien escapó de la atracción gravitacional terrestre, o bien regresó a la superficie alcanzando una elevada temperatura por el rozamiento con la atmósfera.  Así, el material se habría fundido formando las tectitas que fueron encontradas en las branquias de los peces que habrían muerto sofocados. Y todo esto, la ola de agua y la lluvia de tectitas habrían ocurrido poco después del impacto del meteoritoHay expertos, sin embargo, que son escépticos de las conclusiones de DePalma, y antes de aceptarlos prefieren esperar a que publique sus hallazgos de manera formal en una revista científica, después de que sean revisados críticamente por otros expertos. Este es el procedimiento normal para que un resultado científico alcance a una aceptación amplia. DePalma, sin embargo, no es dado a seguir las prácticas usuales y en primera instancia hace públicos sus hallazgos por medios no formales, como es el caso del documental de la BBC. Y como es también el caso de un artículo publicado en 2019 en la revista “The New Yorker” sobre el mismo tema. A quienes somo legos en la materia, los resultados presentados en el documental de la BBC nos parecen impresionantes y los argumentos convincentes. Pero ¿Es realmente posible que podamos saber con una precisión de horas lo que pasó hace 66 millones de años? ¿Y que podamos enterarnos de víctimas de una catástrofe tan remota? Es decir, victimas que nacieron en el Cretácico y murieron en el Paleógeno. De un modo u otro, habría que reconocer que la posibilidad resulta fascinante.",
    "El Sol es uno de los actores principales en el proceso de transición energética por el que estamos atravesando, lo que resulta natural, pues es el Sol, en último término, la fuente de energía de la que depende la vida superior del planeta. Aprender a hacer uso de la energía del Sol para un propósito definido, sin embargo, no ha sido una tarea sencilla. Uno de los primeros ejemplos en este sentido tuvo lugar en el sitio de Siracusa por los romanos, en el año 212 antes de la era cristiana. Recordamos que, en esa ocasión, Arquímedes participó de manera activa en la defensa de la ciudad, creando toda suerte de dispositivos defensivos. Uno de estos dispositivos fue un conjunto de espejos que se empleaban para dirigir la radiación solar hacia los barcos romanos con el propósito de prenderles fuego. No es claro, sin embargo, si, los defensores de Siracusa habrían logrado destruir barcos romanos por esta técnica, o si se trata de una mera leyenda.En todo caso, es interesante señalar que las ideas de Arquímedes para destruir barcos romanos tienen puntos de contacto con una técnica que se emplea en la actualidad para aprovechar la energía del Sol. Esta técnica consiste en dirigir la radiación solar por medio de un conjunto de espejos hacia un depósito que contiene un fluido. Como resultado, el fluido alcanza una alta temperatura y puede ser empleado para mover una turbina que a su vez acciona a un generador de energía eléctrica. Esta manera de aprovechar la energía del sol, no obstante, no es la más extendida. La manera más común de hacerlo es por medio de paneles solares, también conocidos como paneles fotovoltaicos, que convierten directamente la energía solar en energía eléctrica sin pasos intermedios. Este medio de aprovechar la radiación solar está avanzando a pasos acelerados. En este sentido, durante la semana que hoy termina nos enteramos por los medios de comunicación que la empresa Sun Power, con base en Singapur, había sometido a la autoridad correspondiente una declaración de impacto ambiental por la proyectada construcción en Australia de lo que será la mayor planta de paneles fotovoltaicos del mundo. Dicha planta tendrá una potencia de hasta 20 gigawatts y estará localizada en el Territorio del Norte, aproximadamente a la mitad del camino entre la ciudad de Alice Springs en el centro de Australia y la ciudad de Darwin en su costa septentrional.  Para apreciar el tamaño de la planta solar proyectada, habría que mencionar que se extenderá en un área de 12,000 hectáreas, incluirá 28 millones de paneles solares y baterías de almacenamiento para que el flujo de energía no se interrumpa durante la noche y tendrá un costo de 22,000 millones de dólares. Será unas veinte veces más grande que la planta solar Villanueva, en el estado de Coahuila, que se encuentra entre las 10 más grandes del mundo. La planta Villanueva cubre un área de 2,600 hectáreas y cuenta 2.3 millones de paneles solaresPor otro lado, dado que la planta solar se localizará en medio de la nada -el Territorio Norte tiene una extensión de casi un millón y medio de kilómetros cuadrados y una población que no llega a los 250,000 habitantes- la energía producida será enviada ´por una línea de transmisión hasta un punto en la costa norte de Australia distante 800 kilómetros.Una parte de la energía generada será consumida en el Territorio del Norte, mientras que la otra, mayoritaria, será exportada a Singapur desde la costa norte de Australia por medio de cables submarinos de 4,200 kilómetros de longitud. Se anticipa que el año 2028 dicha energía cubra el 15 por ciento de las necesidades de energía eléctrica de Singapur. Además de su enorme tamaño, la planta solar australiana en proyecto contempla bancos de baterías para proporcionar energía de manera continua. Con esto se supera una debilidad de la energía fotovoltaica que se produce solamente en la medida en que brille el Sol. Regresando al sitio de Siracusa, ahora, a más de dos mil años de distancia, nos es claro que Arquímedes hubiera fácilmente llevado a cabo sus propósitos de quemar con la radiación solar a los barcos romanos que los sitiaban de haber contado con elementos de tecnología moderna, tales como espejos cóncavos de vidrio recubiertos de aluminio y monturas mecánicas orientadas hacia un punto fijo por medio de una computadora -habida cuenta, por supuesto, de que los romanos no hubieran  también contado con tecnologías modernas y hubieran fabricado sus barcos con acero y los hubieran equipado con armamento moderno que fácilmente habrían barrido con los espejos de Arquímedes.El hubiera, por supuesto, no existe y no tiene sentido imaginar cosas que no sucedieron. Lo que sí es real, ahora los sabemos, es que el Sol es una fuente potente de energía que puede ser aprovechada y que es económicamente rentable. Como el proyecto australiano lo demuestra.",
    "Según la firma de consultoría PwC, alrededor de un 30 por ciento de los puestos de trabajo en 29 países, en su mayoría industrializados, se habrán perdido a mediados de la próxima década por causa de la automatización. Esto no es algo inusual, pues la pérdida de empleos por el advenimiento de una nueva tecnología se ha dado ya anteriormente. Así, la máquina de vapor sustituyó a la fuerza humana en el siglo XIX y eliminó fuentes de trabajo manual. Del mismo modo, la aparición de la computadora personal y los procesadores de palabras en las últimas décadas del siglo XX cambiaron de manera radical las habilidades requeridas para el trabajo de oficina y marginaron a personas sin estas habilidades.Por otro lado, si bien han eliminado puestos de trabajo, las nuevas tecnologías han creado nuevas oportunidades de empleo con habilidades diferentes a las tradicionales. La introducción de la electricidad en las últimas décadas del siglo XX, por ejemplo, dio origen a puestos de trabajo que demandaron de habilidades propias de la entonces nueva tecnología. Del mismo modo, se espera que la automatización, al mismo tiempo que elimine puestos de trabajo, genere otros nuevos acordes con las nuevas circunstancias.Los especialistas dividen el desarrollo industrial que ha experimentado el mundo en los últimos 250 años en tres revoluciones, cuyos rasgos salientes son, en forma respectiva, la máquina de vapor, la electricidad y las computadoras. En la actualidad está en curso una cuarta revolución industrial impulsada por tecnologías tales como el Internet, la inteligencia artificial y la robótica, la cual, al igual que las revoluciones anteriores, se espera destruya empleos y genere otros nuevos.En esta ocasión, sin embargo, se anticipa que los cambios ocurran con una rapidez tal que habrá profesiones que se harán obsoletas en el curso de una generación. Es decir, existe la posibilidad de que un trabajador tenga que reentrenarse profesionalmente con el fin de conservar su empleo, y en estas circunstancias le sería útil una guía que le permitiera escoger un reentrenamiento profesional. De la misma manera, un joven buscando un primer entrenamiento profesional encontraría útil información sobre la estabilidad futura de tal o cual puesto de trabajo.Con esto en mente, un grupo de investigadores encabezado por Antonio Paolillo, de la Escuela Politécnica Federal de Lausana, Suiza, decidió llevar a cabo una investigación para determinar el riesgo de desaparición de puestos de trabajo por la automatización, determinada ésta tanto por el desarrollo de los robots, como por el avance de la inteligencia artificial que los opera. Los resultados de la investigación fueron publicados el pasado 13 de abril en la revista “Science Robotics”. En su artículo, Paolillo y colaboradores crean un índice para el riesgo de automatización que mide la probabilidad de que un determinado puesto de trabajo sea ocupado por un robot dotado de inteligencia artificial. Esto, basado en las habilidades que dicho puesto requiere, y en las posibilidades presentes y futuras de la robótica y la inteligencia artificial para cubrir dichas habilidades. Los investigadores desarrollaron también un segundo índice que nos da la medida en la que las habilidades de un determinado puesto de trabajo, destinado a desaparecer, pueden ser usadas en un reentrenamiento para un puesto de trabajo que tenga una mayor probabilidad de supervivencia. Paolillo y colaboradores calculan el riesgo de automatización de cerca de mil ocupaciones y encuentran que los físicos son los que tienen el menor riesgo de ser sustituidos por una máquina (lugar 1 en la lista), mientras que los empacadores de carne son los de mayor riesgo (lugar 967). En medio de estos extremos se encuentran los ingenieros en robótica, los economistas y los técnicos eléctricos, que ocupan los lugares 122, 203 y 458, respectivamente.    En cuanto al índice que puede ser usado como guía para cambiar de profesión, Paolillo y colaboradores crearon un sitio en Internet (Resilience To Robots – EPFL) que permite evaluar la mejor opción para cambiar de profesión. Habría, no obstante, que señalar que, si bien en algunos casos el consejo que nos da el sitio es sin duda de utilidad, en otros las respuestas son desconcertantes. Por ejemplo, la primera opción que nos ofrece el sitio para la profesión de físico es la de cirujano, que claramente implica un reentrenamiento profesional casi total. Esto resulta poco práctico, además de que habría una pérdida en el valor del índice de riesgo.Por lo demás, al margen de que el sitio pudiera mejorarse, el artículo de Paolillo y colaboradores nos pone de manifiesto la urgencia de adaptar los planes de estudio de las escuelas profesionales a las cambiantes condiciones del mercado laboral del futuro, que serán determinadas por la cuarta revolución industrial. So pena de ser sustituidos por una máquina.",
    "A raíz del llamado embargo petrolero en el año 1973 se generó un gran interés en el desarrollo de tecnologías para el aprovechamiento de la energía solar. Esta energía es una fuente prácticamente inagotable, pero hace cincuenta años, cuando ocurrió la crisis del petróleo, no existían tecnologías para competir económicamente con las fuentes de energía fósiles. Para ser precisos, en 1973 existían celdas solares que convertían directamente la energía del sol en energía eléctrica, pero su alto costo no las hacía competitivas. A lo largo de los últimos cincuenta años, la tecnología de las celdas solares se ha desarrollado a tal grado que hoy compite económicamente con otras fuentes de generación de energía, además que su operación no genera contaminantes atmosféricos, como es el caso de las fuentes fósiles. Una medida del éxito que están teniendo las celdas solares nos la dan quizá las protestas que están generando en algunas de las comunidades de los Estados Unidos por la instalación de las llamadas granjas solares, constituidas por extensas áreas cubiertas de paneles solares. Las críticas que se hacen a dichas granjas es que ocupan grandes áreas que resultan con daños ecológicos, incluyendo la erosión del suelo y la contaminación con los herbicidas empleados para prevenir el crecimiento de maleza. Además de que afean el paisaje y hacen que baje el valor de las propiedades en sus alrededores.Las celdas solares son particularmente competitivas para la electrificación de zonas rurales o de sitios alejados de la red de distribución de energía eléctrica. Para este tipo de aplicaciones, no obstante, se hace más notoria su mayor limitación: generan energía durante el día, mas no durante la noche. Así, se requiere de un medio de almacenamiento -típicamente un sistema de baterías- que capture la energía generada durante el día para ser usada durante la noche. Las baterías, sin embargo, añaden complejidad al sistema de generación de energía, lo que limita su atractivo. En este sentido, un artículo aparecido esta semana en la revista “Applied Physics Letters” reporta el desarrollo de una celda solar que produce energía, no solamente durante el día, sino también durante la noche. El artículo fue publicado por un grupo de investigadores encabezado por Sid Assawaworrarit de Stanford University, en California.El desarrollo de Assawaworrarit y colaboradores parte de una celda solar de silicio convencional a la que se le adaptó una placa termoeléctrica. La celda de silicio produce energía durante el día a partir de la radiación que recibe del sol, mientras que la placa termoeléctrica lo hace durante la noche. Para que esto último suceda, es necesario establecer una diferencia de temperatura entre la cara superior y la cara inferior de la placa termoeléctrica.  Para entender cómo el generador termoeléctrico puede producir energía durante la noche, cuando no hay radiación solar, consideremos lo siguiente. Primeramente, habría que hacer mención que todos los objetos emiten una cierta cantidad de calor que tiende a enfriarlos. No obstante, si el objeto está, por ejemplo, en el interior de una habitación, absorberá el calor emitido por las paredes o por otros objetos en la habitación y compensará el calor perdido. Si, por el contrario, está expuesto a la intemperie en una noche estrellada, emitirá radiación hacia el cielo que se perderá irremediablemente y no tendrá manera de ser compensada. De este modo, el objeto se enfriará por abajo de la temperatura ambiente- tal como una planta se enfría y puede incluso llegar a helarse en una noche despejada.La celda de Assawaworrarit y colaboradores fue construida uniendo la celda solar a la placa termoeléctrica, con la celda colocada en la parte superior viendo al sol. Durante el día el arreglo opera como una celda típica. Durante la noche, se enfría emitiendo calor hacia el firmamento a través de la celda, lo que produce una diferencia de temperatura de unos pocos grados centígrados entre las dos caras de la placa termoeléctrica, y la consecuente energía eléctrica.  La energía generada durante la noche -que es, ciertamente, mucho menor que la generada durante el día- podría ser usada para iluminación. Para este propósito, un área de 20 metros cuadrados de celdas sería suficiente para una lámpara de 1 watt, según los autores del artículo de referencia. Como hacen notar Assawaworrarit y colaboradores, los elementos empleados para construir su celda no fueron diseñados específicamente para la aplicación y habría mucho espacio para optimizar el arreglo. En estas condiciones, consideran que sus resultados demuestran una opción viable para construir dispositivos para generar energía a partir de la radiación solar, tanto de día como de noche. Y con esto avanzar en la sustitución de las fuentes fósiles de generación de energía. Por más que contribuyan a afear el paisaje con tendidos de celdas solares.",
    "En su novela “Crónicas Marcianas” publicada en 1950, el escritor norteamericano Ray Bradbury hace una recopilación de cuentos cortos publicados a lo largo de varios años en los que se relata la colonización del planeta Marte. La novela inicia con el cuento “El verano del cohete”, en el que Bradbury relata el despegue, en un frío día de invierno de 1999, de un cohete rumbo a Marte con los primeros dos exploradores a bordo. El cohete partió desde un puerto espacial en el estado norteamericano de Ohio, generando un calor tan intenso que, por un momento, el frío de invierno se tornó en calor de verano. El despegue del primer cohete a Marte fue exitoso, pero no la conclusión de la misión, pues sus integrantes fueron ultimados al llegar al suelo marciano. Después de varios viajes de exploración, no obstante, los terrestres llegaron en grandes cantidades a Marte, colonizando un planeta que resultaba muy diferente a su lugar de origen, pero no a tal grado que les impidiera adaptarse. “Crónicas Marcianas” es, por supuesto, un espléndido libro de ficción que no pretende reflejar las condiciones ambientales reales de Marte. De hecho, sabemos que dichas condiciones harían imposible una colonización tal como la describe Bradbury. Para empezar, la atmósfera marciana es unas cien veces más tenue que la de la Tierra, además de que está compuesta casi enteramente de dióxido de carbono. Así, por falta de oxígeno y presión atmosférica, no podríamos sobrevivir en la superficie de Marte sin un traje especial. Habría que considerar también que la temperatura en la superficie del planeta alcanza valores extremos, llegando hasta los menos 100 grados centígrados, aun en el ecuador.De este modo, Marte es diferente a la Tierra en muchos aspectos, algunos tan contrastantes como su falta de atmósfera con oxígeno y sus temperaturas extremas. Marte es también diferente en aspectos más sutiles. Uno de estos nos lo revela un artículo publicado esta semana en la revista “Nature” por un grupo internacional de investigadores encabezado por Sylvester Maurice del Instituto de Investigación en Astrofísica y Planetología de Toulouse, Francia. En su artículo, Maurice y colaboradores reportan el resultado de una investigación llevada a cabo para determinar la velocidad del sonido en la atmósfera marciana, que anticipaban sería diferente a la correspondiente velocidad en la Tierra. Esto, debido a tres factores: las bajas temperaturas ambientales, la tenue atmósfera marciana, y el hecho que esté constituida fundamentalmente por dióxido de carbono.  Para llevar a cabo su estudio, Maurice y colaboradores hicieron uso del explorador Perseverance, que arribó a la superficie de Marte en febrero de 2021. Dicho explorador cuenta con el micrófono SuperCam, alojado en el mástil del explorador, a una altura de 2.1 metros sobre el suelo. El micrófono es parte de un instrumento de análisis que emplea un láser para calentar y evaporar rocas del suelo marciano, y fue primordialmente diseñado para grabar los sonidos producidos por dicho láser durante el proceso. Como una aplicación secundaria, los investigadores emplearon el SuperCam para medir el tiempo que le toma al sonido producido por el láser en llegar hasta el micrófono -después de recorrer 2.1 metros-, y con este tiempo calcular la velocidad del sonido.  Encuentran los investigadores que el sonido en Marte viaja a una velocidad 30 por ciento más lenta que en la Tierra. Además, mientras que en nuestro planeta los sonidos graves y agudos viajan a la misma velocidad, en Marte los sonidos agudos viajan más rápido que los graves. Para ser más precisos, si escucháramos un sonido proveniente de una fuente alejada unos 30 metros, los sonidos graves llegarán dos décimas de segundo más tarde que los agudos. Tendríamos así problemas para, por ejemplo, escuchar un concierto interpretado por una orquesta. Por supuesto, habría que reconocer que, dadas sus condiciones ambientales extremas, es poco probable que tuviéramos la oportunidad de asistir a una sala de conciertos en Marte. Y si fuera el caso, sería menester que acudiéramos en traje de astronauta, al igual que lo tendrían que hacer los integrantes de la orquesta, lo cual se antoja difícil. Los señalamientos acerca de las peculiaridades de la propagación del sonido en la atmósfera marciana no tienen entonces una gran importancia práctica para los futuros colonizadores de Marte, aunque sí son significativos desde el punto de vista científico y contribuyen al conocimiento de un planeta que, supuestamente, colonizaremos en un futuro todavía no determinado.Y en cuanto a “Crónicas Marcianas”, si bien es una novela que relata un planeta que no corresponde a la realidad -sin pretender hacerlo-, es un libro fantástico, altamente recomendable.",
    "El escritor y militar romano Plino el Viejo -muerto durante la erupción del volcán Vesubio en el año 79 de nuestra era- nos dejó una descripción de los monopodos, una raza de hombres que habitaban en la India y que tenían características muy peculiares. Según Plinio el Viejo, los monopodos, como su nombre lo indica, contaban con una sola pierna y con un solo un pie, si bien este último de enormes proporciones. Lejos de constituir un estorbo, el enorme pie les permitía desplazarse por saltos a gran velocidad, además de que lo usaban para protegerse del sol, acostándose de espalda y levantando el pie hasta que su sombra los cubriera por completo. Los monopodos, por otro lado, no eran los únicos monstruos que en la antigüedad y el medioevo se creía que habitaban en tierras lejanas de difícil acceso, y en ese sentido habría que recordar a los blemios, seres sin cabeza y con la cara en el torso que habitaban en Etiopía. Ahora que se ha explorado prácticamente todo el planeta, es difícil argumentar en favor de la existencia de seres exóticos e inteligentes en algún rincón de la Tierra. Nada nos ha impedido, sin embargo, trasladar nuestra fascinación por los seres exóticos a lugares fuera de nuestro planeta. Podemos mencionar, por ejemplo, que, en agosto de 1924, cuando el planeta Marte tuvo un máximo acercamiento con la Tierra, el gobierno de los Estados Unidos estableció el “National Radio Silence Day”, que fijó la suspensión de las transmisiones de radio por cinco minutos cada hora a lo largo de un periodo de 36 horas. Esto, con el objeto de escuchar hipotéticas trasmisiones de radio que hubieran emitido los marcianos para comunicarse con nosotros, aprovechando la cercanía de los dos planetas. Desafortunadamente, los esfuerzos no tuvieron éxito. Hoy sabemos que en Marte no hay marcianos -aunque sí podría haber vida microbiana- como muy probablemente no hay seres inteligentes en ningún otro lugar del sistema solar fuera de la Tierra, por lo que hemos tenido que trasladar nuestros esfuerzos para encontrar vida inteligente a lugares más lejanos. Así, a lo largo de la segunda mitad del siglo pasado y en lo que va del presente, hemos escudriñado el cielo con la esperanza de detectar señales de radio emitidas por una civilización avanzada en busca de establecer comunicación. Desafortunadamente, al igual que en el caso del “National Radio Silence Day”, las iniciativas no han tenido éxito hasta el momento. Al lado de la búsqueda de vida inteligente extraterrestre se están llevando a cabo iniciativas para descubrir la presencia de alguna forma de vida en planetas extrasolares, conocidos como exoplanetas. Lejos del impacto que tendría detectar un mensaje enviado por una civilización extraterrestre, descubrir vida en un exoplaneta, aun si no fuera inteligente, tendría una enorme importancia científica. Según la NASA, se han descubierto alrededor de 5,000 exoplanetas, algunos con características similares a las de la Tierra, y cabe preguntarse si algunos de ellos alojan vida. Para tratar de averiguarlo, es necesario escudriñarlos en búsqueda de bioseñales que indiquen la presencia organismos vivos. Un gas asociado a la actividad biológica es el metano. Así, la presencia de metano en la atmósfera de un exoplaneta podría indicar que dicho planeta aloja vida. Habría que notar que también hay otros gases, como el oxígeno, que también están asociados a la actividad biológica. En estos momentos, sin embargo, el metano resulta particularmente interesante, dado que es fácilmente detectable por el nuevo telescopio espacial James Webb de la NASA. Tenemos, por otro lado, que la mera presencia de metano atmosférico no permite asegurar la presencia de vida, pues este gas puede ser generado también por procesos no biológicos. De este modo, para determinar la probabilidad de que un exoplaneta dado aloje vida, sobre la base de la medición de metano atmosférico, es necesario tomar en cuenta las condiciones globales de dicho planeta. En un artículo aparecido esta semana en la revista “Proceedings of the National Academy of Sciences”, un grupo de investigación encabezado por Maggie Thomson, de la Universidad de California en Santa Cruz, reporta los resultados de un estudio en el que se concluye que en un planeta similar a la Tierra, los procesos no biológicos no pueden fácilmente mantener una concentración de metano atmosférico, y que la detección de dicho gas en la atmósfera de un exoplaneta sería una fuerte indicación de la presencia de vida. Basado en todo lo anterior, podríamos esperar que en los próximos años recibamos noticias sobre el descubrimiento de vida en un planeta alrededor de una estrella lejana. Descubrimiento que será llevado a cabo con el telescopio James Webb, que podrá escudriñar el cielo con una resolución sin precedente. No descubriremos seres sin cabeza o con un solo pie, de los que nos producen fascinación, pero sin duda las noticias serán más que relevantes.",
    "El 24 de febrero pasado inició Rusia la invasión de Ucrania, y ese mismo día tomó control de la central nuclear de Chernóbil, en el norte del país, cerca de la frontera con Bielorrusia. Como bien recordamos, en 1986 esta central sufrió uno de los dos más graves accidentes nucleares de la historia -el otro fue el de Fukushima, en marzo de 2011-, cuando explotó uno de sus cuatro reactores nucleares, dispersando grandes cantidades de material radiactivo a la atmósfera. En la actualidad la central está inactiva, con todos sus reactores apagados y con el reactor accidentado aislado en un enorme sarcófago de 110 metros de alto. No habría así peligro de radiaciones en cuanto a los reactores mismos. En Chernóbil, sin embargo, permanecen cerca de 20,000 barras de combustible nuclear gastado que mantienen todavía niveles de radioactividad y que deben ser cuidadosamente mantenidas en aislamiento. Con relación a esto último, habría que recordar que las barras de combustible gastado, removidas durante la recarga de un reactor nuclear, generan un calor intenso, por lo que hay que enfriarlas por varios años sumergiéndolas en piscinas de agua hasta que se reduzca su nivel de radioactividad. Las 20,000 barras de combustible gastado almacenadas en Chernóbil provocaron preocupación por la posibilidad de que quedaran expuestas al medio ambiente durante la lucha entre rusos y ucranianos por el control de la planta. Afortunadamente, según los expertos, el combustible ha reducido ya su radioactividad a niveles tales que, aun en el caso de que fallaran los sistemas de enfriamiento de las piscinas con las barras radiactivas, las temperaturas que se alcanzarían supondrían un peligro relativamente menor. Ucrania, por otro lado, cuenta con cuatro centrales nucleares con un total de quince reactores nucleares operativos que generan aproximadamente el 50 por ciento de su energía eléctrica; al mismo tiempo que, a nivel global, Ucrania es el séptimo productor de energía nucleoeléctrica. Así, la invasión de Ucrania por Rusia constituye la primera guerra de grandes proporciones que se da en un país con una industria nuclear desarrollada, lo que abre la posibilidad novedosa de que ocurra la dispersión de contaminantes radiactivos por acciones de guerra. Esta posibilidad está comentada por Vadin Chumak, del Centro Nacional de Investigación de Medicina Radiológica de Ucrania, en una entrevista que concedió a la revista “MIT Technology Review” y que fue publicada esta semana. Comenta Chumak que, aparte de Chernóbil, los rusos han tomado el control de la central nuclear de Zaporiyia, en el sur del país, la cual cuenta con seis reactores nucleares, más combustible gastado, el cual “es muy peligroso y contiene muchos materiales radiactivos”. Prosigue Chumak: “El combustible fresco es mucho menos peligroso que el combustible gastado. Después de que las barras de combustible trabajan por un par de años en el núcleo del reactor, acumulan una enorme cantidad de productos de fisión los cuales son muy radioactivos, tales como iodo, cesio y estroncio. Si hubiera un daño a las instalaciones que resguardan el combustible gastado en Zaporiyia, se podría producir una enorme emergencia radiológica, comparable a lo que pasó en Chernóbil”. A la pregunta de si los reactores mismos representan un peligro, Chumak contesta: “No pienso que los reactores pudieran ser destruidos. Están alojados en edificios especiales los cuales son resistentes y muy difíciles de destruir. Esos edificios están diseñados de una manera tal que un jet jumbo de gran tamaño podría caer directamente sobre uno de ellos y aun así se mantendría en pie. El gran peligro es el combustible gastado, el cual es almacenado en instalaciones que no fueron diseñadas para ser atacadas por tanques o misiles, que es lo que Rusia está haciendo ahora”. Si bien, visto a la distancia no esperaríamos que Rusia destruyera de manera dirigida las instalaciones nucleares de Ucrania, dado que, como país vecino, pudiera ser también afectado por la dispersión de material radiactivo, si habremos de conceder que no es posible descartar una destrucción por accidente. Así, a los peligros de una guerra, habremos de añadir los peligros que resultan de insistir en el uso de la tecnología nucleoeléctrica que genera desechos radiactivos que permanecen altamente riesgosos por muchos miles de años. Desechos que, a falta de un sitio de almacenamiento definitivo, se mantienen en instalaciones no demasiado seguras. Susceptibles de ser destruidas, por ejemplo, por un misil mal dirigido.",
    "De las pirámides del complejo de Guiza, en Egipto, la pirámide de Keops es la de mayores proporciones: 230 metros por lado y 140 metros de altura. Es también la que tiene una estructura interna más compleja, la cual incluye una cámara subterránea, y dos cámaras en el cuerpo de la pirámide: la cámara de la reina y la cámara del rey. Es posible acceder a dichas cámaras a través de un túnel que parte del lado norte de la pirámide a una altura de 18 metros desde la base de la misma. Al final de su tramo ascendiente, el túnel se bifurca en un túnel horizontal, que conduce a la cámara de la reina, y en un túnel estrecho y ascendiente de 47 metros de longitud y 8 metros de altura que lleva a la cámara del rey, y que es conocido como “Gran Galería”. La cámara del rey tiene un área aproximada de 10 por 5 metros y una altura de seis metros. Existe también un túnel para acceder a la cámara subterránea.Además de sus grandes dimensiones y complejidad interna, la pirámide de Keops resulta atractiva por los secretos que encierra. En efecto, sabemos que en la actualidad la cámara del rey solamente aloja un sarcófago de piedra vacío, sin que se sepa cuál fue el destino de la momia del faraón Keops que presumiblemente lo habría ocupado. Esto, por un lado, no es algo extraordinario, pues se sabe que las tumbas de los faraones eran por lo común víctimas de los ladrones. Sin embargo, como se señala en un artículo firmado por Mike Dash y publicado en el “Smithsonian Magazine” en el año 2011, si bien se sabe que el túnel que lleva a la cámara subterránea era conocido desde la antigüedad, no hay una evidencia igualmente sólida de que las cámaras superiores hayan sido del conocimiento de griegos y romanos. De hecho, habría sido hasta el año 820 de nuestra era que, por iniciativa del califa Al-Mamún, se había penetrado nuevamente la tumba de Keops después de miles de años de aislamiento. Para esto, se perforó un nuevo túnel en la pared de la pirámide.Por lo demás, la pirámide de Keops no solo guarda secretos acerca de su historia en los últimos 4,500 años, sino también en cuanto a su estructura interna, que aparentemente es más compleja que lo que sabemos en la actualidad. En este sentido, un equipo de investigadores de la Universidad de Nagoya, en Japón, descubrió en 2017 un hueco en el interior de la pirámide que había permanecido oculto. Dicho hueco, revelado empleando una técnica de rayos cósmicos, tiene una longitud de 40 metros, casi igual a la Gran Galería que da acceso a la cámara del rey.    Para profundizar en nuestro conocimiento sobre la pirámide de Keops, un grupo de especialistas encabezado por Alan Bross, del National Acelerator Laboratory, de los Estados Unidos, tiene en marcha un proyecto para investigar su estructura interna empleando igualmente técnicas de rayos cósmicos. Estos proyectos están descritos como en un artículo sometido el pasado 17 de febrero al sitio “arXiv”, que aloja manuscritos de investigación que no han pasado todavía el proceso de revisión por pares.     Habría que mencionar que, más que rayos cósmicos como tales, la técnica de Bross y colaboradores empleara muones producidos por estos rayos. Como sabemos, los rayos cósmicos son partículas de muy alta energía procedentes del espacio, los cuales, al penetrar a la atmósfera de nuestro planeta producen cascadas de partículas secundarias. Entre las partículas secundarias producidas se encuentran los muones. Estas partículas son débilmente absorbidas al penetrar a un material y por tanto son ideales para determinar su estructura interna. En el caso de la pirámide a estudiar, si el material por el que atraviesa el haz de muones es roca sólida, sufrirá una cierta absorción que podrá ser determinada midiendo la cantidad de muones que emergen de la pirámide. Si, por otro lado, los muones en su camino encuentran un hueco, sufrirán una absorción menor que podrá ser determinada al salir de la pirámide. De esta manera podrán detectarse la presencia de huecos en el interior de la pirámide, e incluso cambios en la composición de los materiales sólidos que la componen. Además, llevando a cabo mediciones desde varios ángulos alrededor de la pirámide, los investigadores podrán localizar la posición de los huecos en el interior de la pirámide, de la misma manera en que un tomógrafo toma imágenes tridimensionales en el interior del cuerpo.  De este modo, con seguridad pronto tendremos noticias acerca de los secretos que guarda el interior la pirámide de Keops, que serán desvelados empleando técnicas que hace apenas un siglo habrían parecido de ciencia ficción. Un poco más difícil será desvelar los secretos de su historia en los últimos 4,500 años.",
    "El 25 de agosto de 2012, la sonda espacial “Voyager 1” de la NASA abandonó el sistema solar y con esto se convirtió en el primer objeto humano en penetrar en el espacio interestelar. En estos momentos se encuentra a más de 23,000 millones de kilómetros de nosotros -unas 150 veces la distancia entre el Sol y la Tierra- alejándose a una velocidad de más de 60,000 kilómetros por hora. En decenas o centenas de miles de años arribará a las inmediaciones de sistemas solares vecinos al nuestro, y si bien es improbable que llegue a hacer contacto con civilizaciones alienígenas, la Voyager 1 lleva a bordo una cápsula del tiempo con imágenes y sonidos de la Tierra, así como información de la posición en el espacio de nuestro planeta. La sonda Voyager 2, por su lado, se convirtió en 2018 en el segundo objeto artificial en abandonar el sistema solar, y se espera que en el futuro lo hagan tres sondas más de la NASA, las Pioneer 10 y 11 y la New Horizons.  Así, hemos inaugurado ya la época de los viajes interestelares, al igual que con seguridad lo han hecho también otras civilizaciones alienígenas.Con relación a esto último, en el mes de octubre de 2017 se detectó en el observatorio Haleakala en Hawái, la visita de un objeto inusual, que cruzó por nuestro sistema solar con una velocidad y una trayectoria tales que indicaban un origen fuera del sistema solar. Este objeto, el primero de origen extrasolar detectado en nuestro sistema planetario, fue bautizado como “Oumuamua”, que en hawaiano quiere decir “mensajero de lejos que llega primero”.   Desde su descubrimiento, Oumuamua ha sido motivo de controversia entre los especialistas por sus características singulares. En particular, porque durante su acercamiento al Sol experimentó una aceleración en su velocidad más allá de la que hubiera experimentado por la sola atracción gravitatoria del Sol.  Esto indicaría que Oumuamua tiene la naturaleza de un cometa. En efecto, un cometa está constituido por materiales que se vaporizan al acercarse al Sol, formando la coma y la cola del cometa. Esta vaporización genera una fuerza que le imprime al cometa una aceleración adicional durante su acercamiento al Sol. Así, la aceleración en exceso observada durante el acercamiento de Oumuamua podría ser explicada si asumimos que éste tiene la naturaleza de un cometa, a pesar de que, no hubiera mostrado indicios de la formación de una coma durante su paso por Sol. La conclusión que Oumuamua tiene la naturaleza de un cometa ha sido disputada por Shmuel Bilay y Abraham Loeb, de la Universidad de Harvard, quienes afirman en un artículo publicado en el número de noviembre de 2018 de la revista “The Astrophysical Journal Letters”, que la fuerza que produjo el incremento en la aceleración de Oumuamua fue en realidad debida a la presión de la radiación del Sol. Con relación a esto, sabemos que la radiación del Sol ejerce una fuerza sobre un objeto al incidir sobre su superficie. Esta fuerza, por supuesto, es muy pequeña y no afecta mayormente a los objetos aquí en la superficie de la Tierra, Puede, sin embargo, producir grandes efectos si actúa por un tiempo largo sobre objetos que presenten una gran superficie a la radiación solar. De hecho, es la radiación solar la que impulsa a las sondas espaciales equipadas con velas solares, de la misma manera que el viento impulsa a los barcos de vela.Para apoyar sus conclusiones, Bilay y Loeb hacen un cálculo para determinar la forma que debería tener Oumuamua para experimentar la aceleración observada durante su acercamiento al Sol. Encuentran que debería tener la forma de una delgada lámina con un espesor inferior a un milímetro; justo lo que se esperaría de una vela solar. En estas condiciones, Bilay y Loeb especulan que Oumuamua podría constituir los restos de una nave interestelar, ya fuera de operación, construida por una civilización alienígena.No es difícil imaginar que la especulación de Bilay y Loeb ha sido fuertemente disputada por los especialistas quienes arguyen un origen natural para Oumuamua, con todo y su comportamiento extraño.  Loeb, sin embargo, ha sostenido sus conclusiones originales. Así, en un artículo a publicarse en el mes de abril del presente año en la revista “New Astronomy”, juntamente con otro autor critica las conclusiones de un artículo en el que se sostiene que Oumuamua tiene un origen natural y es en realidad un pedazo de nitrógeno congelado, que se vaporizó al acercarse al Sol generando un incremento en su aceleración.Desafortunadamente, Oumuamua está ya fuera del alcance de nuestros telescopios y no es posible estudiarlo con más profundidad para determinar su verdadera naturaleza. En esta situación, tendríamos que estar atentos por si otro objeto similar cruza por nuestro sistema solar. Después de todo, por experiencia propia, sabemos que una civilización tecnológica puede enviar sondas hacia otras estrellas.",
    "En el año de 1851 se celebró en la ciudad de Londres la exposición mundial “Gran Exhibición de la Industria de todas las Naciones”. Para alojar a dicha exposición se construyó un enorme edificio de acero y cristal, el llamado “Crystal Palace”, que fue desmontado al final de la exposición, y reconstruido en el “Crystal Palace Park”, en la misma ciudad de Londres como parte de un parque de diversiones. En dicho parque se instaló también una exhibición de modelos de dinosaurios y otros animales extintos que habían sido descubiertos no hacía mucho tiempo, la cual, por su valor histórico, se ha mantenido hasta la actualidad. Uno de los modelos en exhibición correspondía al iguanodonte, descubierto en 1821 en el sur de Inglaterra por Gideon Mantell, un paleontólogo aficionado. La identificación del iguanodonte a partir de este primer descubrimiento no fue inmediata, dado que los restos óseos descubiertos eran incompletos. Mandell, no obstante, propuso que el iguanodón era un reptil de grandes proporciones, cuadrúpedo, con un cuerpo pesado y un cuerno en la nariz. Dicha descripción, incluyendo el cuerno en la nariz, corresponde al modelo de iguanodonte en exhibición en Crystal Palace Park. Determinar el aspecto que habría tenido un animal extinto a partir de sus restos óseos fósiles no es una tarea simple. Esto lo hace patente el cuerno que le fue adjudicado al iguanodonte, que ahora sabemos -con más evidencias fósiles- era en realidad parte de un pulgar. No tenemos tampoco restos fósiles de tejido blando que nos pudieran dar una idea precisa del aspecto que habría tenido en vida un animal en tiempos prehistóricos. Y, sin embargo, a pesar de estas dificultades, abundan las imágenes de dinosaurios y otros animales extintos, por lo que cabe preguntarse: ¿Qué tan precisas son estas descripciones y cómo afectan la comunicación científica, tanto entre investigadores, como entre el público en general? Un artículo aparecido esta semana en la revista “Paleontologia Electronica” aborda este tópico. Dicho artículo fue publicado por un grupo de investigadores encabezado por Matt Davis, del Museo de Historia Natural de Los Ángeles, California.Como apuntan Davis y colaboradores, mientras que el contenido de un artículo científico que trate sobre un animal extinto debe pasar por una rigurosa revisión por expertos independientes antes de ser publicado, algunas imágenes del animal en cuestión, que no forman parte del artículo y que comúnmente se dan a la publicidad una vez que el mismo se ha publicado, pueden no ser tratadas con el mismo rigor. Así, dado el gran poder de difusión de una imagen, se propagan ideas falsas sobre el aspecto que habrían tenido animales en el pasado prehistórico. Y esto, no solamente entre el público en general, sino también entre los expertos.En estas condiciones, Davis y colaboradores se propusieron desarrollar un proyecto de realidad aumentada para el Museo La Brea Tar Pits en Los Ángeles, California, con imágenes tridimensionales en movimiento de animales extintos, entre los que se incluyen, mamuts, mastodontes, bisontes, camellos, leones, osos, tigres dientes de sable, lobos y berrendos. Dichas imágenes han sido elaboradas siguiendo el conocimiento científico actual que se tiene de dichos animales. Además, cuando hay lagunas en la información que se tiene de un animal particular, la imagen elaborada es intencionalmente imprecisa para reflejar dichas lagunas. Mediante el proyecto de realidad aumentada, empleando dispositivos de bajo costo como teléfonos celulares, el visitante del museo Tar Pit podrá sumergirse en un escenario del pasado en el que, por ejemplo, interactúan de manera simultánea manadas de bisontes y lobos. Los animales, además, actuarán de manera realista, de acuerdo con el conocimiento científico que se tiene sobre ellos. En contraste, otros aspectos, como por ejemplo el color de la piel, no serán importantes.  Concluyen Davis y colaboradores que su proyecto constituye un ejemplo de las acciones que deben tomarse para difundir imágenes de animales extintos que atiendan al conocimiento científico que se tiene sobre los mismos. Y así evitar la propagación de ideas falsas sobre el aspecto físico que tuvieron animales que se extinguieron en tiempos prehistóricos.Por lo demás, habría que tomar en cuenta que el conocimiento científico es algo que está en continua evolución, y que algo que es hoy aceptado por los expertos podría ser desmentido en el futuro. Así, de la misma manera como los iguanodontes perdieron su cuerno, en el futuro algunas de las imágenes desarrolladas para el museo de Los Ángeles podrían dejar de ser válidas. Y en lo que a esto último respecta, poco podemos hacer.",
    "Si retrocediéramos 100,000 años hacia el pasado a bordo de una máquina del tiempo nos encontraríamos con un continente europeo poblado por neandertales, lo mismo que por mamuts, rinocerontes y osos, entre otros grandes mamíferos. A los hombres modernos les tomaría todavía algunas decenas de miles de años en llegar hasta ahí después de su salida de África.  Hace 100,000 años estaba aún lejos el desarrollo de la agricultura, que permitió el asentamiento de los grupos de cazadores-recolectores y con el tiempo el desarrollo de las civilizaciones. Sin duda, hace 100,000 años el mundo era muy diferente al actual.Si, empleando nuestra máquina del tiempo, nos ahora trasladáramos 100,000 años hacia el futuro ¿con qué nos encontraríamos? Es difícil anticiparlo dado los rápidos y sorprendentes cambios que la civilización ha experimentado en los últimos doscientos años impulsados por el estudio científico de la naturaleza. No obstante, podemos quizá anticipar que nos encontraríamos con huellas de nuestro paso por el mundo, al igual que ahora tenemos huellas de nuestros antecesores decenas de miles de años atrás.    Podemos estar razonablemente seguros de esto último al menos en lo que respecta a una de nuestras actividades: la operación de reactores nucleares para la generación de energía que producen residuos que serán radioactivos hasta por cientos de miles de años.En efecto, como sabemos, a raíz del descubrimiento de la posibilidad de desintegrar algunos átomos bombardeándolos con neutrones, fue posible desarrollar reactores nucleares para producir energía. En la actualidad, según datos de la Wikipedia, existen en el mundo 435 reactores en operación que producen alrededor del 10 por ciento de la electricidad que consume el mundo. El uso de electricidad de origen nuclear, sin embargo, varía ampliamente entre países. En Francia, por ejemplo, el 70 por ciento de la electricidad tiene origen nuclear.La energía nuclear se clasifica como “limpia”, en el sentido que no produce gases de invernadero y por tanto se le considera como una fuente de energía adecuada para combatir el cambio climático. La energía nuclear, no obstante, está lejos de ser limpia en un sentido más amplio, pues produce desechos radiactivos, en la forma de combustible nuclear usado, que pueden ser altamente peligrosos por decenas de miles de años.El combustible desechado en un reactor nuclear en cada operación de recambio típicamente consiste de tubos de un centímetro de diámetro y cuatro metros de largo, rellenados con pastillas de dióxido de uranio. Al ser removido del reactor, el combustible está caliente y es altamente radioactivo, por lo que debe ser reprocesado, o bien sumergido en una pileta de agua por varios años para enfriarlo y después transferirlo a un contenedor de acero sellado y almacenado en un silo de concreto.  Actualmente, mas de un cuarto de millón de toneladas de desechos nucleares altamente radioactivos están almacenados en las cercanías de reactores nucleares y plantas de fabricación de armas nucleares a nivel global.Se considera, sin embargo, que este almacenamiento es solamente temporal, en tanto se resuelve el problema de manera definitiva. En este sentido, Finlandia es uno de los países que más ha avanzado mediante la construcción de un sitio de almacenamiento subterráneo en la isla Olikiluoto en la costa oeste del país. Una descripción del sitio apareció está semana en la revista “Science”, en un artículo firmado por Seeder El Showk.  Los tubos radioactivos desechados por los reactores finlandeses serán colocados en contenedores sellados de hierro de seis metros de alto, y estos a su vez en contenedores sellados de cobre que serán recubiertos con bentonita. En el espacio entre ambos contenedores se inyectará gas argón para evitar su corrosión. Los contenedores con los desechos radiactivos serán enterrados en cerca de cien túneles a 430 metros de profundidad. El sitio está siendo construido en el subsuelo rocoso, entre dos fallas geológicas paralelas, separadas por 800 metros. Con todas estas precauciones, los finlandeses esperan que los materiales radiactivos colocados en el confinamiento -miles de contenedores- se mantengan en un aislamiento total por un espacio de 100,000 años, al final del cual habrán perdido su peligrosidad.Así, si todo sale bien, hipotéticos arqueólogos del futuro podrían descubrir en Olikiluoto un almacén de desechos nucleares -afortunadamente ya inofensivos- producto de una tecnología que con seguridad les parecerá obsoleta. Y con este descubrimiento, posiblemente se pregunten por la razón que llevo a los primitivos habitantes de la Tierra a fabricar materiales altamente peligrosos que después tuvieron que enterrar, empleando grandes esfuerzos y cuidados, a gran profundidad.",
    "Ciertamente, no tenemos noticias de que un orangután en la selva haya sido avistado fabricando una herramienta de piedra y usarla, por ejemplo, para abrir una fruta que pretende comer. No lo hemos presenciado quizá porque para este propósito le bastan sus dientes. O bien, debido a que los orangutanes no son lo suficientemente inteligentes para concebir, fabricar y usar herramientas.Si bien esto último es lo que posiblemente se nos viene a la mente en primera instancia, un artículo publicado esta semana en la revista Plos One arroja dudas al respecto. En efecto, en dicho artículo se describen los resultados de experimentos llevados a cabo con cinco orangutanes que habitan en zoológicos en Noruega y el Reino Unido, los cuales demuestran que estos animales, sin ningún entrenamiento, son capaces de utilizar una herramienta filosa de piedra para abrir una caja. Son, además, capaces de aprender por imitación el proceso de fabricación de una herramienta de piedra. Evolutivamente, nuestra especie y la de los orangutanes están muy separadas y tendríamos que remontarnos trece millones de años hacia el pasado para encontrar un ancestro común. Los diferentes caminos evolutivos nos han hecho lo suficientemente diferentes para que no experimentemos ninguna incomodidad ante la presencia de un orangután -pese a que su aspecto físico comparte algunas semejanzas con el nuestro y a que ha dado pruebas de inteligencia-. Como especie, no hay ninguna duda de que somos superiores a los orangutanes, como lo somos con respecto a cualquier otra especie viviente sobre la tierra.Esto último, sin embargo, es solamente una circunstancia del tiempo que nos tocó vivir y la situación habría sido muy diferente hace cuarenta mil años, cuando nuestra especie convivió en Europa con los neandertales. Como sabemos, los neandertales habitaron el continente europeo por cientos de miles de años antes de extinguirse hace unos cuarenta mil años, coincidiendo con el arribo de nuestra especie a dicho continente. Por esta coincidencia, se ha especulado que los neandertales se extinguieron de manera súbita ante el embate del homo sapiens, cognitivamente superior.Tal parece, no obstante, que dicha especulación es demasiado optimista con respecto a nuestra supuesta superioridad sobre los neandertales. En este sentido son las conclusiones de un artículo aparecido el pasado 9 de febrero en la revista “Science Advances”, publicado por un grupo internacional de investigadores encabezado por Ludovic Silmak de la Universidad de Toulouse en Francia.En su artículo, Silmak y colaboradores reportan el descubrimiento de restos dentales y de instrumentos de piedra en la cueva Mandrin en el sur de Francia, los cuales demuestran la presencia en dicha cueva de humanos modernos hace unos 55,000 años. Esto es, el homo sapiens habría llegado a Europa unos 10,000 años antes de lo que se pensaba. Aún más, los hallazgos indican que la cueva Mandrin estuvo ocupada de manera alternada por humanos modernos y por neandertales. En efecto, los investigadores encuentran evidencias de ocupación por neandertales hace 60,000 años y de la subsecuente ocupación por humanos modernos, seguidamente de la ocupación por neandertales y finalmente por humanos modernos. Así, el proceso de extinción de los neandertales fue más complejo de lo que se creía y no habría ocurrido de manera súbita por el embate de una especie evolutivamente superior. Por lo contrario, al menos en lo que respecta a la cueva Mandrin, se dio un ir y venir lo largo de miles de años, hasta que finalmente prevalecieron los humanos modernos.De este modo, de haber vivido en Europa hace 50,000 años, habríamos convivido con una especie con un aspecto ciertamente distintivo -con un cuerpo más robusto y con arcos ciliares prominentes, entre otras características-, pero no demasiado diferente al nuestro. Y con una inteligencia tampoco alejada de la nuestra en demasía, como lo demuestran las sofisticadas técnicas que desarrollaron para fabricar herramientas de piedra. Incluso, es ahora claro que una cruza entre los neandertales y los humanos modernos, cuyo material genético acusa una herencia de los primeros. Así, hace 50,000 años no teníamos exclusividad en el planeta como la única especie con una inteligencia superior. De esta forma, nuestros ancestros tendrían que haber estado acostumbrados a no tener la exclusividad en este sentido. Por otro lado, si alguno de nosotros, en una máquina del tiempo, se hubiera trasladado 50,000 años hacia el pasado, se hubiera encontrado con una población diferente en muchos aspectos -con respecto a su especie-, pero igual en otros, incluyendo la inteligencia. Lo que, sin duda, habría sido toda una experiencia. En todo caso, de una naturaleza diferente a la que vagamente experimentamos con los orangutanes.",
    "De acuerdo con datos de la Universidad Johns Hopkins, el virus del Covid 19 ha infectado a más de 400 millones de personas y llevado a la muerte a casi seis millones alrededor del mundo; esto, en apenas un par de años desde su aparición en Wuhan, China, en enero de 2020. La variante Ómicron del virus ha sido particularmente rápida en su diseminación, produciendo números récord de nuevos infectados y reemplazando a la variante Delta en apenas unos meses. Afortunadamente, también ha sido menos virulenta.Con todo y lo traumático que nos han resultado los dos últimos años de pandemia, podemos quizá sentirnos afortunados de vivir en una época de avances médicos y de higiene pública. Siglos atrás, nuestros antepasados no tuvieron la misma suerte. En este sentido, la historia nos habla de pandemias que aterrorizaron a poblaciones sin medios para defenderse por ser totalmente ignorantes de las causas de la enfermedad. Uno de los ejemplos más citados al respecto es la peste negra que azotó a Europa en el siglo XIV y que habría acabado entre el 30 por ciento y el 50 por ciento de la población en el curso de unos cuantos años. Esto último es la versión más extendida, basada en textos históricos. Es posible, sin embargo, que la devastación que produjo la peste negra haya sido exagerada. A esta conclusión llega un artículo aparecido esta semana en la revista “Nature Ecology and Evolution”, el cual fue publicado por un grupo internacional de investigadores encabezado por Adam Izdebski, del “Max Planck Institute for Science of Human History” en Jena, Alemania. En su artículo, Izdebski y colaboradores sostienen que, si bien hubo zonas de Europa altamente devastadas por la pandemia -de acuerdo con evidencias históricas-, esto no fue algo generalizado en todo el continente. Llegan a esta conclusión mediante un estudio paleoecológico, empleando el procedimiento que se describe a continuación. Izdebski y colaboradores señalan que del 75 por ciento al 90 por ciento de la población de Europa en el siglo XIV era rural, y que la muerte de un gran porcentaje de esta población hubiera dejado grandes extensiones de tierra sin cultivar. Con esto, las plantas cultivadas en las superficies agrícolas habrían sido sustituidas por pastizales o por la vegetación propia del lugar.  De este modo, si se encontrara un cambio en la vegetación de las tierras agrícolas se obtendría evidencia del colapso poblacional producido por la pandemia de peste.    Pero ¿cómo podrían averiguar Izdebski y colaboradores si en tal o cual lugar hubo una invasión de la superficie agrícola por otras plantas? Lo hicieron estudiando los granos de polen que, arrastrados por el viento, terminaron atrapados en una capa de tierra y rocas en el fondo de lagos y humedales, que se constituyen de este modo en testigos del pasado vegetal de las cercanías. La información proporcionada por el polen será más antigua en cuanto mayor sea la profundidad a la que se encuentra en la capa de tierra en el fondo del lago.   Para llevar a cabo su estudio, los investigadores tomaron 1, 634 muestras del fondo de 261 lagos y humedales en 19 países de Europa y se enfocaron a estudiar los cambios en el polen atrapado en intervalos de cien años, antes y después del año 1,350, alrededor del cual se inició la pandemia. Encontraron, que, si bien hubo lugares en los que se produjo un colapso poblacional, entre las que se encuentran áreas del centro de Italia y de Francia, lo mismo que del sur de la península escandinava y de Grecia, también hubo otras regiones en las que no se registra un nivel apreciable de afectación por la pandemia. En algunas áreas incluso se observa un incremento en la superficie cultivada. Este es el caso de una región en el centro de la península ibérica, y de extensas áreas en Polonia y Rusia.Los resultados de Izdebski y colaboradores muestran que muy posiblemente el colapso de la población de Europa por la peste negra haya sido exagerado. Ciertamente, de acuerdo con los textos históricos, hubo regiones altamente devastadas -lo que es corroborado por el estudio-, pero esto no fue un fenómeno generalizado. De un modo u otro, los europeos sobrevivieron a la peste negra, de la misma manera que hemos sobrevivido a muchas otras pandemias provocadas por diversos patógenos a lo largo de la historia. Y esto nos da esperanzas de que, a la brevedad posible, logremos salir de la pandemia que nos tocó vivir. Después de todo, contamos para defendernos con medios infinitamente superiores en comparación con los de nuestros antepasados de la Europa medieval, quienes atribuían la enfermedad a las causas más dispares, desde la corrupción del aire hasta los fenómenos astronómicos. Lo que, por supuesto, no los llevó a ningún lado.",
    "Uno de los episodios clave en la conquista del Perú por Francisco Pizarro fue la captura en Cajamarca, después de una celada, de Atahualpa, el último emperador Inca libre. Como rescate para lograr su liberación, Atahualpa ofreció entregar a Pizarro objetos de oro y plata en cantidades suficientes para llenar el cuarto en el que se encontraba preso hasta donde alcanzara su mano alzada. Pizarro acumuló así toneladas de oro y de plata, a pesar de lo cual al final condenó a Atahualpa a morir en la hoguera.Como sabemos, la avidez por el oro y la plata era algo común entre los conquistadores españoles, y esto es lo que posiblemente explique el hallazgo arqueológico -y macabro- reportado en un artículo aparecido esta semana en la revista “Antiquity”. Dicho artículo fue publicado por un grupo internacional de investigadores encabezado por Jacob Bongers de East Anglia University en el Reino Unido.En su artículo, Bongers y colaboradores reportan los resultados de una investigación llevada a cabo con restos humanos descubiertos en el valle de Chincha, en la costa peruana, aproximadamente 200 kilómetros al sur de Lima. Dicho valle fue dominado por el pueblo chincha entre los años 1000 y 1400 de nuestra era, pero se encontraba integrado al imperio Inca a la llegada de los conquistadores españoles. De manera específica, los investigadores estudiaron 79 ejemplares de lo que llamaron “vertebras en postes”, que consisten en grupos de vertebras humanas ensartadas en cañas. Dichos ejemplares fueron recuperados en 20 sitios mortuorios conocidos como chullpas. Por medio de dataciones por radio carbón, estiman los investigadores que las vertebras corresponden a personas fallecidas entre los años 1520 y 1550 de nuestra era, mientras que las cañas en las que fueron ensartadas habrían sido cultivadas entre los años 1550 y 1590. De este modo, las vértebras habrían sido ensambladas y vueltas a enterrar en sus tumbas, decenas de años después de la muerte de las personas a las que pertenecieron. Habría que hacer notar que el primer periodo coincide con el arribo de los españoles a Perú, lo mismo que con las epidemias y hambrunas que se sabe que ocurrieron en este país a inicios del siglo XVI.Consideran Bongers y colaboradores que el desfase de épocas entre la muerte de una persona y el uso de sus huesos para ensamblar un arreglo de “vértebras en postes”, posiblemente indique que las tumbas originales fueron saqueadas por colonizadores europeos con la intención de robar los objetos de oro y plata que pudieran contener. De hecho, como hacen notar los investigadores, se sabe que el saqueo de tumbas era una práctica extendida en la región chincha.Por otro lado, la violación de tumbas y la dispersión de los restos mortuorios que trae como consecuencia habría contrastado con la actitud del pueblo chincha, que daba gran valor a la integridad del cuerpo humano. Así, ensamblar vértebras en cañas era una forma de reunir nuevamente los restos humanos dispersados por los colonizadores europeos ávidos de oro y plata. Como comentan Bolgers y colaboradores, el alto valor que los pueblos andinos daban a la integridad del cuerpo humano puede ser apreciada por el hecho que, en los sacrificios rituales de infantes llevados a cabo por los incas, la muerte se producía por métodos “no sangrientos”, ya sea por ahogamiento, estrangulamiento o entierro en vida. En la misma dirección, se afirma que, al enfrentar su condena de muerte, atado a un poste y enfrente de una pila de leña, a Atahualpa se le dieron dos opciones: morir quemado, o convertirse al cristianismo y morir por estrangulamiento -garrote vil-. Atahualpa escogió está última opción, aparentemente porque si muriera en la hoguera su cuerpo perdería toda su integridad al convertirse en ceniza. Atahualpa fue bautizado con el nombre de Francisco -en honor a Pizarro- tras lo cual murió estrangulado; con el cuerpo íntegro, sin embargo.Todo lo anterior nos revela lo mucho que ha cambiado el mundo en quinientos años. Si bien, quizá no tanto en cuanto al aprecio por el oro.",
    "En los últimos cien años la población mundial ha crecido de manera acelerada hasta casi alcanzar hoy en día los 8,000 millones de seres humanos. De manera concurrente, el crecimiento poblacional ha requerido elevar la producción de alimentos y de energía que, entre otros efectos, ha llevado a un incremento acelerado en la emisión de gases de invernadero a la atmósfera. De estos gases, el más pernicioso es el dióxido de carbono, producido por la quema de combustibles fósiles, seguido del metano, que es el componente mayoritario del gas natural. El dióxido de carbono es un gas que permanece en la atmósfera por cientos de años una vez que es dispersado, mientras que el tiempo de vida del metano en la atmósfera es de sólo 10-15 años. La potencia del metano como gas de invernadero, sin embargo, es unas ochenta veces mayor que la del dióxido de carbono cuando se promedian sus efectos en los primeros 20 años. En este sentido, se estima que un 25 por ciento del calentamiento global que ha experimentado la Tierra en los dos últimos siglos es debido a las emisiones de metano.  El metano en la atmósfera se origina, tanto en procesos naturales, como en actividades humanas, incluyendo la producción y el uso de los combustibles fósiles. Una fuente de metano la constituye las fugas que se producen durante el transporte del gas natural en gasoductos. De manera sorprendente, otra fuente significativa de emisiones de metano a la atmósfera son las estufas domésticas. Esto último, según un artículo aparecido esta semana en la revista “Environmental Science and Technology”, publicado por un grupo de investigadores encabezado por Eric Lebel, de Stanford University, en California. En dicho artículo, Lebel y colaboradores reportan los resultados de un estudio llevado a cabo con las estufas de cocina de 53 casas habitación en California, con el objetivo de cuantificar los niveles de metano que emiten a la atmósfera durante su operación. En este sentido, las emisiones de metano pueden producirse, tanto durante el proceso de encendido de los quemadores de la estufa -al abrir la llave de gas y antes de que se inicie la combustión-, como por la combustión incompleta del gas. Adicionalmente, y de manera sorprendente, la emisión de metano a la atmósfera ocurre también cuando la estufa no está en operación y tiene sus llaves de gas cerradas. De hecho, Lebel y colaboradores encuentran que el 75 por ciento de la emisión de metano ocurre cuando la estufa no está en operación. La segunda fuente de emisión de metano son los hornos de las estufas, seguida de la combustión incompleta del gas. El gas que se escapa al inicio del encendido de los quemadores tiene una contribución significativamente menor, equivalente a la emisión por la combustión incompleta en un quemador encendido por 10 minutos.       Así, las estufas domésticas de gas natural son una fuente de gases de invernadero, no solamente de dióxido de carbono que se produce durante la combustión del gas, sino también de metano. Pero ¿qué tan significativa es esta fuente? En este respecto, Lebel y colaboradores estiman que el metano emitido por las estufas domésticas en los Estados Unidos es equivalente al gas de invernadero generado por medio millón de automóviles de gasolina.     Se estima que la población en el mundo hace unos 10,000 años, al inicio de la agricultura, era de algunos millones de seres humanos. Con el transcurso de tiempo dicha población se incrementó lentamente: hasta unos cientos de millones al inicio de nuestra era, a 1,000 millones en 1800, y a 2,000 millones en 1930. En la segunda mitad del siglo el crecimiento anual de la población mundial se elevó considerablemente por sobre sus tasas históricas, alcanzando un máximo de 2.1 por ciento en la década de los años sesenta, a partir del cual disminuyó paulatinamente hasta llegar al 1 por ciento en la actualidad. Como resultado de esta evolución, a pesar de la reducción en el crecimiento anual de los últimos 50 años, llegamos a la población actual de 8,000 millones de personas y con esto pusimos en apuros al planeta, que no ha resultado ser lo suficientemente grande. A tal grado hemos presionado al planeta, que aun el mero acto de cocinar nuestros alimentos puede contribuir al calentamiento global.",
    "Con la declaración de emergencia sanitaria que emitió la Organización Mundial de la Salud el 30 de enero de 2020, el uso de los cubrebocas se incrementó rápidamente como un medio para limitar la trasmisión del coronavirus. Al mismo tiempo, sin embargo, los cubrebocas desechados se constituyeron en una fuente de contaminación por materiales plásticos cuyos efectos podrían trascender a los de la pandemia. En ese sentido, un artículo publicado el pasado mes de diciembre en la revista “Nature Sustainability” documenta el incremento en el número de cubrebocas desechados como basura en lugares públicos a lo largo de un periodo de 14 meses, entre septiembre de 2019 -antes de la aparición de la pandemia- y octubre de 2020. La investigación se llevó a cabo en 11 países en tres continentes, incluyendo a Australia, Alemania y los Estados Unidos, y demostró que en el periodo de estudio el número de cubrebocas-basura se incrementó más de 80 veces. El problema ambiental que representa el uso de cubrebocas puede ponerse en perspectiva si consideramos que en la actualidad se desecha mensualmente la asombrosa cifra de más cien mil millones de cubrebocas, según un artículo publicado en marzo de 2021 en la revista “Frontiers of Enviromental Sciences and Engineering”. Es decir, se desechan unos tres millones de cubrebocas cada minuto, los cuales se suman al agudo problema de contaminación ambiental por plásticos que sufre el planeta.  Con respecto a esto último, los expertos distinguen entre contaminación por macroplásticos, microplásticos -fragmentos de plástico con dimensiones entre 5 milímetros y una milésima de milímetro- y nanoplásticos, con dimensiones inferiores a una milésima de milímetro. Igualmente, los científicos saben que los macroplásticos se degradan de manera continua en fragmentos más pequeños por la acción del medio ambiente, particularmente por la componente ultravioleta de la radiación solar. Los nanoplásticos preocupan de manera particular a los especialistas pues se piensa que, por su pequeño tamaño, podrían ser más tóxicos y podrían dispersarse más ampliamente en el medio ambiente. Esto último, desafortunadamente acaba de comprobarse, como lo reporta un artículo aparecido esta semana en la revista “Environmental Research”, firmado por un grupo de investigadores encabezado por Dusan Materic de la Universidad de Utrecht. En efecto, según lo documentan en su artículo Materic y colaboradores, estudios llevados a cabo en Groenlandia y en la Antártida encontraron la presencia de nanoplásticos en ambas latitudes. En Groenlandia encontraron nanoplásticos a lo largo de una columna de hielo de 14 metros de longitud, que corresponde a hielo depositado desde 1965 a la fecha, lo que demuestra que la dispersión de nanoplásticos hacia la región ártica no es un fenómeno nuevo, sino que ha ocurrido a lo largo de los últimos cincuenta años. En la Antártida, en el otro extremo del mundo, Materic y colaboradores encontraron en el hielo oceánico concentraciones de nanoplásticos cuatro veces más grandes que los encontrados en Groenlandia.Cabe preguntarse por el origen de los nanoplásticos encontrados en las regiones polares. Al respecto, encuentran los investigadores que, tanto en Groenlandia como en la Antártida, alrededor del cincuenta por ciento de los nanoplásticos son de polietileno, que es usado en bolsas y empaques de plástico. En Groenlandia, la segunda y tercera componentes de nanoplásticos es polvo de llanta -aproximadamente un 25 por ciento- y PET -20 por ciento-, el cual es un plástico usado en la industria textil y para fabricación de botellas. En la Antártida, en contraste, no se encontró polvo de llanta y cerca del 30 por ciento de los nanoplásticos son de polipropileno, que es un plástico con una amplia gama de aplicaciones.   Los resultados de Materic y colaboradores confirman que ningún lugar de nuestro planeta, por más alejado que esté de los centros urbanos, está libre de contaminación plástica. En particular, estos investigadores demuestran por vez primera que los nanoplásticos han llegado hasta las regiones más apartadas del mundo. ¿Cuáles son las consecuencias de la contaminación por nanoplásticos para nuestra salud? Al respecto, Materic y colaboradores no ofrecen información concluyente, pero comentan que los nanoplásticos pueden tener “un efecto tóxico para los organismos marinos, afectando su crecimiento, induciendo un retraso en su crecimiento, generando malformaciones y cambios subcelulares”. En todo este contexto, queda claro que, si bien los cubrebocas han sido una herramienta básica para lidiar con la pandemia de coronavirus, al mismo tiempo se han constituido en una fuente de contaminación ambiental por macroplásticos, microplásticos y nanoplásticos, capaz de invadir todo el planeta. Lo que es, por supuesto, una manifestación más de que nada es gratis y de que todo tiene un costo.",
    "El 27 de diciembre de 1984, un grupo de cazadores de meteoritos auspiciados por la Fundación Nacional de la Ciencia de los Estados Unidos, descubrió en un campo de hielo en la Antártida un meteorito de poco menos de dos kilogramos al que se le denominó ALH84001. Si bien en su momento el origen del meteorito no fue correctamente identificado, estudios posteriores relativos a su composición de isótopos de oxígeno, pusieron en claro que se formó en el planeta Marte hace unos 4,000 millones de años, cuando habría existido agua líquida en la superficie marciana. El meteorito ALH84001 fue eyectado de la superficie de Marte por el impacto de un cuerpo celeste, y después de viajar en el espacio por unos 16 millones de años, aterrizó finalmente en la Antártida hace unos 13,000 años.El meteorito ALH84001 alcanzó gran notoriedad en 1996, cuando la NASA anunció que en el mismo se habían encontrado restos orgánicos que apuntaban a la existencia de vida en Marte en un pasado remoto. Dicha evidencia fue detallada en un artículo publicado en la revista “Science” el 16 de agosto de 1996, por un grupo de investigadores encabezados por David McKay del “Lyndon B. Johnson Space Center” de la NASA en Houston, Texas. En su artículo, McKay y colaboradores apoyan sus conclusiones en una serie de observaciones, que incluyen imágenes obtenidas por medio de un microscopio electrónico de lo que serían los restos fosilizados de microbios marcianos, lo mismo que compuestos orgánicos y magnéticos que atribuyen a la actividad biológica. Ciertamente, McKay y colaboradores son cautos y admiten que, tomadas por separado, sus observaciones no constituyen una evidencia contundente del origen biológico de los materiales y estructuras observadas en el meteorito. Afirma, sin embargo, que en su conjunto las observaciones apoyan de manera sólida sus conclusiones. El anuncio de la NASA sobre los descubrimientos de McKay y colaboradores incluso motivó un comunicado de prensa del presidente Clinton en el que expresó: “Hoy la roca 84001 nos habla a través de todos estos miles de millones de años y millones de millas. Habla de la posibilidad de la vida. Si se confirma este descubrimiento, seguramente será una de las visiones más sorprendentes de nuestro universo que la ciencia haya develado jamás. Sus implicaciones son tan trascendentes e impresionantes como se pueda imaginar. Aunque promete respuestas a algunas de nuestras preguntas más antiguas, plantea otras aún más fundamentales”. Por su parte, el famoso divulgador científico Carl Sagan, después de calificar al descubrimiento como “glorioso”, expresó: “La posibilidad de llegar en forma independiente al mismo tipo de vida en dos planetas independientes es muy pequeña. Esta es una de las grandes emociones: ver cómo procede su historia evolutiva en dos planetas diferentes”. Las conclusiones de McKay y colaboradores, sin embargo, han sido muy controvertidas y es posible que en la actualidad haya las suficientes evidencias de que no son ciertas. Los últimos argumentos al respecto aparecieron la presente semana en la revista “Science”, en un artículo publicado por un grupo de investigadores encabezado por Andrew Steele de “Carnegie Institution for Science” en Washington, D.C. De acuerdo con Steele y colaboradores, los compuestos orgánicos que se encontraron en el meteorito ALH84001 son el resultado de la interacción del agua con rocas marcianas y no tienen un origen biológico -de hecho, apuntan, la misma interacción no biológica se observa también en nuestro planeta-. Llegan a esta conclusión después de aplicar técnicas analíticas sofisticadas al estudio del meteorito, incluyendo la obtención de imágenes nanométricas por medio de un microscopio electrónico, y el análisis de su composición química también en una escala nanométrica. Así, no pareciera que el meteorito ALH84001 nos traiga noticias acerca de la evolución de la vida en nuestro planeta vecino que, sin duda, constituiría la noticia científica del siglo. Esto no deja de ser decepcionante, para nosotros y para la NASA, que justifica buena parte de sus proyectos de exploración planetaria en la búsqueda de vida extraterrestre.Así, por lo pronto, tendremos que seguir esperando por evidencias sólidas de la existencia de vida extraterrestre. En este sentido, si bien el meteorito ALH84001 no nos las proporciona de manera inmediata, sí nos demuestra que, como comenta Steele: “En Marte ocurren reacciones químicas no biológicas que dan origen a un conjunto de compuestos orgánicos a partir de los cuales la vida puede haberse desarrollado, y que representan un trasfondo que debe tomarse en cuenta para la búsqueda de vida en el pasado en ese planeta”. Con seguridad, es solo cuestión de tener un poco de paciencia.",
    "Ahora que hemos superado el puente Guadalupe-Reyes, no faltarán, aquellos con sentimientos de culpa por los kilos extras adquiridos durante las fiestas decembrinas que hayan incluido entre sus propósitos para el año nuevo el de reducir peso. Para esto tendrán a su disposición una gran variedad de dietas para adelgazar; algunas más sustentadas que otras, por estrellas de cine incluso. No obstante, y al margen de las características de dichas dietas y de lo famoso de sus proponentes, la fisicoquímica del cuerpo se impone:  para bajar los kilos de más habremos de eliminar la energía química acumulada en el cuerpo por haber ingerido más de la que estrictamente necesitábamos para funcionar. Es decir, el exceso de grasa corporal tiene que ser combinada con el oxígeno que respiramos, liberando la energía requerida y dejando dióxido de carbono y agua como residuos. Estos residuos deberán ser eliminados del cuerpo por alguna vía para hacer efectiva la pérdida de peso y en este sentido surge la pregunta: ¿Cuál es la vía más importante para expulsarlos? Un artículo publicado en diciembre de 2014 -pero de gran relevancia en estos momentos de apuro- nos ofrece una respuesta. Dicho artículo fue publicado en la revista “British Medical Journal” por Ruben Meerman y Andrew Brown de la Universidad de Nueva Gales del Sur en Australia. De acuerdo con Meerman y Brown, para oxidar 10 kilogramos de grasa corporal se usan 29 kilogramos de oxígeno proporcionados por el aire que respiramos. Calcularon que la oxidación genera como residuos 28 kilogramos de dióxido de carbono y 11 kilogramos de agua que deben ser desechados por el cuerpo. Los investigadores calcularon también los porcentajes de grasa corporal que son eliminados, como dióxido de carbono a través de los pulmones, y como agua. Encontraron que por medio de los pulmones se elimina el 84 por ciento de los residuos, mientras que solamente un 14 por ciento de los mismos se desechan como agua. Esto último, mediante la orina, las heces, el sudor y otros fluidos corporales.La respiración es entonces la principal vía para perder peso mediante la exhalación de dióxido de carbono, lo que no deja de ser sorprendente. De hecho, Meerman y Brown incluyen en su artículo una tabla en la que consignan opiniones de diferentes especialistas que resultan erradas en su mayoría. Así, a la pregunta de ¿Cuando alguien pierde peso a dónde se va?, más del 60 por ciento de los médicos familiares y de los dietólogos contestaron que se transforma en calor -violando la conservación de masa-, mientras que menos del 5 por ciento contestó de manera correcta.Basados en sus resultados, Meerman y Brown nos ofrecen ejemplos de interés práctico. Así, calculan que una persona de 70 kilogramos de peso, descansando y desarrollando actividades ligeras cada ocho horas, exhala por día unos 203 gramos de carbono. Si esta misma persona corre por espacio de una hora, incrementa su exhalación de carbono por unos 39 gramos, elevando su emisión total diaria a unos 240 gramos de carbono. A manera de comparación, 500 gramos de azúcar contienen 210 gramos de carbono, al mismo tiempo que un panqué de 100 gramos equivale a un 20 por ciento de los requerimientos energéticos diarios de una persona. Concluyen Meerman y Brown que los esfuerzos por bajar de peso mediante la actividad física pueden ser fácilmente superados por cantidades relativamente pequeñas de comida en exceso. Tenemos así que, en cierto modo y si hemos de mantener la línea, la velocidad con la que trabajan los pulmones tendría que seguir a la velocidad con la que ponemos a trabajar al estómago. A juzgar por la epidemia de sobrepeso y obesidad que asuela al mundo, sin embargo, tal parece que es más fácil que se cansen los pulmones a que se canse el estómago. De este modo y dado que al nivel de la población en general no se tiene conocimiento de la relación directa entre respiración y pérdida de peso -que refuerza la idea de “come menos y muévete más”-, recomiendan Meerman y Brown que: “todos estos conceptos sean incluidos en los programas educativos de las escuelas secundarias y de los cursos de bioquímica en las universidades, y así corregir las ideas erróneas que se tienen acerca de la pérdida de peso”.Por lo pronto, si fuera el caso y dado que palo dado ni Dios lo quita, como propósito de año nuevo habría que someternos a un régimen de dieta y ejercicio físico para perder -vía la respiración- los kilos de más ganados en el pasado puente de fin de año.",
    "Al acercarse el final de cada año es costumbre hacer un recuento de los principales avances y descubrimientos científicos y tecnológicos ocurridos a lo largo de los últimos doce meses. En esta ocasión, por ejemplo, el “Smithsonian Magazine” destaca algunas actividades espaciales, incluyendo las tres misiones enviadas a Marte por los Estados Unidos, China y los Emiratos Árabes Unidos, el nuevo telescopio espacial de la NASA y el despegue del turismo espacial. Destaca, igualmente, el desarrollo de píldoras para tratar el Covid, el descubrimiento en China de lo que podría ser una nueva especie humana arcaica, y los efectos negativos que el cambio climático está produciendo en los bancos de coral.Los avances científicos tecnológicos se han ido acumulando desde hace cuatro siglos, incrementando nuestro conocimiento de la naturaleza, lo que ha posibilitado el desarrollo de tecnologías que han cambiado radicalmente, y en tiempo récord, nuestro estilo de vida.  En este contexto, resulta interesante trasladarnos cien años hacia el pasado, hasta el final de 1921, y preguntarnos por desarrollos científicos o tecnológicos sobresalientes que se llevaron a cabo en dicho año.Para obtener una respuesta, una buena fuente de información es la revista británica “Nature”, que se ha publicado desde 1869. Así, hojeando “Nature”, encontramos en el número del 27 de octubre de 1921 un artículo con el título “Speaking films” publicado por Alexander Rankine del “Imperial College London” en el que describe una técnica para hacer películas sonoras. Como sabemos, en la década de los años veinte del siglo pasado el cine era una industria consolidada basada en películas mudas, cuya proyección se acompañaba con música u otros sonidos grabados. No obstante, y a pesar del éxito del cine mudo, ya desde finales del siglo XIX se dieron intentos para agregar sonido sincronizado –en particular, diálogos– con las escenas proyectadas. El principal problema para hacer una película sonora era la sincronización de la imagen con el sonido. Sin esta sincronización la proyección resultaba poco creíble o aún risible. Así, si la imagen y el sonido se grababan en medios diferentes, era necesario que ambos empezasen a reproducirse de manera simultánea, lo cual no necesariamente ocurría –dependiendo de la habilidad del operador–. Además, si por alguna circunstancia hubiera que reparar la cinta de la película eliminándole algún tramo y volviéndola a unir, el problema de sincronización con la banda sonora se incrementaba de manera considerable. De manera alternativa, la sincronización de la imagen y el sonido podría lograrse de manera automática grabándolos en el mismo rollo de película. Esto último, discutido en el artículo de Rankine, fue la solución que finalmente adoptó la industria cinematográfica. Dicha solución requirió de desarrollos tecnológicos que, si bien no estaban todavía lo suficientemente maduros en 1921, no hubo que esperar mucho para verlos cristalizar en la década de los años treinta. Hace cien años, sin embargo, había quienes dudaban que el cine sonoro tuviera algún futuro. Por ejemplo, un comentario al artículo publicado por Rankine, aparecido el 10 de octubre de 2021 en “Nature” firmado por Lough Pendred, lo pone de este modo: “Si bien el problema mecánico de sincronización podría ser resuelto, hay una dificultad más seria que podría perdurar y que puede ser mejor descrita como psicológica. Hay que recordar que, tanto por las imágenes como por el sonido, una película busca engañar a los sentidos humanos. Al ojo se le engaña haciéndolo creer que ve personas reales en movimiento. Por otro lado, si la película falla en engañar al sentido de la vista, falla también en lograr el efecto psicológico. De la misma manera, el sonido de la película trata de engañar al sentido del oído. A menos que realmente creamos que estamos oyendo a Enrico Caruso, el placer y el efecto solo serán parciales”. Y prosigue Prended: “Ahora, mi propia experiencia es que se puede engañar a un sentido, pero no a dos al mismo tiempo. Se puede engañar al ojo con las imágenes o al oído con el sonido, pero si se intenta engañarlos a ambos de manera simultánea se fracasará y ambos engaños se destruirán”. Pendred apoya sus argumentos con una experiencia personal: una película sonora en la que aparecía una persona bailando al ritmo de un banjo, la cual no le pareció creíble en absoluto. A toro pasado, cien años después, sabemos que Prended estaba equivocado de cabo a rabo y que el cine sí es capaz de engañar a nuestros ojos y oídos y transportarnos a un mundo imaginario.   Por lo demás, la historia del cinematógrafo –con apenas cien años de vida– nos muestra lo mucho que puede cambiar el estilo de vida de tanta gente en un tiempo tan corto. Todo gracias a la ciencia y la tecnología que de ella resulta.",
    "Nos despertamos el pasado día de Navidad con una buena nueva: a las 6 horas y 20 minutos de la mañana despegó exitosamente en el puerto espacial europeo en la Guayana Francesa, un cohete de la Agencia Espacial Europea con el nuevo telescopio espacial de la NASA a bordo. El lanzamiento de dicho telescopio, bautizado como “James Webb Space Telescope”, culminó un esfuerzo de 25 años a lo largo de los cuales el proyecto sufrió de obstáculos administrativos y de sucesivos incrementos presupuestales que lo llevaron a alcanzar un costo final sorprendente, cercano a los 10,000 millones de dólares. El telescopio James Webb tiene grandes ambiciones. Intenta, por ejemplo, detectar radiación infrarroja que se generó hace 13,500 millones de años -es decir, radiación generada en un lugar tan lejano, que le tomó 13,500 millones de años en llegar hasta nosotros-, y con esto estudiar el Universo en una etapa temprana desde su formación. Intenta, también, estudiar la atmósfera de planetas fuera de nuestro sistema solar. ¿Qué ventajas tiene un telescopio espacial con respecto a otro situado en la superficie de la Tierra? Sabemos que la atmósfera obstaculiza de diversas maneras la observación del firmamento. Puede, por ejemplo, absorber parte de la radiación que proviene del espacio haciéndola difícil o imposible de detectar desde la superficie de nuestro planeta. Esto es particularmente cierto de la radiación infrarroja, que es invisible para nosotros, pero que es de gran interés para los astrónomos. De hecho, el nuevo telescopio espacial tiene como propósito obtener imágenes infrarrojas del Universo. Así, es crucial situarlo en el espacio, por encima de la atmósfera.El telescopio James Webb cuenta con un espejo de 6.5 metros de diámetro formado por 18 segmentos hexagonales recubiertos de una delgada película de oro. Será colocado a una distancia de 1.5 millones de kilómetros de nuestro planeta -unas cuatro veces la distancia entre la Tierra y la Luna-, en un punto en donde seguirá a la Tierra en su órbita alrededor del Sol. Está diseñado para trabajar a una temperatura de menos 230 grados centígrados. Para lograr esta baja temperatura, el telescopio cuenta con una “sombrilla” de cinco capas de un material llamado kaptón, que lo protege de la radiación del Sol. Con esta protección el telescopio puede alcanzar su temperatura de operación emitiendo radiación hacia el espacio. ¿Cuál es la razón por la cual el telescopio espacial requiere de una temperatura tan baja para operar? La razón es simple de entender, aunque posiblemente no sea aparente a primera vista. En efecto, sabemos que todos objetos, dependiendo de su temperatura, emiten radiación. Esto es evidente a simple vista si, por ejemplo, calentamos un objeto metálico a unos 500 grados centígrados, punto en el que el objeto se podrá al rojo vivo y emitirá una radiación de color rojizo. Si ahora lo enfriamos hasta unos 200 grados centígrados, el objeto aparentemente deja de emitir. Esto, sin embargo, es incorrecto, pues la emisión seguirá existiendo solo ahora en la forma de radiación infrarroja que no podemos ver -lo podríamos comprobar acercando la mano al objeto, sin tocarlo-. Así, sin importar que tan baja es su temperatura, un objeto un objeto emitirá radiación infrarroja en cierto grado.Pongámonos ahora en el lugar de los diseñadores del telescopio espacial que intentan detectar la radiación infrarroja que llega a la Tierra emitida por objetos muy lejanos. Para esto tuvieron que ser muy cuidadosos con la temperatura a la que operaría el telescopio, la cual debe ser lo suficientemente baja más baja para que la radiación infrarroja que inevitablemente emita no oscurezca la radiación infrarroja proveniente del espacio. De otro modo, el telescopio quedaría prácticamente ciego e inutilizable. Con todo lo anterior, no es difícil entender que el nuevo telescopio espacial sea un instrumento extremadamente sofisticado, que operará a una temperatura de menos 230 grados centígrados, a una distancia de 1.5 millones de kilómetros de la Tierra, obteniendo imágenes de cómo era el Universo hace 13,500 millones de años. Un instrumento que con seguridad nos permitirá avanzar en la comprensión del Universo. Si bien al sorprendente costo de 10,000 millones de dólares.",
    "El pasado 23 de noviembre, la NASA lanzó una nave espacial con la misión de colisionar con el asteroide Dimorfos. El objetivo de la NASA es averiguar en qué medida es posible desviar la trayectoria de un asteroide con el objeto de desarrollar estrategias para defendernos del impacto de asteroides en ruta de colisión con la Tierra. Ciertamente, no es el caso de Dimorfos, que en su máximo acercamiento a nuestro planeta se encontrará a una distancia diez veces mayor que la distancia entre la Tierra y la Luna. No obstante, aun con una probabilidad muy baja, no podemos descartar que en el futuro nos enfrentemos con un asteroide de gran tamaño con resultados desastrosos para el planeta. El ejemplo extremo en este sentido es el bien conocido asteroide de 10 kilómetros de diámetro que provocó la extinción de los dinosaurios al final del periodo Cretácico hace 66 millones de años. Como sabemos, el impacto de este asteroide ocurrió cerca del pueblo de Chicxulub, en la costa norte de la península de Yucatán, generando un cráter de 200 kilómetros de diámetro que produjo terremotos y tsunamis gigantescos. El impacto lanzó a la atmósfera enormes cantidades de polvo y partículas que bloquearon por años la luz solar, impidiendo el proceso de fotosíntesis y llevando a la tercera gran extinción de especies en la historia de la Tierra.Con respecto a la extinción de especies, los especialistas consideran que es esencial determinar la época del año en que ocurrió la caída del asteroide, dado que muchas funciones biológicas, como los patrones de reproducción y las estrategias de alimentación, dependen de las estaciones del año.  En este sentido surge la pregunta: ¿es posible determinar la época del año en que ocurrió un evento, ciertamente catastrófico, pero extremadamente remoto? De manera sorprendente, un artículo aparecido esta semana en la revista “Scientific Reports” nos da una contestación afirmativa. Dicho artículo fue publicado por un grupo internacional de investigadores, encabezado por Robert DePalma de la Universidad de Manchester, en el Reino Unido.  En su artículo, DePalma y colaboradores llegan a la conclusión de que el inicio del fin de los dinosaurios ocurrió en un día de primavera-verano hace aproximadamente 66 millones de años. Basan sus conclusiones en múltiples evidencias recogidas a lo largo de varios años en un sitio llamado Tanis, en el estado norteamericano de Dakota del Norte.  En este sentido, hay que señalar que hace 66 millones de años la parte central del actual territorio de los Estados Unidos estaba cubierta de norte a sur por un mar interior que se conectaba con el golfo de México. El sitio en el que DePalma y colaboradores recogieron sus evidencias habría estado localizado cerca de la costa de dicho mar interior. De acuerdo con el artículo de referencia, a partir de los 13 minutos de que se produjo el impacto del asteroide y a lo largo de las siguientes dos horas, se produjo una lluvia de partículas sobre el área de Tanis, la cual fue posteriormente sepultada con la flora y fauna del lugar por el tsunami que le siguió, juntamente con las especies marinas arrastradas por el mismo. De esta mezcla de animales y vegetales, terrestres y marinos, y preservada por los sedimentos, fue donde DePalma y colaboradores encontraron los materiales para llevar a cabo su investigación.Estudiaron, por ejemplo, los huesos de peces que mostraron que las bandas características que indican el ritmo de crecimiento de los huesos año con año, e incluso de estación a estación, fueron interrumpidos abruptamente, sugiriendo que la muerte de los peces ocurrió durante los meses de primavera y verano. De manera adicional, por medio de un estudio del patrón de isótopos de carbono de los huesos fósiles, el cual está relacionado con la ingesta de alimentos, los investigadores concluyeron que el patrón de isótopos corresponde al periodo de mayor ingesta de alimentos, o sea, los meses de primavera-verano. La edad de los peces fósiles más jóvenes encontrados, y dado que se conocen los meses de desove, indica también que su muerte ocurrió durante la primavera-verano. Así, tal parece que los dinosaurios encontraron su destino un día de primavera-verano de hace 66 millones de años. No tenemos la fecha exacta de tan desafortunado evento, pero posiblemente sea irrelevante. Más relevante es saber que ocurrió durante la primavera, dada la dependencia de muchas funciones biológicas con la época del año y las consecuencias que un evento futuro como el de Chicxulub podría tener sobre la vida en la Tierra. Por lo demás, habría que reconocer que: 1) el evento fue desafortunado para los dinosaurios, pero no para nosotros, pues Chicxulub marcó el inicio de la era de los mamíferos, a los cuales pertenecemos, y 2) que no está de más mantener vigilado nuestro vecindario en busca de asteroides amenazadores.",
    "Nuestro futuro energético está sin duda alguna centrado en el Sol, que no solamente irradia la superficie de nuestro planeta con una cantidad de energía que es varios miles de veces más intensa que la que necesitamos para todas nuestras actividades, sino que además constituye una fuente de energía prácticamente inagotable y, en principio, no contaminante del medio ambiente. A pesar de todas sus cualidades positivas, sin embargo, la radiación solar no ha sido la fuente de energía que el mundo utilizó para llevar a cabo la revolución industrial de los últimos dos siglos que, como sabemos, se ha basado primordialmente en los combustibles fósiles, primero el carbón y luego el petróleo y el gas natural. Una razón de fondo para que así haya ocurrido resulta más o menos obvia: es más fácil prenderle fuego a un material combustible que construir un dispositivo para convertir la energía del sol en otra forma de energía más conveniente. Por supuesto, en su momento no fue fácil construir una máquina que usara el calor producido por la combustión del carbón para generar vapor de agua y mover un pistón mecánico que potenciara la actividad industrial. Más difícil, sin embargo, fue idear y construir un panel solar que capturara la luz solar y la convirtiera en energía eléctrica, pues para esto se necesitaron sofisticadas teorías físicas y el desarrollo de tecnologías igualmente sofisticadas para la purificación, tratamiento y manipulación de materiales, las cuales no estuvieron disponibles hasta mediados del siglo pasado. De manera adicional, al margen de sus cualidades positivas, la energía solar tiene puntos negativos. Uno de ellos es su dilución. Es decir: si bien la cantidad de energía que incide sobre toda la superficie de la Tierra es enorme, la que incide en un metro cuadrado de terreno es apenas suficiente para encender un televisor de tamaño mediano -cuando brilla el sol, por supuesto-. Así, es necesario cubrir con paneles solares un área relativamente grande para casi cualquier aplicación práctica. En contraposición, la energía contenida en un combustible fósil está altamente concentrada -puede incluso explotar, como bien sabemos- y es fácilmente transportable, además de que está disponible a toda hora del día. Por todo lo anterior, no es sorprendente que los combustibles fósiles le hayan tomado la delantera a la energía solar. No obstante, a partir de la crisis del petróleo de 1973 y dado el creciente problema de contaminación ambiental, la energía solar ha tomado un papel cada vez más relevante, a un grado tal que actualmente se considera que es una tecnología clave para limitar el calentamiento del planeta. Con respecto a esto, un reporte dado a conocer el pasado 1 de diciembre por la Agencia Internacional de Energía (AIE), predice que el presente año terminará con un incremento de 290 gigawatts en la capacidad mundial de generación de energía eléctrica por fuentes renovables, de los cuales 160 gigawatts corresponderán a la energía solar. El incremento en la capacidad de generación de electricidad por energía solar que se dará en 2021 -un 17 por ciento-, no tendría precedente, y sería equivalente a 100 centrales nucleares como la de Laguna Verde en el estado de Veracruz. En el mediano plazo, la AIE espera que en el año 2026 la capacidad de generación de energía eléctrica renovable se incremente un 60 por ciento con respecto a su nivel en 2020 y alcance un nivel equivalente al nivel actual combinado de generación de electricidad por combustibles fósiles y energía nuclear.  El incremento en la capacidad instalada de energías renovables se dará a pesar de la elevación de los costos de insumos clave para la fabricación de paneles solares y turbinas de viento, ayudado por la elevación en los costos de los combustibles fósiles. No obstante, la AIE predice que si continúan los altos costos de las materias primas a lo largo de 2022, podrían desaparecer las reducciones de costos de la energía solar que se han acumulado en los últimos tres años.De un modo u otro, y a pesar de su rezago inicial, la energía solar se ha convertido en una tecnología de generación de energía limpia que será clave para combatir el cambio climático en los decenios por venir. No obstante, la AIE advierte que, aun con la impresionante velocidad con la que están creciendo, las energías renovables tendrían que doblar su velocidad de crecimiento si se ha de alcanzar la meta de emisión cero de contaminantes fijada para la mitad del presente siglo. Quedarían, pues, algunos esfuerzos por hacer.",
    "Atrapados por la pandemia de Covid 19 y ante la perspectiva de una cuarta ola de contagios por la variante ómicron, que se anuncia será más mortífera que las anteriores, habría quizá que recordar, a manera de triste consuelo, que las epidemias de grandes proporciones han sido una constante en la historia. Pensemos, por ejemplo, en la epidemia de peste bubónica que diezmó a la población de Europa en el siglo XIV o, más recientemente, a la llamada gripe española que produjo hace un siglo entre 20 y 50 millones de muertos. Por lo demás, la actual pandemia ha traído cambios que se augura serán permanentes, al igual que ha acelerado otros que estaban ya en curso. Se espera, por ejemplo, que el llamado trabajo en casa se haga más común, lo mismo que las reuniones virtuales de trabajo y las compras en línea. Vendrían también cambios menos obvios. A saber, y a manera de ejemplo, habría una tendencia a que el modelo de transporte aéreo basado en “hubs” -aeropuertos de grandes proporciones que concentran el tráfico aéreo y lo distribuyen hacia aeropuertos más pequeños- sea sustituido por un modelo de tráfico de punto a punto -no a través de un “hub”-, con el fin de limitar la concentración de grandes multitudes de personas y evitar contagios.  Esperaríamos, por supuesto, que una epidemia que acabe con un porcentaje sustancial de la población lleve a cambios irreversibles. En ese sentido, se considera que la epidemia medieval de peste bubónica en Europa contribuyó al fin del régimen feudal y al advenimiento de la Edad Moderna.  Tomando en cuenta todo lo anterior, resulta de actualidad comentar un artículo aparecido esta semana en la revista “Past and Present”, publicado por Peter Sarris de la Universidad de Cambridge en el Reino Unido. En dicho artículo, Sarris refuta a Merle Eisenberg de la Universidad de Princeton y a Lee Mordechai de la Universidad de Maryland, quienes consideran que el impacto y significado de la epidemia conocida como Plaga de Justiniano ha sido exagerado por los historiadores y que dicha epidemia fue en realidad “similar a uno de nuestros brotes de influenza”. Habría que recordar que la Plaga de Justiniano fue una pandemia de peste bubónica que se detectó por primera vez en el puerto de Pelusio en el delta del río Nilo en el verano del año 541 de nuestra era, y en menos de un año se propagó hasta Constantinopla -hoy Estambul-, de donde se extendió hacia el Medio Oriente, Europa y el norte de África. Habría también que recordar que en ese tiempo Constantinopla era la capital del Imperio romano de Oriente, gobernado por Justiniano I.   Como menciona Sarris en su artículo, un testigo de primera mano de la situación ocasionada por la Plaga de Justiniano fue el historiador Procopius, quien estaba presente en Constantinopla cuando arribó la epidemia. Procopius escribió sus impresiones en la obra “Historia de las Guerras”, que tenía como propósito relatar las guerras libradas por Justiniano. De acuerdo con Procopio y otros historiadores, en su etapa más álgida, la epidemia cobraba más de 10,000 víctimas por día, en una ciudad de medio millón de habitantes, la mitad de los cuales habían quizá ya sucumbido por la enfermedad. El mismo emperador Justiniano habría enfermado, aunque pudo recuperarse.    Además de los relatos de testigos de primera mano, hay otras evidencias históricas que dan cuenta de la magnitud de la catástrofe que sufrió el imperio de Justiniano. En este sentido, Sarris menciona que, en una ley del año 544, Justiniano intentó imponer límites a los salarios de artesanos y trabajadores agrícolas que “intentaban aprovecharse de la escasez de mano de obra por la pandemia para obtener salarios más altos, o más altos precios por los bienes y servicios que ofrecían”.  En otra ley del año 544, “Justiniano trató de evitar que los arrendatarios de las tierras de la Iglesia negociaran rentas más bajas, mientras que al mismo tiempo permitía que la Iglesia alquilara tierras a perpetuidad para fomentar el cultivo continuo de la tierra”. Así, de acuerdo con los argumentos de Sarris, la Plaga de Justiniano estuvo lejos de ser una gripa intrascendente y tuvo un impacto social y económico importante. En cuanto a nuestra propia pandemia, y al margen de los cambios que inevitablemente nos dejará como herencia, estamos considerablemente mejor equipados -en recursos y conocimientos- para defendernos; esto, en comparación con aquellos que sufrieron pandemias siglos atrás. O sea que en ese sentido tenemos ventajas. Si bien habría que aprovecharlas al máximo.",
    "El 2 de septiembre de 2004, el submarino de exploración Alvin se sumergió a una profundidad de 2,200 metros en el límite de la placa tectónica Juan de Fuca en el océano Pacífico, enfrente de la costa del estado norteamericano de Washington. Como parte de sus actividades, el submarino recogió muestras de fauna marina del fondo del océano y las llevó a la superficie. Para sorpresa de los investigadores que las examinaron, las muestras incluyeron 38 especímenes de molusco de la especie “L. gordensis”, que es propio de un sitio localizado en el límite de la placa de Gorda, más de 600 kilómetros hacia el sur y a 2,700 metros de profundidad. El descubrimiento fue, ciertamente, sorprendente, pues el sitio en la placa Juan de Fuca en el que fueron encontrados los moluscos “L. gordensis” no cuenta con las condiciones ambientales propias para su desarrollo. En efecto, se sabe que dicha especie prospera en la placa de Gorda, en un sitio en donde existen las llamadas chimeneas hidrotermales que proporcionan los nutrientes que necesita para su desarrollo. En el lugar en el que fueron recogidos los 38 especímenes de “L. gordensis”, en contraste, no existen dichas chimeneas.   El misterio pronto fue develado: los especímenes encontrados en la placa Juan de Fuca en realidad no eran propios de ese lugar, sino que fueron llevados ahí por Alvin que dos días antes hacia hecho una sumersión en la placa de Gorda, en donde también recogió muestras de fauna local. Se trató entonces de un descuido de quien se encargó de limpiar el dispositivo de Alvin para recoger muestras y dejó restos de la operación anterior. Por lo demás, en descarga del culpable, habría que añadir que nadie pensaba que un organismo pudiera ser llevado a la superficie desde una profundidad de 2,700 metros, para enseguida ser sumergido hasta 2,200 metros y finalmente vuelto a llevar a la superficie, y que no muriera en el proceso.  Los moluscos “L. gordensis”, sin embargo, fueron capaces de superar todo este maltrato, aparentemente sin daño, y viajar entre dos puntos aislados en el fondo del mar soportando condiciones ambientales extremas. Lo anterior nos sirve como punto de partida para comentar un artículo aparecido está semana en la revista ”BioScience” en el que se discute la posibilidad de que ocurra una contaminación interplanetaria, ahora que la exploración espacial ha tomado un nuevo impulso. Dicho artículo fue publicado por un grupo internacional de investigadores encabezado por Anthony Ricciardi de la Universidad McGill en Canadá.Las condiciones ambientales para un viaje interplanetario son extremas y, ciertamente, los humanos y otros organismos superiores no podrían soportarlas a menos que fueran fuertemente resguardados en un medio ambiente artificial en el interior de una nave espacial. Si bien los moluscos “L. gordensis” tampoco lograrían sobrevivir a un viaje interplanetario sin la debida protección, su ejemplo nos demuestra que hay organismos que pueden soportar condiciones extremas sorprendentes. Y en ese sentido, habría que recordar que un estudio llevado a cabo en 2018 por científicos japoneses demostró la supervivencia de un cierto tipo de bacterias expuestas durante tres años a las condiciones del espacio, en el exterior de la estación espacial internacional. Como apuntan Ricciardi y colaboradores: “Este resultado muestra que es viable transportar células vivas entre la Tierra y Marte”.  Aun si la probabilidad de una contaminación interplanetaria fuera pequeña, en caso de darse podría tener grandes consecuencias. En este respecto, Ricciardi y colaboradores comentan que “la situación es análoga al de los desastres extremos, naturales o tecnológicos -por ejemplo, un terremoto de grandes dimensiones o la fusión del núcleo de un reactor nuclear- que, aunque son típicamente raros, tienen potencialmente consecuencias inaceptables que merecen salvaguardas únicas”.En todo caso, para los especialistas la invasión de un planeta por organismos extraños tendría consecuencias difíciles de anticipar y las mismas podrían significar una catástrofe mayor. Habría motivo para preocuparnos si la invasión se diese en un planeta a cientos de millones de kilómetros de distancia. Más habríamos de preocuparnos si dicha invasión ocurriese en nuestro propio planeta, pues el flujo de microorganismos podría ocurrir por dos vías: desde la Tierra hacia otros planetas, y desde otros planetas hacia la Tierra; esto último en la medida en que se intensifique el regreso de las naves espaciales enviadas para explorar otros cuerpos del sistema solar. Así, nuestros mejores aliados serán las duras condiciones del espacio que imponen barreras para el flujo de microorganismos.  Barreras que esperemos sean lo suficientemente altas.",
    "Cuando viajaron a bordo del satélite estadounidense “Vanguard 1” en marzo de 1958 para proporcionarle la energía que necesitaba para comunicarse con la Tierra, las celdas solares hicieron un debut estelar como dispositivos con la habilidad de aprovechar la energía del Sol. En esos momentos, sin embargo, no se tenía aprecio por la energía solar, a menos que se requiriera proporcionar energía eléctrica en un lugar remoto, lejos de las líneas de distribución de energía eléctrica, como ciertamente era el caso de la órbita de un satélite a cientos de kilómetros sobre la superficie de nuestro planeta. No pasó mucho tiempo para que las cosas cambiaran y aprendiéramos a valorar a la energía del Sol y a las celdas solares. En efecto, a raíz de la crisis del petróleo de 1973, cuando el costo de este combustible se incrementó 500 por ciento en el curso de un año, el Sol saltó a la palestra como una fuente de combustible abundante e inagotable. Los países desarrollados dedicaron entonces abundantes recursos para desarrollar tecnologías para celdas solares, que en esos momentos tenían un costo demasiado alto para competir con los combustibles fósiles como fuentes de energía.Con el tiempo, las celdas solares disminuyeron su precio a un grado tal que en estos momentos hay situaciones en donde generar electricidad por medio de paneles solares tiene un menor costo que hacerlo quemando combustibles fósiles. Además, con la crisis medioambiental por la que atraviesa el planeta, los paneles solares, como fuentes de energía limpia que no generan gases de invernadero, se han convertido en uno de los actores clave para mitigar el cambio climático.Por si todo lo anterior no fuera suficiente, los paneles solares tienen la virtud de la escalabilidad. Es decir, pueden ser usados en instalaciones pequeñas -en el techo de una casa habitación, por ejemplo-, lo mismo que en instalaciones de tamaño mediano -en el techo de un edificio público-, o en granjas solares para generar grandes cantidades de energía para ser distribuida por la red pública de electricidad. De este modo, los paneles solares pueden generar energía en instalaciones centrales o bien de manera distribuida, en el lugar en el que se le consume, sin necesidad de transportarla grandes distancias por líneas de alto voltaje.Las instalaciones de paneles solares colocados en techos de edificios y casas habitación han crecido a gran velocidad en los últimos años, multiplicándose por un factor mayor a 8 entre 2006 y 2016.  En la actualidad, un 40 por ciento de la electricidad generada por paneles solares se produce en instalaciones colocadas en los techos de edificios y casas de habitación. Ante este panorama, cabe preguntarse cuantos techos de casas y edificios existen en el mundo para colocar instalaciones solares. Un artículo publicado el pasado mes de octubre en la revista “Nature Communications” intenta responder a esta pregunta. El artículo fue publicado por un grupo internacional de investigadores encabezado por Siddharth Joshi de “University College Cork” en Irlanda. En su investigación, Joshi y colaboradores se propusieron estimar el área total de los techos de los edificios construidos sobre la superficie de la Tierra. Para este propósito, dividieron la casi totalidad de la superficie sólida de la Tierra -más de 195 países- en una red con más de tres millones y medio de celdas de 10 kilómetros cuadrados con edificios, caminos, áreas verdes, estacionamientos, entre otras estructuras. Hecho esto, el área cubierta por los edificios en cada celda fue estimada a partir de un conjunto de elementos, que incluyeron imágenes de satélite, datos poblacionales y parámetros socio económicos, empleando técnicas de inteligencia artificial. Estimaron que el área total cubierta por edificios- más de 300 millones- es de 0.2 millones de kilómetros cuadrados -un décimo de la extensión de nuestro país-. Con estos números encontraron que si se cubrieran con paneles solares el total de edificios que existen sobre la Tierra, se generaría más energía eléctrica que la que actualmente se genera a nivel global.El potencial de generación de energía solar por paneles colocados en techos de casas y edificios, sin embargo, no es el mismo en todos los lugares del mundo. Este potencial se divide entre Asia -47 por ciento-, América del norte -20 por ciento- y Europa -13 por ciento-, por nombrar a las regiones de mayor potencial.Las celdas solares han recorrido así un largo camino a lo largo de los últimos 70 años, desde su debut, a cientos de kilómetros de la superficie de la Tierra a bordo de “Vanguard 1”, hasta su papel hoy en día como la fuente de energía que nos proporciona la energía que usamos en nuestras casas. Esta vez al nivel del suelo.",
    "Dado el enorme número de planetas con condiciones parecidas a la Tierra que orbitan alrededor de estrellas en la vecindad de nuestro sistema solar, muchos especialistas consideran que el desarrollo de vida inteligente en mundos extraterrestres debe ser algo común. De hecho, a lo largo de muchos años se han llevado esfuerzos para detectar señales de radio enviadas por una civilización tecnológica fuera del sistema solar con el objeto de alertarnos de su presencia; sin éxito hasta el momento, en forma desafortunada. ¿Cuál podría ser el aspecto físico de los posibles seres inteligentes de otros mundos? No hay elementos en los que pudiéramos basarnos para aventurar una hipótesis al respecto. Ciertamente, no los hay, por ejemplo, para apoyar la diversidad de aspectos físicos que podemos apreciar entre los parroquianos de la cantina interestelar que aparece en la película “La guerra de las galaxias”. Entre estos podemos ver personajes con cabeza grande, sin pelo y con ojos almendrados, así como otros con cabeza que recuerda a la de un reptil, o bien con un cráneo rematado con una especie de cuernos dobles apuntando hacia arriba. La pretensión de “La guerra de las galaxias”, por supuesto, es solamente servir como entretenimiento y de ninguna manera presentar posibilidades, con un cierto apoyo científico, para el aspecto físico que tendrían los hipotéticos habitantes de otros mundos. De hecho, casi todos los parroquianos de la cantina tienen un aspecto humanoide, con dos piernas y dos brazos, además de que mantienen una posición erguida, características todas que no podemos asegurar tendrían los nativos de otros sistemas solares, para los cuales la evolución podría haber tomado un camino diferente.En efecto, como comenta el zoólogo Arik Kershenbaum de la Universidad de Cambridge en el Reino Unido en una entrevista aparecida en la publicación en línea “Quanta Magazine” el pasado mes de marzo: “Tenemos cuatro extremidades solo porque descendemos de un pez con cuatro aletas que salió del agua hace casi 400 millones de años. Fácilmente podríamos haber tenido seis extremidades, o incluso ocho, si la historia evolutiva hubiera sido diferente”.Por lo demás, si bien es fascinante y divertido imaginar el aspecto que tendrían nuestros vecinos interestelares en el supuesto de que existieran, el ejercicio no deja de ser simplemente una especulación. Y no dejará de serlo hasta que no tengamos información convincente al respecto, lo cual posiblemente no ocurra en un futuro inmediato.En materia de noticias interestelares, sin embargo, no todo resulta negativo -aunque no tan espectacular como es lo concerniente a los extraterrestres-, si hemos de considerar un artículo publicado esta semana en la revista “Nature Communications” por el astrónomo Siyi Xu del centro de investigación NORILab de la Fundación Nacional de la Ciencia de los Estados Unidos y el geólogo Keith Putirka de la Universidad Estatal de California. En dicho artículo, Putirka y Xu reportan los resultados de una investigación llevada a cabo para averiguar la composición química de planetas en nuestro entorno interestelar.Los planetas motivo del estudio en realidad ya no existen, sino que en algún momento fueron engullidos por sus respectivas estrellas, desintegrándose en su atmósfera. De este modo, los elementos químicos que componían un planeta dado se diluyeron en la atmósfera de la estrella que lo capturó, que se convirtió así en una “estrella contaminada” con elementos químicos diferentes al hidrógeno y al helio que le son propios.La contaminación de una estrella por la captura de un planeta resulta para nosotros un hecho afortunado, pues mediante el análisis químico de su atmósfera podemos saber cuál era la composición química del planeta capturado. Y en ese sentido, después de estudiar las atmósferas de 23 estrellas contaminadas, situadas en un radio de 650 años luz en torno al sol, Putirka y Xu encuentran algunos planetas con composiciones químicas similares a los de los planetas rocosos de nuestro sistema solar. La composición de la mayor parte de los planetas rocosos estudiados, sin embargo, es exótica y ajena a nuestro entorno planetario. De hecho, los investigadores tuvieron que crear nombres para clasificar los compuestos inusuales que encontraron.  Las estrellas a nuestro alrededor pueden entonces albergar mundos extraños: planetas rocosos diferentes a los que encontramos en nuestro sistema solar. Falta averiguar si estos mundos albergan también formas exóticas de vida, incluyendo vida inteligente. No necesariamente con dos piernas y dos brazos.",
    "Según la Organización Mundial de la Salud, mientras que en 1980 108 millones de personas en el mundo sufrían de diabetes, en 2014 este número se había cuadruplicado, alcanzando 422 millones; esto es, el 8.5 por ciento de la población mundial adulta. En los años subsecuentes la tendencia creciente se mantuvo y en 2019 el número de personas con diabetes alcanzó los 463 millones, lo que representó el 9.3 por ciento de la población adulta. Según los expertos, la tendencia continuará en el futuro y se espera que en 2045 casi el 11 por ciento de los adultos sufrirán de diabetes. Dada la situación, la diabetes es ya un problema mayor de salud pública y podría convertirse en la mayor epidemia del siglo XXI.Como nos lo explican los especialistas, el incremento en los casos de diabetes en el último medio siglo está en buena medida asociado a cambios en nuestra alimentación, que nos ha llevado a elevar el consumo de la llamada comida chatarra, con el consecuente incremento en nuestros índices de sobrepeso y obesidad. La prevalencia de la diabetes, por otro lado, no es uniforme a lo largo del mundo y afecta más a algunos países que a otros. En este sentido, es interesante mencionar el artículo publicado en 2017 por Paul Zimmet de la Universidad Monash en Australia en la revista “Clinical Diabetes and Endocrinology” en el que menciona un estudio de prevalencia de la diabetes llevado a cabo en la isla Mauricio en el océano Índico, en el que hicieron levantamientos cada cinco años entre 1987 y 2015: Encontraron que la prevalencia de la diabetes creció de 14.6 por ciento en 1987 a 23.6 por ciento en 2009, un incremento de 60 por ciento en dos décadas. Un punto interesante que Zimmet hace notar, es que la tendencia en el crecimiento de la enfermedad fue la misma para los tres grupos étnicos mayoritarios de Mauricio: chinos, sudafricanos e indios de la India, lo que indicaría que las tendencias observadas en Mauricio podrían extenderse a China y a la India, que conjuntan una buena parte de la población mundial. Estaríamos pues ante una epidemia de alcance global.De hecho, como lo consigna Zimmet, tanto en China como en la India la epidemia de diabetes ha crecido de forma acelerada. Así, la India cuenta actualmente con cerca de 90 millones de enfermos de diabetes, mientras que en China la prevalencia de la enfermedad se ha elevado desde el uno por ciento en 1980, hasta casi 10 por ciento en 2009. En concordancia con este aumento, Zimmet hace notar que el restaurante de McDonalds en la plaza Tiananmen en Beijing, China, es uno de los más activos de la cadena a nivel mundial.Una evidencia de que el consumo de comida chatarra es una de las mayores causas de la epidemia de diabetes nos la proporciona el artículo publicado esta semana en la revista “JAMA Network”, por un grupo de investigadores encabezado por Rania Kanchi, de la Universidad de Nueva York.  En su artículo, Kanchi y colaboradores reportan los resultados de un estudio llevado a cabo con el fin de averiguar si la disponibilidad de restaurantes de comida rápida en un determinado vecindario tiene alguna influencia sobre la probabilidad de que los habitantes del mismo desarrollen diabetes. Para este propósito, integraron un grupo de más de cuatro millones de veteranos del ejército estadounidense dispersos en todo el territorio norteamericano, no enfermos de diabetes, y lo siguieron a lo largo del periodo de 2008 a 2016. Esto, para averiguar si desarrollaban la enfermedad, morían o permanecían sanos a lo largo del periodo de estudio. Consideraron los investigadores cuatro tipos de vecindarios: urbanos de alta densidad, urbanos de baja densidad, suburbanos y rurales. Encontraron que una mayor densidad de restaurantes de comida rápida en un vecindario dado lleva a un incremento modesto en el riesgo de desarrollar diabetes. En contraste, una mayor densidad de supermercados está asociado con un menor riesgo de desarrollar la enfermedad en vecindarios suburbanos y rurales. Esto último, presumiblemente debido a que un supermercado ofrece opciones para adquirir alimentos que pueden cocinarse en casa para una alimentación más sana. Así, o bien la densidad de restaurantes de comida rápida en los Estados Unidos tendría que limitarse, o bien estos tendrían que ofrecer opciones bajas en grasa, sal y azúcares. Por lo demás, al margen de las medidas que se tomen, y a diferencia de otras calamidades que azotan al mundo, podemos de manera individual tomar acciones para protegernos. Y como guía, podríamos quizá recordar cuáles eran los alimentos que se consumían hace medio siglo, y dar marcha atrás a la chatarrización de la comida.",
    "Forzado por el cambio climático, el mundo ha emprendido un cambio energético. No con la decisión suficiente, sin embargo, de acuerdo con la Agencia Internacional de Energía (AIE). En efecto, en su “Energy Outlook 2001”, la AIE considera que, con las políticas energéticas vigentes, el incremento en la temperatura del planeta con respecto a su valor preindustrial podría alcanzar 2.6 grados centígrados al final del presente siglo. Este valor está muy por encima de los 1.5 grados centígrados, valor que los expertos consideran es el máximo permisible para prevenir un desastre climático. Además, ni aun con los recortes en la emisión de gases de invernadero prometidos por los países del mundo, será posible limitar el calentamiento global a 1.5 grados centígradosEn estas condiciones, la AIE considera que es necesario implementar políticas más agresivas con el fin de que en el año 2050 se produzca un equilibrio entre el volumen de gases de invernadero emitidos a la atmósfera y el volumen de gases de invernadero removidos de la misma, lo que sí permitiría limitar el incremento de temperatura global a niveles aceptables. Establece la AIE cuatro ejes para alcanzar la meta fijada. Considera primeramente que debe impulsarse masivamente el uso de la energía eléctrica en sectores tales como el transporte y la calefacción. Esto incluye la sustitución de los vehículos con motor de combustión interna por vehículos eléctricos, que sabemos es un proceso que está ya en marcha. De hecho, hay fabricantes de automóviles que han puesto fecha para dejar de producir vehículos con motor de gasolina. Por otro lado, la electrificación masiva tiene sentido si la energía eléctrica a consumir se genera con un bajo impacto atmosférico, sin quemar combustibles fósiles tal como ocurre en una central termoeléctrica. En este sentido, la AIE considera que, entre otras medidas, se requiere instalar dos veces más paneles solares y turbinas de viento que los que actualmente se contemplan en los planes más ambiciosos.Un segundo aspecto considerado por la AIE es el incremento de la eficiencia energética; es decir, usar menos energía para hacer lo mismo. En ese sentido la electrificación contribuye de manera positiva, pues hay dispositivos eléctricos que son más eficientes que sus contrapartes que usan combustibles fósiles. Es el caso, por ejemplo, de los automóviles eléctricos, que -según apunta la AIE- usan un 70 por ciento menos energía por kilómetro recorrido que los automóviles de gasolina. Una tercera medida es la supresión de las emisiones a la atmósfera de metano durante la extracción y el manejo de los combustibles fósiles. La importancia de esta medida puede apreciarse si recordamos que el metano es un gas de invernadero ochenta veces más potente que el dióxido de carbono, si bien tiene un menor tiempo de vida en la atmósfera. Finalmente, considera la AIE que debe impulsarse en la presente década innovación en tecnologías limpias de generación de energía, aun si el impacto de esta innovación sólo repercuta hasta las siguientes décadas. Se cuenta ya con todas las tecnologías necesarias para alcanzar las metas de reducción de emisiones en 2030. No es el caso de las metas para 2050, para las cuales será necesario contar con tecnologías que están en la actualidad en fase de desarrollo con el fin de cumplirlas en su totalidad.    Afirma la AIE que las medidas a implementar son viables desde el punto de vista tanto tecnológico como económico y que estas crearán a un enorme mercado para tecnologías limpias de generación de energía. En este sentido, tenemos que en la transición energética las energías solares y eólicas juegan un papel central. Estas fuentes energéticas, sin embargo, son intermitentes y requerirán de sistemas de almacenamiento que proporcionen energías cuando dichas fuentes no estén disponibles -por ejemplo, por la noche en el caso de la energía solar-. El mercado para las baterías de litio será entonces enorme. De hecho, en las proyecciones del AIE dicho mercado tendrá un crecimiento en las próximas décadas varias veces más grande que el crecimiento de otras tecnologías limpias, incluyendo las de los paneles solares y los aerogeneradores. Así, el mundo ha entrado en una fase de cambio energético, que si bien no estaría ocurriendo a la velocidad deseada, sí dejaría muy lejos aquella época de derroche energético -a mediados del siglo pasado- en la que no éramos plenamente conscientes de que, con el tiempo, con nuestros desechos, habríamos de poner en peligro al clima de la Tierra.",
    "Como sabemos, el planeta Tierra está experimentando un calentamiento global como resultado del incremento sostenido en la concentración de gases de invernadero en la atmósfera. Con respecto a su nivel previo a la revolución industrial de hace dos siglos, la temperatura del planeta se ha elevado en 1.1 grados centígrados y los expertos pronostican que alcanzará valores catastróficamente más altos si no se toman medidas correctivas. Los expertos también pronostican que el incremento en la temperatura de la Tierra provocará eventos climáticos extremos, que serán más frecuentes y más intensos en cuanto más alta sea la temperatura global. Y el presente año, con su cauda de ondas de calor, sequías, huracanes y lluvias torrenciales, les ha dado la razón.    En la convención climática de Paris de 2015 se establecieron medidas para reducir la emisión de gases de invernadero a la atmósfera y limitar el incremento en la temperatura global a menos de 2 grados centígrados con respecto a los niveles preindustriales, al mismo tiempo que se buscaría que dicho incremento no sobrepasara 1.5 grados centígrados. En seguimiento a la convención de París, y con el objeto de acelerar acciones para alcanzar las metas convenidas, el próximo mes de noviembre se llevará a cabo en Glasgow, Escocia, la Conferencia sobre Cambio Climático de la Organización de la Naciones Unidas (COP26).  Como todos los años, la Agencia Internacional de la Energía (AIE) -fundada en 1974 a raíz de la crisis del petróleo y de la cual México es miembro desde 2018 - publicó su “World Energy Outlook 2021”, en la que plasma sus perspectivas para el desarrollo del sector energético a nivel global para lo que resta del siglo. En esta ocasión, la AIE adelantó un mes su publicación anual como una contribución a la reunión COP26. Tal como escribe el director ejecutivo de la AIE en el prólogo del documento de referencia: “La edición de este año del “World Energy Outlook” ha sido diseñada, excepcionalmente, como una guía para la COP26. Explica claramente lo que está en juego: lo que significan para el sector energético y el clima las promesas de reducción de emisiones hechas por los gobiernos hasta ahora. Y deja en claro hasta donde deben extenderse estos compromisos anunciados para que tengamos buenas posibilidades de limitar el calentamiento global a 1.5 ° C y evitar los peores efectos del cambio climático”. Así, no considera la AIE que los países en su conjunto hayan hecho los compromisos suficientes para atajar al calentamiento global y que mayores compromisos y acciones son necesarias para este propósito. En su “World Energy Outlook”, la AIE considera tres posibilidades o escenarios para el desarrollo del sector de la energía y su impacto en el clima global en lo que resta del siglo. Un primer escenario asume que las políticas energéticas de los países del mundo se mantendrán en su estado actual, lo que llevaría a un incremento de temperatura de 2 grados centígrados en 2050 y de 2.6 grados centígrados en 2100. En un segundo escenario, dichas políticas se modificarán con acuerdo a los compromisos que han asumido los países, y con esto los incrementos de temperatura en 2050 y 2100 serán de 1.8 grados centígrados y de 2.1 grados centígrados, respectivamente. Finalmente, en un tercer escenario las emisiones de gases de invernadero se reducirán paulatinamente hasta alcanzar un equilibrio entre los gases de invernadero emitidos y removidos de la atmósfera en el año 2050. En este último escenario, el incremento en la temperatura global alcanzaría un valor máximo de 1.5 grados centígrados en 2030, mismo que se mantendría aproximadamente constante por el resto del siglo.De este modo, ni aún con los compromisos actuales de reducción de gases de invernadero se limitaría el incremento en la temperatura global de 1.5 grados centígrados con respecto al nivel preindustrial -además, por supuesto, de que dichos compromisos no son vinculatorios y al respecto habría que recordar que los Estados Unidos se retiraron de los acuerdos de París a iniciativa de la anterior administración estadounidense-. De este modo, según el “World Energy Outlook 2021” para mantener el incremento de temperatura a un máximo de 1.5 grados centígrados con respecto a su valor preindustrial, habría que fijar una meta de cero emisiones netas de gases de invernadero en el año 2050. Y para lograr cero emisiones netas, puesto que eliminar por completo a los combustibles fósiles no es posible, será necesario desarrollar medios para remover gases de invernadero de la atmósfera y así equilibrar aquellos que será inevitable emitir.",
    "En su colección de relatos cortos “Crónicas marcianas”, el escritor norteamericano Ray Bradbury describe a Marte como un planeta no demasiado diferente de la Tierra. Así, según Bradbury, Marte cuenta con una atmósfera enrarecida pero respirable, y si bien su suelo es de un “raro color violeta y la hierba de un rojizo pálido”, ahí crecen los árboles, cae la lluvia y corren los ríos. Hay también canales como lo que Schiaparelli creyó observar a finales del siglo XIX cuando dirigió su telescopio -que no tenía la resolución adecuada- hacía la superficie marciana. Marte estaría, además, habitado por marcianos de tez morena, ojos rasgados y de color amarillo, y con capacidades de comunicación telepática. Los marcianos, además, habrían muerto en masa por una epidemia de viruela llevada hasta allá por los primeros exploradores terrestres.Bradbury, por supuesto, no pretendía que tomáramos de manera literal sus descripciones del planeta Marte y solamente las usa para exponer situaciones propias que encara nuestra civilización, incluyendo la colonización de otros planetas y el peligro de destrucción de la Tierra por una guerra atómica -que era motivo de gran preocupación pública en la época, alrededor de 1950, en la que fueron escritos los cuentos de “Crónicas marcianas”. Aun así, es desafortunado que Marte no se acerque ni por asomo a las descripciones de Bradbury, lo que dificulta o hará imposible su colonización en un futuro previsible. Dicho planeta, en efecto, como lo muestran las imágenes que nos han enviado las sondas que han sido colocadas en su superficie, es un lugar frío e hiper árido sin el menor rastro de vegetación, y con una atmósfera tan tenue -y no respirable, puesto que está compuesta fundamentalmente de dióxido de carbono- que el agua no puede existir en forma líquida en su superficie -aunque sí, en forma de hielo, en el subsuelo marciano. Por otro lado, hubo una época en la que los especialistas sospechaban que en la superficie de Marte existieron lagos y ríos de agua líquida. Esto habría ocurrido en un pasado increíblemente remoto: hace 3,700 millones de años. Para poner esta antigüedad en perspectiva, habría que recordar que la Tierra y Marte se formaron como planetas hace unos 4,500 millones de años. Para confirmar la presencia de agua líquida en un pasado remoto sobre la superficie marciana, la NASA hizo uso de la sonda “Perseverance” que, como sabemos, arribó a la superficie de Marte el pasado 18 de febrero; de manera precisa al cráter Jezero de 45 kilómetros de diámetro, dentro del cual la NASA había identificado, mediante fotografías de satélite, lo que aparentaba ser el delta de un río que alimentaba a un lago en el interior del cráter. Una vez dentro del cráter, “Perseverance” pudo fotografiar formaciones rocosas que eran invisibles desde el satélite y que mostraban estratos con la historia geológica de dicho cráter. En un artículo aparecido esta semana en la revista “Science” se reporta un análisis de dichas fotografías. El artículo fue publicado con un grupo internacional de investigadores encabezado por Nicolas Mangold de la Universidad de Nantes en Francia. De acuerdo con el análisis de Mangol y colaboradores, las formaciones rocosas fotografiadas por “Perseverance”, efectivamente, corresponden al delta de un río que alimentaba un lago de decenas de kilómetros de diámetro. Además, los sedimentos de dichas formaciones indican que hubo una época en que el río fluyó con intensidad moderada y que el depósito de sedimentos ocurrió de manera continua. En contraste, los investigadores encuentran también estratos con grandes rocas -con dimensiones de metros- que fueron arrastradas río abajo, posiblemente por decenas de kilómetros, lo que demuestra que ocurrieron también episodios de flujos de agua violentos, cuyo origen por el momento no puede ser establecido.  Hay de este modo evidencia sólida de que en el pasado Marte era un lugar considerablemente menos seco y frío que lo que es en la actualidad, lo que apunta a la posibilidad de que hubiera albergado vida hace 3.700 millones de años. No podemos saber por el momento que tan avanzada habría sido, y para averiguarlo quizá debamos esperar que progresen las exploraciones del planeta Marte. Mismas que, sabemos, se han acelerado en lo últimos años.  Por lo demás, y al margen de lo avanzado que pudiera haber llegado a ser la vida en Marte hace miles de millones de años, no es posible dejar de recomendar la lectura de “Crónicas marcianas”, con sus poéticas descripciones de Marte y de los marcianos.",
    "Esperemos que los eventos climáticos extremos del verano pasado hayan convencido a los escépticos de que el calentamiento global es algo real -de otro modo, muy probablemente no haya esperanza de que algún día lleguen a convencerse-. Como sabemos, en los meses pasados el planeta sufrió numerosos eventos climáticos extremos de tal magnitud, que los expertos consideran son una manifestación inequívoca del calentamiento global. Estos eventos incluyeron: una ola de calor con temperaturas récord que ocasionó la muerte de más de 500 personas en la Columbia Británica y en el noroeste de los Estados Unidos, lluvias torrenciales en Europa Occidental y en China que produjeron más de doscientos muertos en Bélgica y  Alemania  y más de trescientos muertos  en la provincia china de Henan, y  enormes incendios forestales en el oeste de los Estados Unidos y en Canadá, en donde el pueblo de Lytton en la Columbia Británica fue destruido en un 90 por ciento por el fuego.  Los eventos climáticos del pasado verano incluyeron también enormes incendios en Yakutia, una región del norte de Siberia, que habrían destruido seis millones de hectáreas de bosques. Los incendios de Yakutia alcanzaron una magnitud tal que el humo habría llegado hasta el polo norte, a unos tres mil kilómetros de distancia. Más recientemente, el pasado mes de septiembre, la ciudad de Nueva York se inundó por efecto del huracán Ida.Sabemos que el calentamiento global está siendo ocasionado por el incremento sostenido de la concentración de gases de invernadero en la atmósfera desde el inicio de la revolución industrial a finales del siglo XVIII. El gas de invernadero más relevante es el dióxido de carbono, producto de la quema de combustibles fósiles. Este gas, sin embargo, no es el único al que podemos culpar del calentamiento global y en este sentido el metano ocupa el segundo lugar. En efecto, tenemos que si bien la concentración de metano en la atmósfera es unas doscientas veces más pequeña que la del dióxido de carbono, como gas de invernadero el metano es unas 80 veces más potente.  Así, los expertos estiman que un 30 por ciento del incremento en la temperatura global desde la revolución industrial es debido al incremento de metano en la atmósfera. Además de lo anterior, en comparación con el dióxido de carbono, se estima que la concentración de metano en la atmósfera está creciendo a un ritmo más acelerado y que desde el inicio de la revolución industrial se ha elevado unas 2.5 veces.Aproximadamente la mitad de las emisiones de metano a la atmósfera se originan en actividades humanas, fundamentalmente actividades relacionadas con la agricultura y la producción de alimentos, el manejo de basura, y la extracción de combustibles fósiles.  En estas condiciones, ante el crecimiento de la población mundial y de su nivel de vida, consideran algunos especialistas que en el futuro será problemático detener el crecimiento en las emisiones de metano a la atmósfera. Dada esta situación, para mitigar el efecto de invernadero del metano se requeriría extraerlo de la atmósfera con el fin de reducir su concentración en la misma. Un artículo aparecido está semana en la revista “Philosophical Transactions A” discute los beneficios que tendría dicha extracción para el clima del planeta. El artículo fue publicado por un grupo de investigadores de los Estados Unidos y el Reino Unido, encabezados por Sam Abernethy de la Universidad de Stanford. De acuerdo con el estudio de Abernethy y colaboradores, reducir en un 40 por ciento el metano en la atmósfera reduciría el calentamiento del planeta en 0.5 grados centígrados en el año 2050. Para apreciar esto último, habría que recordar que hoy en día la temperatura del planeta es alrededor de 1.2 grados centígrados más alta que al inicio de la revolución industrial, valor que alcanzaría alrededor de 2.5 grados en un escenario con altas emisiones de metano en el año 2050. Así, de estar Abernethy y colaboradores en lo correcto, si bien extraer metano de la atmósfera no resolvería por si mismo el problema del cambio climático, sí contribuiría de manera apreciable a superarlo. Esto, por supuesto, en el caso en que existieran métodos eficientes para llevar a cabo dicha extracción, lo cual por el momento no pareciera ser cierto. De este modo, al menos por los próximos años, tendremos posiblemente que acostumbrarnos a los eventos climáticos extremos.",
    "Como sabemos, las semillas de algunas plantas están diseñadas para viajar grandes distancias transportadas por el viento, lo que posibilita que una planta sedentaria pueda reproducirse en lugares alejados del lugar en donde se encuentra inmovilizada. De algún modo, dichas semillas pueden concebirse como dispositivos biológicos optimizados para viajar en el aire, los cuales fueron desarrollados por la naturaleza mediante el mecanismo de selección natural.  Dada la gran experiencia ingenieril de la naturaleza, resulta natural tratar de copiar sus diseños para construir dispositivos artificiales con funciones específicas. Por ejemplo, hay iniciativas para construir dispositivos de cómputo tomando como base la organización y el funcionamiento del cerebro humano, el cual es un “dispositivo” altamente complejo y eficiente que la naturaleza ha logrado desarrollar a lo largo de millones de años de evolución. Un artículo aparecido esta semana en la revista “Nature” nos da otro ejemplo al respecto: el desarrollo de dispositivos diminutos que pueden permanecer suspendidos en el aire por tiempos largos y viajar grandes distancias. El artículo de referencia fue publicado por un grupo numeroso de investigadores adscritos a instituciones de Corea del Sur, los Estados Unidos, China y el Reino Unido, encabezados por Bong Hoon Kim, de la Universidad Soongsil en Corea del Sur.  El diseño de los dispositivos reportados por Kim y colaboradores, en cuanto a su habilidad viajera, está inspirado en el diseño aerodinámico de las semillas voladoras de las plantas sedentarias. A diferencia de éstas, sin embargo, su propósito no es el de transportar material genético, sino el de realizar una serie de funciones, desde medir la contaminación atmosférica, hasta controlar la propagación de enfermedades.  De manera específica, los dispositivos de Kim y colaboradores, algunos con el tamaño de un grano de arena, están basados en buena medida en las semillas de la planta “tristellateia”, las cuales tienen forma de estrella y cuentan con la habilidad de mantenerse flotando en el aire por largo tiempo, descendiendo lentamente con un movimiento de rotación a la manera de las aspas de un helicóptero. Imitando a las semillas naturales, los chips voladores tienen la forma de una estrella de tres picos, cuidadosamente diseñada para que pueda permanecer en el aire por largo tiempo de manera estable. En el centro del chip pueden colocarse diversos microcomponentes, incluyendo un microcontrolador electrónico, una memoria electrónica para almacenar datos, dispositivos para analizar contaminantes atmosféricos, una fuente de energía, y una antena para radio comunicación.Kim y colaboradores imaginan enjambres de chips voladores arrojados desde un avión a grandes alturas que descienden lentamente con un movimiento estabilizado por su rotación, lo que les permitiría dispersarse en grandes áreas de terreno realizando funciones tales como medir la contaminación atmosférica, estudiar la dispersión de patógenos, o realizar observaciones de vigilancia. Todo esto a un costo reducido en comparación con los métodos tradicionales.Dispersar grandes cantidades de chips voladores en la atmósfera, por supuesto, implicaría generar una fuente de contaminación que incrementaría la presión sobre nuestro ya castigado medio ambiente. En este respecto, Kim y colaboradores reconocen el problema y mencionan en su artículo que se encuentran trabajando para desarrollar chips que puedan degradarse con el tiempo e integrarse al medio ambiente. Lo cual, no obstante, todavía estaría por verse.Kim y colaboradores, por otro lado, mencionan que, en cuanto a su habilidad para mantenerse volando, sus chips han superado a los diseños de la naturaleza, al mismo tiempo que pudieron construir chips voladores con tamaños más pequeños que los de las semillas naturales que los inspiraron. Todo esto a pesar de que a la naturaleza le tomó cientos de millones de años en desarrollarlas. Tenemos así una demostración irrebatible de que la ciencia funciona. Por más que algunas veces se ponga en duda.",
    "En la cultura popular los hombres cavernícolas frecuentemente se representan greñudos y con la barba crecida, algunas veces armados con un hacha de piedra -como Trucutú en las tiras cómicas de los años cincuenta del siglo pasado-, otras con un garrote al hombro, pero siempre vestidos con pieles de animal. Por otro lado, en la recreación de la vida en la prehistoria frecuentemente se superponen épocas que en realidad están separadas por decenas de millones de años, en el mejor de los casos. Por ejemplo, en la película “El cavernícola”, estrenada en 1981, podemos ver a Ringo Starr y a sus compañeros correr despavoridos para ponerse a salvo del ataque de un dinosaurio de cartón. La película nos presenta esta escena por más que los dinosaurios hayan desaparecido cuando a nuestra especie aun le faltaban más de sesenta millones de años para hacerse presente sobre la superficie del planeta.  Por lo demás, si bien sería esperable que los cavernícolas, dada su vida agitada, siempre se mostraran greñudos y barbudos, no resulta claro cuándo empezaron a vestirse con pieles de animal. Afortunadamente, esto último está empezando a develarse, tal como se discute en un artículo aparecido la semana que hoy termina en la revista “iScience”. Dicho artículo fue publicado por un grupo de investigadores encabezado por Emily Hallett del Instituto Max Planck en Jena, Alemania. En su artículo, Hallett y colaboradores describen el hallazgo de herramientas de hueso en la “Cueva de los Contrabandistas”, un sitio arqueológico localizado a unos 250 metros de la costa atlántica de Marruecos. Los investigadores exploraron dicha cueva con el objetivo de averiguar cuál era la dieta de sus primitivos moradores hace 90,000-120,000 años. Con este propósito recogieron huesos de animales encontrados en su interior para determinar a qué especies correspondían y si estos habían sido utilizados como alimento.En su investigación Hallett y colaboradores encontraron huesos de animales que, efectivamente, mostraban signos de haber sido parte de la alimentación de los moradores de la cueva. Al mismo tiempo, sin embargo, encontraron huesos que habían sido tallados cuidadosamente con el propósito claro de fabricar herramientas. Especulan los investigadores que estas herramientas, en la forma de una espátula con extremos redondeados, se usaban para limpiar y curtir pieles de animales para fabricar prendas de vestir. Esta conclusión está reforzada por el hecho de que en la cueva se encontraron huesos con marcas que no correspondían a las marcas típicas encontradas en huesos de animales sacrificados con propósitos de alimentación. Y sí, por el contrario, consistentes con el propósito de remover en una sola pieza la piel del animal.De este modo, encontraron Hallett y colaboradores evidencia de que hace 100,000 años nuestros ancestros ya utilizaban prendas de vestir, y en este sentido surge la pregunta: ¿qué fue lo que los impulsó a los moradores de la Cueva de los Contrabandistas a cubrirse el cuerpo? Ciertamente, dado lo delgado de la piel humana, una cubierta de piel de animales habría proporcionado protección en contra de las temperaturas bajas. Sin embargo, apuntan los expertos, esto que es válido para el clima frío del norte de Europa, lo es menos en Marruecos, en donde la temperatura era más moderada.En estas condiciones surge una segunda pregunta: ¿se habrían vestido los moradores de la Cueva de los Contrabandistas simplemente por el gusto de hacerlo y andar bien presentados? Esta es, por supuesto, una pregunta fascinante para la que desafortunadamente por el momento no tenemos respuesta. Hacen notar los expertos, sin embargo, que la introducción del vestido en la prehistoria coincide con la aparición de adornos personales con conchas marinas, que claramente no tienen un propósito práctico. Queda pues abierta la pregunta de si la moda en el vestir es más vieja de lo que hubiéramos imaginado, y si tiene una antigüedad de cuando menos 100,000 años. Los resultados de Hallett y colaboradores, por otro lado, apoyan la imagen popular del cavernícola vestido con pieles. Esto, al menos en lo que respecta a los cavernícolas que vivieron desde hace unos 100,000 años a la fecha.  No hay apoyo alguno, por el contrario, para escenas de cavernícolas luchando o huyendo de dinosaurios más o menos reales. Con la excepción, por otras razones, de la película “El Cavernícola”, que resulta muy divertida.",
    "La mañana del 24 de agosto del año 79 de nuestra era -aunque, según otras versiones, podría haber sido un par de meses después- la vida de las ciudades romanas de Pompeya y Herculano llegó a un fin abrupto. Como sabemos, ese día el Vesubio hizo erupción sepultando a las dos ciudades bajo una gruesa capa de material volcánico. En el evento murió un número indeterminado de sus habitantes intoxicados con gases venenosos. Y los que no murieron hubieron de huir sin posibilidades de regresar.Al tener un fin abrupto, sepultadas por material volcánico, Pompeya y Herculano fueron congeladas en el tiempo, y en ese sentido fue un evento afortunado, pues nos proporciona una “fotografía” del estilo de vida en las ciudades romanas hace dos mil años. Por supuesto, con seguridad los habitantes de Pompeya y Herculano no estarían de acuerdo en que la erupción del Vesubio y el fin abrupto de sus ciudades haya sido afortunado, por más de que, de alguna manera, hayan sido inmortalizados. Al margen de controversias, sin embargo, habría que conceder que, dado que no es posible retroceder en el tiempo y tratar de mitigar la catástrofe de Pompeya y Herculano con nuestros conocimientos de vulcanología, debemos sacar el máximo provecho al afortunado/desafortunado evento.Con este ánimo, un artículo aparecido el pasado 25 de agosto en la revista “Science Advances” nos proporciona información detallada sobre los alimentos que consumían los habitantes de Herculano en el tiempo de la catástrofe. El artículo fue publicado por un grupo internacional de investigadores encabezados por Silvia Soncin de la Universidad de Nueva York. El estudio fue llevado a cabo con 17 esqueletos de habitantes de Herculano, 11 hombres y 6 mujeres, que constituyen una muestra de 340 esqueletos que fueron encontrados en nueve almacenes de piedra alineados enfrente de la playa. Las víctimas habrían llegado hasta ahí buscado protegerse del flujo piroclástico del volcán.  Como argumentan Soncin y colaboradores, “Este notable conjunto de víctimas de una catástrofe natural no solo es de gran interés público, sino que también ofrece una oportunidad para avanzar sustancialmente en nuestro conocimiento de la sociedad romana mediante la aplicación de enfoques bioarqueológicos. La muestra de esqueletos en Herculano no está limitada por los sesgos que generalmente enfrentan los osteoarqueólogos cuando estudian esqueletos enterrados en cementerios, debido a la mortalidad y entierros selectivos. En lugar de esto, los esqueletos de Herculano proporcionan una ‘instantánea’ de una población antigua que rara vez se ofrece en arqueología”.Soncin y colaboradores se propusieron averiguar la dieta de los habitantes de Herculano a partir de un análisis de isótopos de carbón y nitrógeno en el colágeno de los huesos de las víctimas. De manera interesante, los investigadores encuentran diferencias claras entre las dietas seguidas por hombres y mujeres. Así, en comparación con los hombres, Soncin y colaboradores encuentran que las mujeres en Herculano obtenían menos porcentaje de las proteínas que ingerían de cereales y productos del mar, y relativamente más de animales y productos terrestres, incluyendo carne, huevos y productos lácteos.   ¿Cuál es la razón para la diferenciación de la dieta entre hombres y mujeres? No ofrecen Soncin y colaboradores una explicación definitiva al respecto, pero en cuanto al mayor consumo de productos del mar por parte de los hombres, hacen notar que éstos tenían un mayor contacto con el mar -como pescadores- que las mujeres. También, apuntan los investigadores, los hombres generalmente ocupaban más posiciones socialmente privilegiadas en comparación con las mujeres, y eran liberados de la esclavitud a una edad más temprana que las mujeres. Todo esto habría proporcionado a los hombres mayores oportunidades para conseguir alimentos de más alto precio. En espera de una explicación más sólida, la “instantánea” de Herculano que nos proporcionó el Vesubio muestra claramente una asimetría en la alimentación de hombres y mujeres. Y si bien en la actualidad este hecho pudiera resultar en cierto grado sorprendente, habría que señalar que en la sociedad romana, en la que se practicaba la esclavitud, dicha asimetría no se habría dado a notar. Lo que sí resulta altamente sorprendente es que, aun tomando en cuenta la ayuda del Vesubio, se haya logrado obtener información tan detallada acerca de las costumbres dietéticas de una ciudad romana que desapareció hace dos mil años.",
    "Un artículo aparecido esta semana en la revista “Space Weather” discute posibilidades para dar la vuelta a uno de los múltiples obstáculos que enfrentarían los astronautas de una hipotética misión al planeta Marte: las radiaciones de alta energía que permean el espacio. Dicho artículo fue publicado por un grupo internacional de investigadores encabezado por Mikhail Dobynde del Instituto Skolkovo de Ciencia y Tecnología, en Moscú, Rusia.  En la Tierra estamos protegidos de las radiaciones del espacio por el campo magnético terrestre. En contraste, en el transcurso de un viaje a Marte los astronautas no contarían con esta protección. Una exposición del viajero a dichas radiaciones más allá de cierto límite podría provocarle cáncer y eventualmente la muerte.     Se sabe que los astronautas de una futura misión tripulada a Marte se enfrentarían a dos tipos de radiaciones de alta energía: los rayos cósmicos provenientes del espacio interestelar y las radiaciones generadas por la actividad del Sol, cuya intensidad varía de manera periódica siguiendo un ciclo de once años. En el pasado, la duración de las misiones a la Luna del programa Apollo se midió en días, y el riesgo que corrieron los astronautas por la exposición a las radiaciones de alta energía fue relativamente pequeño. En contraste, la duración de una misión a Marte se mide en años, lo que incrementa de manera sustancial el riesgo por las radiaciones del espacio. La nave que transportaría a los tripulantes de una hipotética misión a Marte tendría entonces que estar adecuadamente protegida contra las radiaciones del espacio. Cómo protegerse de estas radiaciones es el tema abordado por Dobynde y colaboradores en el artículo referido anteriormente.Como señalan Dobynde y colaboradores, las radiaciones solares a las que estarían expuestos los astronautas en su viaje a Marte, crecerían y disminuirían en concordancia con los máximos y mínimos de la actividad solar. Por el contrario, la intensidad de la radiación cósmica que experimentarían disminuiría al incrementarse la actividad del Sol y aumentaría cuando ésta disminuyese; esto, por efecto del viento solar. La radiación cósmica, por otro lado, contiene mayores números de partículas de muy alta energía en comparación con la radiación solar que está compuesta mayormente de partículas de menor energía.Basados en todo lo anterior, y tomando en cuenta que es más difícil protegerse de las partículas de más alta energía -pues más fácilmente penetrarían el blindaje de la nave espacial-, Dobynde y colaboradores determinan, a partir de cálculos de computadora, los mejores momentos para llevar a cabo el viaje a Marte. Así, dicho viaje debe iniciarse durante un máximo de actividad solar, cuando la intensidad de la radiación cósmica es menor.  Además, de acuerdo con los cálculos de los investigadores, aun con estas precauciones, la duración total del  viaje de ida y regreso a Marte de no debe exceder los cuatro años. Establecen también que las dos próximas fechas para emprender un viaje tripulado a Marte son los años 2030 y 2050. Calculan también Dobynde y colaboradores cuál sería el espesor del blindaje de aluminio de la nave espacial. Podríamos quizá pensar que entre mayor fuera dicho espesor, mayor sería la protección. Esto, sin embargo, no resulta del todo cierto, pues al absorberse en el blindaje la radiación de alta energía se produciría una radiación secundaria que penetraría al interior de la nave, aumentando la radiación absorbida por los astronautas. De este modo, el espesor del blindaje debería ser suficientemente grande para proteger el interior de la nave de las radiaciones del Sol, sin exceder, sin embargo, un cierto valor para minimizar la producción de radiación secundaria.     Dobynde y colaboradores ofrecen recomendaciones para minimizar la exposición a las radiaciones de alta energía de los tripulantes de una futura misión a Marte. Concluyen que, si bien la radiación del espacio impone limitaciones estrictas y presenta dificultades técnicas, una misión tripulada a Marte es aún viable. Por lo demás, podríamos quizá concluir que los planes -que de vez en cuando se publicitan- sobre un inminente viaje tripulado a Marte son solamente trucos publicitarios o de relaciones públicas; y que de darse dicho viaje, no se llevará a cabo en un futuro cercano. Después de todo, las radiaciones del espacio son completamente ajenas a nuestro hábitat natural.",
    "Sabemos que, como una consecuencia inesperada de la pandemia de coronavirus hay una escasez de chips semiconductores que ha afectado a la industria automotriz a nivel mundial. En particular, en San Luis Potosí ha forzado paros técnicos a lo largo del año de la armadora de General Motors. Dicha escasez sería el resultado de varios factores, incluyendo el hecho de que al inicio de la pandemia la industria automotriz -previendo una disminución en la demanda de automóviles por el confinamiento forzado- habría bajado su demanda de semiconductores, los cuales habrían sido absorbidos en su lugar por la industria del entretenimiento y los videojuegos. Así, al prolongarse la pandemia y mantenerse alta la demanda de chips para estas últimas aplicaciones, se ha producido una escasez que no ha podido ser superada. Esto último debido a que la industria de los chips semiconductores es altamente compleja y requiere de inversiones extremadamente elevadas. De hecho, existen muy pocas compañías a nivel mundial capaces de fabricarlos.La emergencia sanitaria ha perturbado así la compleja cadena de suministros de la industria automotriz, y en este contexto resulta particularmente interesante un artículo aparecido el pasado mes de julio en la revista “Enviromental Science and Technology”, en el que se analiza el impacto que producirían las fluctuaciones en los precios de las materias primas empleadas en la fabricación de los automóviles, y cómo este impacto difiere entre los automóviles con motores de combustión interna y los automóviles híbridos -que pueden funcionar tanto con motores eléctricos como con motores de gasolina-. El artículo fue publicado por un grupo de investigadores adscritos al Instituto de Tecnología de Massachusetts y al Centro de Investigación e Innovación de la compañía Ford Motor Company, encabezados por Baran Bhuwalka. En su artículo, Bhuwalka y colaboradores reportan los resultados de un estudio llevado a cabo con siete automóviles marca Ford, tanto de gasolina, como híbridos. En este último caso incluyeron automóviles cuyas baterías eléctricas pueden ser recargadas enchufando el vehículo a la red eléctrica o bien mediante la tracción del mismo vehículo. Como primera parte de su estudio, los investigadores identificaron el número total de componentes de cada vehículo -alrededor de 1,700 en promedio- y determinaron los compuestos y los elementos químicos que las integran. Encontraron que en la fabricación de los siete automóviles se emplearon más de 200 compuestos y 76 elementos químicos -más del 80 por ciento de todos los elementos que ocurren en forma natural-. Como era esperable, el hierro y el acero resultaron ser los principales constituyentes, seguidos de los materiales poliméricos y del aluminio.Enseguida, Bhuwalka y colaboradores calcularon cuál sería la “exposición” -incremento en los costos de manufactura- de cada uno de los automóviles estudiados ante las fluctuaciones estimadas en los precios de los elementos que los componen. Encontraron que los automóviles híbridos enchufables tienen una exposición entre el 75 por ciento y el 95 por ciento más alta que los automóviles de combustión interna. Esta diferencia es sobre todo debida a los materiales empleados en la fabricación de la batería del automóvil eléctrico, incluyendo cobalto, níquel y grafito, que tienen precios altos y/o volátiles. Es debida también a los materiales empleados en el sistema de transmisión y embrague.  En contraste, los automóviles híbridos enchufables muestran una menor exposición en comparación con los automóviles de gasolina en lo que se refiere al sistema de expulsión de gases.   En balance, Bhuwalka y colaboradores encuentran que los automóviles eléctricos representan para el fabricante un riesgo por incremento de precios de las materias primas que casi dobla al de los automóviles de gasolina. Ponen esto en perspectiva estimando que un fabricante que produzca un millón de unidades de automóviles híbridos enchufables afrontará un riesgo que es 1,000 millones de dólares más alto que el que afrontaría fabricando el mismo número de automóviles de gasolina.Ciertamente, el cambio climático, cuyos efectos son cada vez más evidentes y extremos, nos impone la sustitución de automóviles de combustión interna por automóviles eléctricos. Los resultados presentados por Bhuwalka y colaboradores, sin embargo, constituyen una muestra -inesperada para aquellos que no somos expertos- de que el camino no está libre de obstáculos.",
    "¿Deben los niños regresar a clases presenciales el próximo 30 de agosto o en su defecto proseguir su educación de manera remota? Como sabemos, esto es algo que se ha discutido de manera intensa en los últimos días. Por un lado, se aduce que sin clases presenciales se priva a los niños del contacto con otros niños y otras personas, con consecuencias negativas para su educación y desarrollo psicológico. Por su lado, quienes se oponen a las clases presenciales señalan que con esto se pondría en peligro su salud física, dada la ola de contagios provocada por la nueva cepa de coronavirus. Habría que reconocer que a los críticos del regreso a la escuela no les falta la razón, si tomamos en cuenta que el programa de vacunación en nuestro país no ha avanzado a la velocidad que hubiéramos deseado. Así, de acuerdo con datos de la Universidad Johns Hopkins, apenas el 22.5 por ciento de la población en México está completamente vacunada. Por otro lado, dejando de lado las obvias consecuencias negativas que tendría para la educación de los niños el no regresar a las clases presenciales, cabe preguntarse que tanta razón asiste a aquellos que aducen que la falta de contacto social los afectará negativamente en cuanto a su desarrollo psicológico. Un artículo hecho público el pasado miércoles en el sitio de preimpresos “medRxiv” nos ofrece información al respecto. Dicho artículo, en el que se estudia el desarrollo de habilidades cognitivas de niños en los primeros años de su vida, pero que todavía no ha pasado por un proceso de revisión por pares, fue publicado por un grupo de investigadores adscritos a instituciones norteamericanas encabezado por Sean Deoni, de la Universidad Brown en Rhode Island.Deoni y colaboradores reportan los resultados de un estudio llevado a cabo para determinar en qué grado el aislamiento social obligado por la pandemia de coronavirus ha afectado el desarrollo cognitivo de los niños. Escriben al respecto: “El cerebro humano es único en su línea de tiempo de desarrollo prolongada. Los bebés nacen con cerebros relativamente inmaduros que, como ellos, son a la vez competentes y vulnerables. Los bebés son intrínsecamente competentes en su capacidad para iniciar relaciones, explorar, buscar significado y aprender; pero son vulnerables y dependen completamente de los cuidadores para su supervivencia, seguridad emocional, modelado de comportamientos y la naturaleza y reglas del mundo físico y sociocultural en el que habitan. Asimismo, el cerebro infantil nace con una inmensa capacidad para aprender, remodelar y adaptarse, pero es sensible y vulnerable al abandono y a las exposiciones ambientales que comienzan incluso antes del nacimiento. La plasticidad adaptativa del cerebro, sin embargo, es una espada de doble filo. Si bien los entornos positivos y enriquecedores pueden promover un desarrollo cerebral saludable, descuidar la inseguridad, el estrés y la falta de estimulación pueden deteriorar los sistemas cerebrales maduros y alterar los resultados cognitivos y conductuales”.El estudio de Deoni y colaboradores fue realizado como parte de un programa iniciado en 2011 que tiene como objetivo estudiar el desarrollo de habilidades cognitivas de niños durante sus primeros años de vida. Esto les permitió comparar el desarrollo de niños nacidos durante la pandemia con el desarrollo de niños de la misma edad nacidos previo a la misma. Consideraron 672 niños del área del estado de Rhode Island, de raza predominantemente caucásica. Encontraron que aquellos nacidos durante los meses de pandemia muestran habilidades motoras y de comunicación verbal y no verbal sensiblemente menores que las de aquellos nacidos antes de la pandemia.  Atribuyen este resultado a la falta de estímulo a los niños, tanto por el aislamiento impuesto por la pandemia, como por el estado de estrés en el que se encuentran sus padres por el temor a infectarse y porque tienen que dividir su tiempo entre el cuidado de los niños en casa y el cumplimiento de sus obligaciones en el trabajo –en el caso, por supuesto, de que lo hubieran conservado–. Encuentran, además, que los niños con niveles económicos menores son más afectados por el mayor grado de estrés al que están sometidos sus padres.De estar Deoni y colaboradores en lo correcto –habría que recordar que su artículo tiene que pasar todavía por una revisión por pares–, la pandemia, aun sin infectarlos, puede afectar al desarrollo cognitivo de los niños en sus primeros años de vida. Y tenderá a hacerlo más en cuanto más bajo sea su nivel socioeconómico. Un ejemplo más de la inequidad del mundo. O sea que al perro más flaco se le cargan las pulgas.",
    "El Sol es, sin duda, la fuente natural de energía que deberíamos emplear para mover al mundo: es prácticamente inagotable y más que suficiente para satisfacer por completo nuestras necesidades energéticas, además de que está, en principio, a nuestra entera disponibilidad. No fue, sin embargo, la primera fuente de energía que usaron nuestros antepasados y, de hecho, sólo hemos empezado a aprovecharla hasta fechas muy recientes. \nPara ser precisos, esto último no es estrictamente cierto, pues desde que los antecesores de nuestra especie aprendieron a controlar el fuego hace cientos de miles de años, hicieron uso de combustibles vegetales que en último término tuvieron un origen solar -a través del proceso de fotosíntesis-. Así mismo, el carbón que impulsó a la Revolución Industrial hace dos siglos, y posteriormente el petróleo y el gas natural, tuvieron un origen vegetal, y en último término solar.  No obstante, si no consideramos aplicaciones tales como la que supuestamente Arquímedes llevó a cabo hace más de dos mil años en Siracusa, destruyendo barcos enemigos por medio de un espejo gigante que concentraba la luz solar, el uso sistemático de la energía del Sol solamente se ha dado en el último medio siglo. Anteriormente, no existían las tecnologías necesarias para aprovechar de manera eficiente dicha energía, más allá quizá de su aprovechamiento para calentar espacios habitables.\nEn la actualidad, la energía solar puede aprovecharse concentrándola -a la manera de Arquímedes- sobre un fluido para alcanzar temperaturas de cientos de grados centígrados. El fluido caliente es posteriormente dirigido a una turbina, la cual a su vez mueve a un generador de electricidad. De esta manera es posible convertir, en dos pasos, a la energía solar en energía eléctrica. De manera alternativa, la energía solar puede ser convertida directamente en energía eléctrica por medio de paneles fotovoltaicos; aplicación que, como sabemos, es cada vez más popular. \nA pesar de todas sus virtudes, sin embargo, la energía del Sol está marcada por un pecado original: es intermitente, siguiendo el ciclo de día y noche. Esto hace necesario contar con un medio de almacenamiento de la energía generada a lo largo del día para usarla durante la noche, lo que ha sido una de las debilidades de la energía solar, para la cual hay dispositivos de generación cada vez más competitivos, pero medios de almacenamiento con un desarrollo comparativamente menor.\nUn posible medio de almacenamiento es el hidrógeno que, como sabemos, es un gas combustible que reacciona químicamente con el oxígeno generando agua. Hay en la actualidad interés en desarrollar tecnologías de producción de hidrógeno empleando energía solar, lo que permitiría que la energía en exceso presente durante el día sea empleada para generar hidrógeno para consumo en horas de la noche. Dado que en la combustión del hidrógeno no interviene el carbono, no se generan gases de invernadero, lo cual es benéfico para el medio ambiente. Se contaría así con un sistema para generar y almacenar energía, que sería amigable con el medio ambiente.  \nPor otro lado, un sistema con estas características tendría que ser competitivo desde el punto de vista económico y su fabricación no será sencilla. En este contexto, resulta interesante un artículo aparecido esta semana en la revista “Nature Communications” en el cual se describen avances en el desarrollo de una celda para obtener hidrógeno y oxígeno a partir de agua. El artículo fue publicado por un grupo de investigadores de los Estados Unidos y China encabezados por Soonil Lee de la Universidad de Texas.  \nSabemos que la molécula de agua está formada por dos átomos de hidrógeno y uno de oxígeno y el dispositivo que están estudiando Lee y colaboradores tiene el propósito de romper el enlace químico que los une empleando la luz solar y de esta manera separar al agua en hidrógeno y oxígeno. Un aspecto interesante es que los materiales empleados en la celda de Lee y colaboradores han sido desarrollados por las industrias electrónica y de las celdas solares, en particular el silicio, que es la base de más del 90 por ciento de los paneles solares. La estructura de la celda de Lee y colaboradores, además, recuerda a la de una celda solar. Por otro lado, la tecnología que emplean es relativamente simple y potencialmente de bajo costo.\nUn dispositivo como el que buscan desarrollar Lee y colaboradores apunta entonces en la dirección de una energía solar no contaminante, disponible durante el día y la noche, y económicamente competitiva con otras formas de generación de energía. Y con estas perspectivas, por el bien del planeta, esperemos que las mismas no resulten demasiado buenas para ser ciertas. Por lo demás, de lo que sí podemos estar seguros es que la energía del Sol es inagotable y que su pecado original es “peccata minuta”.",
    "Si tiene usted espíritu aventurero y le gustan las emociones fuertes, y tiene, además, 17,500 euros que le sobren, quizá pudieran interesarle las ofertas de vuelos a la estratosfera promocionados por la agencia Mig-Flug -fácilmente localizable en Internet-. Por esta cantidad, Mig-Flug ofrece vuelos en un MiG-29, un caza de fabricación rusa capaz de desplazarse a una velocidad 1.7 veces la velocidad del sonido. Durante el vuelo, el avión alcanzará una altitud máxima de 17-22 kilómetros, desde la cual podrá comprobar que la Tierra es, efectivamente, redonda. Podrá igualmente comprobar que el espacio es negro, si bien cubierto de estrellas, aun durante el día. Para mayor atractivo, experimentará durante el vuelo aceleraciones equivalentes a 8 veces la aceleración de la gravedad, y las maniobras y piruetas propias de un avión caza. Para los más audaces, es posible que un vuelo que solo alcance una altura de 20 kilómetros no sea lo suficientemente excitante, en cuyo caso podrían considerar los vuelos suborbitales de las compañías “Virgin Galactic” y “Blue Origin”. Hay, sin embargo, un obstáculo para hacer uso de estas oportunidades: el costo del boleto, que es alrededor de 250,000 dólares en el caso de Virgin Galactic. En el caso de Blue Origin, el costo del vuelo, once minutos en total, sería aparentemente de decenas de millones de dólares. Así, a menos que seamos multimillonarios, nos tendríamos que quedar con las ganas de viajar al espacio.  Virgin Galactic y Blue Origin son dos compañías interesadas en desarrollar el turismo espacial y los viajes suborbitales de las últimas semanas fueron ampliamente publicitados con este propósito. Hay también compañías interesadas en el turismo a nivel orbital e, incluso más allá. Por ejemplo, la compañía “Orbital Assembly Corporation” tiene la intención de construir un hotel en una órbita terrestre, que ha llamado “Voyager Station”, y que sería terminado en el año 2027.El hotel Voyager Station, cuyo proyecto puede ser consultado en Internet, consistirá de dos anillos concéntricos, el mayor con diámetro de 200 metros, unidos por tubos a manera de rayos de rueda de bicicleta. Tendrá 11,600 metros cuadrados de espacio habitable, el cual incluye suites individuales de 30 metros cuadrados y villas de 500 metros cuadrados para un máximo de 16 personas. Las instalaciones incluyen también un gimnasio, un bar y un restaurante. La estructura se mantendrá en rotación -tal como la estructura de doble anillo que aparece en la película 2001 Odisea de Espacio dirigida por Stanley Kubrick-, con el objeto de generar una gravedad de un sexto de la terrestre, de modo tal que una persona con un peso de 72 kilogramos pesaría en el hotel solamente 12 kilogramos. El hotel contará también con una zona de cero gravedad -en el centro de la estructura- con el objeto de que los huéspedes puedan experimentar la ingravidez. Con todas estas amenidades, no es difícil llegar a la conclusión que el hotel Voyager Station será uno de super lujo, que estará a la disposición de un número extremadamente pequeño de habitantes del planeta Tierra.Por esto último, al igual que por otras consideraciones, los proyectos de desarrollo del turismo espacial son objeto de muchas críticas.  Por ejemplo, en un artículo en línea publicado en el sitio “The Conversation”, Eloise Marais, del “University College London”, hace notar que la quema de los combustibles empleados para impulsar a los vehículos espaciales contamina, tanto las capas bajas de la atmósfera como la estratósfera. Blue Origin emplea como combustibles hidrógeno y oxígeno líquidos, los cuales al reaccionar producen vapor de agua, que afecta negativamente al clima terrestre, mientras que Virgin Galactic hace uso de combustibles que generan al quemarse dióxido de carbono, óxidos de nitrógeno y partículas de carbón. Según Marais, el efecto que todos estos contaminantes tienen sobre el clima de la Tierra es incierto y más estudios son necesarios para precisarlo. No obstante, señala Marais, sabemos que dos tercios de los gases de combustión se expelen a alturas entre 12 y 85 kilómetros, en donde contribuyen a destruir la capa de ozono que nos protege de la radiación ultravioleta. Sabemos también que los gases expelidos por los vehículos espaciales son contaminantes atmosféricos que contribuyen al calentamiento global. La misma reentrada de las naves espaciales a la atmósfera que produce altas temperaturas hace que el nitrógeno del aire se combine con el oxígeno para generar óxidos de nitrógeno.Así, a reserva de que sea estudiado con detalle el efecto que tendrían los viajes de turismo espacial sobre el clima del planeta, no pareciera faltarle la razón a aquellos que piensan que no debe prevalecer el interés de los dueños de las empresas espaciales, y de los pocos multimillonarios con capacidad para realizar turismo, espacial sobre el interés de la inmensa mayoría de habitantes de este planeta. Aunque los ricos tengan que conformarse con emociones más de este mundo.",
    "Si bien en materia de contaminación ambiental nadie está por completo libre de culpa, y en ese sentido todos somos iguales, también es cierto que, recordando a George Orwell, algunos son más iguales que otros. En efecto, tenemos que, hablando en términos de extensión territorial, el 90 por ciento de los gases de invernadero, culpables del calentamiento global, son generados en menos del 8 por ciento de la superficie de la Tierra; en buena medida en los países industrializados. Así pues, como contaminadores, algunos son, efectivamente, más culpables que otros. Por otro lado, aunque la emisión de gases de invernadero se concentra en ciertos sitios -grandes centros urbanos, zonas industriales y de extracción de petróleo, entre otras-, su influencia se extiende por todo el planeta. En estas condiciones, hay regiones del mundo que sufren la contaminación atmosférica a pesar de haber hecho poco para generarla. ¿En qué medida la emisión de gases de invernadero en regiones localizadas del mundo afecta al resto del planeta? Un artículo aparecido esta semana en la revista “Science Advances” nos ilustra al respecto. Dicho artículo fue publicado por un grupo de investigadores encabezado por Kyle Van Houtan del Monterey Bay Aquarium en California. Van Houtan y colaboradores llevaron a cabo un estudio para determinar la relación que existe entre la generación de gases de invernadero en un lugar específico del planeta, y el incremento resultante en temperatura. Entre los contaminantes atmosféricos consideraron al dióxido de carbono, al metano, al óxido de nitrógeno, y a las partículas de carbón, los principales gases de invernadero. Hay que notar que la emisión de estos contaminantes está concentrada en el hemisferio norte, fundamentalmente en Europa Occidental y Central, China, Japón, Corea, el norte de la India, y el noreste de los Estados Unidos.  Para llevar a cabo su estudio, Van Houtan y colaboradores construyeron un índice que refleja la disparidad, en un lugar dado, entre la emisión de gases de invernadero y el incremento promedio de temperatura que se esperaría en un periodo de 50 años. Dicho índice puede ser positivo, negativo o cero. En este último caso, el incremento de temperatura depende solamente de los gases de invernadero generados localmente, o sea en el mismo lugar. Un índice positivo indica que el incremento de temperatura en el sitio considerado es mayor al que se esperaría dada su generación local de gases de invernadero, mientras que un índice negativo indica lo contrario.   Los resultados obtenidos por Van Houtan y colaboradores son interesantes. Encuentran, por ejemplo, que en el periodo comprendido entre los años 2050-2099, en el 99 por ciento de la superficie del planeta se tendrán índices de disparidad positivos. Esto será particularmente acusado en las regiones árticas y en países norteños como Canadá, Rusia y Finlandia. Pero también en países de Asia Central, el norte de México, Bolivia, Perú y la región de Amazonas. Les corresponderán también índices positivos a países del norte y del sur de África, y a gran parte de Australia. En contraste, las regiones industrializadas de Europa, Asia y Norteamérica tendrán índices de disparidad negativos, fundamentalmente por sus altos niveles de emisión de contaminantes.El estudio de Van Houtan y colaboradores nos muestra detalles finos de algo que de entrada ya sabíamos: mientras que los gases de invernadero son generados fundamentalmente por el mundo industrializado, sus efectos se extienden a todo el planeta, afectando a países no industrializados. A pesar de que éstos han jugado un papel menor o marginal en el proceso de calentamiento global.Por lo demás, al margen de los detalles finos que nos muestra el artículo de referencia, es claro que un incremento dado de temperatura en un país con pocos recursos tendrá un efecto amplificado, en comparación con el mismo incremento en un país industrializado. Así, por cualquier lado en que lo veamos, el calentamiento global tendrá un mayor costo para países con menos recursos. Pagando de este modo justos por pecadores. O lo que es lo mismo, al perro más flaco se le cargarán las pulgas.",
    "¿Fue la ola de calor que golpeó la semana pasada el noroeste de los Estados Unidos y el suroeste de Canadá y que produjo cientos de muertos producto del calentamiento global? Estaríamos quizá tentados a suponer que éste fue el caso, dado que se rompieron -y por mucho- todos los récords de temperaturas máximas en esa región del mundo, que incluye a los estados de Washington y Oregón en los Estados Unidos y la Columbia Británica en Canadá. El caso extremo en este sentido fue el pueblo de Lytton en la Columbia Británica, que registró una temperatura de 49.6 grados centígrados el pasado 29 de junio, registro que estableció un récord para todo el territorio canadiense. Además, para desgracia de sus habitantes -unos 250-, las altas temperaturas combinadas con un ambiente seco provocaron un fuego que arrasó con el 90 por ciento de las construcciones del pueblo.Habrá que notar, sin embargo, que los expertos son cautos en cuanto a asociar un fenómeno climático particular al calentamiento global. Esto solamente puede establecerse, con una cierta seguridad -nunca absoluta- después de un análisis riguroso del fenómeno climático en cuestión. Un primer análisis del evento climático de la semana pasada fue llevado a cabo por la iniciativa “World Weather Attribution” (WWA) que agrupa a investigadores de instituciones en el Reino Unido, los Estados Unidos, Holanda, Francia, Alemania y Suiza. De acuerdo con su página de Internet, la iniciativa WWA es un esfuerzo internacional para analizar y comunicar la posible influencia que el cambio climático tiene sobre eventos climáticos extremos, tales como tormentas, lluvias extremas, ondas de calor, olas de frío y sequías. La iniciativa WWA elabora informes rápidos con sustento científico, sobre eventos climáticos extremos y su relación con el cambio climático. Esto, con el propósito de minimizar su impacto social, económico y medio ambiental. Como expone WWA en su sitio electrónico, el análisis de un evento climático extremo se hace de manera acelerada con el fin de proporcionar respuestas rápidas a preguntas tales como qué tan probable es que se produzca un evento similar en el futuro cercano, y así influir en aquellos que toman decisiones sobre futuras medidas de mitigación de los desastres climáticos.¿Cuáles son las conclusiones del estudio de WWA sobre el evento climático de la pasada semana? Los investigadores aventuran que posiblemente se trató de un fenómeno extremadamente raro, que solo ocurre una vez cada 1,000 años. De ser el caso, los habitantes de Oregón, Washington y Columbia Británica -y particularmente los de Lytton- habrían corrido con una muy mala suerte. El estudio de WWA, sin embargo, también concluye que el calentamiento global hizo 150 veces más probable que ocurriera un evento climático similar al de la semana pasada.  Es decir, sin cambio climático dicho evento habría ocurrido solamente una vez cada 150,000 años. O lo que es lo mismo, no habría nunca ocurrido en términos prácticos.Un evento climático extremo que ocurre una vez cada 1,000 años posiblemente no sea motivo de preocupación por más desastres que pudiera ocasionar. Desafortunadamente, con la contaminación creciente que está experimentando la atmósfera de nuestro planeta, la probabilidad de que ocurra un fenómeno climático como el de la semana pasada es igualmente creciente. Para ser precisos, como sabemos, la temperatura del planeta se ha incrementado en aproximadamente 1.2 grados centígrados desde el inicio de la revolución industrial por la emisión de gases de invernadero a la atmósfera y se espera que dicho incremento se eleve hasta 2 grados centígrados en el año 2040 de no moderarse dicha emisión. Una vez que se alcance esta temperatura, el estudio WWA estima que un evento climático similar al de la semana pasada ocurrirá cada 5 o 10 años. Además, las temperaturas que se alcanzarían serían 1 grado centígrado más altas.Los investigadores concluyen que sus resultados son motivo de alarma y que el rápido calentamiento del planeta nos está llevando hacia un territorio desconocido, con consecuencias significativas para nuestra salud, bienestar y sustento. Así, el mundo tendrá que prepararse para vivir un futuro que posiblemente sea diferente al presente que a nosotros nos tocó vivir.",
    "En septiembre de 1941, los fósiles del llamado Hombre de Pekín fueron empacados en dos cajas para ser transportados desde China a los Estados Unidos a bordo del buque norteamericano President Harrison. Como recordamos, el Hombre de Pekín, con una antigüedad de 500,000-250,000 años, fue descubierto en la década de los años veinte del siglo pasado. En su momento, gozaba de gran celebridad como un “eslabón perdido” que demostraba la teoría de la evolución de Darwin. La determinación de enviar dichos fósiles fuera de China tuvo el propósito de ponerlos a salvo de las hostilidades de la guerra chino-japonesa, en curso desde el año 1937.Los fósiles, sin embargo, nunca llegaron a su destino y desaparecieron sin dejar rastro. ¿Fueron robados por los japoneses en el camino hacia el puerto en donde serían embarcados hacia los Estados Unidos? ¿Fueron sustraídos por los marinos estadounidenses que habrían de transportarlos hasta su destino final? O bien, ¿el vehículo que los transportaba en territorio chino habría sido asaltado y robado por ladrones sin la menor idea de la importancia científica de los fósiles y que simplemente los hubieran desechado por considerarlos sin utilidad alguna? En la medida en que no sean finalmente localizados, no hay manera de encontrar una respuesta segura y la desaparición del Hombre de Pekín queda sumida en el más absoluto misterio, que sin duda podría dar material para una película.  A la historia anterior se añade otra, también de fósiles descubiertos en territorio chino, que podría igualmente proporcionar material para el cine. Esta segunda historia gira alrededor de un fósil del género homo, un cráneo sorprendentemente bien conservado y con una antigüedad de cientos de miles de años, que es motivo de tres artículos publicados esta semana en la revista “The Innovation”, por investigadores en instituciones de China, Australia y el Reino Unido. Este segundo fósil, tiene, efectivamente, una historia peculiar, misma que está relatada en uno de los artículos mencionados anteriormente: “El cráneo de Harbin reportado en este documento fue supuestamente descubierto en 1933. Un hombre (mantenido en el anonimato por su familia), que trabajaba para los ocupantes japoneses como contratista de mano de obra, descubrió el cráneo cuando su equipo de trabajadores estaba construyendo un puente para los japoneses cerca de Ciudad de Harbin en el noreste de China. El hombre fue astuto y se dio cuenta del valor potencial del descubrimiento, probablemente porque el descubrimiento del primer cráneo del Hombre de Pekín en 1929 había atraído un gran interés en China. En lugar de pasarle el cráneo a su jefe japonés, lo enterró en un pozo abandonado, un método tradicional chino para ocultar tesoros. Después del establecimiento de la república china moderna, el hombre volvió a la agricultura e hizo todo lo posible por ocultar su experiencia como contratista de mano de obra para los invasores japoneses. El cráneo permaneció desconocido para el público y la ciencia durante décadas, pero sobrevivió a la invasión japonesa, la guerra civil, el movimiento comunista, la revolución cultural y el comercio desenfrenado de fósiles en los últimos años. La tercera generación de la familia del hombre se enteró de su descubrimiento secreto antes de su muerte y recuperó el fósil en 2018.”  Así, en contraste con el Hombre de Pekin, el fósil de Harbin tuvo suerte y logró sortear la guerra chino-japonesa. Y más aún, los familiares del descubridor original accedieron a donarlo a una institución china de investigación. Hay que mencionar, no obstante, que no todo fue miel sobre hojuelas, pues el contratista chino murió sin revelar el sitio exacto en donde descubrió el fósil, lo que dificultó a los investigadores la determinación de su antigüedad.  No se los impidió, sin embargo, y se encontró que el fósil tiene una antigüedad mínima de 146,000 años. Las fotografías incluidas en los artículos publicados por los investigadores muestran un cráneo, con una conservación casi perfecta, relativamente alargado y bajo, y con un volumen que podría alojar un cerebro de un tamaño comparable al de un humano moderno. En contraste, tiene cuencas oculares de gran tamaño y cejas gruesas y salientes, boca ancha y grandes dientes. De acuerdo con los investigadores, las características morfológicas del fósil de Harbin lo hacen diferente a otras especies homo. Esto, los lleva a afirmar que pertenece a una nueva especie, que se convertiría en nuestro pariente más cercano. Algo en lo que, sin embargo, no están de acuerdo otros especialistas.De un modo u otro, ya sea que el fósil de Harbin pertenezca a una nueva especie o a otra ya establecida, resulta afortunado que un espécimen de esa antigüedad y con ese excelente estado de conservación, haya logrado llegar hasta nosotros, aun en medio de una guerra entre chinos y japoneses.",
    "Imaginemos que tenemos un problema con la bomba que sube el agua desde el aljibe al tanque de almacenamiento en el techo de la casa, la cual dejó de funcionar después de despedir humo negro y oler a quemado. Ante esta situación, lo más probable es que tengamos que enviar dicha bomba a un taller de reparación para su rebobinado. El problema tendría una solución relativamente sencilla, sin embargo, pues las bombas de agua domésticas son dispositivos relativamente simples, con una estructura interna y principios de funcionamiento que están al alcance de un técnico medianamente entrenado. Podría, por supuesto, ocurrir que el daño sea extensivo y que no pueda la bomba ser reparada. En tal caso, posiblemente tengamos que comprar una bomba nueva. Sirva lo anterior como una introducción al tema que queremos tratar en este artículo: la fabricación de órganos artificiales para el cuerpo humano como sustitutos de órganos defectuosos. En este contexto se concibe al cuerpo humano como una máquina, si bien bastante más compleja que aquellas máquinas con las que estamos más familiarizados -aun las más altamente sofisticadas como los teléfonos móviles o las computadoras digitales-. La fabricación de un órgano artificial es pues una tarea bastante más complicada que sustituir la bobina un motor.Es conocido que en el mundo hay una gran escasez de órganos donados para trasplante y que muchos pacientes mueren antes de tener oportunidad de recibir uno. En estas condiciones hay un enorme interés en la producción de órganos artificiales para satisfacer una demanda creciente. Cabe, sin embargo, la pregunta: ¿qué tan factible es fabricar un órgano artificial que pudiera ser trasplantado en sustitución de un órgano defectuoso? La respuesta es que, si bien los órganos artificiales no están todavía disponibles para trasplante, hay grandes avances en esta dirección.Una técnica empleada para fabricar órganos artificiales es similar a la impresión 3D regularmente empleada para fabricar objetos tridimensionales con materiales inanimados, como plásticos y metales. Para esto, se desarrolla primeramente una versión digital del objeto a fabricar, la cual es alimentada a una impresora 3D. Siguiendo el diseño digital, la impresora 3D fabrica el objeto capa por capa, depositando material de manera controlada a través de una cabeza inyectora móvil. El objeto se construye de abajo hacia arriba, con las capas inferiores sirviendo de soporte a las capas subsecuentes.Las impresoras 3D son también utilizadas para fabricar tejido orgánico y, a través de un modelo digital, para desarrollar órganos completos. No es difícil entender, sin embargo, que fabricar un órgano artificial funcional es bastante más complicado que fabricar un objeto inanimado. Para construir material orgánico, la impresora fabrica un armazón o andamio a partir de un material biodegradable, sobre el que deposita células funcionales. El armazón permite colocar a las células en el lugar correcto, dando forma al órgano a construir. En una etapa posterior, el armazón con las células en su superficie es colocado en una incubadora, para que las células se reproduzcan formando el órgano. La pretensión de los expertos es que, una vez llegado a este punto, el órgano artificial -corazón, riñón, hígado, etc.- pueda ser trasplantado al paciente que lo requiera.Desafortunadamente, si bien se han logrado fabricar tejidos funcionales, incluyendo tejido cardiaco, no se ha llegado a cristalizar esta pretensión. Un aspecto particularmente difícil es el desarrollo de la red vascular necesaria para llevar nutrientes al tejido orgánico. El interés en el desarrollo de órganos artificiales, sin embargo, es muy grande y se están haciendo muchos esfuerzos por hacerlos realidad. Hace seis años, por ejemplo, la NASA lanzó el concurso “Vascular Tissue Challenge”, que ofreció un premio 500,000 dólares que se dividiría entre los tres primeros equipos que “logren crear con éxito un tejido orgánico humano vascularizado grueso y metabólicamente funcional, en un entorno controlado de laboratorio”.  El interés de la NASA es el de usar los tejidos orgánicos funcionales para estudiar el efecto que el espacio profundo -la radiación de alta energía, por ejemplo-, tendría sobre los futuros astronautas.Podríamos así esperar que en un futuro todavía no precisado, pero que esperemos no sea muy lejano, tengamos disponibles piezas artificiales de recambio para partes u órganos del cuerpo con problemas de funcionamiento, Tal como se reemplaza el embobinado quemado de una bomba de agua. Desafortunadamente, algunas veces el daño será tal que no permita una reparación. En tal caso, la sustitución será total, por un modelo nuevo.",
    "Si retrocediéramos 60 años en el tiempo hasta la ciudad de Londres nos encontraríamos con una generación de jóvenes que busca de muchas formas romper con la generación de sus padres. Encontraríamos jóvenes que, entre otras cosas, buscan andar a la moda como un elemento de identidad. El clima de la ciudad de Londres en este respecto está retratado de manera satírica en la canción “Dedicated follower of fashion” del grupo británico “The Kinks” -muy popular en el Reino Unido en los años sesenta, pero poco conocido en México-, que en una de sus estrofas dice: “Y cuando hace sus rondas pequeñas. Alrededor de las boutiques de la ciudad de Londres. Buscando ansiosamente todas las modas y tendencias. Porque él es un seguidor dedicado de la moda”.No resulta sorprendente que una generación de jóvenes en la ciudad de Londres hace poco más de medio siglo hubiera estado tan interesada en andar a la moda para diferenciarse de la generación anterior. Después de todo la moda sigue siendo un elemento importante aún hoy en día, aunque no necesariamente por las mismas razones que impulsaron a los jóvenes sesenteros. De hecho, la moda tiene seguidores aun a costa de la salud. Es el caso, por ejemplo, de los zapatos estrechos y los tacones altos, que no están de acuerdo con la estructura de los pies humanos, que es resultado de millones de años de evolución.   Por lo demás, la pretensión de vestir a la moda no es algo que sea característico de los tiempos recientes, y en ese sentido es ilustrativo un artículo que está por aparecer en la revista “International Journal of Paleopathology”, publicado por un grupo de investigadores encabezados por Jenna Dittmar de la Universidad de Cambridge en el Reino Unido. En dicho artículo se pone de manifiesto la importancia que tuvo la moda, particularmente la moda del calzado, en la Europa medieval. Y tal como sucede en la actualidad, aun a costa de la salud de los pies.  En su artículo, Dittmar y colaboradores reportan los resultados de un estudio llevado a cabo con 177 esqueletos humanos excavados de la ciudad británica de Cambridge. Los esqueletos, con antigüedades que van de los siete a los diez siglos, fueron desenterrados de cuatro cementerios. Un cementerio parroquial en el que fueron enterradas personas de todas las clases sociales, un cementerio de hospital de caridad que contenía restos de personas pobres que ahí murieron, el cementerio de un monasterio agustino con restos de personas con un nivel social alto, y un cementerio rural con restos de campesinos, tanto libres como no libres. El interés de los investigadores fue determinar si los huesos de los pies de los esqueletos muestran evidencia de juanetes, que podrían haber resultado del uso de zapatos demasiado estrechos.Con respecto a esto último, es conocido a lo largo del siglo XIV en Europa se desarrolló una moda de zapatos estrechos y puntiagudos conocidos como poulaines. Esto se puede constatar en pinturas de la época que muestran individuos calzando zapatos muy estrechos y con puntas tan alargadas que resultan ridículas. La longitud de estas puntas denotaba la posición social y podían alcanzar una longitud cercana a medio metro. No es difícil concluir que quien usara zapatos tan exagerados habría tenido problemas para caminar, con el peligro de tropezar y romperse una pierna. Así, para evitar esta eventualidad, las puntas de los zapatos se amarraban a las espinillas.  Desafortunadamente, si bien este último recurso solucionaba un problema potencialmente serio, no evitaba que la estrechez de los zapatos presionara de más a los huesos del pie. En efecto, Dittmar y colaboradores concluyen que 18 por ciento de los individuos estudiados sufrieron en vida de juanetes. El problema era más acusado entre los monjes agustinos -43 por ciento de los restos mostraban juanetes- que entre aquellos enterrados en el cementerio del hospital -23 por ciento-, en el cementerio parroquial -10 por ciento- y en el cementerio rural -3 por ciento-. Además, los juanetes eran menos frecuentes entre aquellos que murieron en los siglos XI-XIII, que entre aquellos fallecidos en los siglos XIV y XV, después de que las poulaines se pusieron de moda. Así, Dittmar y colaboradores establecen una conexión entre los juanetes y los poulaines, que, sin embargo, mantuvieron su popularidad a pesar del dolor que ocasionaban. Con el tiempo, la moda desapareció por razones difíciles de anticipar. Una de ellas la esgrimió la iglesia: las poulaines dificultaban hincarse de rodillas para orar.   Los seguidores dedicados de la moda no son, pues, algo propio de nuestros días, y se pierden en la noche de los tiempos. A pesar de los problemas que la moda pueda ocasionar.",
    "Ciertamente, no hay nada gratis en este mundo y por todo tenemos que pagar. Y en este sentido, las medidas que se están implementando para mitigar el cambio climático no son la excepción. Pensemos, por ejemplo, en la sustitución de los automóviles de gasolina por los automóviles eléctricos que, sin duda, tendrá un impacto benéfico sobre el clima de nuestro Planeta. Al mismo tiempo, sin embargo, las baterías de los automóviles eléctricos al final de su vida útil se convertirán en desechos contaminantes -en algunos casos tóxicos- que tendrán que ser eliminados de alguna forma, so pena de que se acumulen en el medio ambiente. Una idea de la inminencia y magnitud del problema que enfrentará el mundo cuando esto ocurra nos la proporciona un artículo publicado en la revista “Nature” en noviembre de 2019, por un grupo de investigadores encabezado por Gavin Harper de la Universidad de Birmingham en el Reino Unido. Harper y colaboradores hacen notar que en 2017 se vendieron aproximadamente un millón de vehículos eléctricos y que, asumiendo de manera conservadora, que el banco de baterías de un automóvil tiene un peso promedio de 250 kilogramos, al final de la vida útil de dichas baterías -los fabricantes las garantizan por unos ocho años- se tendrán que procesar 250,000 toneladas de desechos, solamente por aquellas baterías vendidas en 2017. El problema, además, se escalará rápidamente en los años por venir en la medida en que los fabricantes de automóviles disminuyan o paren la producción de vehículos de gasolina y los sustituyan por automóviles eléctricos. Así, se estima que en el año 2030 se tendrán en circulación más de cien millones de automóviles eléctricos, con la consecuente generación de desechos contaminantes al final de su vida útil.¿Cuáles son las posibilidades y los problemas asociados para el manejo de las baterías ya fuera de uso? Harper y colaboradores, al igual que un artículo de divulgación aparecido en la revista “Science” el pasado 20 de mayo, firmado por Ian Morse, nos ofrecen un panorama al respecto. Dichas baterías tendrían básicamente tres destinos: 1) su acumulación en un depósito de desechos contaminantes, 2) su reciclaje para recuperar los elementos más valiosos y 3) su reúso en aplicaciones diferentes a la de los automóviles eléctricos.  Como sabemos, las baterías empleadas en los automóviles eléctricos utilizan el metal litio para su funcionamiento, además de otros elementos como cobalto y níquel. El reciclaje implicaría la destrucción de las baterías y su procesamiento -ya sea a altas temperaturas o bien empleando sustancias químicas- para recuperar metales tales como el cobalto y el níquel -más no el litio, dado que su abundancia no haría el proceso económicamente viable-. Con el reciclaje de las baterías usadas se tendría una fuente de materiales que de otra manera serían proporcionadas por la industria minera. Esto es particularmente importante en el caso del cobalto, cuyas reservas mundiales están presionadas por la industria de las baterías eléctricas. De acuerdo con Morse, sin embargo, los fabricantes de baterías están tratando de eliminar el cobalto de sus procesos. En tal caso se perdería el incentivo económico para el desarrollo de las técnicas para el reciclado de baterías, que está basado en la recuperación de cobalto, y las compañías recicladoras se encontrarían en la posición de “tratar de vender montañas de basura”.  La opción ideal, por otro lado, sería la del reúso de las baterías mediante un reacondicionamiento. Las baterías reacondicionadas encontrarían aplicación en el campo de las energías renovables, tales como la solar y la eólica, que requieren de sistemas de almacenamiento dada su intermitencia.El reacondicionamiento de baterías de automóvil, sin embargo, encuentra también obstáculos importantes. Esto en parte es debido a que la estructura y tecnología de las baterías difiere según el fabricante. En efecto, un banco de baterías está constituido por un conjunto de módulos, cuyo número varía ampliamente según el fabricante. Cada módulo, a su vez, está formado de celdas cuyo número y geometría depende también del fabricante. Así, hay celdas cilíndricas y celdas en forma de prisma rectangular. Las celdas, además, pueden estar unidas con un pegamento difícil de remover. Todo esto, unido a la falta de información técnica acerca de la estructura y materiales de las baterías, hace que su desensamble se dificulte, con el consecuente incremento en los costos de reacondicionamiento.De todo lo anterior concluimos que, efectivamente, todo en esta vida tiene un precio, y si bien los automóviles eléctricos contribuirán a mitigar el problema climático, lo mismo harán para incrementar el problema de contaminación ambiental. No hay pues solución perfecta.",
    "La obsolescencia de ciertas habilidades profesionales en la medida que avanza la tecnología -y la consecuente desaparición de los puestos de trabajo asociados- no es, por supuesto, algo nuevo.  Durante la revolución industrial de los siglos XVIII y XIX, por ejemplo, el trabajo manual y habilidades artesanales fueron sustituidas por máquinas para la fabricación industrial.  Más recientemente, la aparición de las computadoras personales y los editores de texto han hecho en gran medida obsoletas las habilidades tradicionales de una secretaria para tomar dictado y escribir textos. Al mismo tiempo, sin embargo, las nuevas tecnologías han creado nuevas especializaciones profesionales y puestos de trabajo para sustituir a los perdidos. En el contexto anterior, no es de sorprender que, en la medida en que avanzan las tecnologías de la robótica y la inteligencia artificial, se pierdan puestos de trabajo en líneas de producción industrial y en el sector de servicios y que al mismo tiempo se generen nuevos puestos de trabajo basados en dichas tecnologías. Un artículo aparecido en la edición de junio de la revista “Journal of Archeological Science” nos da un ejemplo ilustrativo al respecto. Dicho artículo fue publicado por Leszek Pawlowicz y Christian Downum de la Universidad del Norte de Arizona. En su artículo, Pawlowicz y Downum reportan los resultados de una investigación que tuvo como objetivo determinar, por un método novedoso, la posibilidad de clasificar cerámicas fabricadas por pueblos indígenas del norte de Arizona durante el periodo que abarca los años 825 y 1,300 de nuestra era. Dichas cerámicas son conocidas como loza blanca de Tusayan. Como explican los autores, las vasijas de cerámica de Tusayan están pintadas con motivos geométricos en colores negro y café oscuro, y han sido tradicionalmente clasificadas en siete tipos principales, de acuerdo a su estilo de decoración. Sin embargo, argumentan Pawlowicz y Downum, a pesar de su utilidad, esta clasificación adolece de los mismos problemas de tipologías de otros lugares arqueológicos. Entre otras cosas, fue diseñada con criterios subjetivos y para propósitos que ya no se consideran válidos.Dadas las anteriores circunstancias, Pawlowicz y Downum se propusieron evaluar el uso de un algoritmo de inteligencia artificial para clasificar la cerámica de Tusayan, el cual emula crudamente el proceso que sigue el cerebro humano para analizar información visual.  Para entrenar al algoritmo, los investigadores lo alimentaron con miles de fotografías de fragmentos de dicha cerámica. Una vez entrenado, se evaluó su capacidad para clasificar un conjunto de fragmentos de cerámica de Tusayan, que no contenía ninguno de los fragmentos empleados en su entrenamiento. Para establecer una comparación con sus contrapartes humanos, Pawlowicz y Downum pidieron a cuatro arqueólogos, con amplia experiencia en la cerámica de Tusayan, que también clasificaran los fragmentos de cerámica proporcionados al algoritmo.Los resultados de experimento fueron notables, pues el algoritmo aprendió a identificar la cerámica en un tiempo relativamente corto, con una precisión que igualó a la de dos de los expertos voluntarios y superó a la de los otros dos. Adicionalmente, fue capaz de explicar cuáles fueron los criterios que empleó para dar su clasificación en cada caso, algo que los expertos humanos tienen dificultad en precisar. Para aclarar esto último, supongamos que se nos presenta una fotografía y se nos dice es de alguien a quien conocemos bien, pero que en realidad corresponde a otra persona muy parecida. Es posible que reconozcamos este hecho al instante sin ninguna duda, pero que al mismo tiempo tengamos dificultades para explicar el porqué de nuestra seguridad. Así, de acuerdo con los resultados de Pawlowicz y Downum, las computadoras y sus algoritmos de inteligencia artificial pueden hacer el trabajo de análisis y clasificación de objetos arqueológicos con mayor precisión y rapidez que los humanos. Tienen, además, la virtud de no cansarse, lo que es, ciertamente, una ventaja para llevar a cabo un trabajo que es de suyo largo y tedioso. Así, a primera vista, amenazarían con quitarle el trabajo a los arqueólogos.   No sería el caso, sin embargo, de acuerdo con Pawlowicz y Downum, quienes no piensan que las computadoras y sus algoritmos puedan resolver todos, o aun la mayor parte, de los problemas de clasificación de las cerámicas. Pero sí que son, potencialmente, herramientas poderosas para mejorar la objetividad, replicabilidad, velocidad y eficiencia en la clasificación del diseño de las cerámicas. Y en ese sentido, abren nuevas posibilidades de especialización para los arqueólogos, amenazados, como todo el mundo, con quedarse sin trabajo por el avance tecnológico.",
    "De acuerdo con el sitio “Our World in Data”, a la fecha se han administrado aproximadamente 1,600 millones de dosis de vacunas contra el Covid-19 a nivel global. Como resultado, el 9.5 por ciento de la población del mundo ha recibido cuando menos una vacuna, mientras que un 5 por ciento adicional está completamente vacunada. En la aplicación de vacunas el liderazgo lo tiene Israel, que ha logrado inmunizar por completo al 59 por ciento de su población. Los Estados Unidos, por su lado, es el país que más vacunas ha aplicado con un total de 282 millones. Con esto, el 38 por ciento de su población está completamente vacunada.México no destaca en cuanto al porcentaje de su población completamente vacunada, aun en comparación con otros países latinoamericanos. Así, apenas el 8.8 por ciento de la población de México está completamente vacunada, en contraste con Chile y Uruguay que han vacunado completamente al 40 por ciento y al 28 por ciento de su población, en forma respectiva.Por lo demás, si bien de manera dispareja, los países del mundo están avanzando en el proceso de vacunación y tenemos la esperanza que pronto será superada la emergencia sanitaria. Con esta perspectiva, algunos se preguntan por el rumbo que tomará el coronavirus en el futuro y especulan sobre la posibilidad de que se convierta en un virus endémico -sin su virulencia actual-, tal como otros virus de la gripe común que aparecen de manera estacional. Y es esta una posibilidad que se explora en un artículo aparecido el pasado 7 de mayo en la revista “Virus”, publicado por un grupo de investigadores de la Universidad de Utah encabezado por Alexander Beams. Beams y colaboradores hacen notar que se pueden encontrar claves sobre la evolución futura del coronavirus causante de la epidemia actual si se examinan otras epidemias del pasado. Consideran en particular la llamada “gripe rusa”, que se originó en Asia central en 1889. Desde su punto de origen, la epidemia llego hasta San Petersburgo y de ahí se expandió rápidamente a Europa y al resto del mundo.  25 millones de personas fueron infectadas, de las cuales un millón fallecieron. Los investigadores hacen notar que hay evidencia genética de que la gripe rusa fue debida a un coronavirus que es actualmente estacional. Dicho coronavirus habría perdido así su virulencia, ya sea porque sufrió cambios genéticos, o bien porque la población desarrolló defensas inmunológicas. Al respecto, Beams y colaboradores traen a colación el caso de un grupo de personas que permanecieron aisladas por cinco meses en una base británica de la Antártida en la década de los años setenta del siglo pasado, los cuales enfermaron gravemente por un virus de gripe común. Esto, según los investigadores, fue debido a que el sistema inmunológico pareciera requerir de la exposición frecuente al virus para mantener su efectividad.  Para explorar la posible evolución futura del Covid-19, Beams y colaboradores desarrollaron un modelo matemático simplificado que asumió que la gravedad de la enfermedad depende del tamaño de la dosis de virus a la que el paciente fue expuesto. Supusieron también que los enfermos graves expulsan más virus que las personas con enfermedades leves, de modo que los primeros tenderán a producir enfermos graves y los segundos enfermos leves. Igualmente, asumieron que los niños tienden a sufrir infecciones leves y por tanto son fundamentalmente generadores de enfermos leves.  Finalmente, consideraron que, en la medida en que la población genera una inmunidad parcial, se podrían generar casos de enfermedades leves, tal como sucede con otros coronavirus de la gripe común.Con las anteriores suposiciones, Beams y colaboradores encuentran que en el largo plazo la población estará predispuesta a desarrollar enfermedades leves, llevando a la desaparición de las enfermedades graves en el curso de una década. Esto sería resultado de una adaptación de nuestro sistema inmunológico - que no estaba preparado para combatir al virus cuando hizo su aparición-, mas que de una evolución genética del coronavirus. En el futuro, quienes encontrarían al virus por vez primera serían los niños, pero estos tenderían solamente a desarrollar enfermedades leves.  De estar Beams y colaboradores en lo cierto, en el futuro el coronavirus, que tantos dolores de cabeza nos ha causado a lo largo del último año, no sería sino un mal recuerdo. Quizá la posteridad nos recordará con algo así como: “en los años veinte del siglo XXI se desarrolló una pandemia de coronavirus que puso de cabeza al mundo, tal como lo hizo la gripe española un siglo antes”. Además de que, por supuesto, el bicho será una monserga cada invierno cuando tengamos que cuidarnos de otro virus más.",
    "No es de sorprender que el relato del historiador griego Heródoto sobre la batalla de Himera les resulte poco convincente a los historiadores modernos. Después de todo, como sabemos, la historia la hacen los vencedores, además de que la batalla de Himera ocurrió hace 2,500 años. No podríamos pues esperar que Heródoto hubiera sido rigurosamente imparcial al describir un acontecimiento que llevó a la derrota contundente de los cartagineses a manos de los ejércitos de Siracusa y Agrigento y salvó a Himera, en la costa norte de Sicilia, de la invasión de Cartago. La salvó hasta su caída y destrucción 70 años después en manos de los cartagineses en una segunda batalla de Himera, después de la cual nunca fue reconstruida.De acuerdo con Heródoto, cuando los “bárbaros” cartagineses atacaron la colonia griega de Himera fueron derrotados por una coalición de aliados de ciudades sicilianas. Un artículo publicado esta semana en la revista en línea “PLOS ONE”, sin embargo, tiene otros datos. En efecto, dicho artículo sugiere que el ejército que derrotó a los cartagineses no estaba compuesto exclusivamente por soldados griegos, sino que incluía a mercenarios provenientes de diversos lugares, no solamente de Sicilia, sino de otras regiones alrededor del mar Mediterráneo e incluso más allá. El artículo de referencia fue publicado por un grupo de investigadores adscritos a instituciones norteamericanas, encabezados por Katherine Reinberger de la Universidad de Georgia.¿Cómo es posible llegar a una conclusión como la que nos ofrecen los investigadores sobre un acontecimiento que ocurrió hace 2,500 años? Para entenderlo, habría primeramente que mencionar que Reinberger y colaboradores estudiaron los restos de 62 soldados de la coalición griega muertos en las batallas de Himera encontrados en 8 fosas comunes. Los investigadores estaban interesados en determinar las concentraciones de dos isótopos del elemento estroncio en el esmalte dental de las víctimas. Se sabe que estos isótopos se incorporan al cuerpo a través de la ingesta de alimentos y agua, y que sus concentraciones en el mismo son un reflejo de las concentraciones respectivas en los suelos y rocas de las regiones en donde crecieron los soldados.    De este modo, mediante la medición de la concentración de isótopos de estroncio en los dientes de los soldados muertos, Reinberger y colaboradores pudieron determinar los orígenes geográficos de los combatientes en las batallas de Himera. Y, en cuanto a la primera de estas batallas, librada en el año 480 antes de nuestra era, lo que encontraron contradice a Heródoto. De hecho, lo hace de manera frontal, pues dos tercios del ejercito que venció a los cartagineses en esta primera batalla provenían de regiones fuera de Sicilia, desde el mar Negro hasta la península Ibérica. En contraste, los investigadores encuentran que en la segunda batalla de Himera -año 409 antes de nuestra era- tres cuatros de los muertos fueron originarios de Sicilia.    Lo encontrado por Reinberger y colaboradores coincide con los relatos históricos según los cuales en la primera batalla de Himera una coalición de aliados griegos enfrentó exitosamente a los cartagineses, mientras en la segunda, la ciudad de Himera los enfrentó sola, con resultados funestos.  La coalición que derrotó a Cartago hace 2,500 años, sin embargo, habría estado formada mayoritariamente por soldados mercenarios.Como hacen notar Reinberger y colaboradores, en el periodo clásico griego, los solados mercenarios podrían haber sido vistos como la antítesis de los soldados ciudadanos griegos -hoplitas- que gozaban de prestigio social. En particular, las cambiantes lealtades de los mercenarios habrían ofendido los ideales de ciudadanía y lealtad, y Heródoto, que escribía en el periodo clásico, habría compartido las actitudes en contra de los mercenarios. No sorprendería de este modo su parcialidad al escribir sobre la composición del ejército griego que venció a los cartagineses en Himera.Parcialidad que habría imaginado pasaría impune en los años por venir. No lo ha sido, entre otras cosas por un estudio -2,500 años después- que ha puesto de manifiesto la necesidad que hubo en su momento de incorporar mercenarios en los ejércitos de la Grecia clásica.  Un estudio que empleó técnicas de laboratorio de alta sofisticación que, seguramente, a Heródoto le habrían parecido propias del diablo.",
    "¿Qué tanta confianza le merecen las computadoras? O más propiamente dicho ¿qué tanto confía en los programas o algoritmos que corren en las mismas? Es posible que la mucha o poca confianza que le inspiren dependa de cada aplicación en particular. Por ejemplo -si fuera el caso-, muy probablemente confíe en que los impuestos que le descuentan de su salario mensual, que con seguridad es calculado por un algoritmo de computadora, haya sido fijado correctamente. En contraste, es posible que se muestre más cauteloso con respecto al algoritmo que guía a los automóviles autónomos, los cuales, como sabemos han sido protagonistas de accidentes fatales. Igualmente, habría una cierta probabilidad de que se preguntara por lo acertado del diagnóstico médico que le habría extendido una computadora después de que le hiciera saber los síntomas de su enfermedad. Lo cierto es que en lo sucesivo tendremos que acostumbrarnos a que las computadoras y sus algoritmos jueguen un papel cada vez más importante en nuestras vidas. Los algoritmos que controlan el movimiento de los automóviles sin conductor, por ejemplo, con seguridad progresarán a tal grado que en un futuro cercano será más seguro viajar en uno de estos automóviles que en uno manejado por un humano -si bien habría que reconocer que en algunos casos como el de nuestra ciudad, en donde el tráfico es altamente desordenado y las calles están llenas de baches y de zanjas que tardan meses en desaparecer, dichos algoritmos se encontrarán con desafíos importantes.   De un modo u otro, sin embargo, nos acostumbraremos a los algoritmos y seremos cada vez más dependientes de ellos. Y en ese sentido cabe preguntarnos por el grado de confianza que nos inspiran, pregunta que busca contestar un artículo aparecido esta semana en la revista “Scientific Reports” y que tiene como autores a un grupo de investigadores de la Universidad de Georgia encabezado por Eric Bogert.En su artículo, Bogert y colaboradores reportan los resultados de un estudio diseñado para determinar el grado de confianza que inspiran los algoritmos en comparación con la opinión de un grupo de personas. Para este propósito reunieron a un grupo de 1,500 voluntarios en forma virtual y les pidieron contar el número de personas que aparecían en una serie de fotografías que fueron divididas en dos grupos. Además de las fotografías, a los voluntarios se les proporcionaron los números correctos de personas en cada imagen, con la anotación de que habían sido obtenidos por un algoritmo entrenado con 5,000 imágenes en un primer grupo de fotografías, y por un grupo de 5,000 personas en el segundo. En ningún caso se proporcionaron dos números de personas para una misma fotografía. De manera natural, en la medida que se elevaba el número de personas en una fotografía, creció la probabilidad de que los voluntarios no las contaran y en su lugar basaran sus estimaciones en los números que les fueron proporcionados. En estas circunstancias, Bogert y colaboradores encontraron que los voluntarios tendieron a usar más las sugerencias proporcionadas por el algoritmos -que son percibidos como particularmente capaces para manipular números- que en el juicio de otras personas. No tendríamos así reticencia, de estar Bogert y colaboradores en lo correcto, en aceptar la intromisión creciente de los algoritmos en nuestra vida diaria. Intromisión que, por otro lado, apenas comienza.  Con respecto a esto último, si bien según la llamada paradoja de Moravec, lo que es fácil para los humanos y que requiere habilidades sensomotoras -podar un rosal, por ejemplo- es extremadamente difícil para un algoritmo. Estaríamos así lejos de que una computadora pueda emular totalmente a un ser humano. No obstante, en otras actividades que resultan difíciles para los humanos -demostrar un teorema matemático, jugar ajedrez, o realizar cálculos matemáticos con grandes números, por ejemplo-, las computadoras nos rebasan y lo harán más ampliamente en el futuro cercano. Así, habilidades profesionales que son valiosas en la actualidad tenderán a ser obsoletas en la medida que sus puestos de trabajo asociados sean tomados por asalto por los algoritmos. Sería el caso, por ejemplo, de las profesiones centradas en actividades administrativas. En contraste, paradójicamente no lo sería de profesiones tales como la jardinería o aquellas que demanden de habilidades sensomotoras.",
    "En un escenario de ciencia-ficción podríamos imaginar que nuestro planeta alcanza un nivel de contaminación tal por la sobreexplotación de nuestros recursos naturales que parte de la población del mundo se ve obligada a emigrar a otro planeta. Las olas migratorias, por supuesto, se han dado en el pasado y nunca en condiciones óptimas. Estaríamos de acuerdo, sin embargo, que cambiar de planeta resultaría un poco más traumático. El destino más probable sería Marte, que está relativamente cerca, aunque con condiciones ambientales nada acogedoras, pues además de no tener una atmósfera con oxígeno para respirar, está bombardeado continuamente por mortíferas radiaciones de alta energía, entre otros inconvenientes.La emigración planetaria no debería ser entonces una opción -a menos que no nos quede otra- y más nos valdría tratar de arreglar nuestros problemas ambientales a la brevedad. Estos problemas incluyen las crisis por la contaminación atmosférica, la contaminación de los océanos, la contaminación por desechos plásticos, la contaminación por desechos radiactivos, el congestionamiento de satélites en órbitas terrestres y, por supuesto, el calentamiento global y el cambio climático que no se han podido frenar.  Aunado a lo anterior, el planeta podría enfrentar en un futuro una escasez de agua por la sobreexplotación de los mantos acuíferos que proporcionan este líquido a miles de millones de personas, además de la mitad del agua consumida a nivel global por la agricultura. Este problema es abordado por un artículo publicado esta semana en la revista “Science”  por Scott Jasechsko y Debra Perrone de la Universidad de California en Santa Bárbara. En dicho artículo, Jasechsko y Perrone reportan los resultados de un estudio en el que hicieron una compilación de 39 millones de pozos de agua en 40 países, los cuales producen la mitad del total de agua extraída del subsuelo a nivel global. Los investigadores compararon el nivel del agua en cada pozo con su profundidad y encontraron que hasta un 20 por ciento de los pozos tienen profundidades no mayores de 5 metros por abajo de su nivel del agua. Jasechsko y Perrone examinaron también las memorias de construcción de cada pozo y encontraron que, si bien los pozos perforados más recientemente tienden a ser más profundos que los anteriores, esto no es siempre es así, aun si se tienen datos de que hay una disminución de los niveles de agua por la sobreexplotación. Los datos compilados por Jasechsko y Perrone muestran que en una mayoría de países, incluyendo a los Estados Unidos, el Reino Unido y México, hubo una tendencia en el periodo 2000-2015, a perforar pozos más profundos que más someros, para compensar el abatimiento de los niveles de agua. En algunos otros países, como Argentina, Alemania y Japón, se dio la tendencia opuesta en el mismo periodo de tiempo,   En estas condiciones, Jasechsko y Perrone estiman que hasta un 20% de los pozos a nivel mundial, se encontrarían en riesgo de secarse si el nivel del agua disminuyera en unos pocos metros, generando una crisis de grandes proporciones. Y esto podría suceder tanto con pozos antiguos como con pozos perforados recientemente. A pesar de esto, como señalan los autores, no todos los países que estudiaron cuentan con una red de monitoreo del agua.Consideran Jasechsko y Perrone que deben de implementarse redes de monitoreo de pozos que proporcionen datos sobre sus profundidades y niveles de agua, datos que deben de hacerse públicos, con el fin de determinar las profundidades a las que deben perforarse los nuevos pozos; esto, a la luz de predicciones sobre la disminución de los niveles de agua en el futuro. Tenemos así la tarea de abordar y remediar los problemas ambientales por los que atraviesa el planeta, incluyendo el de la sobreexplotación de los mantos acuíferos, so pena de vernos obligados a emigrar a otro planeta.No obstante, en previsión de esta última eventualidad, habría que mencionar que la semana que hoy termina la NASA anunció que su explorador “Perseverance” había logrado producir oxígeno en Marte a partir del dióxido de carbono de la atmósfera marciana. La cantidad producida fue muy pequeña, alrededor de cinco gramos, que apenas es suficiente para que pueda respirar una persona por diez minutos. El resultado es, sin embargo, alentador, pues involucra la utilización de materia prima marciana para fabricar un elemento que no solamente es indispensable para que futuros astronautas puedan sobrevivir en Marte, sino que también es esencial para impulsar a los cohetes que los traerían de regreso a la Tierra. Además, combinándolo con el hidrógeno, será posible obtener agua, que también es indispensable para la supervivencia. Un paso importante, sin bien pequeño, para avanzar en la colonización de Marte.",
    "Una de las noticias de la semana fue el fallecimiento de Bernard Lawrence Madoff -mejor conocido como Bernie Madoff- en una prisión del estado de Carolina del Norte. Madoff, que contaba con 82 años al morir, recibió en 2009 una sentencia de 150 años en prisión por haber estafado algo así como 20,000 millones de dólares a miles de inversionistas, quienes le confiaron sus capitales con la promesa de obtener rendimientos por arriba de los que ofrecía el mercado. Al final resultó que el esquema de Madoff era fraudulento y que no solamente las supuestas ganancias -unos 40,000 millones de dólares- eran inexistentes, sino que los capitales invertidos estaban en riesgo de perderse.Para estafar miles de millones de dólares a miles de incautos -que incluyeron al director de cine Steven Spielberg, al presentador de televisión Larry King y al equipo de beisbol Metz de Nueva York, entre muchas otras celebridades y organizaciones- se necesitan, por supuesto, habilidades fuera de lo común. Entre otras cosas, es necesario ser un caradura a toda prueba y contar con una gran capacidad de simulación y convencimiento, cualidades que Madoff poseía en grado sumo. Así, en una mesa redonda llevada a cabo en la ciudad de Nueva York en octubre de 2007, poco antes de que se descubrieran sus actividades criminales, Madoff afirmó: “No significa que no haya abusos, sin duda, pero en general, en el entorno regulatorio actual, es prácticamente imposible violar las reglas. Esto es algo que el público realmente no comprende. Si lees cosas en el periódico y ves que alguien viola una regla, dices bueno, siempre está haciendo esto. Pero es imposible que una infracción pase desapercibida, y ciertamente no durante un período de tiempo considerable”.   Y no obstante sus declaraciones -que no convicciones-, Madoff estuvo violando las reglas del sistema financiero de los Estados Unidos por varias décadas antes de ser finalmente descubierto. Esto ocurrió durante la crisis económica de 2008, la cual le impidió seguir sosteniendo su simulación. Curiosamente, si bien sus víctimas fueron personajes y organizaciones prominentes, Madoff utilizó para estafarlos un sistema que no es para nada sofisticado: el llamado esquema Ponzi o fraude piramidal. Este esquema consiste en una recaudación de fondos con la promesa de altos dividendos.  Estos, sin embargo, no se pagan por las ganancias generadas por el capital invertido, como sería lo esperado, sino empleando los recursos recibidos de nuevos inversionistas, lo que no es autosostenible  No es difícil entender que un esquema Ponzi, por necesidad, se colapsará en algún momento. En efecto, este es el esquema por el que se rigen las llamadas pirámides que aparecen de cuando en vez. A manera de ejemplo, consideremos una pirámide que es iniciada por una persona que invierte una cierta cantidad -1,000 pesos, por ejemplo- y, en el curso de un mes, tiene la obligación de incorporar a dos personas más que invertirán 1,000 pesos cada una de ellas. A estas dos nuevas personas se les pide que a su vez incorporen a dos personas cada una que a su vez aporten 1,000 pesos al capital común. El proceso de repite de manera continua con el consecuente crecimiento del tamaño de la pirámide, tanto en número de miembros como en capital. Al mismo tiempo, para hacer atractiva su incorporación a la pirámide, a cada interesado se le asegura un rendimiento a su inversión -10 por ciento, por ejemplo, al cabo de un mes. Si el flujo de nuevos miembros es continuo, los recursos aportados por los recién llegados siempre alcanzarán para pagar los altos réditos mensuales de todos los miembros en los niveles inferiores de la pirámide, la cual crecerá de manera paulatina. La incorporación de nuevos miembros, sin embargo, es demasiado acelerada y pronto no habrá más candidatos. En ese momento no podrán pagarse los réditos pactados y todo el esquema se colapsará.El entorno financiero en el que se desarrolló el esquema Ponzi de Madoff es, por supuesto, más complejo que el ejemplo considerado líneas arriba. En el fondo, sin embargo, se tienen las mismas bases: el pago de réditos atractivos empleando el dinero de otros. Madoff posiblemente sabía que su esquema fraudulento sería detectado en algún momento. Durante su juicio, el 12 de marzo de 2009, declaró: “Cuando comencé mi esquema Ponzi, creí que terminaría pronto y podría liberarme a mi y a mis clientes del esquema. Con el paso de los años me di cuenta de que mi arresto y este día llegarían inevitablemente”.Según algunos, sin embargo, dicho día tardó demasiado en llegar, dado que existían más que suficientes indicios de que lo que Madoff operaba era un fraude tipo Ponzi.",
    "Si, como punto de referencia, consideramos que el 28 por ciento de la energía que consumen los Estados Unidos lo emplea la industria del transporte, y que de este porcentaje el 53 por ciento corresponden a los vehículos de gasolina, no es difícil llegar a la conclusión que los automotores de combustión interna contribuyen de manera significativa a la contaminación atmosférica causante del cambio climático. En estas condiciones, resultan alentadores los anuncios de Volvo y General Motors en el sentido que descontinuarán la producción de automotores de combustión interna a partir de 2030 y 2035, en forma respectiva, para ser sustituidos por vehículos eléctricos.Se esperaría, por supuesto, que en la medida en que se sustituyan los automotores de combustión interna por vehículos completamente eléctricos se limitaría la emisión de gases de invernadero a la atmósfera, dado que un motor eléctrico no emite este tipo de gases. Habría que matizar, no obstante, esta última afirmación. Ciertamente, durante la operación de un motor eléctrico no se emiten gases de invernadero. Para que el motor funcione, sin embargo, hubo que cargar previamente el banco de baterías del automóvil y para esto hubo de transferirse energía eléctrica desde una estación generadora a través de la red eléctrica -la estación termoeléctrica de Villa de Reyes, por ejemplo-.  Cabe preguntarse por la forma en que dicha estación obtiene la energía que transfiere al banco de baterías: ¿la obtiene por medio de una fuente de energía no contaminante como la del Sol, o por una fuente que emite gases de invernadero quemando combustibles fósiles? En el segundo caso no podemos afirmar que los automóviles eléctricos sean totalmente no contaminantes. En realidad, tampoco podemos hacerlo en el primer caso, pues aún si el banco de baterías del automóvil obtuviese su energía de una fuente totalmente limpia, como sería el caso de un banco de paneles solares, para fabricar dichos paneles habría sido preciso utilizar procesos industriales y de extracción de minerales que requirieron de energía que, con seguridad, no fue generada por una fuente limpia sin producción de gases de invernadero.  Podemos también añadir que la misma fabricación del automóvil, incluyendo su carrocería, sus interiores, su banco de baterías, etc., requirió de procesos que produjeron gases de invernadero. Podemos así afirmar que no existe un automóvil que no genera contaminación, y que todos lo harán en cierto grado.Dicho lo anterior, lo relevante es comparar los grados de contaminación generados por ambos tipos de automóviles, eléctricos y de combustión interna. Un documento preparado para el Congreso de los Estados Unidos por Richard Lattanzio y Corrie Clark, especialistas en políticas ambientales y energéticas, nos ofrece esta comparación.  El documento, con fecha 16 de junio de 2020, puede ser consultado libremente en Internet.  Cuando Lattanzio y Clark comparan los niveles de emisión de gases de invernadero durante la fabricación de los automotores -incluyendo aquellos producidos durante la extracción y el procesamiento de materias primas- encuentran que son mayores para los vehículos eléctricos que para los de combustión interna. Encuentran que, durante la fase de producción, las emisiones de gases de invernadero para vehículos del mismo tamaño, los automotores eléctricos emiten entre 1.3 y 2 veces mas gases de invernadero que los de combustión interna. La desproporción en la generación de contaminantes entre los dos tipos de vehículos también se cumple en cuanto a la emisión de otros gases como óxidos de nitrógeno y dióxido de azufre. Por otro lado, hay que señalar que, según Lattanzio y Clark, los automóviles eléctricos aprovechan un 77 por ciento de la energía que reciben de la red eléctrica, mientras que los automóviles de combustión interna emplean apenas del 12 al 30 por ciento de la energía almacenada en la gasolina. En estas circunstancias, la mayor producción de gases de invernadero durante la fabricación de los automóviles eléctricos en comparación con los automóviles de combustión interna es compensada en favor de los primeros durante el tiempo de vida de los vehículos, a pesar de que las baterías les añaden un peso adicional. Del análisis de Lattanzio y Clark se desprende que los automóviles totalmente eléctricos podrán tener un impacto positivo en la mitigación del cambio climático. En qué grado se producirá esta mitigación dependerá, sin embargo, de la mezcla de fuentes de energía, contaminantes y limpias, de las que se abastezcan las redes eléctricas. Con el peligro de que si esta mezcla no es favorable podría incluso resultar peor el remedio que la enfermedad. Después de todo, no es posible crear energía de la nada y la que usen los automóviles eléctricos tendrá que ser generada de algún modo. Que más nos vale procurar que sea no contaminante.",
    "Si Alexander Graham Bell, quien frecuentemente es acreditado como el inventor del teléfono, hubiera viajado en el tiempo hasta nuestros días -por medio de la máquina del tiempo de H.G. Wells, por ejemplo-, difícilmente habría reconocido a los teléfonos inteligentes como los sucesores de su invento. Esto sin duda, pues los teléfonos inteligentes, aparte de servir para hablar por teléfono, se usan en muchas otras aplicaciones por demás disímbolas, desde tomar fotografías y videos, y ver las fotografías y videos que otros nos envían por Internet, hasta consultar el estado del tiempo para el día de mañana y averiguar cuál es el mejor camino para viajar por carretera entre dos puntos. Por otro lado, habría que reconocer que, de darse, la confusión de Bell sería en realidad atribuible a un problema de semántica, pues posiblemente no deberíamos llamar “teléfono” a un dispositivo que trasciende con mucho las funciones del teléfono tal como fue originalmente concebido. De hecho, la evolución de los teléfonos inteligentes ha estado ligada al de las computadoras, que igualmente han trascendido su función original como máquinas para realizar cálculos con números y se han hecho cada vez más inteligentes.   De este modo, en la medida en que los teléfonos han sido dotados de cerebros electrónicos cada vez más potentes -como resultado de los avances en la tecnología de las computadoras-, sus funciones se han multiplicado y diversificado, al mismo tiempo que se han hecho paulatinamente más complejas. Así las cosas, resulta oportuno comentar un artículo que será presentado la próxima semana en el congreso “Conference on Health, Inference and Learning” de la “Association for Computing Machinery” por un grupo de investigadores encabezados por Xin Liu de la Universidad de Washington en Seattle.  En dicho artículo, Liu y colaboradores describen el desarrollo de una aplicación para teléfonos inteligentes que permiten la determinación del ritmo cardiaco y la frecuencia de respiración de una persona a partir de un video de 18 segundos. Mediante un análisis de cambios sutiles de la luz reflejada por la cara, que dependen de variaciones del flujo sanguíneo, la aplicación es capaz de determinar los ritmos cardiaco y respiratorio. Como explican los investigadores, la pandemia de coronavirus por la que atravesamos ha hecho más frecuentes las consultas médicas virtuales. La información del paciente que obtiene el médico mediante sesiones telefónicas o por Zoom es, sin embargo, limitada y en estas condiciones la aplicación desarrollada proporcionaría información relevante sobre sus condiciones de salud.  Por otro lado, la determinación de los ritmos cardiaco y respiratorio a partir de un video, como discuten Liu y colaboradores, requiere que la aplicación esté adaptada a diferentes características físicas y fisiológicas de la población, incluyendo el color de la piel que afecta la cantidad de luz reflejada. La aplicación debe ser también capaz de obtener información en condiciones ambientales cambiantes -por ejemplo, de niveles y tonalidades de iluminación-, diferentes a aquellas en la que se lleva a cabo el entrenamiento. De la misma manera, la aplicación debe tomar en cuenta la sensibilidad relativa a los colores de cada teléfonoPara lograr todo esto, se puede entrenar a la aplicación mediante técnicas de inteligencia artificial, proporcionándole un gran número de datos cardiacos y respiratorios de la población en general, obtenidos mediante instrumentos profesionales. Entrenar teléfonos inteligentes para monitorear los ritmos cardiaco y respiratorio de este modo, sin embargo, requeriría de tiempo, esfuerzo y una gran cantidad de recursos. Una manera de darle la vuelta a estas dificultades es personalizando el entrenamiento, proporcionando al teléfono las características físicas y fisiológicas de su propietario obtenidas mediante instrumentos profesionales. De mejor manera, como mencionan Liu y colaboradores, el entrenamiento se haría sin la ayuda de dichos instrumentos y con una cantidad pequeña de datos fisiológicos. Esto contribuiría a extender el uso de la técnica entre la población.  La aplicación desarrollada por Liu y colaboradores está dirigida a superar todas estas dificultades, y de acuerdo con su artículo, supera a otros sistemas existentes por un margen apreciable. Los investigadores reconocen, sin embargo, que la aplicación tiene dificultades con la piel oscura que refleja menos luz para su análisis.  Tenemos así una nueva aplicación, por si hiciera falta, de los teléfonos inteligentes. Mismos que probablemente debiéramos dejar de llamar teléfonos -pero sí dispositivos extremadamente inteligentes-. Y no confundir así a Alexander Graham Bell en su hipotético viaje desde el pasado. Después de todo, más que suficientes sorpresas se habría de llevar.",
    "De acuerdo con estadísticas actualizadas el día de ayer por el periódico New York Times, los Estados Unidos han aplicado a su población casi 137 millones de vacunas contra el coronavirus, lo que lo convierte en el país que más vacunas ha aplicado. En porcentajes, el 27 por ciento de la población estadounidense ha recibido cuando menos una dosis de la vacuna mientras que el 15 por ciento está totalmente vacunado. Al ritmo actual, los Estados Unidos vacunarían al 70 por ciento de su población en el curso de cuatro meses. En números absolutos, a los Estados Unidos le siguen China, India y el Reino Unido con 97 millones, 58 millones y 32 millones de vacunas aplicadas, en forma respectiva. \nDe manera poco sorprendente, los países que más vacunas han aplicado tienden a ser aquellos que más han contribuido a su desarrollo, y en ese sentido destacan los Estados Unidos que han canalizado o comprometido más de 10,000 millones de dólares de dinero público a compañías farmacéuticas privadas para acelerar el desarrollo de las vacunas, incluyendo a Pfizer, Moderna y Johnson y Johnson, que han producido las tres vacunas cuyo uso hasta la fecha ha sido autorizado en los Estados Unidos. \n  \nEn contraste con países involucrados en el desarrollo de las vacunas, en otros países no involucrados las estadísticas de vacunación son más modestas. En México, por ejemplo, según las estadísticas del New York Times, se han aplicado alrededor de 6.5 millones de vacunas, lo que representa apenas 5.1 vacunas por cada 100 habitantes. \n  \nNo es difícil entender que los Estados Unidos, que han invertido masivamente en el desarrollo de las vacunas, busquen como prioridad asegurar el número suficiente de vacunas para cubrir a su población. De igual forma y de manera también predecible, los países ricos, hayan o no financiado el desarrollo de las vacunas, han buscado asegurar suficientes vacunas para proteger a sus respectivas poblaciones.\n  \nComo resultado de todo lo anterior los países ricos han negociado contratos con las compañías farmacéuticas y acaparado las vacunas disponibles, en detrimento de los países con menos recursos que poco interesan a dichas compañías. Como lo hizo notar el pasado 18 de enero Tedros Adhanom Ghebreyesus, Director de la Organización Mundial de la Salud, “Más de 39 millones de dosis de vacuna se han administrado en al menos 49 países de ingresos altos. Solo se han administrado 25 dosis en uno de los países de ingresos más bajos. No 25 millones; no 25 mil; solo 25”.\n    \nEl acaparamiento de vacunas por los países ricos ha sido motivo de fuertes críticas, dado que el desarrollo de las vacunas solo fue posible con el concurso del dinero público y como resultado las compañías farmacéuticas recibirían grandes beneficios. En efecto, el desarrollo de vacunas altamente efectivas en menos de un año, sólo fue posible por los resultados de investigación básica que se acumularon a lo largo de las últimas décadas y que fueron financiadas con dinero público, como lo relata un artículo aparecido en la revista “Scientific American” el pasado 18 de noviembre. La acumulación de conocimientos científicos, sin embargo, no fue suficiente y tuvo que venir la emergencia sanitaria para que, con la inversión masiva de dinero público, las compañías farmacéuticas pudieran desarrollar las vacunas en tiempo récord. A no dudarlo, a dichas compañías la epidemia también les ha caído como anillo al dedo.    \nAsí las cosas, hay propuestas para suspender provisionalmente, en tanto se supera la pandemia, los derechos de patente de las vacunas que poseen las compañías farmacéuticas, con el objeto impulsar su producción masiva y reducir su costo. Después de todo, dado que el mundo está altamente intercomunicado, la pandemia es un problema global que tiene que resolverse como tal para evitar su propagación, aun desde regiones remotas. En este sentido es ilustrativo el caso de la ciudad brasileña de Manaos, situada en medio de la selva amazónica y la cual, a pesar de estar relativamente aislada y prácticamente sin comunicaciones por vía terrestre, ha tenido dos olas mortíferas de coronavirus e incluso ha generado una nueva cepa más contagiosa que ya se ha propagado a otros países sudamericanos.   \nSin importar las buenas intenciones de aquellos que pugnan por que las farmacéuticas renuncien a sus ganancias en tanto pasa la pandemia, es posible que no pasen de ser eso, buenas intenciones. AstraZeneca, por ejemplo, ha manifestado que no lucrará con su vacuna hasta pasada la pandemia. Tal parece, no obstante, que piensa declararla finalizada el próximo mes de julio. Ojalá, pero está de dudarse.",
    "El paso peatonal de Shibuya en Tokio tiene fama de ser el más concurrido del mundo. En la red Internet no es difícil encontrar videos en los que es posible observar una multitud de peatones en espera de que los semáforos les permitan cruzar a la acera de enfrente. Una vez que esto sucede, y a través de cinco pasos señalados en el pavimento con líneas blancas, los peatones de lanzan a alcanzar su objetivo, entrecruzándose sin interferir con los que vienen en sentido opuesto.Las multitudes que cruzan por el paso peatonal de Shibuya -miles de personas a la vez, según la Wikipedia- reflejan la alta movilidad de la población de Tokio y el hecho que Shibuya es una de las principales estaciones de la extensa red de trenes urbanos y suburbanos de esta ciudad.  Habría que mencionar que las aglomeraciones en Shibuya no son únicas en Tokio -una ciudad cuya área metropolitana es en población la primera o segunda más grande del mundo- y constituyen más la regla que la excepción.Por lo anterior, no causa sorpresa el artículo aparecido esta semana en la revista “Science Advances” en el que se analizan situaciones como las que se presentan en el paso peatonal de Shibuya, con la circunstancia de que algunas de las personas que cruzan la calle podrían ir distraídas tecleando un mensaje en su teléfono móvil. El artículo de referencia fue publicado por un grupo de investigadores encabezado por Hisashi Murakami de la Universidad de Tokio, y en el mismo se reportan los resultados de un estudio llevado a cabo con dos grupos de 47 estudiantes cada uno, que caminan en sentidos contrarios e ingresan y se cruzan en un pasillo de 3 metros de ancho y 10 metros de largo. Los experimentos se llevaron a cabo con cuatro condiciones. En la primera se les pidió a tres de los elementos de uno de los grupos, seleccionados al azar, que hicieran uso de sus teléfonos móviles durante el experimento y se les colocó en las tres posiciones delanteras al ingresar el grupo al pasillo. De manera específica, se les pidió que teclearan un mensaje y mantuvieran fija la vista sobre su teléfono la mayor parte del tiempo. En otros dos experimentos se seleccionaron también tres personas al azar para hacer uso de sus teléfonos móviles, pero se les colocó en la parte media del grupo en un caso y en la parte trasera del mismo en el otro. Finalmente, en un cuarto experimento ningún participante hizo uso de su teléfono móvil.Los experimentos fueron filmados y los resultados pueden ser consultados libremente en la página electrónica de “Science Advances”. De acuerdo con Murakami y colaboradores, al entrar dos grupos en contacto, las personas que los encabezan toman decisiones sobre el camino a seguir para evitar colisiones, basadas en una evaluación de los movimientos de la persona que tienen enfrente, mientras que los que vienen atrás simplemente les siguen. Así, en el video correspondiente al experimento en el que no se hace uso de teléfonos móviles, se observa que al hacer contacto los dos grupos se dividen de manera fluida en tres filas que se entrecruzan de manera alternada. En contraste, en el experimento en el que los tres participantes a la cabeza caminan distraídos con su teléfono celular y sólo ven hacia adelante una fracción del tiempo, les cuesta trabajo evaluar los movimientos futuros de la persona que tienen enfrente y la interacción es mucho más desordenada. Esto hace que al grupo le tome más tiempo formar filas y retrase su salida del pasillo. La desorientación, además, no ocurre solamente con aquellos que usan sus teléfonos, sino también con los líderes del otro grupo, que tienen dificultades para evaluar los movimientos futuros de los líderes distraídos y tomar decisiones en consecuencia. En los casos en los que los distraídos se insertan en la mitad y al final del grupo, los resultados son intermedios entre los dos casos anteriores.Así, los resultados de Murakami y colaboradores nos muestran que hay ocasiones en las que caminar compartiendo nuestra atención con un teléfono móvil no es lo más recomendable. Esto quizá no es de sorprender. Como posiblemente sí lo sea enterarnos que al caminar distraídos tecleando un mensaje podríamos también resultar inconvenientes para aquellos con los que interactuamos, quienes pudieran tener dificultades para predecir nuestros movimientos inmediatos. Habría así tiempos para usar nuestro teléfono móvil para consultar y enviar mensajes escritos, para enterarnos de las últimas noticias, para saber si lloverá el día de mañana, para tomar fotografías, para grabar mensajes de voz, para tomar clase en línea y, por supuesto, para hablar por teléfono. Pero quizá no para escribir un mensaje cuando cruzamos por un paso peatonal en horas pico.",
    "En el año de 1901, cerca de la isla griega de Anticitera en el mar Egeo, un grupo de buzos que se sumergieron en el mar en busca de esponjas descubrieron entre los restos de un antiguo naufragio un objeto que de momento no identificaron, pero que a la postre resultaría célebre.  Dicho objeto, con una antigüedad de unos dos mil años, es hoy conocido como el Mecanismo de Anticitera, una especie de computadora empleada para describir el movimiento de la Luna, el Sol y los cinco planetas conocidos en la antigüedad. El Mecanismo de Anticitera se encuentra hoy en día en el Museo Nacional de Arqueología en Atenas.Como es fácil de entender, después de dos mil años de permanecer sumergido en el mar, el estado en el que se encuentra el Mecanismo de Anticitera no es el que hubiéramos deseado y, de hecho, se encuentra incompleto y dividido en 82 fragmentos que constituyen apenas un tercio del total. Esto, sin embargo, no impide vislumbrar que se trata de un objeto en cuya construcción se emplearon técnicas de fabricación de partes mecánicas que se antojan avanzadas para la época, lo mismo que los conocimientos astronómicos entonces en boga. Si bien el Mecanismo de Anticitera ha sido objeto de estudio de varios grupos de investigación, por su estado de deterioro no se han podido desentrañar todos los detalles de su construcción y funcionamiento. Los resultados del último esfuerzo en este sentido fueron publicados la semana que hoy termina en la revista “Scientific Reports”, por un grupo de investigadores encabezado por Tony Freeth del University College London, en el Reino Unido. En dicho artículo se desarrolla un modelo para el Mecanismo de Anticitera que los autores consideran desvela finalmente los secretos que encierra.Según el modelo de Claudio Tolomeo, aceptado en el tiempo en que fue construido el Mecanismo de Anticitera, la Tierra es el centro del Universo, con la Luna, el Sol y los cinco planetas entonces conocidos orbitando a su alrededor. Si bien esto es cierto para la Luna y podría pasar como tal para el Sol, está lejos de serlo para los planetas que, como sabemos, giran alrededor del Sol.  De este modo, las trayectorias que siguen los planetas vistos desde la Tierra son bastante más complicadas que las de la Luna y el Sol. Y, si como Tolomeo nos empeñamos en poner a la Tierra en el centro del Universo, tendremos que asumir que los planetas se mueven en una trayectoria más compleja que una simple órbita circular alrededor de nuestro planeta.En estas circunstancias, Tolomeo ideó la teoría de los epiciclos, según la cual los planetas se mueven en círculos alrededor de un punto que a su vez se mueve en una órbita circular alrededor de la Tierra. Además, dado que los epiciclos no fueron suficientes para explicar las trayectorias de los planetas, Tolomeo se vio forzado a asumir que las órbitas de los epiciclos son excéntricas, es decir que no están centradas en la Tierra sino en otro punto del espacio. Y como esto último no fue tampoco suficiente, Tolomeo sin desanimarse postuló la existencia de un punto adicional, el ecuante, desde el cual la velocidad con la que el centro de los epiciclos surcaba el firmamento era constante.Toda esta complejidad la tuvieron que tomar en cuenta los diseñadores y constructores del Mecanismo de Anticitera. Un punto a su favor fue que los movimientos de la Luna, el Sol y los planetas se describían en términos de círculos, lo que facilitó que fueran simulados por medio de engranes. Así, como lo ilustran las imágenes publicadas por Freeth y colaboradores -que pueden ser libremente consultados en Internet en el sitio de Scientific Reports- el Mecanismo de Anticitera podría haber consistido en un complejo ensamble de engranes sobre engranes, algunos rotando sobre un eje común y otros alrededor de ejes excéntricos fijos sobre los primeros.  El diseño producido por Freeth y colaboradores es sin duda fascinante y de corresponder al diseño original resultaría también sorprendente. ¿Podrían los griegos hace dos mil años haber producido un ingenio con el nivel de sofisticación mostrado en el modelo desarrollado? Como lo señalan Freeth y colaboradores, los constructores del Mecanismo de Anticitera tendrían, entre otras cosas, que haber construido múltiples ejes huecos, embebidos uno en otro como muñecas rusas, sin contar con tornos para metal.De estar Freeth y colaboradores en lo cierto, el Mecanismo de Anticitera -que no habría sido de ningún modo obra única- constituyó un diseño superlativo de ingeniería y tecnología. En contraste, el modelo de Tolomeo implementado en dicho mecanismo resulta complicado en exceso. Además de claramente erróneo.",
    "En el prefacio de su novela “Frankenstein o el moderno Prometeo”, publicada en 1818, Mary Shelley escribió: “Pasé el verano de 1816 en los alrededores de Ginebra. La temporada estaba fría y lluviosa, y por las noches nos amontonamos alrededor de un fuego de leña ardiente, y ocasionalmente nos divertimos con algunas historias alemanas de fantasmas que cayeron en nuestras manos. Estos cuentos despertaron en nosotros un juguetón deseo de imitación. Otros dos amigos y yo accedimos a escribir cada uno una historia basada en un hecho sobrenatural”. Refiere así Mary Shelley las circunstancias que dieron origen a la novela por la que es más famosa. El año de 1816 fue particularmente frío, con un descenso promedio en la temperatura global de aproximadamente medio grado centígrado. La alteración climática fue a tal grado severa que 1816 que es conocido como el “Año que no tuvo verano”. Los expertos explican que todo esto fue debido a que en los años previos ocurrieron una serie de erupciones volcánicas que arrojaron grandes cantidades de contaminantes a la atmósfera que bloquearon parcialmente la radiación solar. La última de dichas erupciones fue la del Monte Tambora en Indonesia, ocurrida en 1815.En el verano de 1816, Mary Shelley, coincidió por varios días con Lord Byron y otros escritores en una casa alquilada a orillas del lago Ginebra en Suiza.  El tiempo frío y lluvioso debido a la alteración climática los obligó a resguardarse de manera permanente en el interior de la casa, circunstancia que dio origen, además de “Frankenstein y el moderno Prometeo”, a una novela de horror inconclusa de Lord Byron y a la novela corta “El vampiro” de John William Polidori, otro de los asistentes a las veladas obligadas a orillas del lago Ginebra. Como Mary Shelley y sus colegas lo experimentaron en su tiempo, la temperatura del planeta puede ser sustancialmente modificada por la emisión de gases y polvo volcánico a la atmósfera. Doscientos años después experimentamos esto en carne propia, si bien mediante la emisión de gases diferentes a los arrojados por el Monte Tambora que elevan, más que disminuyen, la temperatura global.Tomando en cuenta lo anterior, cabe preguntarse si es factible que el actual calentamiento global pudiera ser contrarrestado empleando técnicas de geoingeniería solar que inyecten a la atmósfera gases que produzcan el efecto contrario al de los gases de invernadero. La respuesta de los expertos es que nadie sabe si esto pudiera hacerse de manera segura. Es decir, los fenómenos atmosféricos son de una complejidad tal que resultaría muy peligroso intentar modificar el clima a escala global por los efectos colaterales, difíciles de anticipar, que se podrían producir, incluyendo una alteración en los patrones de lluvias a lo largo del planeta.      Aun así, algunos científicos del clima están interesados en investigar las posibilidades que tendríamos de modificar el clima del planeta empleando técnicas de geoingeniería. El argumento es que, aún si no se pusieran en práctica, es necesario conocer el potencial que ofrecerían dichas técnicas, dada la lentitud con la que el mundo está adoptando medidas para limitar la emisión de dióxido de carbono a la atmósfera. En este sentido, un artículo aparecido el pasado 19 de febrero en la revista de divulgación científica, MIT Technology Review, firmado por James Temple, destaca el proyecto que actualmente tiene en curso Frank Keutsch en la Universidad de Harvard para estudiar formas para reflejar la radiación solar antes de que llegue a la superficie terrestre. Keutsch y su grupo planean enviar una serie de globos que inyectarían en la estratósfera un polvo fino de carbonato de calcio y estudiarían su dispersión y habilidad para reflejar la radiación solar. Posteriormente experimentarían con otro tipo de partículas, incluyendo ácido sulfúrico.Los experimentos de geoingeniería solar, sin embargo, tienen críticos. Uno de éstos es Wil Burns, citado por Temple, quien considera que “los desafíos de gobernar una herramienta de este tipo son inmensos: un solo país podría realizar geoingeniería solar por sí solo, pero todos los países se verían afectados…y no podemos saber qué ocurrirá realmente a escala planetaria”.  Así, si bien para Mary Shelley y sus colegas las molestias del encierro fueron pasajeras, pues como concluye en su prólogo: “El clima, sin embargo, de repente se volvió sereno; y mis dos amigos me dejaron de viaje en los Alpes”, para las futuras generaciones los efectos negativos de la geoingeniería solar podrían ser sustancialmente más duraderos. Y sin que haya garantía de que tengamos ganancias literarias.",
    "¿Cuándo se descubrieron los rotíferos y quién fue su descubridor? No existe un acuerdo definitivo al respecto, pero se sabe que su existencia fue puesta de manifiesto por primera vez hacia el final del siglo XVIII, según algunos por el inglés John Harris y según otros por el holandés Antoni van Leeuwenhoek. De acuerdo con la Wikipedia, existen unas 2,200 especies de rotíferos y éstos son comunes a nuestro alrededor. A pesar de esto, tardamos en descubrirlos a causa de su pequeño tamaño. En beneficio del lector, quizá valga la pena mencionar que -otra vez de acuerdo con la Wikipedia- los rotíferos son organismos que en su mayoría tienen un tamaño microscópico, entre un décimo de milímetro y medio milímetro, y que se encuentran en el charcos y estanques, lo mismo que en lugares húmedos. Dado su pequeño tamaño, para observar a los rotíferos en detalle Harris y Leeuwenhoek tuvieron que recurrir al uso de microscopios, primitivos, pero con los aumentos suficientes. En un artículo publicado en 1694 en la revista “Philosophical Transactions” de la Royal Society, Harris relata sus observaciones: “Examiné una pequeña gota de agua de lluvia que había estado en un recipiente de barro en mi ventana por cerca de dos meses…Vi allí un animal como un gran gusano que podía contraerse en una figura esférica y luego estirarse de nuevo; la punta de su cola aparece como un fórceps como el de una tijereta; pude ver claramente que abría y cerraba la boca, de donde salían frecuentemente burbujas de aire”.Para los primeros afortunados que pudieron ver a través de un microscopio un mundo hasta entonces oculto, la experiencia tuvo que haber sido fascinante. Equivalente a la que experimentó Galileo cuando apuntó su telescopio hacia Júpiter y descubrió cuatro puntos brillantes -los satélites galileanos- orbitando a su alrededor. Hoy en día existen microscopios mucho más potentes que los que emplearon Harris y Leeuwenhoek que nos permiten ver al micromundo, y aun al nanomundo -mil veces más pequeño- con lujo de detalles. Si bien, posiblemente no con el mismo y mayúsculo asombro que experimentaron Harris y Leeuwnhoek, sí ciertamente sin dejar de fascinarnos.Un ejemplo de lo anterior lo constituye el artículo publicado esta semana en la revista “International Journal of Molecular Sciences” por un grupo de investigadores encabezado por Katarzyna Turnau, de la Universidad de Cracovia, en Polonia. En dicho artículo, Turnau y colaboradores ponen de manifiesto un teatro de guerra que se lleva a cabo dentro de una gota de agua proveniente de una planta de tratamiento de aguas residuales, y en el que participan una cierta especie de rotíferos, por un lado, y una alianza de hongos microscópicos, bacterias y virus, por el otro. En esta guerra, los rotíferos son la parte agredida: los agresores se interesan en ellos en calidad de ganado para su alimentación y actúan en consecuencia.  De hecho, como veremos, en lugar de guerra podríamos hablar de una masacre.Empleando microscopios ópticos y microscopios electrónicos, Turnau y colaboradores pudieron seguir los detalles de las hostilidades. Como parte de la estrategia conjunta de ataque, los hongos establecen una red de filamentos con los cuales logran atrapar e inmovilizar al rotífero con la ayuda de bacterias adheridas a la superficies externas e internas de dichos filamentos. Seguido de esto, los filamentos penetran al interior de la presa para absorber nutrientes, al mismo tiempo que las bacterias adheridas a las paredes externas e internas de los filamentos se lanzan al abordaje, también en busca de nutrientes. Al final, el hongo retira el filamento del interior del rotífero, dejando sólo una carcaza con bacterias en su interior. Durante todo este proceso, Turnau y colaboradores encuentran gran cantidad de virus durante el proceso y aunque consideran que cumplen una parte activa en el mismo, no tienen de momento una claridad al respecto.Ciertamente, aun a varios siglos de distancia desde la invención del microscopio hay amplio espacio para el asombro en nuestra exploración del mundo microscópico. En particular, el trabajo de Turnau y colaboradores nos demuestra que allá abajo puede haber guerras y alianzas tales como las que ocurren aquí arriba.",
    "En su novela de 1898 “La guerra de los mundos”, el escritor británico H.G. Wells imagina que la Tierra es invadida por un ejército de marcianos que hacen uso de máquinas de guerra gigantescas y de cañones de rayos calóricos capaces de incinerar todo lo que tocan. Las primeras señales, no comprendidas en su momento, del peligro que acechaba a nuestro planeta, fue una serie de resplandores que pudieron observarse desde la superficie de Marte durante 1894, y que no eran otra cosa que el disparo de cañones de tamaño enorme empleados para lanzar hacia la Tierra a las naves invasoras. Quien haya leído la novela sabrá que tuvo un final feliz -dentro de lo que cabe, por la enorme destrucción que causaron los marcianos- pues los invasores fueron finalmente aniquilados por los microbios terrestres.  \n  \nCuriosamente, en el improbable caso de que en Marte hubiera marcianos, podrían éstos haber atestiguado recientemente hechos similares a los descritos por Wells en su novela: tres resplandores ocurridos en la superficie de nuestro planeta entre los días 19 y 30 de julio del año pasado, cuyo significado no les hubiera sido claro en un primer momento, pero que después habrían averiguado fueron originados por la ignición de tres cohetes que despegaron de la superficie de nuestro planeta con destino a Marte.  \n  \nEstos cohetes llevaron a bordo sendas naves espaciales de misiones de la NASA, la agencia espacial china y el programa espacial de los Emiratos Árabes Unidos. La misión de la NASA tuvo como objetivo posar suavemente sobre la superficie marciana al robot explorador Perseverance. Un objetivo similar buscaba la misión china, a la que se le ha dado el nombre de Tianwen-1, y que incluye, además, una nave para orbitar Marte. Por su lado, la misión de la Emiratos Árabes Unidos incluye solamente un orbitador.  \n  \nLa noticia científica de la semana fue, sin duda, el arribo exitoso del explorador Perseverance a la superficie de Marte el pasado día 18. El Perseverance tiene como misión identificar lugares propicios para el desarrollo de vida microbiana y explorarlos en busca de signos de vida. Levará también a cabo experimentos para la fabricación de oxígeno a partir de la atmósfera marciana que está formada fundamentalmente por dióxido de carbono. Recogerá igualmente muestras de rocas y suelo marciano y las almacenará en la superficie del planeta para que sean recolectadas y traídas a la Tierra en misiones futuras.  \n  \nComo un elemento adicional, la misión de la NASA incluye un pequeño helicóptero para explorar la superficie marciana desde una perspectiva novedosa: por encima de dicha superficie, pero no tan alejada como la que tienen los satélites en órbita. Todo esto con un costo:  alrededor de 2,800 millones de dólares, que incluye el desarrollo y construcción del Perseverance, sus costos de lanzamiento y operación, y 85 millones de dólares para diseñar, construir y operar el helicóptero.   \n  \nEl Tianwen-1, por su parte, entró en órbita alrededor de Marte el pasado 10 de febrero -un día después de que lo hiciera la sonda árabe- con el robot explorador a bordo. Se espera que dicho explorador intente posarse suavemente sobre la superficie marciana el próximo mes de mayo.  \n  \nHay de este modo una actividad renovada en la exploración de Marte que ahora incluye no solamente a los Estados Unidos y Rusia, los países que iniciaron la exploración del espacio -el último todavía como Unión Soviética- sino también a China, la India, Japón, el Reino Unido, la Unión Europea y, de manera sorprendente, a los Emiratos Árabes Unidos.  \n  \nDe la misma manera hay una actividad renovada en la exploración de la Luna y de algunos asteroides y todo esto de alguna manera nos trasporta hacia el pasado, al tiempo en el que H.G. Wells escribió su novela La Guerra de los Mundos, la cual algunos han entendido como una crítica al colonialismo de la época mediante el cual algunos países europeos se apoderaron prácticamente de todo el continente africano, como los marcianos quisieron apoderarse de la Tierra. ¿Estaremos ahora en una situación en algún modo similar en una carrera entre países para apoderarse de los recursos del espacio?  \n    \nPor supuesto, en Marte no hay marcianos -como sí había africanos en África en el siglo XIX- a quienes robarles las tierras y en ese sentido no habría que desarrollar conceptos tales como el de “razas inferiores” que se inventaron los europeos para justificar el despojo, y que fue incluso usado por el mismo H.G. Wells al inicio de su novela, con todo y su crítica al colonialismo europeo.  \n  \nPor lo demás, al margen de intenciones ocultas, con seguridad las sondas marcianas nos harán llegar, desde el remoto lugar en el que se encuentran, noticias e imágenes que seguramente serán fascinantes.",
    "En enero de 2020 y en relación a la entonces naciente pandemia de coronavirus, el Director de la Organización Mundial de la Salud (OMS) escribió en la página oficial de la misma:\n“Como saben, acabo de regresar de China. Ayer tuvimos oportunidad de reunirnos con el Presidente Xi Jinping, el Ministro de Salud Ma Xiaowei y el Ministro de Relaciones Exteriores Wang Yi. Antes de mi visita, mantuve un contacto prácticamente diario con el Ministro Ma para tratar sobre la respuesta al brote y sobre la ayuda que puede prestar la OMS, porque necesitamos centrarnos en el epicentro del brote para contribuir a evitar que se propague al resto del mundo.”\n  \n“Durante mi visita mantuvimos una serie de conversaciones basadas en la honestidad y el entendimiento mutuos. Tratamos sobre la colaboración continua para aplicar medidas de contención en Wuhan, medidas de salud pública en otras ciudades y provincias, la realización de nuevos estudios sobre la gravedad y la transmisibilidad del virus y el intercambio de datos y material biológico. Me sentí muy impresionado y esperanzado por el conocimiento detallado que tiene el Presidente sobre el brote y por su implicación personal en la respuesta, con los que demostró un liderazgo fuera de lo común. Según me transmitió, las medidas que han tomado benefician no solo a China, sino también al resto del mundo.”\n    \nA la luz de los acontecimientos, habríamos de concluir que, a pesar de lo mucho que se impresionó el Director de la OMS, el coronavirus no se habría enterado de las medidas tomadas en su contra por el gobierno chino -o bien que no las habría tomado muy a pecho- y se expandió rápidamente por el mundo. Aunque, en descargo de la OMS y el gobierno chino, habría que acotar que para esto contó con la cooperación de muchos países, incluyendo el nuestro, que no se tomaron la pandemia demasiado en serio. \n  \nPor otro lado, los expertos descartan que el coronavirus tenga un origen artificial y en su lugar, sostienen que se originó en una especie silvestre, posiblemente murciélagos, y de ahí se transfirió a los humanos a través de una especie intermediaria. Los detalles de cómo ocurrió esto, sin embargo, son todavía un misterio.  \n  \nLa epidemia actual, por lo demás, no es un caso único -aunque sí el más grave- de patógenos recientemente trasmitidos a los humanos desde una especie animal y ejemplos de esto son los coronavirus que ocasionan el SARS y el MERS, detectados por vez primera en 2003 y 2012, de manera respectiva. En este contexto, la manera cómo se ha gestionado la actual pandemia resulta poco esperanzadora. \n  \nY lo es todavía menos a la luz de un artículo aparecido esta semana en la revista “Science for the Total Environment” que introduce una conexión inesperada entre la pandemia de coronavirus y el cambio climático. Dicho artículo fue publicado por un grupo de investigadores encabezado por Robert Beyer de la Universidad de Cambridge en el Reino Unido.   \n  \nEn su artículo, Beyer y colaboradores reportan los resultados de un estudio llevado a cabo para determinar las modificaciones que han ocurrido por efecto del cambio climático en la vegetación de una región que incluye la provincia de Yunnan en el sur de China y las áreas colindantes de Myanmar y Laos. Esto, desde las primeras décadas del siglo pasado hasta la actualidad. Encontraron que la flora del área estudiada ha cambiado de arbustos a bosques y lo que ha provocado un crecimiento en la población de murciélagos, que estiman se ha incrementado en 40 especies. Y dado que cada especie porta un promedio de 2.7 coronavirus diferentes, se desprende que en los últimos cien años ha habido un incremento en el área de alrededor de 100 coronavirus que son transmitidos por murciélagos. \n  \nTodo esto, según Beyer y colaboradores, incrementa la probabilidad de que los patógenos portados por los murciélagos salten a los humanos, posiblemente a través de un mamífero intermediario como el pangolín o la civeta, que son parte de la fauna de la región, sugiriendo una conexión entre el cambio climático y la pandemia de coronavirus actualmente en curso. \n  \nEsta conclusión debe ser, por supuesto, corroborada -o desechada en su caso- por estudios posteriores, pero ciertamente resulta poco alentadora dado el lento avance en las medidas para mitigar el calentamiento global. Y también por el manejo que muchos países le han dado al coronavirus, que sin duda se ha sentido a sus anchas a lo largo del último año.",
    "Nuestra historia tiene lugar en la Ciudad de México en el mes de diciembre de 1959, y en la misma intervienen espías mexicanos, espías norteamericanos y miembros de la KGB soviética. Como veremos, si bien la historia podría servir de argumento para una película de James Bond, corresponde a hechos verídicos según la versión de varios de sus protagonistas, y gira alrededor del robo por parte de los norteamericanos de los secretos de una sonda espacial que la Unión Soviética tenía en exhibición en el Auditorio Nacional. La historia está relatada en un artículo aparecido el pasado 28 de enero en la revista de divulgación “MIT Technology Review” firmado por Jeff Maysh.\n  \nComo sabemos, el 4 de octubre de 1957 la Unión Soviética logró poner en órbita terrestre al Sputnik 1, el primer satélite artificial de la historia, seguido dos meses después por el Sputnik 2 con la perra Laika a bordo. Los Estados Unidos, por su parte, colocaron en órbita su primer satélite, el Explorer 1, el 31 de enero de 1958. Se inició de esta manera la carrera espacial entre los Estados Unidos y la Unión Soviética. De manera dispareja, sin embargo, pues el Sputnik 2 tenía un peso de media tonelada mientras que el Explorer 1 pesaba apenas unos 15 kilogramos, lo que evidenciaba una diferencia sustancial en tecnología espacial entre los dos países. \n\n  Pronto esta diferencia se hizo más clara cuando la Unión Soviética logró enviar las sondas Luna 2 y Luna 3, la segunda de las cuales logró fotografiar la cara oculta de nuestro satélite natural en octubre de 1959. Con esto, los norteamericanos entraron en pánico pues consideraban que la superioridad espacial soviética reflejaba su capacidad para fabricar misiles nucleares -el verdadero interés de la carrera espacial- que pudieran hacer blanco en su territorio.\n\n    Así las cosas, la Unión Soviética organizó una exposición itinerante para exhibir su desarrollo tecnológico, en la que la sonda lunar ocupaba el lugar central.  La exhibición abrió del 21 de noviembre de 1959 en la Ciudad de México y se prolongó hasta finales de diciembre. \n      \nAprovechando la ocasión, los norteamericanos resolvieron tratar de averiguar los secretos de la superioridad espacial rusa examinando subrepticiamente a la sonda lunar. Dado que estaba fuertemente resguardada por los agentes soviéticos, esto no era posible durante la exhibición, por lo que idearon un plan para secuestrarla por algunas horas durante su traslado desde el Auditorio Nacional hasta la estación del tren en la Ciudad de México.\n  \nPara llevar a cabo su plan, los espías norteamericanos contaron con la ayuda de espías mexicanos comandados por un miembro del Estado Mayor Presidencial según relata Maysh que, aprovechando el caos de tránsito característico de la Ciudad de México, lograron separar el camión en el que se trasladaba a la sonda, del vehículo en el que se transportaban los agentes soviéticos que la tenían a su cuidado. Así, los agentes mexicanos llevaron la sonda lunar a un sitio previamente escogido en donde los espías norteamericanos abrieron la caja en la que se transportaba y la desarmaron por completo para fotografiarla. Después de esto la volvieron a armar con todo cuidado para que los soviéticos no notaran la violación. Antes de rearmarla, no obstante, removieron todas las piezas removibles de su interior. Particularmente, estaban interesados en obtener muestras del combustible empleado por los soviéticos, que consideraban era uno de los factores que hacían superior a la tecnología espacial rusa.\n  \nTodo lo anterior lo lograron durante la noche en el espacio de unas cuantas horas, de modo que antes del amanecer la sonda lunar llegó a la estación del ferrocarril en la que la esperaban los soviéticos que nada notaron. Todo esto demuestra el alto grado de habilitación de los espías norteamericanos, uno de los cuales formaba parte de un grupo de ladrones e intrusos conocido de manera afectuosa como “hombres del segundo piso” por su capacidad para entrar a edificios a través del segundo piso.\nHabría que decir también que, según Maysh, el escuadrón que llevó a cabo el latrocinio contó con la valiosa ayuda de una persona de la embajada norteamericana, quien les pidió que se olvidaran de las enchiladas y las margaritas y que sólo consumieran avena y agua, dado que una vez dentro de la caja en la que se transportaba la sonda, el espacio sería tremendamente reducido y un mal gas daría al traste con toda la operación.\n  \nLa información proporcionada por el secuestro de la sonda soviética habría sido de gran ayuda para que los norteamericanos ganaran finalmente la carrera a la luna y con esto la historia tuvo un final feliz. De acuerdo, por supuesto, del lado en que se le vea.\n \n \n \n.",
    "Según estima la Agencia Internacional de Energía (AIE), por la desaceleración económica debida a la pandemia de coronavirus, la demanda mundial de energía durante 2020 se redujo aproximadamente un 5 por ciento. De manera concurrente, la emisión de dióxido de carbono a la atmósfera disminuyó en 7 por ciento, paliando el creciente problema de calentamiento global, al mismo tiempo que la inversión en energía se redujo en un 18 por ciento. Una vez superada la pandemia, sin embargo, se reactivará la economía y se incrementará el consumo de energía y con esto la emisión de gases de invernadero. En estas condiciones, cabe preguntarse, cómo se verán afectados el consumo de energía y el calentamiento del planeta en los años por venir. \n\nEn este sentido, la AIE considera dos posibles escenarios. En el primero se asume que las medidas y políticas implementadas por los diferentes países para combatir el calentamiento global serán las mismas que antes de la pandemia, y que ésta será puesta bajo control durante 2021. Dadas estas condiciones, el consumo de energía mundial retornará en 2022 al nivel que tenía en 2019 y a partir de ahí reasumirá su ritmo de crecimiento pre-pandemia.  \nEn un segundo escenario, la AIE asume que la crisis por la pandemia se prolongará y sólo será hasta 2025 que la demanda de energía recuperará su nivel pre-pandemia. En este contexto, la economía sufrirá un mayor nivel de daño y la pandemia marcará el inicio de una década con el crecimiento en la demanda de energía más bajo desde 1930.  \n\nDe acuerdo con las proyecciones de la AIE, en el segundo escenario se estabilizarán las emisiones de dióxido de carbono en los próximos diez años, mientras que en el primer escenario se mantendrán creciendo, sin bien a un menor ritmo que el que tenían antes de la pandemia. En ambos escenarios los niveles de emisión de gases de dióxido de carbono serán sustancialmente menores que los que se habrían dado de no haber ocurrido la pandemia, sobre todo para el segundo escenario en el que el mundo tarda más tiempo en superarla.\n\nEn todos los escenarios considerados por la AIE se predice un declive en el uso del carbón, que, como sabemos, fue el combustible más importante durante la revolución industrial. Este declive estaba ya presente antes de la pandemia, pero su caída se acelerará en los años por venir. Por el contrario, el gas natural, se mantendrá al alza, mientras que la demanda de petróleo, de cumplirse el segundo escenario, alcanzará un límite en la década de 2030.    \nPor su lado, las energías renovables, incluyendo la solar, la eólica y  la hidroeléctrica, jugarán un papel central en el futuro energético del planeta. Y entre todas éstas, la energía solar fotovoltaica –que emplea módulos solares para convertir la energía del sol en energía eléctrica– se llevará la parte del león en la generación de energía eléctrica. Esto, debido a la espectacular reducción por un factor de 100 en el precio que los módulos solares en las últimas cuatro décadas. \nLa pandemia de coronavirus actualmente en curso tendrá repercusiones en la demanda global de energía, incluso después de que sea superada. Esto al menos según la AIE, que proyectaba antes de la pandemia un crecimiento de 12 por ciento en la demanda de energía entre 2019 y 2020.  Ahora, las proyecciones en el mismo periodo son de 9 por ciento en el primer escenario y de sólo 4 por ciento en el segundo. El curso del mundo post-pandemia es entonces dependiente del tiempo que le tome al mundo superar la crisis sanitaria.\n\nY en este contexto, cabe hacer notar que algunos países ricos, notablemente los Estados Unidos y la Gran Bretaña, han acaparado buena parte de la producción mundial de vacunas y que estarían en posibilidad de sortear la crisis en el curso del presente año. Además, por su desarrollo tecnológico, los países industrializados tendrán herramientas –como las tuvieron para fabricar vacunas– para desarrollar los esquemas de energía sustentable que demanda el mundo del futuro.\n\nEn México estamos en una situación muy diferente. Por un lado, contamos con un número de vacunas que es insuficiente, al menos por el momento, para superar la pandemia antes de que ésta siga su curso natural. Y, por otro lado, dadas nuestras limitaciones tecnológicas ¿cómo vamos a sortear la crisis climática en las décadas por venir?",
    "Una característica de nuestro tiempo es el avance vertiginoso del conocimiento científico y con esto de las tecnologías basadas en dicho conocimiento. Que dicho avance es verdaderamente vertiginoso lo ejemplifica el desarrollo, en menos de un año, de varias vacunas para frenar el avance de la pandemia de coronavirus. Ante la gran velocidad con que avanza el conocimiento científico y tecnológico, las universidades y centros de educación profesional deben renovar sus programas de estudios a la misma velocidad, so pena de que queden rápidamente obsoletos. En este sentido, es interesante comentar un artículo publicado el pasado mes de octubre en la revista “Physical Review Physics Education Research”, por un grupo de investigadores encabezado por Michael Fox, del Instituto de Estándares  y   Tecnología, y de la Universidad de Colorado en Boulder.En el artículo de referencia, intitulado: “Preparándonos para la revolución cuántica: ¿Cuál es el papel de la educación superior?”, Fox y colaboradores hacen un análisis de los cambios que deben de llevar a cabo la universidades de los Estados Unidos para formar a los ingenieros y técnicos de alta calificación que necesitan la compañías involucradas con el campo de la información cuántica. Todo esto en el marco de la “National Quantum Initiative”, convertida en ley por el Congreso de los Estados Unidos el 21 de diciembre de 2018.Dado que el término “revolución cuántica” podría quizá resultar poco familiar, vale la pena mencionar que esta revolución está basada en la “mecánica cuántica”, disciplina desarrollada en Europa en las primeras décadas del siglo pasado por una generación excepcionalmente brillante de físicos. La mecánica cuántica es una teoría que explica el funcionamiento de la naturaleza en una escala atómica, en términos radicalmente diferentes de la física que se aplica a los objetos de tamaño ordinario. De hecho, la mecánica cuántica predice fenómenos que nos resultan sorprendentes, tales como que un objeto pueda estar en dos lugares a la vez.     No obstante lo inesperado de sus predicciones, la mecánica cuántica ha resultado ser extremadamente útil. Fue crucial, por ejemplo, para el desarrolló en 1947 del transistor y en 1960 del láser, que son piedras angulares en las que se apoya la red Internet. Hoy en día, a un siglo de su invención, la mecánica cuántica ha dado origen al campo de la información cuántica, que promete avances tecnológicos todavía más grandes, notablemente en el área de la computación. En este contexto se ha acuñado el término “segunda revolución cuántica”, y atendiendo a la misma, Fox y colaboradores llevaron a cabo una encuesta entre compañías relacionadas con la informática cuántica para averiguar sus necesidades de ingenieros e investigadores. En base a las respuestas obtenidas, los investigadores determinaron que las actividades de las compañías en la industria cuántica se agrupan en: detectores cuánticos, comunicaciones cuánticas, y computación cuántica, incluyendo hardware y software. Para estas áreas la industria cuántica norteamericana necesitará una fuerza de trabajo de ingenieros, científicos teóricos y de laboratorio, y técnicos altamente especializados.  Parte de esta fuerza de trabajo deberá contar con un entrenamiento formal en el campo de la información cuántica, mientras que el resto tendría que haber acreditado uno o dos semestres de cursos introductorios en este campo.Cabe preguntarse la importancia de todo lo anterior para México. Como un país subdesarrollado tecnológicamente, sin duda iremos a la saga de los países ricos en cuanto al desarrollo de la industria cuántica. Dado el proceso de globalización, sin embargo, es posible que en algún momento futuro se abran puestos de trabajo en México en esta industria y por tanto deberíamos adelantar acontecimientos y contemplar las adecuaciones conducentes en nuestros planes de estudio. Al mismo tiempo que impulsamos la investigación científica en el área de la información cuántica, que es un prerrequisito para la implantación de programas educativos sólidos que se mantengan permanentemente actualizados.Por lo demás, al no hacerlo estaríamos admitiendo que no tenemos más ambición que la de ser un país eternamente dependiente.",
    "Como sabemos, la pandemia de coronavirus en curso ha ocasionado una desaceleración económica a nivel global. Esto, a su vez, según la organización “Global Carbon Project”, ha llevado en el año 2020 a  una reducción de siete por ciento en la emisión de gases de invernadero a la atmósfera, en comparación con las emisiones de 2019. Esta reducción, es la más grande observada desde que se mantienen records. Al mismo tiempo, producto de esta pandemia se han incrementado actividades tales como el trabajo en casa, las teleconferencias y el consumo de videos de entretenimiento vía el Internet, que son a su vez generadoras de gases de invernadero. En efecto, todas estas actividades requieren de energía eléctrica, parte de la cual se genera en centrales termoeléctricas que consumen combustibles fósiles. Y aun la energía eléctrica generada a partir del sol o del viento, requiere de quemar combustibles fósiles para fabricar los necesarios paneles solares o molinos de viento, según sea el caso. La red Internet depende también de bancos gigantes de computadoras y memorias para almacenar datos cuya fabricación requiere de combustibles fósiles. De hecho, aunque resulta un poco sorprendente, los expertos estiman que la Internet contribuye con alrededor del 3.7 por ciento de la emisión global de gases de invernadero, porcentaje que es equivalente a la contribución correspondiente de la industria de la aviación. Un artículo aparecido esta semana en la revista “Resources, Conservation and Recycling” nos da una perspectiva sobre el impacto en la emisión de gases de invernadero sobre las diferentes actividades propias de la red Internet. Dicho artículo fue publicado por un grupo de investigadores encabezado por Renee Obringer, de la Universidad de Maryland, en los Estados Unidos, empleando datos de 13 países distribuidos a lo largo del planeta, incluyendo a México. Obringer y colaboradores consideran también el uso del agua y del suelo por la red Internet, el cual, argumentan, debe considerarse de manera conjunta con la generación de gases de invernadero para evaluar su impacto ambiental. Hacen notar, por ejemplo, que para el procesado y la transmisión de datos por Internet, Brasil genera 68 por ciento menos gases de invernadero que el promedio mundial, pero se encuentra un 210 por ciento por arriba de dicho promedio en cuanto al uso del agua. Esto es debido a que Brasil obtiene un 70 por ciento de su electricidad a partir de centrales hidroeléctricas. Así, dependiendo del origen de su energía eléctrica, otros países tendrán números muy diferentes. Por otro lado, más allá de estadísticas y de números, nos sería de mucha utilidad saber las posibilidades que tenemos, como usuarios de la red, de contribuir a mitigar su impacto ambiental. Y sobre esto Obringer y colaboradores nos ofrecen algunas guías.   Por ejemplo, si recibimos videos de ultra alta definición por cuatro horas al día, generaremos en un mes 53 kilogramos de dióxido de carbono –el más abundante de los gases de invernadero–. Si, por el contrario, disminuimos la calidad del video a estándar reduciremos dicha generación a 2.5 kilogramos, ahorrando el equivalente al dióxido de carbono producido durante un viaje de 150 kilómetros en automóvil. Obringer y colaboradores hacen notar que si esto se multiplica por 70 millones de usuarios, el ahorro será sustancial. En cuanto al uso del agua, si 100,000 usuarios redujeran la calidad de los videos que reciben se ahorraría suficiente líquido para crecer 185 toneladas de papa.Así, según Obringer y colaboradores, algunas acciones simples pueden ayudar a mitigar el impacto ambiental de la red Internet. Entre estas acciones mencionan: apagar el video en una reunión virtual, disminuir la calidad de los videos de entretenimiento, disminuir el tiempo de juegos, limitar el tiempo en redes sociales, borrar correos electrónicos y archivos almacenados en la nube –dado que para esto se requieren bancos de memorias– y darse de baja de grupos de correo electrónico.Por supuesto, todo lo anterior es más fácil decirlo que hacerlo realidad con los costos actuales de trasmisión de datos por Internet. Si estos costos fueran suficientemente altos, quizá lo pensaríamos dos veces antes de decidir si realmente queremos enviar tal o cual imagen o video. O si para la película que queremos ver es suficiente una resolución estándar. Por lo demás, en tanto los costos sigan a la baja, quizá debamos considerar que, según la NASA, el pasado 2020 es, juntamente con 2016, el año que ha experimentado en promedio la temperatura más alta desde que se mantienen records.",
    "Para un país con tantas carencias como el nuestro ¿vale la pena invertir en ciencia? Sobre todo porque en la medida en que las herramientas e instrumentos para la investigación científica se han hecho cada vez más sofisticados, su costo se ha incrementado de manera concurrente. Para quienes nos dedicamos a este tipo de actividad la respuesta es obvia: la inversión que podamos hacer para desarrollar la ciencia en México será altamente redituable en el futuro. Con relación a esto último podemos argumentar lo que sigue.El objetivo de la investigación científica es determinar las causas de los fenómenos naturales. Esto tiene consecuencias prácticas, pues el entender qué es lo que origina un fenómeno nos permite ejercer un cierto grado de control sobre el mismo. Por ejemplo, sabemos que en las noches estrelladas la temperatura ambiente tiende a bajar en comparación con las noches nubladas y con esto una planta puede helarse. Este fenómeno lo explica la física tomando en cuenta que la planta emite hacia el cielo de manera continua energía por radiación, lo que tiende a disminuir su temperatura hasta posiblemente helarse. En una noche nublada, no obstante, dicha radiación es  reflejada por las nubes y esto permite que pueda ser reabsorbida por la planta limitando su disminución de temperatura; muy al contrario de lo que sucede en una noche sin nubes, cuando la radiación se pierde hacia el espacio. Dotados de este conocimiento, podríamos diseñar un procedimiento –o tecnología– para prevenir la helada colocando una tela sobre la planta que simule una nube y obstruya la vista del cielo.  Así, el conocimiento científico nos permite desarrollar tecnologías –algunas de ellas muy sofisticadas– para resolver problemas de todo tipo, y un ejemplo dramático al respecto nos lo da la actual pandemia de coronavirus. Como sabemos, el pasado mes de diciembre dio inicio en varios países una campaña de vacunación en contra del COVID-19, empleando vacunas desarrolladas en un tiempo récord mediante ingentes cantidades de dinero y sofisticados conocimientos científicos.Para acelerar el desarrollo de las vacunas, el gobierno de los Estados Unidos puso en marcha el pasado mes de mayo el programa “Warp Speed” al que destinó más de 10,000 millones de dólares. Mediante este programa, dicho gobierno otorgó fondos por miles de millones de dólares a cinco compañías farmacéuticas para gastos de investigación, desarrollo y producción de las vacunas. El apoyo se dio, no obstante, con una condición: que los Estados Unidos tuvieran acceso prioritario a las vacunas. Como consecuencia de esto, a la fecha los Estados Unidos disponen de 200 veces más vacunas de las que dispone México, y la situación no parecería que será muy diferente en el futuro. En estas condiciones, hay el peligro de que la pandemia de coronavirus desaparezca en forma natural antes de que se logre inmunizar al total de la población del país. Tendríamos así que haber invertido recursos considerablemente mayores durante el desarrollo de las vacunas con el fin de asegurar números más altos. O bien, que hubiéramos contado con la infraestructura científica y tecnológica para desarrollar nuestras propias vacunas. Esto habría requerido de inversiones a largo plazo que desafortunadamente no estamos acostumbrados a hacer. La pandemia de coronavirus nos muestra las ventajas de invertir en ciencia, con todo y lo cuantioso de los recursos que demanda, habida cuenta que la tecnología actual descansa fuertemente en el conocimiento científico. Un país sin ciencia es un país sin tecnología, y por lo mismo es un país obligado a importarla, con todas las desventajas que esto pudiera representar en un momento dado.Y lo que podemos mencionar para México con respecto al desarrollo de vacunas contra el coronavirus, es igualmente válido para otros campos científicos y tecnológicos de importancia central para el país. Después de todo, México es la quinceava economía del mundo.",
    "Al igual que la revista “Science” en retrospectiva hizo un análisis de los avances científicos del año que acaba de terminar, en su último número nos da también sus predicciones sobre lo que considera serán los principales avances científicos del año que acaba de comenzar. Y entre estos avances, de manera poco sorprendente, la revista incluye dos aspectos relativos a la pandemia de coronavirus. Por un lado, considera “Science” que el equipo de diez científicos auspiciado por la Organización Mundial de la Salud que visitará China varias veces a lo largo del presente año, logrará desentrañar el misterio relativo al origen de la pandemia de coronavirus, lo que sin duda será de gran importancia para prevenir otras pandemias en el futuro.Por otro lado, anticipa “Science” que otro descubrimiento científico de primera importancia en 2021 será el desarrollo de medicamentos específicos para el tratamiento del Covid-19, los cuales serán esenciales dado que en el futuro el virus será endémico. En la actualidad, para atacar a esta enfermedad se emplean antivirales que han sido desarrollados para el tratamiento de otras enfermedades con resultados limitados. Por lo demás, en tanto aparecen los nuevos medicamentos, y dado que vacunar a los miles de millones de habitantes del planeta Tierra no es algo que ocurrirá el día de mañana, habría que seguir con todas las precauciones recomendadas por los especialistas para amortiguar la propagación del virus. Relativo a esto, se sabe que una persona puede infectarse por la inhalación de partículas contaminadas flotando en el aire, o bien al recoger virus con las manos de una superficie contaminada y tocarse posteriormente los ojos, la nariz o la boca.Con relación a esta última vía de infección, la misma puede suprimirse lavando de manera frecuente las superficies contaminadas con agua y jabón. Es posible también descontaminar superficies de virus por medio de luz ultravioleta. Como sabemos, esta luz -invisible para nosotros- es una forma de radiación electromagnética que está más allá del color violeta del arco iris, al igual que la luz infrarroja, en el otro extremo del arco iris, está más allá del color rojo. La luz ultravioleta es más energética que la luz visible y por irradiación puede dañar diferentes componentes del coronavirus e inactivarlo.   La desinfección por medio de luz ultravioleta no es algo nuevo y ha sido practicada ya por varias décadas. Hoy en día, sin embargo, existen nuevas fuentes de luz ultravioleta que, de acuerdo a un artículo aparecido esta semana en la revista “Photochemistry and Photobiology”, resultarían particularmente adecuadas para desinfectar superficies contaminadas por coronavirus. El artículo fue publicado por un grupo de investigadores encabezado por Yoram Gerchman de la Universidad de Haifa en Israel, y en el mismo reportan los resultados de un estudio llevado a cabo para evaluar el uso de fuentes LED de luz ultravioleta para inactivar el coronavirus del Covid-19. Habría que mencionar que las primeras fuentes de luz LED aparecieron en la década de los años 60 del siglo pasado, primeramente, como fuentes de luz infrarroja y después de luz roja y verde. Posteriormente, en los años noventa de ese siglo, se desarrollaron los LEDs azules y con estos los LEDs blancos que iniciaron una revolución en el campo de la iluminación.Una extensión de la tecnología de los LEDs azules ha llevado al desarrollo de los LEDs ultravioleta que ahora Gerchman y colaboradores afirman pueden ser usados para desinfectar superficies de manera rápida y eficiente; por ejemplo, para desinfectar el aire que es inyectado en los sistemas de aire acondicionado y de esa manera limpiar el aire en espacios cerrados. De manera rápida, además, pues en su estudio, los investigadores encuentran que el 99.9 por ciento de los virus pueden ser destruidos en medio minuto por irradiación con LEDs ultravioleta. En cuando a su costo, los LEDs, compiten favorablemente con las fuentes de luz ultravioleta convencionales. Gerchman y colaboradores, por otro lado, hacen notar que la luz ultravioleta es peligrosa y que su manejo debe ser cuidadoso. No sería adecuada, por ejemplo, para desinfectar la mesa del comedor o la perilla de la puerta por una persona no experta en su manejo  –como no lo sería tampoco para desinfectar el interior del cuerpo, como el presidente de los Estados Unidos lo sugirió en una ocasión.  Si bien con la aparición de las vacunas podemos ver una luz al final del túnel, habría que reconocer que la solución a nuestros problemas no es inminente. Y en ese sentido, Gerchman y colaboradores nos ofrecen un recurso tecnológico para ayudarnos a paliar la crisis. Recurso que, después de que termine la pesadilla, nos ayudará a prevenir la siguiente.",
    "Al igual que en años pasados, el último número de la revista “Science” incluye un listado de lo que considera son los descubrimientos científicos más relevantes del año que está por terminar. En dicha lista incluye como finalistas, entre otros, a la cura de dos enfermedades de origen genético por medio de tijeras moleculares para editar genes, al desarrollo de modelos más precisos para predecir el posible curso futuro del calentamiento global, y al descubrimiento de materiales superconductores a temperatura ambiente. El ganador absoluto, sin embargo, y en eso estaríamos con seguridad todos de acuerdo, es el desarrollo en tiempo récord de vacunas para combatir la pandemia de coronavirus que asuela al planeta.Recordemos que fue apenas el 31 diciembre pasado que se dio a conocer un misterioso brote de casos de neumonía en la provincia de Wuhan, China, lo que propició un rápido desarrollo de acontecimientos. El 8 de enero se identificó que el causante del brote era un coronavirus y dos días después los científicos chinos hicieron pública su secuencia genómica. El 20 de enero se confirmó que el coronavirus se puede transmitir de humano a humano, y el 23 del mismo mes se inició al cierre de Wuhan. Con gran éxito, como se comprobaría posteriormente.Desafortunadamente, esto último no fue el caso en otros lugares del mundo. Así, el 23 de febrero se declara el brote de coronavirus en Italia, y posteriormente en otros países europeos, mientras que el 26 de marzo la ciudad de Nueva York se convierte en el centro de la pandemia en los Estados Unidos. Entre tanto, el 16 de marzo, las vacunas de Moderna y CanSino entran en una fase de pruebas y, con velocidad sin precedentes, el 9 de noviembre, Pfizer y BioNTech anuncian que su vacuna tiene una eficacia del 90% y el 16 de noviembre Moderna hace lo propio al dar a conocer una eficacia del 95%. Y con todo esto, en un tiempo record, los programas de vacunación están ya en marcha en un buen número de países, incluyendo el nuestro. Sin embargo, no todo será miel sobre hojuelas por los problemas logísticos que implica vacunar a miles de millones de personas a lo largo del mundo. Los Estados Unidos, por ejemplo, tiene planeado vacunar a 20 millones de estadounidenses antes de terminar el año, pero hasta la mañana de ayer sábado apenas había logrado aplicar unos dos millones de vacunas, según el portal de noticias de CNN. Restan entonces seis días para vacunar a 18 millones de personas, a razón de dos millones de personas por día. En estas circunstancias, la cadena CNBC cita al director de los Institutos Nacionales de Salud quien declaró que si el gobierno de los Estados Unidos no alcanza la meta trazada al final del año, tiene la esperanza que los norteamericanos “entenderán que se trata de un desafío logístico de enormes proporciones”.El programa de vacunaciones masivo –incluyendo el de México- empezó con la vacuna producida por Pfizer BioNTech, la cual presenta problemas de distribución particularmente serios, dado que debe mantenerse a una temperatura de menos 70 grados centígrados para evitar su degradación. Para el transporte de la vacuna, Pfizer diseñó una caja que puede mantener esta temperatura hasta por 30 días empleando hielo seco como refrigerante. Con el objeto de asegurar que durante el transporte no ocurra un accidente que eleve la temperatura por arriba de lo indicado, la caja está equipada con sensores de temperatura cuya lectura es enviada a un centro de control. Una vez entregada la caja con las vacunas, sin embargo, Pfizer no se hace responsable de su manejo y desconecta los sensores de temperatura, especificando que una vez descongelada, la vacuna puede almacenarse a la temperatura de un refrigerador común solo por un máximo de 5 días. En estas condiciones, el gobierno norteamericano ha tomado cartas en el asunto y ha contratado a una compañía para que se haga cargo del monitoreo de la temperatura de las vacunas en tanto se usan. Ciertamente, la vacuna de Moderna -la otra disponible por el momento- requiere solamente de una temperatura de menos 20 grados centígrados, que está al alcance de un congelador común. En un inicio, sin embargo, según los expertos, será necesario utilizar todas las vacunas que sean aprobadas dadas las limitaciones en su producción.Con la aparición de las vacunas para el Covid-19 tal parece que estamos viendo una luz al final del túnel. Habría que tomar en cuenta, sin embargo, que dicha luz es apenas un punto pequeñito y que posiblemente la solución a nuestros problemas no está tan a la vuelta de la equina como hubiéramos querido. Con todo y que se ha actuado a una velocidad sorprendente y sin precedentes.",
    "Una ojeada a la Tierra desde el espacio nos permite corroborar lo que bien sabemos: que durante las fiestas de fin de año hacemos gala de un despliegue masivo de luces multicolores que elevan la luminosidad nocturna de la Tierra por arriba de lo normal. En efecto, en el documento de la NASA que lleva por título “La Tierra durante la noche” -que es posible encontrar en el sitio de internet de la agencia espacial norteamericana-, se incluyen imágenes nocturnas de la Tierra en las que se compara la luminosidad nocturna de nuestro planeta durante los meses de diciembre de 2012 y 2013, con la luminosidad promedio de los años 2012-2014. La NASA concluye que durante la temporada navideña dicha luminosidad se incrementa entre un 20% y un 50%; en los centros urbanos dicho incremento es de 20-30%, mientras que en los suburbios es de 30-50%.Al margen de explicaciones para nuestro entusiasmo por las luces eléctricas navideñas, habría que decir que éstas se originaron hacia el final del siglo XIX durante los primeros años de la industria eléctrica. Edison, quien como sabemos jugó un papel central, como inventor y empresario, en el desarrollo de dicha industria, decoró en la víspera del año nuevo de 1879 el edificio de su laboratorio con luces eléctricas y con esto inauguró la costumbre. Posteriormente, en 1882, Edward Johnson, vicepresidente de la compañía de Edison, decoró un árbol navideño con luces de tres colores y lo puso a rotar. Hecho esto, como relata un artículo publicado en diciembre de 2016 por el “Smithsonian Magazine”, Johnson invitó a ver su creación a un reportero del periódico “Post and Tribune” de Detroit, quien escribió: “En la parte trasera de los hermosos salones, había un gran árbol de Navidad que presentaba un aspecto muy pintoresco y extraño. Estaba brillantemente iluminado con ochenta luces en total encerradas en estos delicados huevos de vidrio, y divididas casi por igual entre blanco, rojo y azul. Difícilmente se puede imaginar algo más bonito”. Por su invento, a Johnson se le reconoce como el padre del árbol navideño iluminado con luz eléctrica. Una década después de su invención, en 1895, el árbol navideño con luces eléctricas llegó a la Casa Blanca. Ese año, el presidente Grover Cleveland dispuso que el árbol de la residencia fuera decorado con cientos de luces multicolores, lo que constituyó un impulso importante para el crecimiento de la iluminación eléctrica navideña. Así, la compañía General Electric comercializó en 1903 las primeras lámparas miniatura para la decoración de árboles de Navidad. De la misma manera, en 1919 la General Electric introdujo las lámparas decorativas con forma de flama, mientras que en 1946 la compañía NOMA comercializó las lámparas de burbujas que generan un flujo ascendente de burbujas de gas cuando se encienden. En la actualidad las lámparas incandescentes navideñas están siendo sustituidas por la lámparas LED, que tienen muchas ventajas, incluyendo la de ser considerablemente más eficientes, durables y resistentes al impacto, lo que sin duda contribuye al crecimiento de la iluminación navideña.Ha pasado ya casi un siglo y medio desde que a Edward Johnson se le ocurrió decorar el árbol navideño con las recién inventadas lámparas eléctricas. No inventó, por supuesto, el  árbol de Navidad, que es anterior a la luz eléctrica. Antes de Johnson, sin embargo, el árbol navideño era iluminado por medio de velas, las cuales, además del peligro de provocar un incendio, no tenían la espectacularidad de la luz eléctrica. En sus primeros tiempos, cuando se iniciaba la luz eléctrica y la población no le tenía plena confianza, la iluminación navideña avanzó lentamente. En cuanto la población se habituó a la nueva tecnología, sin embargo, las luces navideñas avanzaron rápidamente, cubriendo árboles, postes, bardas, techos, y todo tipo de superficies en las que se pueda fijar una guía de luz. Al grado tal que hoy en día la iluminación navideña puede ser fácilmente detectada desde el espacio.",
    "Una fotografía es una ventana que nos permite echar una mirada hacia el pasado, inmediato o relativamente alejado. ¿Qué tan alejado? No más allá de 1826, año en que Joseph Nicéphore Niépce tomó en Francia, desde la ventana de su despacho, lo que se reconoce es la primera fotografía de la historia en la que se captura -y preserva para el futuro- un paisaje natural. Al margen de su relevancia como la primera fotografía de la que tenemos noticia, ésta es de mala calidad y apenas se aprecian detalles del paisaje capturado. Más interesante, sin duda, es la fotografía Louis Daguerre de 1838 en la que se muestra con gran detalle el Boulevard du Temple de París, con la imagen de un hombre de pie y con una pierna levantada, aparentemente lustrándose las botas. Y todavía más interesante resulta lo que es la primera fotografía “selfie” de la historia, que data de 1839. En dicha fotografía podemos ver la imagen del químico aficionado Robert Cornellius viendo hacia la cámara. La fotografía es de una calidad notable, dada la época temprana en la que fue tomada y los pocos recursos de los que dispuso el fotógrafo. Nos transporta un par de siglos hacia el pasado y nos muestra a una persona tal como era en vida.Desafortunadamente no podemos ir mucho más hacia el pasado y recuperar imágenes de personas fallecidas en tiempos más antiguos. Tenemos, por supuesto, pinturas y esculturas de personas que vivieron hace cientos e incluso miles de años, pero en las cuales el artista imprimió sus puntos de vista, lo que podría hacer que reflejaran una realidad distorsionada. Por otro lado, si bien no existen fotografías de personas con más de dos siglos de antigüedad, las técnicas analíticas desarrolladas por la ciencia moderna nos permiten en algunos casos obtener información de personas fallecidas hace bastante más tiempo. Quizá no a un grado tal de obtener imágenes fidedignas de cómo lucían en vida, pero sí de otras circunstancias que marcaron su paso por este mundo. Es el caso de personas que después de muertas fueron momificadas, como fue una práctica común en el antiguo Egipto durante más de dos milenios. Durante el proceso de momificación se removía la humedad de los tejidos orgánicos para evitar la putrefacción y se envolvía el cuerpo con vendas impregnadas con resinas. Con esto nos han llegado a través de los siglos restos humanos que de otro modo se hubieran desintegrado hace ya mucho tiempo. Así, si bien dichos restos solamente nos dan una imagen limitada de la apariencia que la persona tuvo en vida, al mismo tiempo nos permiten investigar las circunstancias por las que atravesó a lo largo de su existencia. Las momias egipcias han sido estudiadas por medio de técnicas de rayos X, empleando métodos no invasivos. Un ejemplo notable fue publicado esta semana en la revista “Interface”, por un grupo de investigadores encabezado por Stuart Stock de la Universidad del Noroeste en el estado norteamericano de Illinois. El estudio de Stock y colaboradores se llevó a cabo con una momia de 1,900 años de antigüedad, descubierta en Hawara, Egipto, en 1911, y es notable por el uso de una instalación de investigación de gran magnitud: la Fuente Avanzada de Fotones, del Laboratorio Nacional de Argonne del Departamento de Energía de los Estados Unidos. Empleando un potente haz de rayos X generado por la fuente de fotones, los investigadores escudriñaron el interior de la momia a través de su recubrimiento de vendas y resinas, auxiliados por un estudio previo de tomografía computarizada, por medio del cual obtuvieron una imagen del interior de la momia. Concluyeron que muy probablemente se trataba de una niña que mostraba signos traumáticos y por tanto había tenido una muerte natural. Determinaron también que fue enterrada con un amuleto de escarabajo de calcita, el cual tenía el propósito de protegerla durante su tránsito a la vida después de la muerte. Y toda esta información se obtuvo sin destruir a la momia. Es muy difícil que pudiéramos recuperar algún día la imagen que en vida tuvo la niña de Hawara. Para empezar, la fotografía no se inventó sino hasta unos 1,800 años después de que murió. Además, la luz que en su momento reflejó la niña y con la que pudiera formarse una imagen, en el mejor de los casos abandonó nuestro planeta hace ya cerca de dos milenios y es impensable que con las técnicas de que disponemos podamos recuperarla. Tendremos así que limitarnos a la información que podemos obtener con técnicas como las empleadas por Stock y colaboradores.  Al menos por el momento.",
    "En diciembre de 2016, arqueólogos alemanes descubrieron en Schöningen, norte de Alemania, un objeto por demás interesante: una vara de madera de unos 64 centímetros de largo, ligeramente curvada y terminada en punta en ambos extremos, y con una sección transversal que es aproximadamente circular en su parte media y que se aplana hacia los extremos. Los arqueólogos concluyeron que dicho objeto fue un arma de caza empleada para matar o aturdir a animales y que habría podido ser arrojada hasta una distancia de unos cien metros. De acuerdo a sus descubridores, el objeto encontrado en Schöningen es el arma de caza más antigua de la que se tenga noticia, fabricada y usada por la especie antecesora del neandertal hace unos 300,000 años.Por cientos de miles de años, en una etapa de cazadores-recolectores, nuestro género recurrió a la caza de animales para hacerse de comida. Con el transcurrir del tiempo, sin embargo, se desarrolló la ganadería -hace unos diez mil años- y no hubo ya necesidad de cazar animales salvajes. En su lugar, se domesticaron algunas especies y se les mantuvo en cautiverio, sacrificando animales según fuera necesario. Así las cosas, en la actualidad en el mundo se sacrifican diariamente 130 millones de pollos y 4 millones de puercos. Estos números, además, están aumentado rápidamente, pues mientras que en los últimos cincuenta años la población del mundo se ha doblado, el consumo de carne se ha triplicado.Por lo demás, podría ser que en un futuro cambiemos nuevamente la forma de obtener carne para alimentarnos, la cual no provendría del sacrificio de especies vivientes sino de procesos de cultivo, en forma análoga a como cultivamos plantas en un campo agrícola. Como una indicación de que esto no es ya más un tópico de la ciencia ficción sino algo factible, la semana que hoy termina nos enteramos por los medios de comunicación que la empresa estadounidense “Eat Just” logró que la agencia de alimentos de Singapur aprobara para su venta carne cultivada de pollo, obtenida sin el sacrificio de animales vivos. Esta es la primera ocasión que una agencia gubernamental aprueba el consumo de carne cultivada.Para fabricar su producto, bautizado como “chicken bits”, “Eat Just” combina carne cultivada en un biorreactor de 1,200 litros con componentes de origen vegetal. Las células necesarias para iniciar el proceso de cultivo son obtenidas de animales vivos, sin sacrificar al donador. Los nutrientes necesarios para el crecimiento de las células son de origen vegetal.  Todo esto es sin duda impresionante, pero cabe preguntarse ¿saben diferente los “chicken bits” cultivados que aquellos obtenidos por medios más tradicionales? Según el periódico británico “The Guardian”, la respuesta de “Eat Just” es: “Por supuesto que saben diferente. Pero esperamos que a través de una comunicación transparente con los consumidores sobre lo que es el producto y cómo se compara con la carne convencional, seremos capaces de ganar. Pero no hay seguridad”.De la compañía que comercializa los “chicken bits” esperaríamos, por supuesto, una opinión positiva acerca de su producto. Para tener más elementos de juicio, no obstante, habría que considerar más puntos de vista. Uno de estos lo encontramos en un artículo publicado el pasado jueves en “The Guardian”, firmado por Jenny Kleeman, quien habría probado los “chicken bits” hace un par de años. Si bien el testimonio de Kleeman no es reciente y el producto podría haber mejorado en dos años, vale la pena considerarlo.Según Kleeman, el pollo de “Eat Just”: “Sí, sabía a pollo: tenía el inconfundible aroma del pollo en mi lengua y en mi nariz. Tenía algo de la jugosidad de la carne animal que esperas cuando comes pollo: esa sensación pegajosa en los dientes cuando muerdes un trozo de carne. Pero tenía la consistencia de la carne procesada de calidad menor imaginable.  Esto no era un trozo de pollo, un corte de carne, sino una masa de células de pollo, abultada y presionada en forma de nugget. Me habían dicho que éste era el futuro de la comida. Pero me costaba trabajo tragarla”.Habríamos de admitir que con el testimonio de Kleeman nuestro calificativo de “impresionante” al producto de “Eat Just” resulta quizá un poco exagerado, a menos que en los dos últimos años los “chicken bits” hayan mejorado sustancialmente. Por lo pronto, sin embargo, no tenemos mayor evidencia al respecto, y resulta complicado predecir si entraremos en el corto plazo en una tercera etapa como consumidores de carne, diferente de la que ha prevalecido en los últimos 10,000 años.",
    "¿Cuáles son las medidas más apropiadas para combatir la propagación de la epidemia de coronavirus? A lo largo de los últimos meses hemos recibido abundante información al respecto; contradictoria en algunos casos, sin embargo. Al inicio de la pandemia, por ejemplo, algunos no aconsejaban el uso del cubrebocas por considerar que podría dar una sensación falsa de seguridad y hacer que quien la usara bajaría la guardia. El cubrebocas resultaría así contraproducente. Con el tiempo, no obstante, se ha extendido su uso, e incluso se ha hecho obligatorio en algunas circunstancias. En contraste, mantener una distancia mínima entre personas es una medida que es ampliamente aceptada, si bien no es claro cuál debe ser esta distancia. Esto último habida cuenta que el contagio puede ocurrir por medio de gotas de saliva, que caen al piso después de recorrer distancias relativamente pequeñas, o bien por aerosoles que se mantienen en el aire por más tiempo y recorren distancias mayores.Por lo demás, las medidas óptimas para mitigar la propagación del coronavirus dependen de la situación particular de que se trate, como concluye un grupo de investigadores de Canadá y el Reino Unido, en un artículo aparecido el pasado 19 de noviembre en la revista “Proceedings of the National Academy of Sciences” de los Estados Unidos. El grupo de investigación es encabezado por Paul Tupper de la Universidad Simon Fraser en Vancouver, Canadá.Tupper y colaboradores se propusieron determinar las medidas más apropiadas para minimizar el número de nuevos infectados y hacer recomendaciones para la organización de eventos. Como primer paso desarrollaron un modelo matemático simple para calcular el número de personas infectadas por un enfermo de COVID 19 en función de cuatro parámetros: intensidad de la trasmisión, duración de la exposición, proximidad de personas, y, de ser el caso, el grado en el que se mezclan los diferentes subgrupos que componen una determinada reunión de personas. Los investigadores obtuvieron valores de estos parámetros a partir de reportes de diferentes eventos que incluyen fiestas, restaurantes, clubs nocturnos y reuniones en general. Incluyeron, por ejemplo, el evento ocurrido en el estado de Washington en el que 52 personas de un coro de 60 miembros resultaron infectadas durante un ensayo. Incluyeron también el caso, ocurrido en China, en el que 5 pasajeros de un autobús, de un total de 39, resultaron infectados durante un viaje de dos horas por una persona enferma que no usó cubrebocas. Al mismo tiempo que, durante el viaje subsequente de 50 minutos de esa misma persona, pero ahora con cubrebocas, ninguno de los 14 pasajeros resultaron infectados.Por medio de su modelo, afirman los investigadores, es posible determinar cuáles son las medidas más efectivas para minimizar los contagios en un evento dado. Por ejemplo, reducir la velocidad de transmisión usando cubrebocas, incrementar el distanciamiento físico disminuyendo el número de participantes, o crear burbujas que incluyan un número pequeño de participantes sin que haya mezclas entre las mismas.Tupper y colaboradores consideran dos casos extremos: una reunión de larga duración y muchos participantes, en la que la probabilidad de resultar infectado es grande, y un segundo caso en el que esta probabilidad es pequeña. Los investigadores encuentran –de manera poco sorprendente- que en todos los casos el distanciamiento físico es una medida efectiva para prevenir la propagación del virus. No sucede lo mismo, sin embargo, con el cubrebocas, que es efectivo cuando la probabilidad de infección es baja, pero lo es en menor grado cuando se ha alcanzado un estado de saturación de infecciones.  Con la vacuna contra el coronavirus todavía a meses de distancia y con dificultades logísticas para hacerla extensiva a toda la población, tendremos en los meses por venir que considerar todas las opciones para limitar su propagación, basados en estudios como el que nos presentan Tupper y colaboradores. A pesar de que, como todo estudio científico, tiene limitaciones y hay margen para que resulte impreciso. Si bien nunca tanto como las versiones que señalaban al principio de la pandemia que los cubrebocas, lejos de ser efectivos, eran contraproducentes.",
    "De acuerdo con los científicos del clima, la elevación de la temperatura de los océanos por el calentamiento global está incrementando la intensidad de los ciclones tropicales. En el norte del Océano Atlántico, los ciclones -que en esa región reciben el nombre de huracanes- se originan en las aguas cálidas del océano frente a la costa de África. Es ahí donde existen condiciones adecuadas de humedad y temperatura necesarias para el nacimiento y desarrollo de estos fenómenos naturales.Como nos lo explican los expertos, los huracanes prosperan con la humedad que recoge el aire atmosférico en contacto con el agua del océano. De hecho, dicha humedad es el combustible que genera y mantiene a un huracán. En efecto, al absorber humedad, el aire se hace más ligero y se eleva por sobre la superficie del océano, hasta alturas en donde encuentra temperaturas más bajas que provocan que la humedad se condense formando nubes. Con esta condensación se libera la energía que mantiene al huracán en movimiento.La cantidad máxima de humedad que puede contener el aire, por otro lado, depende de su temperatura; y al mismo tiempo, entre más alta es la temperatura del agua, más fácilmente puede transferir humedad al aire. Por todo lo anterior, los huracanes se desarrollan en la medida en que se mueven sobre las aguas cálidas del océano. Al llegar a la costa, en contraste, pierden la fuente de energía que los mantiene en movimiento y eventualmente se extinguen. Esto es afortunado pues  solamente una franja a lo largo de la costa de arribo es la que sufre los embates de los huracanes.Esta franja, sin embargo, se está ensanchando por efecto del cambio climático. Esto, al menos, de acuerdo con un artículo publicado el pasado 11 de noviembre en la revista “Nature” por Lin Li y Pinaki Chakraborty del Instituto de Ciencia y Tecnología de Okinawa, Japón. De acuerdo con Li y Chakraborty, en la medida en que se incrementa la temperatura de los océanos por el cambio climático, hay una mayor incorporación de humedad a la atmósfera, y por tanto tienden a aparecer huracanes con mayores cargas de humedad y mayores intensidades. Esto incrementa la distancia de penetración de los huracanes tierra adentro una vez que tocan la costa, dado que, con una mayor carga de humedad, los huracanes tienen más reservas para sobrevivir activos por tiempos más largos, aun sobre terreno firme.   Li y Chakraborty llegaron a esta conclusión mediante un estudio de los huracanes que tocaron tierra en los últimos 50 años. Encontraron que el tiempo de decaimiento de un huracán una vez que toca tierra, ha aumentado en consonancia con el incremento en la temperatura del océano. Así, mientras que al final de la década de los años 60 un huracán típico perdía el 75 por ciento de su intensidad después del primer día de haber tocado tierra, en la actualidad esta pérdida es de sólo el 50 por ciento. Además, por medio de simulaciones de computadora, Li y Chakraborty demostraron que el incremento en la duración de los huracanes en tierra está, efectivamente, vinculado con una mayor carga de humedad.Los resultados de Li y Chakraborty indican que aun si la intensidad de los huracanes permaneciera igual, su poder destructivo se está incrementando en virtud de su mayor penetración en tierra firme. Con relación a esto, los investigadores hacen notar que las poblaciones tierra adentro están menos preparadas para resistir los embates de los huracanes que las poblaciones en la costa, y por lo tanto son más vulnerables.Dado que el clima del planeta es algo extremadamente complicado, los resultados de Li y Chakraborty hubieran sido difíciles de predecir. Con seguridad, en un futuro cercano, en la medida en que se estudien con detalle los efectos del cambio climático, se encontrarán resultados que no llamarán a la sorpresa, pero que no serán evidentes a primera vista. Después de todo, la temperatura del planeta no había estado tan alta en los últimos seis mil años.",
    "Después de que llegó a su fin el proyecto Apollo que llevó a los estadounidenses a la superficie de la Luna, la carrera espacial entró en un periodo de relativa calma. La actividad espacial no cesó por completo, por supuesto, y en el último medio siglo fuimos testigos de numerosos acontecimientos espaciales, incluyendo el nacimiento y muerte de los transbordadores espaciales, y el envío de sondas para explorar los planetas del sistema solar con resultados espectaculares. En el presente siglo, en contraste con el periodo de calma relativa, somos testigos de un renovado interés en el espacio, por parte no solamente de los protagonistas originales, los Estados Unidos y la ahora Rusia, sino también de la Unión Europea, Japón, China, la India, e incluso Israel.El interés en el espacio, además, se ha extendido al capital privado, que ve la posibilidad de hacer negocios más allá de nuestras fronteras planetarias. Esto, con la complacencia de la NASA, que propició el nacimiento de empresas privadas espaciales al suspender el programa de transbordadores espaciales, lo cual eliminó su capacidad para colocar astronautas en órbita. La actividad espacial está así en plena ebullición, con planes para establecer bases lunares y viajar hasta el planeta Marte, con el fin de extender nuestra presencia más allá de los confines del planeta. Para esta expansión, sin embargo, las misiones espaciales necesitarían de recursos materiales, incluyendo metales y agua, que no podrían llevar consigo desde Tierra por lo oneroso que resultaría -llevar al espacio una botella de agua, por ejemplo, podría costar decenas de miles de dólares-. El alto costo para enviar materiales al espacio está asociado con la necesidad de vencer la gravedad de nuestro planeta, lo que requiere de una gran cantidad de energía. Las misiones espaciales tendrían de este modo que hacerse de recursos en el espacio.Los asteroides, localizados entre las órbitas de Marte y Júpiter, serían candidatos para obtener materiales en el espacio, dada su pequeña masa y gravedad también pequeña. La minería de asteroides, sin embargo, requeriría de maquinaría pesada que habría que llevar desde la Tierra. En estas circunstancias, la NASA ha planteado como alternativa el uso de las técnicas de biominería. La biominería emplea bacterias para extraer materiales valiosos, por ejemplo, cobre u oro, de los depósitos minerales. Dado que no implica el uso de maquinaria pesada, la biominería sería entonces, en principio, adecuada para la extracción de minerales en el espacio. En el caso, por supuesto, de que las bacterias no se vean afectadas en su funcionamiento por las condiciones de microgravedad que ahí imperan. Para evaluar este funcionamiento, un grupo internacional de investigadores encabezado por Charles Cokell de la Universidad de Edinburgo, Escocia, llevó a cabo un estudio de biominería en condiciones de microgravedad en la Estación Espacial Internacional. Los resultados de dicho estudio aparecieron publicados esta semana en la revista Nature Communications. De manera específica, Cokell y colaboradores estudiaron la extracción de materiales conocidos como tierras raras, de rocas basálticas empleando microorganismos. Encontraron que dichos microorganismos no fueron afectados en su desempeño por las condiciones de microgravedad imperantes en la Estación Espacial Internacional, concluyendo que la biominería es posible en el espacio.En realidad, los resultados de Cokell y colaboradores van más allá de sus implicaciones meramente para la exploración espacial y demuestran que la biominería de tierras raras es técnicamente posible en el espacio. Y dicho esto, habría que señalar que las tierras raras son materiales de una gran importancia tecnológica, que podemos encontrar en numerosos dispositivos, incluyendo memorias de computadora, lámparas LED, y pantallas de computadora y teléfonos celulares, entre muchos otros. Y que, además, se plantea que la demanda de las tierras raras pronto superará a la oferta. Así, en este contexto, la minería de asteroides se presenta como una opción para obtener tierras raras, que, según los expertos, sería económicamente viable  en un futuro cercano.¿Veremos en los años por venir una expansión de nuestra especie a otros confines del sistema solar como augura la ebullición presente de la actividad espacial? Y de ocurrir esta expansión ¿de quién serían propiedad los recursos que se obtengan de la probable minería, por microorganismos o por otros medios, de asteroides? No es quizá descabellado concluir que serán propiedad del primero que los alcance. Por más que, en principio, se supone, son propiedad de toda la humanidad.",
    "Una recomendación que muy comúnmente reciben los turistas extranjeros que visitan nuestro país es que tengan cuidado con el agua que beben, so pena de verse obligados a emplear una parte sustancial de su tiempo conociendo baños de hoteles y restaurantes, en lugar de disfrutar de las muchas atracciones turísticas que les ofrece México. Por otro lado, el problema con el agua contaminada no es, por supuesto, exclusivo de nuestro país. De hecho, según la Organización Mundial de la Salud, 485,000 personas mueren anualmente a lo largo del mundo a consecuencia de diarreas ocasionadas por beber agua contaminada.Dada esta cifra, sorprende enterarnos que hace 2,000 años, en la ciudad que hoy conocemos como Tikal, en el norte de Guatemala, existieron instalaciones para purificar agua que empleaban técnicas que son hoy usadas para este propósito. Esto, al menos según un artículo aparecido el pasado 22 de octubre en la revista Scientific Reports, publicado por un grupo de investigadores encabezado por Kenneth Barnett Tankersley de la Universidad de Cinncinati.Como explican Tankersley y colaboradores, en contraste con Tenochtitlan, que contaba con abundantes fuentes naturales de agua potable, Tikal estaba situada en un territorio en el que los periodos de sequía eran cosa corriente y por tanto se tenía que almacenar el agua para consumo humano. Dado que el agua almacenada es susceptible de contaminación bacteriana y por materiales tóxicos, esto implicaría que los pobladores de la ciudad habrían desarrollado medios para purificarla.Esta es la conclusión a la que llegan  Tankersley y colaboradores basados en un estudio de campo llevado a cabo en Tikal, durante el cual descubrieron lo que fue un depósito de agua, con una capacidad de unos 58 millones de litros, que mostraba menos niveles de contaminación que otros depósitos a su alrededor. En el fondo del depósito, además, encontraron sedimentos con partículas de cuarzo del tamaño de un grano de arena y materiales conocidos como zeolitas, que son hoy en día empleados en los sistemas de purificación de agua. Las zeolitas son materiales altamente porosos, que contienen una intrincada red de poros microscópicos que penetran a su interior y se ramifican. Tan densa e intrincada es esta red que sumando el área de todos los poros, un gramo de zeolita puede sumar una superficie de 500 a 800 metros cuadrados. Es esta alta porosidad la que permite atrapar contaminantes en los sistemas de filtrado y purificación de agua usados hoy en día.  Las zeolitas encontradas en Tikal no son propias del lugar en el que se encuentra el depósito de agua, sino que habrían sido llevadas hasta ahí desde un lugar distante unos 30 kilómetros, presumiblemente con el propósito de construir un sistema de purificación de agua.No hay seguridad de cual habría sido la estructura de este sistema, pero Tankersley y colaboradores especulan que el filtro de agua estaba colocado en el punto de entrada del agua al depósito. Ahí, los materiales de cuarzo y zeolita se colocaron  entre dos paredes laterales sólidas y dos petates de palma que permitían la entrada y salida del agua a través del filtro. De cuando en vez, avenidas de agua ocasionadas por los ciclones tropicales arrastraban los materiales del filtro hacia el depósito de agua formando capas en el sedimento del mismo. Así, el filtro tendría que haber sido reconstruido después de cada avenida de agua. En el caso del material zeolita, formado por partículas de tamaño menor que las de cuarzo, había transporte de material hacia el depósito, aun durante el funcionamiento normal del filtro. Es sin duda notable que los mayas de Tikal hayan desarrollado hace dos milenios un sistema de purificación de agua empleando principios que son válidos hoy en día. Y más notable aun es que el de Tikal sea el más antiguo sistema de purificación de agua en el mundo, empleando zeolitas, del que se tenga noticia. De hecho, según Tankersley y colaboradores, la zeolita no se usó nuevamente para purificar agua sino hasta los inicios del siglo XX. Así, los mayas de Tikal se adelantaron en esta materia un par de miles años al resto del mundo.Con el tiempo declinó la civilización maya y con esto se perdió la tecnología de purificación de agua por medio de zeolitas y no podemos hacer nada al respecto. Es, sin embargo, en algún sentido interesante preguntarnos si, de haber sobrevivido la civilización maya ¿tendríamos en México un agua de mayor calidad? Lo cual, por supuesto, no pasa de ser un ejercicio ocioso.",
    "Como sabemos, la temperatura corporal de una persona es un indicador de su estado de salud, misma que empeoraría en cuanto más difiriera de su temperatura normal. Es pues esencial conocer esta última. De acuerdo con el médico alemán del siglo XIX Carl Reinhold August Wunderlich, la temperatura normal del cuerpo es de 37 grados centígrados en promedio. Wunderlich, llegó a esta conclusión después de llevar a cabo millones de mediciones de temperatura corporal con 25,000 personas.Tal parece, sin embargo, que las cifras de Wunderlich no son  necesariamente válidas en los días que corren. Es el caso, por ejemplo, de la población de los Estados Unidos cuya temperatura corporal promedio ha ido disminuyendo en los últimos 200 años a una velocidad de 0.03 grados centígrados cada diez años, según un artículo publicado en enero del presente año en la revista eLife por un grupo de investigadores encabezado por Myroslava Protsiv de la Universidad Stanford. De este modo, concluyen los autores de dicho artículo, la temperatura corporal de la población estadounidense -lo mismo que la de otros países desarrollados- es en la actualidad aproximadamente medio grado centígrado menor que la medida por Wunderlich a mediados del siglo XIX.Según Protsiv y colaboradores, es probable que la disminución en la temperatura corporal esté asociada a la aparición de los antibióticos y al desarrollo económico de los países industrializados, que ha elevado los niveles de vida, mejorado las condiciones de higiene y abatido las infecciones por tuberculosis y malaria, todo lo cual ha disminuido los niveles de inflamación crónica desde el siglo XIX.  Un factor adicional para esta disminución, arguyen Prostiv y colaboradores, podría estar asociado al uso de sistemas de calefacción y de aire acondicionado, que limitan las variaciones de temperatura ambiental, con lo que el cuerpo gasta menos energía para adaptarse a dichas variaciones. Un artículo aparecido esta semana en la revista “Science Advances” nos brinda un ejemplo más de disminución de temperatura corporal. Esta vez, además, no en un país desarrollado, sino en uno en desarrollo. El artículo fue publicado por un grupo internacional de investigadores, encabezado por Michael Gurven de la Universidad de California en Santa Bárbara.Gurven y colaboradores reportan los resultados de un estudio llevado a cabo con un grupo representativo de la población indígena tsimané en la amazonia boliviana. Los resultados del estudio son relevantes pues, de acuerdo con los investigadores, los tsimané son un grupo indígena con un estilo de vida de subsistencia, que no cuenta con agua corriente y que está altamente expuesto a patógenos diversos. Están así lejos de las condiciones que prevalecen en un país industrializado y se podría predecir que su temperatura corporal se acercaría a la que midió Wunderlich en el siglo XIX. Lejos de esto, los tsinamé han experimentado una disminución de temperatura corporal equivalente a la observada en los países industrializados.  Esta disminución, además, se ha dado en las últimas dos décadas –los años que corresponden a la presidencia de Evo Morales- cuando los tsinamé disfrutaron de una mejora en su infraestructura de salud pública y servicios médicos, lo mismo que de  campañas de vacunación y acceso a medicamentos. En los países industrializados la disminución de temperatura corporal se dio en consonancia con un incremento en la esperanza de vida que se duplicó en los años que han transcurrido desde la revolución industrial. Los tsinamé, por su parte, incrementaron su esperanza de vida de 43 a 53 años en la segunda mitad del siglo XX, aunque están todavía muy lejos de la esperanza de vida de los países industrializados.De manera adicional, habida cuenta que algunos poblados tsinamé no cuentan con electricidad, resulta obvio  que el uso de sistemas de calefacción y aire acondicionado no sea algo corriente, y en este respecto la energía empleada en la termorregulación del cuerpo no pareciera ser un factor que  determine la temperatura corporal.    Por lo demás, sin duda hay grandes diferencias en las condiciones que experimentan las poblaciones en las ciudades norteamericanas en comparación con las poblaciones tsinamé en la amazonia boliviana. Y a pesar de esto, como señalan Gurven y colaboradores, ambas poblaciones han experimentado una disminución en su temperatura corporal coincidente con una mejora en sus condiciones epidemiológicas y socioeconómicas. Y si bien todavía no son claras las explicaciones para este fenómeno, arguyen que la temperatura corporal podría ser un indicador de la salud de la población, al igual que lo es la esperanza de vida.",
    "El pasado mes de marzo, después de 20 años de operación, se suspendió el proyecto SETI@home operado por la Universidad de California, Berkeley, dedicado a escudriñar el firmamento en la búsqueda de señales de radio provenientes de inteligencias extraterrestres. El proyecto hizo uso de señales obtenidas por los radio telescopios de Arecibo, Puerto Rico, y de  Green Bank en el estado de Virginia Occidental.Para el análisis de las señales recibidas desde el espacio interestelar, el proyecto SETI@home hizo uso de una estrategia pelicular, en la que participaron más de cinco millones de voluntarios distribuidos a lo largo del mundo que pusieron su computadora personal al servicio del proyecto. De acuerdo con esta estrategia, los datos obtenidos por los radio telescopios fueron concentrados en la Universidad de California, en donde fueron segmentados en pequeñas porciones que fueron enviadas a los participantes a través de Internet. Una vez recibidos los datos, cada computadora llevó a cabo su análisis por medio de un programa desarrollado para tal propósito y regresó los resultados al centro de control en la Universidad de California. De este modo, el proyecto SETI@home hizo uso de un poder de cómputo distribuido a lo largo de todo el mundo. Desafortunadamente, después de buscar por 20 años, no se han encontrados señales que nos indiquen que no estamos solos en el Universo y el proyecto ha sido suspendido.No obstante, es posible que no hayamos todavía detectado la existencia de inteligencia extraterrestre porque la búsqueda no ha sido lo suficientemente extensa. Y en este sentido, un artículo aparecido esta semana en la revista “Monthly Notes of the Royal Astronomical Society” nos brinda una perspectiva interesante para enfocarla.Dicho artículo fue publicado por Lisa Kaltenegger de la Universidad Cornell y Joshua Pepper de la Universidad Lehigh, y en el mismo, en lugar de considerar la búsqueda de inteligencias desde la perspectiva terrestre, se pusieron en el lugar de los extraterrestres y se preguntaron por la posibilidad de que éstos hubieran ya llegado a la conclusión que la Tierra es un planeta apto para albergar vida. Es decir, se propusieron determinar en qué direcciones del firmamento existen planetas desde los cuales quede claro que alrededor del Sol –nuestra estrella- orbita un planeta con las condiciones adecuadas para el desarrollo de vida. Y así, una vez determinadas, estas direcciones son hacia donde deberíamos enfocar nuestros esfuerzos, pues es posible que los hipotéticos extraterrestres a los que habría llamado la atención nuestro planeta, nos hubieran ya enviado señales para establecer comunicación.  ¿Cómo podría un extraterrestre llegar a la conclusión que, orbitando a una estrella lejana existe un planeta que promete albergar vida? Ciertamente tendría maneras, pues nosotros mismos desde la Tierra hemos comprobado la existencia de más de 3,000 planetas fuera de nuestro sistema solar. Una técnica usada para este propósito consiste simplemente en observar pequeños cambios periódicos en la luminosidad de la estrella bajo estudio, que indiquen la presencia de un planeta orbitándola. Para que esto suceda, la órbita del planeta debe ser tal que lleve al planeta a interponerse entre nosotros y la estrella cada que completa una vuelta.    De lo anterior, podemos concluir que para que un extraterrestre detecte nuestra existencia como un planeta apto para la vida debe estar colocado en un sitio particular desde donde observe que de manera periódica transitamos enfrente del Sol. Esto nos restringe a extraterrestres que habiten planetas colocados en el plano de la órbita de la Tierra, desde donde se tiene la impresión que nuestro planeta avanza hacia adelante y hacia atrás en línea recta, Esto, por supuesto, limita considerablemente las opciones, y, de hecho, Kaltenegger y Pepper encuentran que, dentro de una distancia de 326 años luz –la distancia que recorre la luz en un año– solamente existen alrededor de 1,000 estrellas –la más cercana a 28 años luz– desde donde hipotéticos extraterrestres podrían haber detectado a nuestro planeta como probable asiento de una civilización inteligente.Todo lo anterior, por supuesto, no pasa de ser una especulación, ciertamente fascinante, pero al fin de cuentas especulación. Hasta que algún día logremos detectar una señal que nos indique que no estamos solos en el Universo. Y ante esta posibilidad, podríamos entretanto sonreír y enviar un saludo a nuestros vecinos cercanos –posiblemente a decenas de años luz de distancia.",
    "En la actualidad, con casi ocho mil millones de habitantes en el mundo, aproximadamente la mitad de las tierras habitables a nivel global son empleadas para producir alimentos. Se esperaría, además, que en las siguientes décadas, en la medida en que se incremente la población del planeta -que se espera casi alcance los diez mil millones de personas en 2050- la superficie empleada en la producción de alimentos debería aumentar. Por otro lado, los desafíos para lograr incrementar la producción de alimentos en función del aumento de su población no serán iguales para todos los países del mundo y un artículo aparecido esta semana en la revista “MIT Technology Review”, firmado por Megan Tatum, nos proporciona un ejemplo dramático al respecto.  El artículo de marras lleva por título “Dentro de la gran apuesta de Singapur por la agricultura vertical” y se refiere a la producción de alimentos de vegetales en espacios cerrados que está llevado a cabo Singapur, que, como sabemos, es una ciudad-estado con una superficie de 697 kilómetros cuadrados –equivalente a una superficie  cuadrada de poco más de 25 kilómetros de lado- y una población de más de seis millones de habitantes. Estos números hacen a Singapur el tercer país con mayor densidad poblacional en el mundo. En su calidad de ciudad-estado, con un denso perfil urbano de edificios de gran altura, Singapur dedica apenas el 1 por ciento de su superficie a la producción de alimentos y tiene que importar el 90 por ciento de la comida que consume. Según Tatum, esto puso a Singapur en una situación de vulnerabilidad en los meses previos a la crisis de 2008, cuando ocurrió un incremento mundial en los precios de los alimentos. En este contexto, compañías privadas en Singapur han desarrollado granjas verticales para el cultivo de vegetales en espacios cerrados empleando técnicas hidropónicas. En su artículo, Tatum describe las instalaciones de VertiVegies, una de estas compañías dedicada a la producción de vegetales. Escribe Tatum: “En el interior de la instalación, bandejas de plástico desiguales fueron apiladas cuidadosamente en estantes de metal, extendiéndose desde el piso de concreto hasta el techo de acero corrugado. En cada bandeja había pequeñas plantas verdes de diferentes especies y tamaños, todas con sus raíces bañadas con la misma solución acuosa, sus hojas curvándose hacia el mismo resplandor rosado de las luces de barra LED que zumbaban levemente por arriba de las plantas”.Los vegetales son de esta manera cultivados en instalaciones resguardadas que son insensibles a las condiciones climáticas externas, y con iluminación artificial LED y no por medio de la luz solar. Con respecto a esto último, hay que notar que al emplear LEDs como fuente de iluminación es posible combinar los colores rojo y azul –y no el color verde-, que son los que optimizan el crecimiento de las plantas. Por otro lado, un aspecto de importancia central para Singapur es la estructura vertical de la granja que optimiza espacio en su densamente poblada superficie urbana.Granjas verticales como la de VertiVegies podrían ser una solución a los problemas de vulnerabilidad de Singapur con respecto a su dependencia en alimentos con el exterior. Con respecto a esto,  Tatum cita a  Paul Teng, de la Universidad Tecnológica de Nanyang, quien estima que por medio de granjas verticales Singapur podrá producir en diez años el 30 por ciento de los vegetales que consuma. Las críticas a las granjas verticales como VertiVegies se centran, de manera comprensible, en los altos costos de instalación –por el alto valor del terreno urbano-, aun para instalaciones verticales, lo mismo que los costos de producción que involucran el uso de luz LED artificial – cuyo costo, sin embargo, se ha reducido significativamente en los últimos diez años.De un modo u otro,  se extienda o no el uso de las granjas verticales en las décadas por venir, habríamos de conceder que las instalaciones de VertiVegies nos hacen vislumbrar un futuro que hasta hace muy pocos años era sólo propio de la ciencia-ficción. Un futuro en el que los alimentos se producen en instalaciones cerradas altamente tecnificadas, más eficientes y menos contaminantes del medio ambiente en comparación con la agricultura tradicional. Y que, además, son insensibles a las condiciones climáticas externas. Esto último es un punto para no desestimar en el contexto de calentamiento global que produce eventos climáticos extremos cada vez más frecuentes.",
    "El pasado mes de enero, la policía de Londres anunció la instalación en la ciudad de un sistema de reconocimiento facial que ayudaría a la detención de personas buscadas por la policía. Para este propósito, el sistema capturará y analizará imágenes del rostro de personas en lugares públicos, y buscará coincidencias con las características de rostros incluidos en un banco de datos de  sospechosos de haber cometido un crimen. En el caso en que el sistema haya señalado a un sospechoso, la policía lo abordará y le pedirá identificarse y lo arrestará en caso de confirmar que es la persona que busca. Los sistemas de reconocimiento facial se han convertido en realidad gracias a los avances que ha experimentado la tecnología de la inteligencia artificial y los bancos masivos de imágenes  faciales que se han acumulado en las últimas décadas.La medida de policía londinense, por otro lado, ha sido motivo de críticas, por la aparente ineficiencia del sistema para la identificación de sospechosos. Así, si bien la policía de Londres argumenta que el sistema genera sólo una alerta falsa por cada 1,000 casos, un estudio de la Universidad de Essex encuentra que de 42 casos estudiados, solamente en 5 funcionó correctamente. No es claro qué es lo que motiva la gran diferencia entre lo reportado por la Universidad de Essex y la policía londinense, pero ciertamente se esperarían diferencias sustanciales en la eficiencia del sistema de reconocimiento facial en función de las imágenes faciales que tiene que analizar. Así, esta eficiencia sería mayor con imágenes obtenidas a propósito, bien iluminadas y resueltas, que con imágenes borrosas obtenidas en un lugar público. Por lo demás, si bien el caso de Londres no es único –Moscú, por ejemplo, poco después del anuncio de Londres, dio a conocer el despliegue de un extenso sistema de reconocimiento facial–, otras ciudades, como San Francisco y Boston, han prohibido su uso por la policía o las agencias gubernamentales por considerarla una tecnología todavía no lo suficientemente desarrollada. Se señala, por ejemplo, que la tecnología produce resultados sesgados en contra de las minorías, que acusan un mayor porcentaje de falsos positivos en comparación con la población mayoritaria blanca.La causa del sesgo no reside, por supuesto, en la naturaleza de los sistemas de reconocimiento facial, sino en la información que les fue suministrada, en la forma de imágenes faciales, durante su entrenamiento. Así, los sistemas de reconocimiento facial son expuestos a un mayor número de imágenes de personas de raza blanca que de otras minorías, y como resultado se genera un sesgo en el número de sus equivocaciones.Un caso que se ha hecho visible en los últimos días y que involucra también un sesgo de los sistemas de reconocimiento facial –esta vez hacia las personas de menor edad– es el de la ciudad de Buenos Aires, Argentina. Como comenta un artículo publicado esta semana en la revista “MIT Technology Review” bajo la firma de Karen Hao, la ciudad de Buenos Aires tiene instalado un sistema de reconocimiento facial para la captura de personas buscadas por la policía. Dicho sistema está ligado a una base de datos conocida como Consulta Nacional de Rebeldías y Capturas, o CONARC, en la que se concentran datos de personas sospechosas de crímenes.El sistema de Buenos Aires ha mostrado numerosas fallas de identificación de sospechosos. En un caso, por ejemplo, un hombre fue arrestado por 6 días, siendo transferido a una prisión de máxima seguridad antes de ser liberado. En otro caso, la víctima fue advertida que podría sufrir más detenciones en el futuro y para evitarlos se le dio un pase para que lo mostrara a los agentes que pretendieran arrestarlo.Además, de acuerdo con una investigación de “Human Right Watch”, CONARC contiene información de menores de edad, la mayor parte con edades de 16 y 17 años, pero también algunos considerablemente más jóvenes, incluso de hasta un año de edad. Si bien no se ha documentado que los infantes hubieran sido motivo de un arresto, los jóvenes de mayor edad si están expuestos a esta contingencia. De hecho, lo están más que los adultos, dado que los sistemas de reconocimiento facial generan más equivocaciones en el caso de los más jóvenes, pues fueron entrenados para reconocer personas de mayor edad.Podríamos quizá concluir que los sistemas de reconocimiento facial en la medida en que avancen tecnológicamente tendrán sin duda aspectos positivos. Pero en tanto eso sucede, habría que poner en la balanza tanto sus aspectos positivos como negativos. Sin perder de vista la visión distópica orwelliana.",
    "Han pasado ya casi 50 años desde la última misión Apollo de la NASA que llevó astronautas a la superficie de la Luna. Desde entonces no hemos vuelto a pisar la superficie de nuestro satélite natural. En fechas recientes, sin embargo, ha resurgido el interés en regresar a la Luna y para este propósito la NASA ha implementado el proyecto “Artemis”, el cual busca establecer una base permanente en la superficie lunar para, entre otros propósitos, preparar un viaje tripulado al planeta Marte en un futuro todavía no determinado. Lo que sí estaría determinado es la fecha del viaje inicial a la Luna que se llevaría a cabo en 2024.Si bien la mayoría de nosotros no pensaríamos en cambiar nuestro lugar de residencia de la Tierra a una base en la Luna, o aun pasar ahí algunos días o semanas en plan turístico, pudiéramos quizá tener  curiosidad por saber qué condiciones climáticas imperan en nuestro satélite natural. Como sabemos, la Luna no tiene atmósfera, de modo que para caminar sobre su superficie sería imperativo hacerlo dentro de un traje espacial. Otro inconveniente es que al no tener la Luna ni atmósfera ni un campo magnético como el de la Tierra –mediante el cual una brújula apunta siempre hacia el norte- estaríamos de manera continua bombardeados por las radiaciones de alta energía que arriban desde el espacio interestelar y que en nuestro medio son atenuados por la atmósfera o son desviados por el campo magnético terrestre. En la Luna estaríamos, además, a merced de las intensas radiaciones que emite el Sol durante las tormentas solares.La Luna es, entonces, un mundo nada amigable. No obstante, puesto que no faltará quien habrá de pasarse un rato más o menos largo en la Luna, es vital saber qué tan intensas son las radiaciones de alta energía en su superficie. Este es, ciertamente, el caso de los astronautas que participarán en el proyecto “Artemis”.¿Qué tan altas son las radiaciones de alta energía en la superficie de la Luna? La respuesta nos la da un artículo publicado esta semana por un grupo de investigadores e instituciones en China y Alemania, encabezados por Shenyi Zhang del Centro Nacional de Ciencia Espacial de China. En su artículo, Zhang y colaboradores reportan valores de radiaciones de alta energía medidas por la sonda china Chang´e 4, que alunizó al inicio del año pasado en el lado oculto de la Luna. Zhang y colaboradores encuentran que los astronautas en la Luna estarían expuestos a radiaciones de alta energía una 200 veces más intensas que las experimentadas en la Tierra. Tendrían, en consecuencia, que protegerse con algún tipo de cubierta que absorba las radiaciones.     Con respecto a esto último, los investigadores calculan que una cubierta de cuando menos 50 centímetros de espesor de tierra lunar sería protección suficiente. En el caso de que ocurra una tormenta solar, los astronautas tendrían que guarecerse en un refugio protegido por un volumen de agua de 10 metros de espesor. Con precauciones y muchos recursos y trabajo para construir una base lunar, podríamos quizá sobrevivir en la Luna por tiempos más o menos largos.Si bien durante medio siglo se suspendieron las misiones tripuladas de exploración de la Luna, en la actualidad existe un interés renovado en visitar por diversos medios nuestro satélite natural. No solamente por los países que  fueron pioneros en la exploración espacial, sino también por otros países como Japón, la India, Israel y –no podría ser de otro modo- China, que se ha convertido en el tercer país en lograr posar suavemente sondas robóticas sobre la superficie lunar.Como sabemos, en las décadas de los años cincuenta y sesenta los Estados Unidos y la Unión Soviética se enfrascaron en una carrera espacial. En un inicio, los soviéticos superaban claramente a los norteamericanos. A los pocos años, sin embargo, mediante inyecciones masivas de dinero a la NASA, lograron que fuese un astronauta  norteamericano el primero en poner un pie sobre la superficie de la Luna, superando de este modo a los soviéticos.Cabrá preguntarse si 50 años después de la primera carrera espacial podría surgir una nueva carrera por la Luna, esta vez entre los Estados Unidos, en conjunto con los países y compañías privadas que están colaborando en el proyecto “Artemis”, por un lado, y China por el otro. Solo el tiempo lo dirá.",
    "“Los científicos del clima están aterrorizados por un segundo periodo de Trump”, así se titula un artículo publicado el pasado 24 de septiembre por James Temple en la revista MIT Technology Review del Instituto Tecnológico de Massachusetts. Y los que no estén aterrorizados, muy probablemente estarán cuando menos profundamente preocupados, dadas las iniciativas que ha tomado el presidente de los Estados Unidos en cuanto a medidas para mitigar el cambio climático. Como sabemos,  el presidente norteamericano no es un creyente –al menos públicamente- en que el planeta esté sufriendo un cambio climático como consecuencia de nuestras actividades y lo reconoce abiertamente. Un ejemplo de esto lo dio el pasado 2 de septiembre durante su viaje a California con motivo de los incendios que asuelan al estado, mismos que el presidente norteamericano atribuye a que no se retiran de los bosques los troncos y hojas secas. En la reunión con los funcionarios californianos,  según relata el periódico New York Times, el secretario de la Agencia de Recursos Naturales del estado exhortó al presidente a “reconocer el cambio en el clima y lo que esto significa para nuestros bosques”. Lejos de reconocerlo, sin embargo, el presidente habría respondido: “Va a empezar a enfriar, ya verás”. Y ante la insistencia del secretario que le respondió: “Quisiera que la ciencia estuviera de acuerdo con usted”, el presidente cerró el intercambio afirmando: “En realidad, no creo que la ciencia sepa”.   En cuanto a esta última afirmación, si bien el clima de la Tierra es un sujeto de estudio extremadamente complejo, existe un consenso científico en que el incremento sostenido en la concentración de gases de invernadero en la atmósfera -que se ha elevado alrededor de un 50 por ciento con respecto a su nivel pre-industrial, la más alta en 800,000 años-, ha puesto al planeta en la vía de un desastre climático. Hay algunos científicos, sin embargo, que consideran que las altas concentraciones de gases de invernadero en la atmósfera en realidad son beneficiosas. Es el caso de William Happer, profesor emérito de la Universidad de Princeton y, hasta septiembre del pasado año, asesor del presidente  como director de la oficina de tecnologías emergentes del Consejo Nacional de Seguridad de los Estados Unidos. Happer, quien es profesor de física pero que no tiene un entrenamiento formal en la ciencia del clima, sostiene que con más dióxido de carbono en la atmósfera tendremos plantas más saludables –recordemos que las plantas toman el dióxido de carbono para llevar a cabo la fotosíntesis-. Una mayoría de expertos opina, no obstante, que si bien las plantas posiblemente fueran más felices con mayores concentraciones de dióxido de carbono, el incremento de este gas en la atmósfera trae consecuencias climáticas que a la larga son negativas –un incremento en la temperatura global y una mayor acidificación de los océanos, entre otras.  La incredulidad del presidente Trump en el cambio climático lo ha llevado a deshacer o intentar deshacer reglamentaciones e iniciativas que buscan de mitigar el cambio climático. Sabemos que en junio de 2017 anunció que los Estados Unidos se desligarían del Acuerdo de Paris que ha sido ratificado por 187 países. Este acuerdo busca reducir la emisión de gases de invernadero para limitar a 2 grados centígrados el aumento de la temperatura global desde la época pre-industrial, y a realizar esfuerzos para limitar este incremento a 1.5 grados centígrados. La separación formal de los Estados Unidos del Tratado de Paris está programada para el próximo 4 de noviembre. Internamente, como lo discute James Temple, la administración de los Estados Unidos está desmantelando las iniciativas de su antecesor dirigidas al control de la emisión de gases de invernadero, incluyendo las emisiones de metano y de hidrofluorocarbonos –usados en los refrigeradores- que son gases de invernadero sustancialmente más potentes que el dióxido de carbono. Según un análisis de la firma “Rhodium Group”, las desregulaciones en curso llevarán a un incremento en la emisión de gases de invernadero en el año 2035, que sería equivalente a la emisión anual de Rusia.Así, quienes están aterrorizados se preguntan qué sucederá si la presente administración cuenta con otros cuatro años para consolidar sus iniciativas de cambios de reglamentaciones ambientales. O si, como lo puntualiza Temple, el país está en el curso de un cambio político profundo, en cuyo caso el clima no sería la mayor de las preocupaciones.",
    "Si bien Venus posiblemente no sea un planeta tan popular como Marte, esta semana se convirtió en noticia cuando un grupo internacional de investigadores, encabezado por Jeane Graves de la Universidad de Cardiff, publicó un artículo en la revista Nature Astronomy en el que  reporta haber detectado gas fosfina en su atmósfera. De acuerdo con las conclusiones de dicho artículo, si bien la fosfina puede tener un origen no biológico, también podría ser originado por vida microbiana. Esto último, a pesar de que las condiciones ambientales en Venus son extremas y muy alejadas de las que prevalecen en la Tierra. Pero vayamos por partes.Consideremos primeramente que la órbita de Venus alrededor del Sol es la segunda más alejada después de la de Mercurio, lo que nos llevaría a concluir que la superficie de Mercurio es más caliente que la de Venus. No es el caso, sin embargo, debido a que Venus tiene una atmósfera extremadamente densa de dióxido de carbono –el gas de invernadero que tantos problemas le está dando al clima de la Tierra-, la cual mantiene la superficie del planeta a una temperatura superior a los 400 grados centígrados, más que suficiente para fundir plomo, pero poco amigable para la vida.  Esto último es responsable de la poca popularidad de Venus en comparación con Marte, que tiene un clima más moderado. Venus, por otro lado, no siempre ha sido un planeta con poco atractivo. Después de todo, visto desde la Tierra, es el segundo objeto nocturno más brillante en el cielo después de la Luna y ha sido bien identificado desde que empezamos a escudriñar el cielo. Así, en su condición de cuerpo espacial destacado, Venus fue en las décadas de los años sesenta y setenta del siglo pasado un planeta de gran interés para las agencias espaciales de los Estados Unidos y, sobre todo, de la entonces Unión Soviética. De hecho, fue Venus y no Marte el primer planeta en cuya superficie se posó de manera controlada una sonda planetaria.  Esto ocurrió en diciembre de 1970, cuando la Unión Soviética colocó sobre la superficie venusina a  la sonda Venera 7, que logró trasmitir desde ahí por 23 minutos información sobre las condiciones ambientales del planeta. La información enviada por la sonda Venera 7 confirmó que la temperatura en la superficie de Venus ronda los 450 grados centígrados, y que su atmósfera es tan densa que la presión atmosférica es unas 100 veces más grande que la de nuestro planeta. Antes de que las sondas planetarias nos dieran a conocer las verdaderas condiciones ambientales de Venus, se conjeturaba que el clima de este planeta era cálido, seco o húmedo, pero no incompatible con la vida. Competía de este modo con Marte, entre los escritores de ciencia ficción, como asiento de civilizaciones extraterrestres. Desafortunadamente, las sondas planetarias nos enseñaron que, lejos de ser un lugar amigable, Venus es un infierno con una atmósfera extremadamente densa y seca, y con altas concentraciones de ácido sulfúrico. El atractivo del planeta sufrió entonces un deterioro considerable.La temperatura ambiental de Venus, sin embargo, se modera conforme nos elevamos por encima de su superficie, y a algunas decenas de kilómetros de altura alcanza unos 30 grados centígrados. Es ahí, conjeturan Greaves y colaboradores, flotando en la atmósfera, en donde podría florecer la vida microbiana responsable de la fosfina detectada en Venus. De conformarse esta posibilidad, Venus con toda seguridad recuperaría su lugar que siempre ha tenido como uno de los objetos más destacados del firmamento.No obstante, hay que hacer notar que, en contraste con la estridencia de los medios de comunicación que destacaron la conjetura de que exista vida microbiana en Venus, Graves y colaboradores son muy cautos en su artículo y escriben en el mismo que “Aun si se confirma, enfatizamos que la detección de fosfina no es una evidencia robusta de la existencia de vida, sino de un proceso químico anómalo y para el cual no hay explicación por el momento”. Al mismo tiempo, sin embargo, enfatizan que, a pesar de sus esfuerzos, no han logrado encontrar una explicación no biológica para su hallazgo de fosfina en Venus.  ¿Se confirmará el origen no biológico de la fosfina encontrada en Venus? ¿O, por lo contrario, se confirmará la existencia de vida microbiana en este planeta? Para encontrar respuestas tendremos que esperar a que se lleven a cabo más investigaciones que arrojen luz al respecto. En tanto eso ocurre, Venus ha sido afortunado al recuperar, así sea temporalmente, algo de su lustre perdido.",
    "Con el tratado de Tordesillas de 1494, los Reyes Católicos y el rey de Portugal se dividieron el mundo por descubrir por medio de una línea imaginaria que corría de polo a polo y pasaba a 370 leguas al oeste de las islas Cabo Verde. De acuerdo con este tratado, a España le corresponderían los territorios que se descubrieran al oeste de dicha línea, mientras que Portugal tomaría posesión de aquellos que lo fueran al este de la misma. Para ponerse de acuerdo y repartirse el mundo del modo en que lo hicieron, España y Portugal no contemplaron, ciertamente, la posibilidad de pedirle una opinión a los habitantes del mundo por descubrir sobre la conveniencia o no conveniencia de tal repartición. No se la pidieron tampoco a otros países. De hecho, Holanda, Inglaterra y Francia no reconocieron el tratado de Tordesillas y emprendieron sus propios viajes de exploración y de toma de posesión de territorios a lo largo del mundo. Todo lo anterior viene a colación por un reciente anuncio de la NASA, de que pondrá a concurso contratos con compañías privadas para impulsar el desarrollo de la minería en la Luna. El proyecto fue presentado por Jim Bridenstine, administrador de la NASA, en un comunicado fechado el pasado 10 de septiembre. De acuerdo con lo dicho por Bridenstine, la compañía ganadora del concurso deberá recoger pequeñas muestras de polvo o rocas lunares en alguna localización de la Luna y proporcionar a la NASA fotografías de las mismas. Deberá también proporcionar información y fotografías del lugar en que fueron recogidas. Finalmente, deberá transferir la propiedad de las muestras lunares a la NASA. El contrato no incluye el traslado del material lunar hasta la Tierra, el cual sería implementado por la NASA en algún momento futuro.La iniciativa anunciada por Bridenstine se da en el contexto del programa Artemis, mediante el cual la NASA busca reanudar las misiones tripuladas a la Luna en 2024, e iniciar un programa de exploración lunar con el objetivo a largo plazo de establecer una presencia sostenible en la Luna, que eventualmente sirva como un punto de apoyo para el envío de una misión tripulada a Marte. Para cumplir estos objetivos, la NASA busca el concurso de la empresa privada que, como sabemos, está incursionando de manera agresiva en la industria espacial.La NASA anticipa que pagará entre 15,000 y 25,000 dólares por muestras lunares que pesen entre 50 gramos y 500 gramos. Un 10 por ciento del pago se realizará a la firma del contrato, otro 10 por ciento en el momento del lanzamiento de la misión a la Luna, y el 80 por ciento restante después de que concluya exitosamente. Estos serán los únicos pagos que realizará la NASA de modo que los costos y riesgos de la misión, que deberá completarse antes de 2024, correrán a cargo de la compañía involucrada. En estas condiciones, la empresa a cargo del proyecto tendría que llevarlo a cabo como un complemento de otro proyecto que de cualquier manera la iba a llevar a la superficie de la Luna. Si bien la empresa no hará un gran negocio, la iniciativa constituiría el inicio de la explotación comercial del espacio por empresas privadas. Como lo expresa la NASA de manera explícita: “La recolección y transferencia será una prueba de concepto para realizar comercio espacial en la Luna”.  En el mismo tenor, Casey Dreier, de la Sociedad Planetaria, manifiesta: “La importancia de este anuncio no es tanto el incentivo financiero (que es minúsculo) sino en establecer el precedente legal de que las empresas privadas pueden recolectar y vender materiales celestes (con la bendición de la NASA/gobierno de los EE.UU.)”. En contraste con los tiempos del tratado de Tordesillas, en la actualidad existe el concepto de que el destino de los recursos que son comunes a todos los habitantes de la Tierra, como es el caso de la Luna y de todos los cuerpos espaciales -en la medida en que no estén habitados por seres inteligentes-, es una decisión colectiva. Con este respecto, el tratado de la Naciones Unidas sobre el espacio extraterrestre, vigente desde 1967, establece que “La exploración y utilización del espacio extraterrestre se llevará a cabo en beneficio y en interés de todos los países y será competencia de toda la humanidad”.De acuerdo con la NASA, su proyecto de minería lunar no contradice el acuerdo sobre el espacio extraterrestre. Aun así, habría que preguntarse en qué medida la decisión de la NASA es en beneficio del mundo y no de intereses particulares. Es decir, ¿en qué medida nos encontraríamos ante una especie de eco de Tordesillas?",
    "No es demasiado difícil entender que los vehículos automotores de gasolina o diesel constituyen una fuente de contaminación atmosférica. Sobre todo si, como llega a suceder, despiden una espesa nube de humo negro cuando aceleran. Tomándolo con filosofía, podríamos quizá convencernos que el humo de los escapes es el precio del progreso y que, en todo caso, podríamos superarlo diseñando vehículos menos contaminantes –tal como de hecho ha ocurrido- o mediante el uso de vehículos eléctricos.Sucede, sin embargo, que los inconvenientes –por no decir desgracias- no siempre vienen solos y ahora nos enteramos que las mismas pistas de asfalto –calles y carreteras- sobre las cuales corren automóviles y camiones son fuentes de contaminación atmosférica. A esta conclusión llega un artículo publicado esta semana en la revista “Science Advances” por un grupo internacional de investigadores de los Estados Unidos y Alemania, encabezado por Peeyusk Khare de la Universidad Yale. Para entender cómo una carretera de asfalto puede producir contaminación atmosférica, habría que considerar que la mayor parte del asfalto que se emplea comercialmente se obtiene a partir del petróleo crudo y que el mismo contiene componentes orgánicos que se volatilizan, tanto durante su proceso de transporte, como de colocación en calles y carreteras. Una vez liberados, estos gases reaccionan químicamente con otros componentes de la atmósfera incorporándose a la misma como aerosoles contaminantes. Estos aerosoles consisten de partículas microscópicas suspendidas en el aire, que son altamente peligrosas para la salud. Khare y colaboradores se propusieron cuantificar el volumen de gases desprendidos durante la colocación y el tiempo de vida de las superficies asfaltadas. Como parte de su investigación, los investigadores sometieron muestras de asfalto a una temperatura de 140 grados centígrados, que corresponde a la temperatura de transporte y de colocación del pavimento de asfalto, y midieron empleando sofisticadas técnicas el volumen de gases desprendidos. Encontraron como un primer resultado que después de una semana de calentamiento  la muestra dejó de emitir gases.Encontraron también que durante las primeras cinco horas de calentamiento a 140 grados centígrados se desprendió el 14 por ciento del total de gases contenido en la muestra. De este modo, puesto que son precisamente cinco horas el tiempo estimado durante el cual el asfalto de mantiene a esta temperatura durante el transporte y el proceso de pavimentación, los investigadores concluyeron que una vez colocado contiene todavía un 86 por ciento de gases susceptibles de liberarse a la atmósfera. Esto de hecho ocurre en días calientes de verano cuando el pavimento puede alcanzar una temperatura de 60 grados centígrados. Por otro lado, la velocidad de desprendimiento de gases a 60 grados centígrados es mucho menor a la que se observa a alta temperatura, lo que convierte al pavimento de asfalto en una fuente de contaminación atmosférica de larga duración. Para hacer las cosas peores, Khare y colaboradores encuentran que la exposición del pavimento a una intensidad moderada de luz solar, incrementa el desprendimiento de gases hasta en un 300 por ciento a la temperatura ambiente. Entre áreas pavimentadas y techos de casas –en donde también se usa el asfalto- aproximadamente un 65 por ciento de la superficie de las ciudades norteamericanas está recubierto de asfalto. En estas condiciones, Khare y colaboradores estiman que el potencial de generación de aerosoles contaminantes por las superficies asfaltadas en la ciudad de Los Ángeles es tan grande como el de los vehículos de combustión interna.De acuerdo con todo lo anterior, resolver el problema de contaminación por los automotores de combustión interna pasaría no solamente por hacerlos más eficientes y menos contaminantes, sino también por mitigar la contaminación de las superficies pavimentadas sobre las que corren. Sin olvidar que el calentamiento global eleva la temperatura de las superficies pavimentadas y por tanto incrementa la emisión de gases contaminantes y agrava el problema. Por lo demás, la contaminación por superficies pavimentadas se agravaría en función de la frecuencia con la que se renuevan los pavimentos. Y en ese sentido, en México podemos estar razonablemente tranquilos.",
    "Quienes tenemos la suficiente edad hemos sido testigos a lo largo del último medio siglo del incremento sustancial en la eficiencia del consumo de gasolina que han experimentado los automóviles. Al respecto, recordamos los automóviles fabricados por las compañías de nuestro vecino del norte en las décadas de los años cincuenta y sesenta, de gran tamaño y dotados con potentes motores, y para los cuales la eficiencia en el consumo de gasolina era la menor de las preocupaciones. La situación cambió en la década de los años setenta del siglo pasado con motivo del llamado embargo petrolero que produjo grandes incrementos en el precio del petróleo, y hoy en día la eficiencia energética es una de las virtudes que más apreciamos en un automóvil.En los días que corren la eficiencia energética de los automóviles es también una virtud por la contaminación atmosférica ocasionada por la quema de combustibles fósiles que amenaza con un colapso climático. De hecho, un incremento en la eficiencia energética en general -y no solamente de los automóviles- es una de las estrategias que se han implementado para mitigar el cambio climático. De acuerdo con esta estrategia, entre más eficientes seamos, disminuiremos el consumo de energía en el transporte, la industria, la iluminación, y en general en todas nuestras actividades como sociedad, y con esto mitigaríamos nuestros problemas climáticos.Estaríamos quizá de acuerdo en que esto último suena razonable. No lo es necesariamente, sin embargo, según un artículo aparecido esta semana en la revista en línea PLOS ONE. El artículo fue publicado por Timothy Garret de la Universidad de Utah, Matheus Grasselli de la Universidad McMaster y Stephen Keen del University College de Londres, quienes llegan a la conclusión que la eficiencia energética, más que disminuir el consumo de energía tiende a aumentarlo. Pero vayamos por partes.Garret y colaboradores conciben -de manera poco usual- a la civilización humana como un ente biológico o físico que consume y desperdicia energía para el cumplimiento de sus funciones. En este contexto, el motor de un automóvil, por ejemplo, obtiene energía a partir de la combustión de la gasolina y parcialmente usa esta energía para impulsar al vehículo. Al mismo tiempo, y de manera inevitable de acuerdo con las leyes de la física, desperdicia parte de la energía que consume. De manera análoga, la civilización humana consume y desperdicia energía pero, a diferencia del automóvil, no solamente la emplea para mantenerse en funcionamiento -en todas las facetas de la actividad humana- sino que también la usa para crecer en tamaño. Y en la medida en que la civilización crece, crece también su apetito por la energía.Los investigadores ejemplifican esto último con la evolución que sufrimos desde que nacemos. Es decir, cuando niños tomamos energía de lo que comemos y la empleamos para mantener nuestro metabolismo como seres vivos, y para crecer hasta llegar a ser adultos. Lo mismo sucede con la civilización humana que consume energía para mantener su metabolismo y para crecer en tamaño. Y en esto último, según Garrett y colaboradores, es donde radica el problema, pues en la medida en que transcurre el tiempo crece la economía global y en esa medida se incrementa su demanda de energía. Así, por un efecto de inercia, la demanda de energía en el presente depende del nivel de consumo de energía que se tuvo en el pasado. Para corroborar esta última hipótesis, Garret y colaboradores compararon el consumo anualizado de energía a nivel global en el periodo 1980-2017, con la producción económica histórica que se había alcanzado en cada uno de los años de periodo estudiado. Encontraron que en dicho periodo, ambas cantidades –que se multiplicaron por un factor mayor a dos- cambiaron de manera proporcional, apoyando la hipótesis.¿Qué consecuencia tiene el que la demanda de energía en un determinado momento dependa del comportamiento de la economía en el pasado? En ese sentido, Garret y colaboradores concluyen que el crecimiento económico es estimulado por las innovaciones en materia de eficiencia energética. Es decir, que dichas innovaciones en lugar de contribuir al ahorro de energía estimularían su consumo.Y en estas circunstancias, no nos queda sino cruzar los dedos para que la conclusión sea incorrecta y que el remedio no agrave la enfermedad.",
    "Masahiro Ono, científico del Laboratorio de Propulsión a Reacción de la NASA, hace la siguiente reflexión en una entrevista publicada esta semana por la Universidad de Texas en Austin. “Imagine que usted es un extraterrestre que sabe casi nada acerca de la Tierra, y que aterriza en siete u ocho puntos de la superficie de nuestro planeta y se mueve sobre la misma por unos pocos cientos de kilómetros. ¿Sabría usted suficiente acerca de la Tierra?” Obviamente la respuesta es no: con una exploración tan superficial –en sentido literal y figurado- se obtendría una visión muy limitada de nuestro planeta. Ono hace sus comentarios en referencia a la exploración de la superficie del planeta Marte por medio de robots exploradores y concluye: “Si queremos representar la enorme diversidad de Marte necesitamos más mediciones de su superficie y la clave para esto son las exploraciones a lo largo de distancias sustancialmente mayores, con suerte cubriendo miles de millas”. Como sabemos, los sucesivos exploradores que la NASA ha colocado sobre la superficie marciana han tenido movilidades cada vez más extendidas. En diciembre de 1971, la Unión Soviética fue el primer país en lograr posar suavemente una sonda sobre la superficie de Marte. Cuatro años más tarde, la NASA colocó en Marte las sondas Viking 1 y Viking 2 que nos proporcionaron las primeras imágenes panorámicas y a color de la superficie del planeta.  Las primeras sondas de exploración en Marte, sin embargo, no tenían movilidad y solamente nos proporcionaron información desde un punto de vista fijo. Misiones posteriores de la NASA enviaron a Marte robots exploradores que pudieron moverse a lo largo de su superficie y nos hicieron llegar imágenes e informaciones más variadas. El primero de ellos, el “Sojourner” arribó a Marte en julio de 1997 y recorrió aproximadamente 100 metros a lo largo de 91 días marcianos. Posteriormente, en enero de 2004, los exploradores “Spirit” y “Opportunity” se posaron sobre la superficie marciana. El primero viajó 4.8 millas y el segundo 28 millas, a lo largo de cinco y quince años, en forma respectiva. En la actualidad, el explorador “Curiosity” está activo en Marte y ha recorrido 12 millas desde su arribo en agosto de 2012.  Por otro lado, como explica Masahiro Ono, si queremos tener un conocimiento más preciso de Marte es necesario extender la movilidad de los futuros exploradores. Hacer esto, sin embargo, es más fácil decirlo que hacerlo. En efecto, para moverse sobre la superficie marciana los robots deben sortear múltiples obstáculos y para esto toman decisiones en las que intervienen los operadores desde la Tierra. Idealmente, los exploradores deberían ser autónomos y tomar sus propias decisiones sin necesidad de comunicarse con los operadores en la Tierra. Como sabemos, en principio la navegación autónoma en Marte sería posible tal como lo hacen los vehículos autónomos aquí en la Tierra. En la práctica, dicha navegación ha sido difícil de alcanzar por la limitada capacidad de cómputo de la que han dispuesto en Marte los exploradores –inferior a la de un teléfono iPhone- y también por limitada disponibilidad de energía –proporcionada por el Sol. En el futuro cercano, según explica Ono, los exploradores incrementarán su capacidad de cómputo sin elevar el consumo de energía mediante el desarrollo de nuevos procesadores, y se espera que alcancen un cierto nivel de inteligencia, si bien no al nivel de los vehículos autónomos que conocemos. Una estrategia para aumentar la movilidad de los exploradores combinará la capacidad de cómputo de la que se dispone aquí en la Tierra para el desarrollo de planes de movimiento empleando sofisticadas técnicas de inteligencia artificial, que indiquen los pasos a seguir dadas las condiciones del terreno. Estos planes se harán llegar al explorador en la superficie de Marte que hará uso de su capacidad de cómputo incrementada para ejecutarlos. Por lo demás, las fotografías que los exploradores nos han enviado desde la superficie de Marte muestran un paisaje seco y pedregoso, de alguna manera similar a algunos lugares de nuestro planeta. No tendríamos que engañarnos, sin embargo, pues los dos planetas son muy diferentes en cuanto a su capacidad de albergar vida tal como la conocemos. En estas condiciones, y por el momento, no habría razón alguna para viajar hasta Marte en persona. Pero sí para explorarlo por medio de robots que cada vez se sentirán más a sus anchas en Marte, sin sufrir de nuestras limitaciones como seres vivos.",
    "La historia relatada aquí se inicia hace medio siglo cuando Mara, una elefanta asiática nacida en cautiverio en la India, fue trasladada después de su nacimiento al zoológico Tierpark Hagenberck de Hamburgo. No permaneció Mara mucho tiempo en el zoológico, pues en mayo de 1970 fue vendida al “Circo África” y trasladada a Montevideo, Uruguay. Como parte del circo, Mara fue exhibida en Uruguay, Argentina y Brasil haciendo las suertes propias de los elefantes en estas circunstancias. Aparentemente, y dentro de lo que cabe, esta etapa de la vida de Mara no habría sido del todo mala. Su suerte cambió, sin embargo, cuando sus dueños se retiraron del negocio circense y la vendieron al circo argentino Rodas.En el Circo Rodas Mara no era feliz. Evidencia de esto es que en algún momento mató a su entrenador cansada de los malos tratos. Paso ahí, sin embargo, 25 años de su vida hasta que en 1995 fue rescatada por el zoológico de la ciudad de Buenos Aires después de que fue encontrada encadenada en un estacionamiento. Sin bien las condiciones de vida de Mara en el zoológico de Buenos Aires habían mejorado, en los últimos años fueron motivo de críticas por parte de activistas preocupados por el bienestar de los animales. En particular, Mara no tenía el espacio suficiente para moverse y era vista frecuentemente balanceando por horas su cabeza en círculos, lo cual se interpreta como un signo de estrés. Tenía, además, que compartir su espacio con dos elefantas africanas. Los legos en materia de elefantes posiblemente no veamos demasiadas diferencias entre los elefantes asiáticos y los africanos -fuera quizá de su tamaño-. Mara, en cambio, no es aparentemente de la misma opinión y, de hecho, no tenía la menor simpatía por sus compañeras de cautiverio.En estas circunstancias se planteó trasladar a Mara a una reserva de elefantes en la región de Mato Grosso en Brasil, distante de Buenos Aires unos 2,700 kilómetros. Mara tendría así que cambiar de país de residencia, lo que implicaba realizar una serie de trámites burocráticos que son de suyo complicados. Y que lo fueron más porque el traslado coincidió con la emergencia sanitaria por la pandemia de coronavirus que cerró la frontera entre Argentina y Brasil.    Entre otros requisitos para su traslado, Mara tuvo que someterse a una cuarentena. No se sabe que los elefantes sean susceptibles al coronavirus y la razón para poner en cuarentena a la elefanta no tuvo ciertamente que ver con la pandemia, y sí con la necesidad de asegurar, antes de entrar a Brasil, que Mara estaba libre de otras enfermedades propias de los elefantes.El traslado se había planeado originalmente para el mes de marzo del presente año, pero se suspendió por la emergencia sanitaria. Pudo finalmente realizarse el 9 mayo y para tal fin la elefanta fue confinada en una estrecha caja que fue subida con una grúa a la plataforma de un camión para su traslado al Mato Grosso. Dos días después de su partida el convoy arribó a la frontera con Brasil, la cual pudo cruzar después de nueve horas de trámites burocráticos. Tres días más le tomó arribar a su destino final. Antes de esto, sin embargo, hubo que transferir, por medio de una grúa, la caja con la elefanta a otro trasporte que pudiera recorrer los últimos 65 kilómetros hasta la reserva. A estas alturas, con su encierro y el ajetreo del viaje, posiblemente Mara haya considerado que iba de mal en peor, y que más le hubiera valido quedarse en el zoológico.La historia, sin embargo, tuvo un final feliz, pues parece que Mara se adaptó rápidamente a su nuevo hábitat. Incluso se hizo amiga a primera vista de Rana, otra elefanta -asiática, por supuesto- de la reserva y de la que es inseparable. Calculan los que saben que Mara tiene ahora una edad de aproximadamente 50 años y que le quedan por delante unos 25 años de vida. Mismos que esperan sus cuidadores sean mejores que los primeros 50, cuando fue obligada mediante golpes a sentarse o a pararse de manos, o bien a languidecer, meciendo por horas la cabeza, en un espacio demasiado reducido. Y en compañía, además, de dos tipas con las que no se llevaba.",
    "Hoy, 9 de agosto, se cumplen 75 años del bombardeo atómico de Nagasaki llevado a cabo por los Estados Unidos al final de la Segunda Guerra Mundial, tres días después de que la ciudad de Hiroshima hubiese corrido la misma suerte. Ambos bombardeos produjeron un estimado de 250,000 víctimas fatales, entre los que fallecieron en el momento de las explosiones y aquellos que enfermaron por envenenamiento radiactivo y murieron posteriormente.El impacto sicológico de estos ataques en la población japonesa fue, sobra decirlo, de grandes proporciones. Y no tanto por el enorme número de muertos entre la población civil, sino por el poder destructivo de las nuevas armas. En este sentido, habría que recordar  que si bien el ataque a la ciudad de Tokio durante la noche del 9 al 10 de marzo de 1945 con bombas convencionales produjo alrededor de 100,000 muertos, para que esto se diera fue necesario desplegar 279 bombarderos B29 que lanzaron sobre la ciudad 1700 toneladas de bombas incendiarias. En contraste, la destrucción de Hiroshima la llevó a cabo un solo bombardero B29 mediante una sola bomba de uranio.Los bombardeos de Hiroshima y Nagasaki han provocado, no podría ser de otro modo, que la población japonesa tenga sentimientos negativos en contra de las armas nucleares y que en forma mayoritaria se oponga al desarrollo, e incluso a la presencia, de dichas armas en su territorio. Por más que Japón dependa de la potencia nuclear de los Estados Unidos para su defensa.Una manifestación de la traumática experiencia de Japón con las armas nucleares es la película “Godzilla”, dirigida por Ishiro Honda y estrenada en el año 1954, apenas nueve años después del fin de la guerra. Dicha película, que tuvo gran éxito, constituye una alegoría de los bombardeos atómicos de las ciudades japonesas e incluye mensajes explícitos en contra del desarrollo de las armas nucleares.De acuerdo con la trama de la película, las pruebas con bombas de hidrógeno llevadas a cabo por los Estados Unidos en los atolones del Océano Pacífico, incomodaron y sacaron de su hábitat en el fondo del mar a Godzilla, un gigantesco monstruo sobreviviente del periodo Jurásico. El aspecto de Godzilla es el de un tiranosaurio con cola de lagartija, hileras de prominentes placas dorsales, y una piel extremadamente rugosa remedando las cicatrices queloides de los sobrevivientes de Hiroshima y Nagasaki. Al inicio de la película varios barcos son atacados y hundidos por Godzilla, episodios que habrían sido inspirados en el incidente ocurrido el 1 de marzo de 1954 en el que el barco atunero japonés Daygo Fururyu Maru estuvo expuesto a la radiación proveniente de una prueba nuclear llevada a cabo por los Estados Unidos en el atolón de Bikini. Como resultado de esta exposición, uno de los tripulantes murió por envenenamiento radiactivo.Después de sus primeros encuentros con los humanos, Godzilla se encaminó hacia la ciudad de Tokio, resistiendo sin mayor problema todos los ataques que le fueron dirigidos para frenarlo. Una vez en Tokio, el monstruo produjo, con su aliento ardiente y enorme fuerza, una destrucción equiparable a la de Hiroshima.Godzilla fue finalmente puesto fuera de combate por el “Destructor de oxígeno”, un poderoso dispositivo inventado por el doctor Daisuke Serizawa. Tal era el poder de dicho dispositivo, que Serizawa temía que pudiera caer en manos de gobiernos que lo utilizarían como un arma de destrucción masiva. En estas condiciones, el científico asumió el papel de héroe y decidió inmolarse y morir junto con Godzilla, para así evitar que en el futuro lo obligaran a reproducir su invento con fines aviesos. La película Godzilla original es pues una metáfora de los ataques nucleares a Hiroshima y Nagasaki y una propaganda en contra de las armas atómicas. Tomada en forma literal, por otro lado, es inexacta en cuanto a que un arma con el suficiente poder para vencer a Godzilla pudo ser fabricada por un científico aislado, y que bastó una decisión personal para ponerla fuera del alcance de gobiernos dispuestos a usarla como un arma de destrucción masiva. En efecto, sabemos que las armas nucleares que devastaron a Hiroshima y Nagasaki fueron resultado de un esfuerzo nacional en el que participaron grupos numerosos de científicos e ingenieros apoyados con cantidades ingentes de recursos; y que, ciertamente, bastaría algo más que una decisión personal para ponerlas fuera de combate.Por lo demás, la película original de Godzilla de 1954 resulta más que recomendable.",
    "Las medidas que se han implementado para desacelerar la propagación del coronavirus en el mundo han llevado, entre otros efectos a nivel global, a una disminución en la emisión de gases de invernadero y a una reducción de los niveles de contaminación atmosférica.  Esto, por ejemplo, ha posibilitado que los montes Himalayas hayan sido visibles por primera vez en treinta años desde una distancia de cientos de kilómetros en el norte de la India. Otro efecto colateral de la pandemia, quizá más sorprendente, al menos para los legos en la materia, ha sido la reducción en los movimientos sísmicos que las actividades humanas producen en la corteza terrestre. Esta reducción está documentada en un artículo aparecido el pasado 23 de julio en la revista “Science”. Dicho artículo fue publicado por un numeroso grupo de investigadores adscritos a instituciones de investigación a lo largo de todo el mundo, incluyendo México, encabezado por Thomas Lecocq, del Observatorio Real de Bélgica. De acuerdo con Lecocq y colaboradores, de manera continua se producen ondas sísmicas de muy pequeña amplitud por la interacción de la corteza terrestre con diversas fuerzas naturales; por ejemplo, por las fuerzas que las olas marinas producen sobre la superficie de la Tierra.  De la misma manera, hay actividades humanas que generan ondas sísmicas. Este es, ciertamente, el caso de las explosiones nucleares, pero también de actividades ordinarias como el transporte y la manufactura industrial. Para llevar a cabo su estudio, los investigadores hicieron uso de datos de sismicidad reportados a lo largo del presente año por 268 estaciones de medición en 155 países, incluyendo, tanto zonas urbanas como zonas aisladas con poca concentración de personas. De estas estaciones, 185 reportaron una baja en la actividad sísmica por efecto de la pandemia por hasta un 50 por ciento a lo largo de meses. Además, sin bien las bajas de sismicidad más acusadas se dieron en las zonas urbanas -de manera entendible-, éstas también se observaron en zonas con densidades poblacionales menores a una persona por kilómetro cuadrados. Lecocq y colaboradores hacen notar que el periodo de quietud sísmica que ha provocado la pandemia es el más largo y más prominente del que se tenga memoria.Normalmente, los movimientos sísmicos naturales se mezclan con los antropogénicos -es decir, producidos por las actividades humanas- dificultando su identificación. Como hacen notar Lecocq y colaboradores, durante la pandemia ha sido posible capturar movimientos sísmicos sutiles de origen natural que en condiciones normales hubieran estado mezclados con la sismicidad antropogénica. También señalan que el mayor conocimiento de la sismicidad natural de la Tierra obtenida durante el periodo de quietud sísmica -bautizado como antropopausa- podría ayudar a distinguir de mejor manera la actividad sísmica natural de la Tierra de aquella antropogénica y contribuir así a la predicción de sismos y desastres naturales.Igualmente, los investigadores hacen notar que un mayor conocimiento de la actividad sísmica natural de la Tierra permitirá, una vez que superemos la contingencia sanitaria, identificar más fácilmente las componentes antropogénicas de sismicidad terrestre y utilizarlas como un monitor para nuestra actividad sobre la superficie de la Tierra.  Así, la pandemia de coronavirus nos ha proporcionado herramientas que podrían ser de gran utilidad futura. Por un lado, el periodo de quietud sísmica que ha provocado nos ha permitido escuchar las vibraciones naturales de la Tierra sin la interferencia antropogénica y con esto contribuir de manera potencial a la prevención de sismos en el futuro. Por otro lado, ha puesto en la palestra a la sismicidad antropogénica como un monitor para nuestras actividades de movilidad. Por lo demás, si bien como dice el dicho, de los males el menos, con seguridad cualquiera de nosotros -de haber tenido la opción- hubiéramos gustosamente renunciado a las herramientas de marras si con ello nos hubiésemos evitado la pandemia. A la que no se le ve un fin en el corto plazo.",
    "“Vámonos haciendo menos”, rezaba el eslogan que el Consejo Nacional de Población lanzó en la década de los años setenta en un intento de mitigar el crecimiento poblacional del país que en esos momentos rebasaba el 3 por ciento anual. A este ritmo de crecimiento la población de México casi se duplicaba en 20 años y constituía un escollo para el desarrollo social del país En la actualidad, a pesar del eslogan –y de algunas caricaturas que hacían escarnio de los machos mexicanos que dejaban hijos regados por todas las colonias– no nos hemos hecho menos y seguimos creciendo. Lo hacemos, sin embargo, a  un ritmo considerablemente menor: aproximadamente el 1 por ciento anual.La reducción en el ritmo de crecimiento poblacional en los últimos cincuenta años ha sido un fenómeno global. Esta reducción, sin embargo, no ha sido uniforme y mientras que algunos países mantienen todavía una tasa de crecimiento poblacional mayor al 3 por ciento, otros han hecho realidad el eslogan de marras y su población está disminuyendo. Esto último afecta a países ricos, como es el caso de Italia  y Japón, por poner dos ejemplos.  Obviamente, un factor fundamental que determina que la población de un país crezca o disminuya es el número de nuevos nacimientos. Para llegar a un equilibrio poblacional cada mujer deber procrear en promedio 2.1 hijos. Por arriba de este número la población crecerá en términos absolutos y disminuirá por abajo del mismo.  En este último caso, al haber menos niños y personas jóvenes se incrementará la edad promedio de la población. Un artículo aparecido esta semana en la revista “The Lancet” discute en forma amplia las consecuencias económicas, sociales y geopolíticas que tendrán los cambios poblacionales a lo largo del presente siglo. Dicho artículo fue publicado por un grupo de investigadores de la Universidad de Washington en Seattle encabezado por Stein Emil Vollset.Stein y colaboradores modelan el crecimiento de la población a lo largo del siglo XXI y hacen predicciones sobre la evolución que experimentará dicha población en 195 países. Contrario a las predicciones de la Organización de la Naciones Unidas que pronostican que la población del mundo continuará aumentando desde su nivel actual de 7,800 millones hasta alcanzar 11,000 millones en el año 2100, Stein y colaboradores encuentran que muy posiblemente dicha población alcanzará un máximo de 9,700 millones en el año 2064, y después disminuirá paulatinamente hasta llegar a 8,800 millones en 2100.Los investigadores encuentran también que hacia el final del siglo XXI, en 183 países, incluyendo a México, la fertilidad será inferior a los 2.1 nacimientos por mujer necesarios para mantener el equilibrio poblacional. Todos estos países estarían entonces entrampados en un proceso de disminución de población, a menos que el abatimiento de la fertilidad sea compensada por la inmigración. En España, por ejemplo, la población disminuiría de los 46 millones actuales a 23 millones en 2100, mientras que en Italia la población lo haría de 60 millones a 30 millones.En los Estados Unidos, un país hecho de inmigrantes pero que en años recientes ha endurecido sus políticas en este respecto, la población tendría un aumento marginal al pasar de 324 millones en 2017 a 335 millones en 2100, alcanzando un máximo de 364 millones de 2060. México, por otro lado, pasará de 127 millones en 2017 a 146 millones en 2100, con un pico máximo de 170 millones en 2062.Los cambios poblacionales tendrían impactos económicos y modificarían la clasificación de las economías del mundo. Así, China adelantaría en 2050 a los Estados Unidos como la primera economía global, para después perder esta posición a favor de los Estados Unidos en 2100. Esto último debido a que China disminuirá drásticamente su población de los 1,400 millones actuales a 730 millones en 2100. La India por su lado, que contaría en 2100 con una población de 1,100 millones, sustancialmente más grande que la de China, se convertiría en el tercer país económicamente más poderoso desplazando a Japón.En este escenario de disminución del índice de fertilidad, Stein y colaboradores apuntan que la inmigración será un factor clave para mantener el nivel poblacional y la salud de las economías más grandes del mundo. Momento en el cual quizá podamos condicionar y controlar el flujo de inmigrantes mexicanos a los Estados Unidos. Al mismo tiempo que lanzamos el eslogan “vámonos haciendo más”.",
    "El pasado viernes 10 de julio se cumplieron 164 años del nacimiento de Nikola Tesla en la aldea de Smiljan en Croacia, en ese entonces parte del Imperio Astro-Húngaro. Para mayor precisión, el acontecimiento ocurrió justo a la media noche entre el 9 y el 10 de julio, según relata Margaret Cheney en su libro: “Tesla: Un hombre fuera de tiempo”.Es posible que el nombre de Nikola Tesla no nos resulte tan familiar como el de Thomas Alva Edison, mejor conocido este último como el inventor y empresario que comercializó la luz eléctrica. Es incluso posible que lo primero que nos venga a la mente al escuchar la palabra Tesla sea el automóvil eléctrico fabricado por la compañía norteamericana Tesla, Inc. En los hechos, no obstante, el nombre de esta compañía hace referencia a Nikola Tesla, quién fue un inventor cuyas ideas y desarrollos dieron forma en buena medida a la industria eléctrica actual.A través de las páginas del libro de Margaret Cheney se descubre a un Nikola Tesla singular y extravagante que, por ejemplo, aborrecía que las mujeres llevaran aretes, especialmente si eran de perlas, y que contaba el número de pasos que daba. Tenía igualmente la costumbre de calcular los centímetros cúbicos del plato de sopa o de la taza de café que tenía enfrente, so pena de no  disfrutarlos. En cuanto a sus habilidades intelectuales, tenía la singular capacidad de diseñar sistemas mecánicos y eléctricos enteramente en su mente, sin necesidad de recurrir a esquemas y planos. Esto último, por supuesto, limitaba su capacidad de interacción con otros colegas que no contaban con la misma habilidad.En los inicios de su carrera, después de emigrar a los Estados Unidos en 1884, Tesla trabajó para la compañía de Edison. Pronto, sin embargo, surgieron problemas. De acuerdo con Cheney, Edison habría encargado a Tesla el rediseño de los generadores de electricidad de la compañía y le habría ofrecido 50,000 dólares por el trabajo. Tesla puso manos a la obra por varios meses, con jornadas de trabajo que corrían desde las 10:30 de la mañana hasta las cinco de la mañana del día siguiente. Al terminar con el encargo de manera exitosa Tesla reclamó el pago ofrecido. Edison se lo negó con el argumento de que todo había sido una broma: “Tesla, tu no entiendes el humor americano”, le habría dicho. Nikola, sin embargo, fue poco receptivo a los dichos de Edison y renunció de manera inmediata a la compañía.Según escribe Cheney, el rompimiento entre los dos inventores podría haber sido anticipado, pues si bien Edison reconocía las grandes habilidades de Tesla como ingeniero eléctrico, en realidad desconfiaba de él. En efecto, recordemos que hacia el final del siglo XIX competían por el mercado de la electricidad dos sistemas: el esquema de corriente directa y el de corriente alterna. El sistema eléctrico desarrollado por Edison eran los de corriente directa, mientras que Tesla defendía al de corriente alterna. Después de dejar la compañía de Edison, Tesla se asoció con la compañía Westinghouse para desarrollar comercialmente el sistema de corriente alterna basado en sus patentes y con esto se inició la llamada “Guerra de las Corrientes”. Durante esta guerra Edison se defendió denunciando a la corriente alterna como altamente peligrosa, habida cuenta que su red de distribución emplea -en contraposición con la corriente directa- etapas de alto voltaje. Con este propósito, contrató a un grupo de escolares para que secuestraran perros y gatos del vecindario –pagando 25 centavos de dólar por cabeza- los cuales fueron electrocutados en su laboratorio con corriente alterna. Con esto Edison pretendía demostrar los peligros que enfrentan los humanos al entrar en contacto con la corriente alterna. Inventó incluso la palabra “Westinghoused” en referencia al acto de morir electrocutado por la corriente alterna de la compañía Westinghouse.       Al final, las ideas de Tesla, por razones técnicas, se impusieron a las de Edison y  con esto la industria eléctrica se decantó primordialmente por la corriente alterna, que es la  que actualmente tenemos en nuestras casas.Nikola Tesla vivió como una persona singular que, con todas sus extravagancias, tuvo un impacto enorme en el desarrollo de la  industria eléctrica. Murió, igualmente, de manera singular: solo y de trombosis coronaria en un hotel de Nueva York a los 86 años de edad. Sin hacer caso del letrero de “No molestar”, la recamarera accedió a la habitación y lo encontró muerto en su cama el día siguiente a su fallecimiento.",
    "¿Por qué las plantas son por lo general de color verde? Esta pregunta a primera vista parece frívola, pero en realidad está lejos de serlo, pues el color de las plantas obedece a principios físicos bien definidos relacionados con el color de la luz solar. Esto último de acuerdo con un artículo publicado el pasado 25 de junio en la revista “Science” por un grupo internacional de investigadores encabezado por Trevor Arp de la Universidad de California en Riverside.Para entender los argumentos de Arp y colaboradores, recordemos primeramente que el Sol contiene todos los colores del arcoíris, desde el rojo hasta el violeta pasando por el verde. No todos, sin embargo, están igualmente representados, siendo el verde el más prominente. Recordemos también que el color de un objeto está determinado por la luz que refleja, de modo que el color de las plantas es debido a que reflejan la luz verde de manera más eficiente que la luz de otros colores. Por otro lado, sabemos que las plantas cumplen una función esencial, la fotosíntesis, por medio de la cual sintetizan la materia orgánica que posibilita la existencia de vida en el planeta. Para impulsar la fotosíntesis, las plantas hacen uso de la radiación solar que incide sobre su superficie.Así las cosas, hubiéramos esperado que las plantas tuvieran cualquier color excepto el verde, dado que con este color se desperdicia la componente más abundante de la luz solar. Las plantas, no obstante, sabemos que son generalmente verdes, lo que nos lleva a preguntarnos por qué es así. ¿Por qué no son rojas o azules -o bien negras, que es ausencia de reflexión- para hacer un uso más eficiente de la radiación solar?En su artículo, Arp y colaboradores ofrecen una explicación a esta aparente contradicción. De acuerdo con estos investigadores, las plantas desperdician la luz verde porque esta es demasiado intensa y varía demasiado a lo largo del día. De hecho, esta intensidad en un determinado momento puede ser lo suficientemente alta para dañar a la planta. Sabemos por experiencia propia que la luz del Sol puede variar grandemente y resultarnos incómoda en una cierta circunstancia. Por suerte, nosotros muy probablemente podríamos ponerle remedio. Por ejemplo, si estamos a la intemperie a pleno sol a las doce del día, podemos movernos hacia un lugar con sombra y así sufrir menos. Con excepciones, las plantas no tienen esta posibilidad y han de recurrir a otro tipo de defensa. En el caso que nos ocupa, según nos explican Arp y colaboradores, las plantas adquieren un color verde y con esto rechazan una parte importante de la radiación solar. En realidad, según el artículo de referencia, las plantas hacen más que eso, pues para una operación eficiente del proceso de fotosíntesis resolvieron el problema que representa la variabilidad de la intensidad de la radiación solar, tanto hacia arriba como hacia abajo del nivel necesario.  Podemos quizá entender este problema si pensamos en un lavamanos que por un lado se llena de agua por medio de una llave, y por otro lado se desaloja por el caño. Si queremos mantener un cierto nivel de agua en el lavamanos, es necesario ajustar el flujo de agua que entra con aquel que se va por el caño. De otro modo, el lavamanos nunca se llenaría o bien se desbordaría. Aplicado esto al proceso de fotosíntesis, el flujo de agua entrante corresponde a la energía solar que incide sobre la planta, y el flujo saliente a la energía empleada en la fotosíntesis.  Para implementar el mecanismo de regulación de energía solar, las plantas desarrollaron dos tipos de captadores de energía, cada uno de ellos sensible a un solo color de la radiación solar. Estos colores fueron cuidadosamente escogidos por la naturaleza con el fin de amortiguar las variaciones de la energía solar que incide sobre la planta. Así, entendemos que las plantas son verdes, no por casualidad y sí por un diseño cuidadoso, para manejar la alta variabilidad de la radiación solar. En otro planeta, con una estrella más fría o más caliente que la nuestra es posible que las plantas no sean verdes, sino azules o rojas o de algún otro color. Esto, por supuesto, no es sino una especulación.Por lo demás, lo que no es especulación es el asombroso diseño de ingeniería del proceso de fotosíntesis patentado por la naturaleza.",
    "Hace dos domingos comentábamos en este espacio un artículo publicado en la revista “Proceedings of the National Academy of Sciences” (PNAS) de los Estados Unidos en el que se argumentaba sobre las bondades del uso de los cubrebocas como un medio para frenar el avance de la epidemia de COVID-19. Dicho artículo concluye que la principal vía de propagación de la epidemia es el aerosol generado por las personas infectadas al toser, estornudar o simplemente hablar, el cual se mantiene en el aire por largo tiempo. En tal caso, los cubrebocas pueden bloquear la emisión de aerosoles por una persona infectada y prevenir que una persona sana se infecte por el contacto con los mismos. Entre los autores del artículo de referencia se encuentra Mario Molina, quien compartió el premio Nobel de Química 1995 por la identificación de los compuestos químicos destructores de la capa de ozono en la atmósfera.Las conclusiones de Molina y colaboradores fueron rápidamente puestas en entredicho por un grupo de 45 investigadores en una carta enviada a la revista PNAS. En dicha carta, los signatarios afirman que el artículo “está basado en suposiciones falsas y adolece de fallas metodológicas de diseño”, y dada la “severidad y el alcance de los asuntos tratados y su enorme e inmediato impacto público”, demandan el artículo sea retirado de inmediato por la revista.    Para entender las objeciones que se ponen al artículo de Molina y colaboradores hay que mencionar que las conclusiones de mismo están basadas en un análisis del ritmo de crecimiento del número de casos de coronavirus en la ciudad de Nueva York y en el norte de Italia, antes y después de que fuera hecho obligatorio el uso del cubrebocas en lugares públicos. A través de este análisis, los investigadores encontraron que en la ciudad de Nueva York - al igual que en el norte de Italia- dicho ritmo de crecimiento  disminuyó significativamente una vez que se implantó el uso de cubrebocas en el mes de abril. En contraste, en el resto de los Estados Unidos, en donde los cubrebocas no habrían sido obligatorios pero sí otras medidas de contención de la pandemia, no hubo una disminución equivalente.  A partir de estos hechos, concluyen que la principal vía de propagación del coronavirus son los aerosoles que dispersan las personas infectadas y que otras medidas de contención no son por sí solas suficientes.  Los críticos del trabajo de Molina y colaboradores argumentan que sus conclusiones se basan en suposiciones demostrablemente falsas. Por ejemplo, arguyen que la afirmación que a partir del mes de abril la única diferencia en medidas de contención del virus entre la ciudad de Nueva York y el resto de los Estados Unidos fue la implantación de los cubrebocas en Nueva York es falsa. Argumentan, asimismo, que los autores del artículo no tomaron en cuenta el retraso que existe entre la implantación de una medida de mitigación y la reducción resultante en el número de nuevos infectados; ni tampoco el hecho de que la implantación de una medida pública no significa su inmediata adopción por toda la población. Denuncian los críticos, igualmente, lo que consideran son fallas metodológicas del estudio, las cuales serían atribuibles al hecho de que ninguno de los autores del artículo son expertos epidemiólogos.  Los autores del artículo, por su lado, argumentan que los críticos no lo entendieron realmente y que algunas de las críticas que han recibido han sido hechas empleando frases del artículo puestas fuera de contexto. Molina reconoce, por otro lado, que quizá el lenguaje que emplearon fue demasiado fuerte y con algunas frases exageradas, debiendo haber sido más cuidadosos con el lenguaje. Por lo demás, los autores del artículo se sorprenden por las increíbles e ingenuas críticas que han recibido, por el solo hecho –así lo ven- de que ninguno de ellos son expertos en COVID-19. El hecho que un artículo sobre la propagación del coronavirus haya sido escrito por investigadores especialistas en ciencias atmosféricos y no por expertos epidemiólogos no es algo necesariamente negativo. En cierto modo podría tener aspectos positivos si proporcionara una visión no ortodoxa de un problema de salud pública que ha rebasado a los expertos.En cualquier caso, todos, tirios y troyanos –incluyendo a los críticos del artículo de Molina y colaboradores- consideran que los cubrebocas– de un modo u otro- contribuyen a frenar la propagación de coronavirus. Lo que es un avance con respecto a lo que algunos expertos sostenían hace apenas un par de meses.",
    "Dado que los planetas en la Vía Láctea  similares al nuestro se cuentan por miles de millones, es de llamar la atención que un artículo publicado esta semana en la revista “The Astrophysical Journal” concluya que solamente 36 de ellos albergan una civilización inteligente que sería capaz de comunicarse a través del espacio interestelar. El artículo fue publicado por Tom Westby y Christopher Concelice de la Universidad de Nottingham en el Reino Unido. Para ser más precisos, Westby y Concelice establecen un rango entre  4 y 211 para el número de planetas que en nuestra galaxia albergan vida inteligente con la suficiente capacidad tecnológica para hacer notar su presencia a través del espacio. Dado que aun 211 es un número minúsculo en comparación con los miles de millones de planetas existentes en nuestra galaxia con las condiciones adecuadas para el desarrollo de la vida, se concluiría que el desarrollo de la vida inteligente es un fenómeno más bien raro, lo que explicaría que no haya podido ser detectada a pesar de las campañas que para este propósito se han montado a lo largo del último medio siglo.  El artículo de Westby y Concelice, sin embargo, ha provocado a bote pronto reacciones negativas en diversos artículos publicados en Internet. Esto no es de sorprender dado que para calcular el número de mundos habitados por seres inteligentes se requiere de informaciones básicas de las que no se dispone. La más importante, por supuesto es la prueba de la existencia de al menos una civilización aparte de la nuestra. Así, sin bien Westby y Concelice basan parcialmente sus resultados en datos objetivos sobre la velocidad de formación de estrellas en la Vía Láctea y en la detección de exoplanetas similares a la Tierra, por necesidad tuvieron que hacer suposiciones sobre otros aspectos desconocidos y en este sentido su trabajo en buena medida es especulativo.De este modo, y en virtud de que el único caso probado de desarrollo de vida inteligente en la Vía Láctea es justamente el que se ha dado en nuestro planeta, Westby y Concelice asumieron que el proceso que se dio en la Tierra es universal y que el mismo ha ocurrido en otras partes de la Vía Láctea. Asumieron también que, dadas las condiciones adecuadas, el desarrollo de la vida inteligente es inevitable. Así, como un fenómeno universal, la vida inteligente se habría desarrollado en todos aquellos planetas con las mismas condiciones de la Tierra, en un intervalo de tiempo entre los 4.5 y 5.5 miles de millones de años a partir  de su formación. Esto es, por supuesto, una especulación, con la que algunos no están de acuerdo.Una segunda suposición, también controvertida, es la relativa al intervalo de tiempo que una civilización ha gozado de la capacidad de comunicarse a través del espacio interestelar -como sabemos, nuestra civilización ha tenido esta capacidad solamente a lo largo de los últimos cien años-. Westby y Concelice asumen que una vez llegado a un cierto grado de desarrollo tecnológico de comunicación, las civilizaciones desarrollan también una capacidad de autodestrucción que eventualmente las llevan a desaparecer. En el caso de nuestra civilización, el fantasma de la autodestrucción gira alrededor de las armas atómicas y el calentamiento global. Westby y Concelice fijan en cien años el tiempo promedio que una civilización extraterrestre goza de la capacidad de comunicación interestelar antes de su autodestrucción.El corto tiempo de vida de cien años asumido por los investigadores para una civilización tecnológica los llevan a calcular el número reducido de 36 civilizaciones extraterrestres en la Vía Láctea con capacidad de hacernos saber de su presencia. Treinta y seis civilizaciones son, ciertamente, muy pocas para el tamaño de nuestra galaxia, que tiene un diámetro de unos 100,000 años luz –un año luz es la distancia que recorre la luz en un año-. En estas condiciones, la civilización inteligente más cercana a la Tierra estaría a unos 17,000 años luz, lo que, según Westby y Concelice, haría imposible que pudiéramos establecer comunicación con nuestros actuales medios tecnológicos. Así, carecen de sentido los esfuerzos que se hacen para escudriñar el espacio en busca de señales de civilizaciones extraterrestres. A menos que eventualmente se lograra detectar un mensaje proveniente de un  planeta similar al nuestro circulando alrededor de una estrella lejana. En cuyo caso habría que tirar al bote de la basura las suposiciones de Westby y Concelice.",
    "Hasta hace poco tiempo, el uso de cubrebocas como un medio para combatir la propagación de la pandemia de coronavirus fue controvertido. Por un lado, se argumentaba que si bien los cubrebocas bloquean en cierto grado  la dispersión de virus por parte de una persona infectada, en realidad no protegen a quien los lleva. En contraposición, George Gao, director general del Centro para la prevención de Enfermedades afirmaba categórico que los Estados Unidos habían cometido un gran error al no implantar el uso de cubrebocas desde el inicio de la epidemia. Como sabemos, hoy en día la controversia ha disminuido y los cubrebocas son ampliamente usados por la población. Y en apoyo a esta práctica, un artículo aparecido esta semana en la revista “Proceedings of the National Academy of Sciences” de los Estados Unidos proporciona evidencia científica sobre las bondades del cubrebocas como una herramienta simple para frenar el avance de la epidemia de coronavirus. Dicho artículo fue publicado por un grupo de investigadores de universidades norteamericanas -entre los que se encuentra Mario Molina, premio Nobel de Química- encabezado por Renyi Zhang de la Universidad Texas A&M.  En el artículo referido, Zhang y colaboradores presentan un análisis de la evolución temporal del número de infectados por COVID-19 en tres países: China, Italia y los Estados Unidos, que de manera sucesiva han sido epicentro de la pandemia. Como sabemos, la epidemia de COVID-19 se inició en China en el mes de diciembre de 2019 y de ahí se extendió a Italia y a otros países de Europa, y eventualmente a los Estados Unidos y al resto del continente americano. China tomó medidas rápidas y drásticas y logró frenar la propagación del virus en su territorio. Italia y los Estados Unidos, en cambio, tuvieron respuestas menos efectivas y el virus tuvo las consecuencias bien conocidas. Zhang y colaboradores enfocan su análisis en los cubrebocas y hacen notar que, mientras que China implantó su uso en la primera etapa de la epidemia, simultáneamente con otras medidas agresivas de distanciamiento social y de identificación de personas infectadas, la respuesta al virus en Italia y en los Estados Unidos no fue igual de contundente. De manera específica, los investigadores hacen notar que estos dos países no implementaron todas las medidas de mitigación al mismo tiempo y fue solo hasta el 6 de abril que Italia implantó el uso del cubrebocas en Lombardía –la región más fuertemente afectada por la pandemia-, mientras que en los Estados Unidos, el estado de Nueva York los hizo obligatorios hasta el 17 de abril. El retraso en las medidas para obligar al uso de los cubrebocas con respecto a otras medidas de mitigación, permitió a Zhang y colaboradores evaluar su impacto en la dispersión de la epidemia.Con argumentos convincentes, a partir de un análisis del crecimiento de infectados antes y después de la implantación de los cubrebocas, concluyen los investigadores que entre el 6 de abril y el 9 de mayo en Italia, y entre el 17 de abril y el 9 de mayo en Nueva York, los cubrebocas evitaron, de manera respectiva, 78,000 y 66,000 nuevas infecciones.De manera adicional, Zhang y colaboradores llegan a la conclusión que la principal vía de infección es el aerosol -partículas menores a 5 micrómetros- dispersado por una persona infectada al estornudar o toser, e incluso simplemente al hablar, que se mantiene suspendido en el aire por largo tiempo. Aquí habría que recordar que los expertos consideran que la trasmisión del virus puede darse también por gotas  de saliva mayores de 5 micrómetros dispersadas por la persona infectada. Estas gotas, por su tamaño, solo logran avanzar distancias pequeñas en el aire antes de caer y depositarse en el suelo o en alguna superficie. Esto último ha dado origen al concepto de “sana distancia”, que dicta mantenerse a una distancia más allá de la cual las gotas se mantienen suspendidas en el aire.Mantenerse a la “sana distancia”, sin embargo, no evitaría el contagio por la vía del aerosol. En este caso, como explican Zhang y colaboradores, la única defensa practicable nos la proporcionan los cubrebocas y el evitar, en la medida de los posible, lugares cerrados con aglomeraciones y sin ventilación. Y dado que la principal vía de contagio serían los aerosoles, quizá valdría la pena que consideremos seriamente estas opciones de defensa.",
    "Un artículo aparecido esta semana en la revista “Nature” da cuenta del descubrimiento en el estado de Tabasco de un sitio arqueológico con estructuras arquitectónicas de grandes proporciones y una antigüedad cercana a los tres mil años. El sitio incluye, además de otras estructuras de menores dimensiones, una enorme plataforma rectangular de casi un kilómetro y medio de largo y 10-15 metros de altura. El artículo referido fue publicado por un grupo de arqueólogos encabezado por Takeshi Inomata de la Universidad de Arizona, y en el que se incluye a María Belén Méndez Mauer de  la UNAM.El descubrimiento se llevó a cabo por medio de un levantamiento topográfico en el sitio conocido como Aguada Fénix, localizado a poca distancia de la frontera entre Tabasco y Guatemala. Dicho levantamiento fue llevado a cabo por la técnica LIDAR, la cual emite un pulso de luz láser hacia el punto de interés y mide el tiempo que dicho pulso –el eco- tarda en regresar. Una vez determinado este tiempo, la distancia al objeto se obtiene a partir de la velocidad –conocida- de la luz láser. El levantamiento topográfico de Aguada Félix se obtuvo montando el LIDAR en un avión y apuntando lateralmente el láser hacia diferentes blancos por medio de un espejo en la medida en que avanzaba, barriendo así toda el  área de interés. Como apuntan Inomata y colaboradores, las estructuras de Aguada Fénix habían permanecido ocultas debido a que su gran extensión y altura relativamente pequeña las hacía confundirse con el paisaje natural. El LIDAR, en contraste, no experimenta ninguna confusión y revela claramente la existencia de un sitio con numerosas estructuras arquitectónicas. La mayor de ellas, una plataforma rectangular alineada aproximadamente en la dirección norte-sur, de 1413 metros de largo y una elevación sobre el terreno de 10-15 metros. Algunas estructuras más pequeñas son también visibles sobre la superficie de dicha plataforma. En particular, sus bordes están coronados  por hileras  de plataformas rectangulares más pequeñas y en su centro es visible una estructura alargada de 400 metros de longitud. Además de la plataforma principal, el sitio incluye otras estructuras y calzadas que dan al conjunto una gran complejidad. Inomata y colaboradores fijan la construcción del sitio Aguada Fénix entre los 1000 y los 800 años antes de nuestra era. Para apreciar las dimensiones del sitio, hacen notar que el volumen de la plataforma principal excede substancialmente al de las pirámides construidas durante el periodo clásico en las tierras bajas mayas. Los investigadores, además, estiman que la construcción de Aguada Fénix habría requerido entre 10 millones y 13 millones de días-hombre de trabajo.Aguada Fénix está colocada en un punto intermedio entre las zonas de cultura olmeca de Tabasco y Veracruz y las tierras bajas en las que se desarrolló la civilización maya del periodo clásico. Con relación a esto, apuntan Inomata y colaboradores que si bien la plataforma de Aguada Fénix estuvo probablemente influenciada por las tradiciones del sitio de San Lorenzo que perteneció a la cultura olmeca, las tradiciones sociales  de ambas culturas diferían en cierto grado. En particular, en comparación con la cultura olmeca, las desigualdades sociales en Aguada Fénix no habrían sido tan marcadas, como lo indica la ausencia de esculturas que representan a individuos de alta posición social. Lo mismo podría decirse de la cultura maya -de la cual Aguada Fénix habría sido precursora- que desarrolló marcadas desigualdades sociales.De este modo, especulan los investigadores, mientras que en San Lorenzo y otras áreas olmecas las construcciones habrían sido realizadas por una mayoría bajo el mando de una clase social jerárquicamente superior, en Aguada Félix la participación de la población habría sido resultado de trabajo comunitario. Esto estaría reflejado en la horizontalidad de las construcciones de Aguada Fénix, en contraste con las pirámides mayas posteriores, cuya verticalidad facilitaba el acceso de sólo unos cuantos privilegiados. ¿Fue Aguada Fénix asiento de una cultura en la que las jerarquías sociales jugaban un papel relativamente menor? De ser así, sus logros arquitectónicos, que nos muestra el eco del LIDAR, serían una muestra de las posibilidades del trabajo colaborativo. Lo cual, por cierto, no sería algo de sorprender.",
    "El lanzamiento el día de ayer de una misión tripulada a la estación espacial internacional fue el primero de este tipo llevado a cabo por una compañía privada, Space X, en colaboración con la NASA. El acontecimiento es indicativo del papel creciente que jugarán en el futuro las compañías privadas en el espacio. ¿Cómo será la industria espacial en el futuro? Como sabemos, esta industria ha tenido un vertiginoso desarrollo en el último medio siglo, sin duda con muchos impactos positivos. Al mismo tiempo, sin embargo, el gran número de satélites puestos en órbita ha congestionado el espacio alrededor de la Tierra, no solamente con satélites en operación, sino también con satélites ya fuera de uso y restos de vehículos de lanzamiento y otros objetos  producto de la actividad espacial. Actualmente se sabe que aproximadamente 20,000 objetos artificiales giran alrededor de la Tierra. Más de 15,000 de estos objetos circulan en la llamada órbita terrestre baja, definida por una banda localizada entre los 160 kilómetros y los 2,000 kilómetros por arriba de la superficie de la Tierra.     El creciente número de satélites y objetos en órbita alrededor de la Tierra incrementa la probabilidad de colisiones entre ellos. De ocurrir, dichas colisiones elevarían el número de objetos sin control en órbita y por consecuencia la probabilidad de ocurrencia de más colisiones. Así, se podría disparar un proceso de generación de desechos espaciales en cascada que imposibilitaría el uso del espacio para el tránsito de satélites, según algunas especulaciones.Aun sin llegar a este extremo, la congestión del espacio introduce costos en la industria espacial por el riesgo de colisiones en órbita con la consecuente destrucción de los satélites involucrados. Para disminuir este riesgo se han propuesto varias alternativas, incluyendo la de hacer un levantamiento y determinar con precisión las trayectorias de todos los objetos de basura espacial. Conociendo dichas trayectorias sería posible anticipar una colisión y evitarla modificando el curso del satélite en peligro.  Se ha propuesto también remover de sus órbitas a los satélites obsoletos y demás objetos de basura espacial por diferentes técnicas, obligándolos a reingresar a la atmósfera en donde se desintegrarían por el rozamiento con el aire.Un artículo publicado esta semana en la revista “Proceedings of the National Academy of Sciences” de los Estados Unidos, sin embargo, afirma que estas propuestas, que han sido solo tecnológicas y de gestión, no atienden al problema de fondo y no darían resultados satisfactorios. El artículo fue publicado por un grupo de investigadores encabezado por Akhil Rao del “Middlebury College”, en el estado de Vermont, Estados Unidos.  Rao y colaboradores plantean que el problema de congestión espacial se origina porque los operadores de satélites no consideran los costos que imponen a otros operadores vía un incremento en el riesgo de colisión. En estas condiciones, limpiar las órbitas espaciales de basura solo incentivaría el lanzamiento de más satélites, hasta el punto en que el riesgo de perderlos iguale a su costo. En su lugar, los investigadores proponen el establecimiento de una “tarifa de uso de órbita” –similar al impuesto al  carbono, que desincentiva la generación de gases de invernadero- que corregiría el problema de incentivos.    En la propuesta de Rao y colaboradores, la tarifa por uso de órbita se fijaría en 2020 en 14,900 dólares norteamericanos por satélite por año. Dicha tarifa se incrementaría 14 por ciento anualmente hasta llegar a unos 235,000 dólares por satélite por año en 2040. Con este esquema, los investigadores estiman que se cuadruplicaría el valor de la industria espacial.  El congestionamiento de las órbitas espaciales alrededor de nuestro planeta corre en vías paralelas a las de otros fenómenos a escala global que han ocurrido a lo largo de las últimas décadas y que han hecho que el planeta Tierra -que en principio luce enorme-  nos haya quedado chiquito. Con la irrupción de las compañías privadas en el espacio, cabe preguntarse si empeorará la congestión espacial. O si bien, por medio de propuestas como la de Rao y colaboradores, se logrará limitar su crecimiento. Por lo demás, en la búsqueda de respuestas, quizá valga la pena preguntarnos en qué medida el impuesto al carbono ha contribuido a mitigar la contaminación ambiental.",
    "Un artículo publicado el pasado 8 de mayo en la revista “Science Advances” por un grupo de investigadores encabezado por Colin Raymond del Instituto de Tecnología de California, reporta datos alarmantes sobre la ocurrencia de episodios extremos de calor y humedad en diferentes lugares del mundo. En dicho artículo se reporta un análisis de datos proporcionados por cerca de 8,000 estaciones meteorológicas en el periodo 1979-2017. Los autores encuentran que los eventos climáticos en los que se combinan altos niveles de humedad con altas temperaturas ambientales son cada vez más frecuentes por efecto del calentamiento global.Como sabemos, la temperatura normal del cuerpo humano ronda los 36.5 grados centígrados, misma es mantenida por un balance entre el calor internamente generado por nuestro metabolismo y el calor disipado al medio ambiente a través de la piel. Con relación a esto último, hay que notar que la temperatura de la piel es de aproximadamente 35 grados centígrados, de modo que si la temperatura ambiental es menor a este valor, la piel no tiene impedimento para emitir calor hacia el medio ambiente y de esta manera mantener nuestra temperatura corporal. Si, por otro lado, la temperatura ambiental es mayor que la temperatura de la piel, no es posible que ésta disipe calor hacia el medio ambiente de manera directa. En estas condiciones, un segundo mecanismo entra en funcionamiento y el cuerpo libera el exceso de calor por medio del sudor. Dicho en pocas palabras, el sudor se evapora al estar en contacto con la piel y para esto necesita de una cierta cantidad de calor que extrae del cuerpo a través de la piel.  Hasta aquí, el mecanismo de disipación de calor de nuestro cuerpo funciona de acuerdo a como fue diseñado. El diseño, sin embargo, presupone ciertas condiciones ambientales que Colin y colaboradores encuentran se están violando cada vez más frecuentemente. Para entender esto, hay que tomar en cuenta que la velocidad con que se evapora el sudor depende de la humedad ambiental, y que ésta es más baja en cuanto más alta es la humedad del aire. De hecho, la velocidad de evaporación se anula teóricamente cuando la humedad ambiental es de 100%. Asumiendo que esto ocurre, una temperatura ambiental superior a los 35 grados centígrados constituye teóricamente la máxima compatible con la vida, al menos por periodos de tiempo relativamente largos –si bien, por supuesto, es posible soportar una temperatura mayor a los 35 grados centígrados con una humedad ambiental menor al 100%.En su estudio, Colin y colaboradores encontraron muchos casos en los que se alcanzaron combinaciones de humedad y temperatura cercanas a la máxima tolerable. Igualmente, encontraron dos localizaciones en las que se alcanzaron, en múltiples ocasiones, combinaciones humedad-temperatura incluso superiores a la máxima compatible con la supervivencia, si bien sólo por espacio de una o dos horas. Los valores de humedad-temperatura más altos se encontraron, entre otros sitios, en puntos localizados en la costa del golfo Pérsico, la costa este de la India, el norte de la India y Pakistán, el noroeste de Australia y la costa del mar Rojo. Se encontraron también altos valores de humedad-temperatura en la costa del golfo de California y en la costa del Golfo de México. En San Luis Potosí, específicamente en Matlapa, la combinación humedad-temperatura alcanzó, en algún momento, el máximo valor compatible con la vida. De acuerdo con Colin Raymond,  se anticipaba que en algunas décadas a partir de ahora crecerían en frecuencia los episodios extremos de humedad-calor en la medida en que avanzaba el calentamiento global. Los resultados de su análisis, sin embargo, demuestran que las proyecciones estaban equivocadas y que el fenómeno ocurre ya en la actualidad.Tenemos así una faceta más de los efectos que está generando el calentamiento global. En los países ricos, los episodios de humedad-calor pueden ser mitigados por medio del aire acondicionado. Esto no es necesariamente cierto con respecto a países con recursos limitados. Así, al igual a como ocurre con la actual pandemia, los más afectados pueden llegar a ser los países más pobres. O, dicho de otro modo: al perro más flaco muy probablemente se le cargarán las pulgas.",
    "En un comunicado de prensa del Programa de las Naciones Unidas para el Medio Ambiente del pasado 26 de noviembre, se puede leer: “En la víspera de un año en que las naciones deben fortalecer sus compromisos climáticos de París, un nuevo informe del Programa de las Naciones Unidas para el Medio Ambiente (PNUMA) advierte que, a menos que las emisiones mundiales de gases de efecto invernadero disminuyan un 7.6 por ciento cada año entre 2020 y 2030, el mundo perderá la oportunidad de encaminarse hacia el objetivo de temperatura de 1.5 grados centígrados del Acuerdo de París”.Como sabemos, en el Acuerdo Climático de París de 2016 se establece como una meta  limitar el incremento de la temperatura global al final del presente siglo, a un valor muy por debajo de los 2 grados centígrados con respecto a su nivel preindustrial. Igualmente, plantea realizar esfuerzos para que dicho aumento no sobrepase los 1.5 grados centígrados. La pandemia de coronavirus actualmente en desarrollo nos proporciona una medida de lo que significaría una reducción anual de 7.6 por ciento en la emisión de gases de invernadero como lo quiere el PNUMA. En efecto, un reporte de la Agencia Internacional de Energía, hecho público el pasado mes de abril, estima que, como resultado del freno económico provocado por la pandemia, habrá una reducción de un 8 por ciento en la emisión de dichos gases a lo largo del presente año. Como resultado, algunos lugares del mundo muestran ya una reducción sustancial de la contaminación atmosférica. Esto es evidente, por ejemplo, en imágenes satelitales del noroeste de China o del norte de Italia, tomadas antes y después de la contingencia. En algún sentido, la epidemia de coronavirus podría de este modo concebirse como un experimento llevado a cabo por la naturaleza que nos muestra fehacientemente que la contaminación atmosférica -y el consecuente calentamiento global- es producto de nuestras actividades económicas. Es interesante hacer notar que la cifra de 8 por ciento en reducción de gases de invernadero citada por la Agencia Internacional de Energía coincide aproximadamente con la cifra de 7.6 por ciento de reducción de emisiones fijada por el PNUMA como meta para los próximos diez años. Esto demuestra lo difícil que sería alcanzar las metas que persigue el Acuerdo de París. Ciertamente –como el experimento actualmente en desarrollo lo demuestra-, podríamos alcanzar dichas metas frenando la economía a los niveles de 2020. Esta opción, por supuesto, no es algo que de manera alguna podamos contemplar, so pena de morir, no por efectos de la contaminación, sino de inanición. El PNUMA, por otro lado, está urgiendo a empezar el programa de reducción de emisión de gases de invernadero a la brevedad, y al respecto apunta que si dicho programa se hubiera implementado hace diez años, habría sido necesaria una reducción de solamente el 3.3 por ciento anual. Sí, por otro lado, demoramos el inicio del programa hasta el año 2025, según la misma fuente, habrá necesidad de llevar a cabo reducciones anuales del 15.5 por ciento.  Un programa sostenible para reducir la emisión de gases de invernadero implica entonces una sustitución acelerada de fuentes de energía basadas en combustibles fósiles por fuentes menos contaminantes, incluyendo las energías eólica y la solar.Con todas sus consecuencias devastadoras, la epidemia de coronavirus nos ha traído al menos un punto positivo: nos ha mostrado un mundo menos contaminado hacia el que deberíamos transitar. Una vez superada la contingencia, sin embargo, la contaminación regresará a su normalidad. Y esta normalidad nos podría llevar a un incremento de temperatura global al final del siglo muy por encima de los 2 grados centígrados, pues en los últimos años las emisiones de gases de invernadero, lejos de disminuir, han aumentado. Por lo demás, vale la pena recordar que, según la Organización Mundial de la Salud, 4 millones de personas mueren anualmente por enfermedades asociadas a la contaminación atmosférica. Y que esto no produce, ni de lejos, el mismo nivel de ansiedad que la pandemia en curso.",
    "Como sabemos, con el fin de reactivar su economía nuestro vecino del norte está empezando a levantar las medidas de aislamiento por la epidemia de COVID-19, si bien cada estado de la unión siguiendo una estrategia particular. Los expertos, por otro lado, han advertido de posibles rebrotes epidémicos si no se toman las precauciones debidas, dado que el número de infectados es todavía muy elevado. En este respecto, el Centro de Ética de la Universidad de Harvard hizo público el pasado 20 de abril un documento intitulado “Hoja de ruta a la resiliencia pandémica”, en el que se propone una estrategia para reabrir la economía de los Estados Unidos sin que nuevos brotes epidémicos obliguen a implementar nuevas cuarentenas. Ante la ausencia de una vacuna o de tratamientos específicos para combatir el virus, las principales estrategias para frenar su expansión han sido la del aislamiento social, la higiene personal y la prohibición de eventos de asistencia masiva. Estas estrategias son las mismas que se emplearon hace cien años para frenar la propagación de la gripe española, y en ese sentido poco hemos avanzado.La estrategia del aislamiento parte de la suposición de que todos pudiéramos ser portadores del virus, aun siendo asintómaticos, y podríamos infectar a otras personas. A diferencia de la pandemia de gripe española, sin embargo, ahora existen pruebas que nos permiten diferenciar entre aquellos que son portadores del virus y aquellos que no lo son. De hecho, la estrategia que siguió Corea del Sur para mitigar el brote de COVID-19 durante los pasados meses de febrero y marzo fue la de aislar con rapidez a las personas infectadas, lo mismo que identificar a aquellos con los que tuvieron contacto y asilarlos también en caso de que resultaran positivos al virus. Entre otros factores, el éxito de Corea del Sur se basó en la realización masiva de pruebas para detectar personas infectadas.Siguiendo la experiencia coreana, la hoja de ruta recomienda realizar pruebas masivas, no solamente a aquellos con síntomas de la infección y a todos los que se presume podrían haber estado expuestos al virus –por ejemplo, los trabajadores de la salud-, sino también a sus contactos, incluyendo a aquellos que fueran asintomáticos. Todos aquellos que resultaran positivos  tendrían que ser puestos en cuarentena. El programa propuesto requeriría realizar de 5 a 20 millones de pruebas por día y sería acompañado de un seguimiento cuidadoso de la velocidad de trasmisión y prevalencia de la epidemia.Un elemento esencial de la hoja de ruta es el seguimiento de aquellos que entraron en contacto con una persona infectada. Esto incluye notificarlos sobre su exposición al virus, lo mismo que asegurar que, en su caso, realicen una cuarentena efectiva. Para esto se pueden emplear herramientas digitales como las usadas en su momento en China para el seguimiento de las personas a través de sus teléfonos celulares. En este punto, sin embargo, y a diferencia de lo que ocurre en China, las exigencias de privacidad que imperan en los Estados Unidos harían menos efectivas dichas herramientas.Llevar a cabo de 5 a 20 millones de pruebas diarias implicaría realizar un gran esfuerzo, aun para un país con los recursos de los Estados Unidos. En este contexto, ¿cuáles serían las perspectivas de México en el momento de terminar la cuarentena e iniciar la reactivación de la economía? Al respecto, el Centro de Ética de la Universidad de Harvard hizo público un documento en el que analiza la situación de los que denomina el “Sur global”, que incluye a la India, África y América Latina y que concentra el 40 por ciento de la población mundial. Las perspectivas planteadas por el documento para América Latina no son halagadoras y entre otras cosas recomienda que los países de la región hagan un esfuerzo conjunto para que la industria farmacéutica desarrolle la capacidad para producir los millones de kits de prueba necesarios, lo que se antoja difícil de llevar a cabo. Por otro lado, y en comparación con la India y el continente africano, la población de América Latina es mayormente urbana lo que en cierto modo es un punto positivo.En resumen, aun sumando puntos positivos, el panorama no luce halagador. Al menos según la Hoja de ruta a la resiliencia pandémica.",
    "Un artículo aparecido el pasado 30 de abril en la revista “Current Biology” nos trasporta 500 años hacia el pasado; de manera precisa, al inicio del periodo colonial de México, cuando se desataron una serie de epidemias que llevaron a la muerte al 90% de la población indígena indefensa ante los patógenos traídos por los conquistadores. Y al transportarnos hacia el pasado, el artículo nos ilustra sobre algo que ha permanecido en buena medida en la oscuridad: que la población de nuestro país tiene una apreciable herencia genética africana, producto de la importación de esclavos africanos a nuestro país durante el periodo colonial. El artículo en cuestión fue publicado por un grupo de investigadores de Alemania y México, encabezado por Rodrigo Barquera del Instituto Max Planck y de la Escuela Nacional de Antropología e Historia.En su artículo, los investigadores  reportan los resultados de un estudio llevado a cabo con restos humanos descubiertos en el centro de la ciudad de México, durante la construcción de la línea 8 del metro en los primeros años de la década de los noventa. De manera precisa, los restos fueron desenterrados en el sitio en el que estuvo localizado el Hospital Real de San José de los Naturales y que actualmente ocupa la estación San Juan de Letrán.El Hospital Real de San José de los Naturales fue fundado por Fray Pedro de Gante en 1531, apenas diez años después de la caída de Tenochtitlan. Como su nombre lo indica, el hospital fue fundado para atender a la población indígena. No de manera exclusiva, sin embargo, pues durante las excavaciones fueron desenterrados los restos de 600 personas, 20 de los cuales aparentaban tener un origen africano. Esto último, a juzgar por la forma de sus dientes, tallados en cono según la usanza de algunos lugares de África, y por sus características craneales. Aquí hay que hacer notar que el hospital incluía un cementerio, entre otras instalaciones.   Barquera y colaboradores dirigieron sus esfuerzos a confirmar el origen africano de tres de las personas cuyos restos fueron desenterrados en el sitio que ocupó el hospital.  Para esto, diseñaron una estrategia multidisciplinaria, empleando técnicas genéticas, osteológicas, e isotópicas para caracterizar los restos. Por medio de sus estudios genéticos, los investigadores confirmaron el origen africano, subsahariano, de los restos estudiados. Dichos restos habrían pertenecido a individuos muertos al inicio del periodo colonial a una edad alrededor de los 30 años. Además, habrían sido presumiblemente esclavos, a juzgar por el evidente maltrato físico que presentaban, incluyendo heridas de bala y una fractura de pierna mal soldada. Los investigadores encontraron también evidencias de las enfermedades que sufrieron en vida. Un individuo sufría de hepatitis B y otro de una enfermedad cercana a la sífilis, enfermedades que los investigadores concluyeron fueron contraídas antes de su llegada al continente americano.Esto último da una indicación del papel que el tráfico de esclavos africanos habría tenido en la introducción de patógenos desconocidos en el Nuevo Mundo, y que llevó a la casi extinción de la población indígena en la Nueva España. Con relación a esto, Barquera y colaboradores hacen notar que hasta 1779, año en el que fue prohibida la importación de esclavos, habían entrado a la Nueva España entre 130,000 y 150,000 esclavos africanos, que sin duda se mezclaron entre la población en general.Los investigadores, además, aventuran que la muerte de los individuos estudiados podría haber sido precisamente por una epidemia, si bien no tienen evidencias al respecto. Basan su hipótesis en que los individuos bajo estudio fueron desenterrados de una fosa común.Los resultados publicados por Barquera y colaboradores, empleando un enfoque multidisciplinario, son sólidos desde un punto de vista científico y arrojan luz sobre un hecho ocurrido hace cinco siglos: el tráfico de esclavos africano hacia nuestro país, que ha dejado una huella genética indeleble entre nosotros. Tráfico que, además de moralmente indefendible aun en época en que ocurrió, habría contribuido a disparar las epidemias que devastaron a una población indígena sin defensas ante los nuevos patógenos.",
    "¿Cuál es la probabilidad de morir golpeados por un meteorito? Ciertamente, no debe ser muy grande, dado que hay pocas noticias en la prensa al respecto. Ha habido casos, sin embargo, de encuentros cercanos con meteoritos. Se sabe, por ejemplo, que el 30 de noviembre 1954 Ann Hodges tomaba una siesta en su casa en el estado de Alabama cuando un meteorito del tamaño de una pelota de softbol perforó el techo del cuarto en el que se encontraba y después de rebotar impactó en su cadera produciéndole una escoriación en forma de hoja de árbol de más de veinte centímetros de largo. De manera afortunada, Ann sobrevivió al incidente y en una fotografía suya, que podemos encontrar en Internet, la vemos acostada mostrando su herida.En otro incidente ocurrido en octubre de 1992 en el pueblo de Peekskill, al norte de la ciudad de Nueva York, un meteorito del tamaño de una bola de boliche se precipitó sobre un automóvil estacionado, un Chevy Malibú 1980 de color rojo que resultó severamente dañado. Gracias a esto, el automóvil se convirtió en una celebridad, al grado que ha sido exhibido en museos a lo largo del mundo. De hecho, en el sitio oficial del “Automóvil del meteorito de Peekskill” se le ofrece precisamente para propósitos de exhibición.En otras ocasiones, los encuentros cercanos con los meteoritos no han sido tan afortunados. El caso más connotado es quizá el del meteorito de Chicxulub, que hace 65 millones de años acabó con los dinosaurios y provocó una extinción masiva de especies. En este caso, los dinosaurios sobre los que directamente aterrizó el meteorito resultaron sin duda muertos de manera instantánea. Habrían sido los menos, sin embargo, y la extinción masiva de especies sobrevino por las posteriores consecuencias climáticas que tuvo el meteorito.Dejemos de lado, no obstante, situaciones tan extremas y limitémonos a incidentes de muerte de personas directamente golpeadas por un meteorito. Encontramos que de manera notable, no hay casos comprobables en este sentido. En 2016 un hombre fue muerto en la India por lo que inicialmente se había pensado fue un meteorito. Hoy en día, sin embargo, se considera que no fue por un objeto extraterrestre lo que provocó el incidente. Las muertes ocasionadas directamente por el impacto de un meteorito son entonces muy escasas y una estimación de su probabilidad de ocurrencia es difícil de establecer.   En estas circunstancias, resulta interesante enterarnos que, finalmente, se ha encontrado la primera evidencia sólida de muerte producida directamente por un meteorito. Esta habría ocurrido el 22 de agosto de 1888 en Sulaymaniyah, Irak, entonces parte del Imperio Otomano, tal como se reporta en un artículo publicado por un grupo de investigadores encabezado por Ozan Unsalan de la Universidad Ege en Turquía.Durante el incidente estudiado por los investigadores, alrededor de las 20:30 horas se avistó en el cielo una gran bola de fuego y se escuchó una explosión, seguida de una lluvia de meteoritos que cayó sobre una aldea y que duró cerca de diez minutos. Como resultado, falleció un hombre y otro resultó con heridas que lo dejaron paralítico. Este relato fue descubierto por Unsalan y colaboradores en tres manuscritos oficiales del gobierno turco recientemente digitalizados, en los que las autoridades locales informan a las autoridades centrales sobre el incidente y les hacen llegar fragmentos del meteorito.Si bien por el momento no se han localizado estos fragmentos, los investigadores consideran que la evidencia que encontraron en los documentos oficiales es suficientemente sólida –por, precisamente, provenir de informes oficiales- para concluir que han encontrado la primera evidencia de un hombre muerto por un meteorito.      La dificultad para encontrar esta evidencia nos indica que tendríamos que ser extraordinariamente desafortunados para que un meteorito cayera justo en el lugar en el que estamos parados. Establecer la probabilidad de que esto ocurra es difícil, pero Clark Chapman del Instituto de Investigación del Suroeste, en Boulder, Colorado, aventura una cifra: la probabilidad de morir por el impacto de un meteorito es de 1 en 1,600,000. Para efectos de comparación, la probabilidad de morir por un rayo en una tormenta eléctrica es más de diez veces mayor. Podemos así vivir tranquilos sin preocuparnos de que el cielo nos pueda dar una sorpresa desagradable.",
    "Un rebaño de cabras salvajes deambulando a sus anchas por las desiertas calles de Llandudno -un pueblo costero en el norte de Gales-, puede ser visto como un ejemplo de recuperación por parte de los animales silvestres de espacios de los que los hemos excluido, al mismo tiempo que revela nuestra cercanía con la vida salvaje. En videos que podemos fácilmente encontrar en Internet, podemos ver a un rebaño de cabras peludas mordisqueando setos y flores, cruzando calles, o bien simplemente descansando echadas en bien cuidados jardines.   En realidad, el rebaño de cabras salvajes –alrededor de 120 animales- es bien conocido por los habitantes de Llandudno. Las cabras viven en una colina cercana y aparentemente no es inusual que bajen al pueblo en periodos de mal tiempo. Esta vez, sin embargo, la parálisis de Llandudno por la epidemia de coronavirus les ha facilitado las cosas y se han aventurado en el pueblo más allá de lo acostumbrado.Por lo demás, si bien, como mencionábamos líneas arriba, las cabras de Llandudno pueden ser vistas como un ejemplo de recuperación de hábitats, en un sentido estricto se podría alegar que en este caso la vida silvestre no puede reclamar derechos de propiedad. De hecho, los antecesores de las cabras habrían llegado a Llandudno desde Cachemira -en el norte de la India- apenas en el siglo XIX. La colina en la que viven, en contraste, ha estado habitada desde la edad de piedra, según podemos leer en la Wikipedia. Por otro lado, la invasión de Llandudno por las cabras de la colina no es el único ejemplo en los tiempos de coronavirus de la presencia de fauna silvestre en espacios urbanos en los que no era vista normalmente. Así, nos enteramos que la ciudad de Lopburi en Tailandia fue invadida por hordas de macacos en busca de comida. Estos macacos eran alimentados por los turistas que hoy han desaparecido. Una situación similar ocurre en Nara, Japón, en donde la ciudad ha sido invadida por grupos de venados. Avistamientos inusuales de coyotes, jabalíes, ardillas, y ratas han sido también reportados en varios lugares del mundo. Un caso extremo es el de un puma que fue encontrado vagando en las calles de Santiago de Chile. Nuestra cercanía con la vida silvestre, revelada por la toma oportunista que hace de los espacios urbanos que nos hemos visto forzados a abandonar, promueve, según los especialistas, la aparición de enfermedades infecciosas como la que ahora estamos padeciendo. En efecto, como sabemos, el virus del COVID-19, saltó desde una especie animal hasta nuestra especie, y no es difícil entender que la posibilidad de la aparición de enfermedades humanas de origen animal se incremente en la medida hagamos más estrecho nuestro contacto con otras especies. La aparición de un patógeno de origen animal, por otro lado, no es algo que haya sorprendido a los expertos. De hecho, ha sucedido en varias ocasiones en lo que va del siglo. Nunca, sin embargo, con las graves consecuencias de la pandemia actual, provocada por un virus que pareciera no ser particularmente letal, pero que sí se expande rápidamente.Dadas las particularidades del virus, en los últimos días el gobierno norteamericano ha considerado la posibilidad de que hubiera escapado del laboratorio de viriología existente en la ciudad de Wuhan, en contraposición con la explicación ofrecida inicialmente por el gobierno chino en el sentido de que el origen del virus habría sido un mercado en dicha ciudad en el que se comercializa fauna silvestre. Hay que notar que no se considera la posibilidad de que el virus hubiera sido fabricado, pues existe evidencia de que tiene un origen natural, pero sí que hubiera escapado por una falla de seguridad del laboratorio.  Del origen del virus, sin embargo, no hay evidencias claras. Esperemos que en un futuro no muy lejano las tengamos en un sentido o en otro. Al margen de lo anterior, para los expertos la frecuencia en la aparición de nuevas enfermedades infecciosas de origen animal está creciendo por la invasión que estamos haciendo de los hábitats de la fauna silvestre y el creciente contacto con la misma. Y para muestra basta el botón de las cabras de Llandudno.",
    "El pasado miércoles 8 de abril se levantó la cuarentena en la ciudad de Wuhan que había permanecido aislada desde el pasado 23 de enero. Algunos especialistas, sin embargo, expresan preocupación por la posibilidad de que aquellos que abandonan la ciudad y que son portadores asintomáticos del virus, provoquen brotes de la enfermedad en otras ciudades chinas. A su debido tiempo, y en la medida en que superen la epidemia, otros lugares del mundo enfrentarán peligros similares.Idealmente, con el fin de prevenir una segunda oleada de la epidemia, y en ausencia de vacunas o tratamientos médicos para combatir la enfermedad, las medidas de distanciamiento social tendrían que mantenerse hasta que el porcentaje de personas inmunes -por haberse ya infectado y recuperado- alcance un valor que los expertos estiman alrededor del 70%. Al alcanzarse este porcentaje, el virus solo podría infectar al 30% restante y esto sería insuficiente para mantener la epidemia. Así, idealmente, sería necesario determinar el grado de inmunidad de una población antes de declararla libre del peligro del COVID-19, y esto es algo que apenas está tomando forma. Una idea del grado de inmunidad alcanzado por las poblaciones afectadas por la epidemia nos lo da un estudio hecho público por un grupo de investigadores encabezado por Hendrik Streeck de la Universidad de Bonn. Dicho estudio -que no ha sido todavía publicado formalmente, sujeto a una evaluación por pares- fue llevado a cabo con pobladores del pueblo alemán de Gangelt, de aproximadamente 12,000 habitantes, situado en la frontera con Holanda. El estudio de referencia fue publicado en línea el pasado 9 de abril en el sitio de red del estado de Renania del Norte-Westfalia.  De acuerdo con Streeck y colaboradores, el inicio de la epidemia de COVID-19 en Ganglet habría ocurrido durante las celebraciones de carnaval el pasado 15 de febrero, durante las cuales hubo más de una ocasión para la trasmisión del virus. De hecho, se sabe que después del carnaval varias personas resultaron positivas al COVID-19, y con el tiempo todo Ganglet resultó fuertemente afectado por la epidemia.El estudio llevado a cabo por Streeck y colaboradores tuvo como propósito determinar el nivel de infecciones que priva en la actualidad en Ganglet, así como el porcentaje de personas que son inmunes. Para este propósito, los investigadores entrevistaron y tomaron muestras de la garganta de aproximadamente 1,000 personas de 400 hogares. En su artículo, reportan resultados obtenidos con 500 personas. Streeck y colaboradores encontraron que un 2% de las personas estudiadas estaban infectadas con el virus, mientras que un 14% portaban anticuerpos que indicaban que habían estado expuestos previamente al virus, algunos con síntomas de la enfermedad y otros asintomáticos. De manera interesante, encuentran que la tasa de mortalidad, basada en el número total de personas infectadas, es de 0.37%. Hacen notar que este porcentaje es cinco veces menor que el 1.98% estimado para Alemania por la Universidad Johns Hopkins y lo explican porque en sus cálculos están incluyendo el número total de infectados, graves, leves y asintomáticos. De lo anterior, Streeck y colaboradores concluyen de manera preliminar que un 15% de la población de Ganglet es inmune al virus del COVID-19 y ya no podría enfermarse. Si bien este porcentaje está lejos del 70% necesario para extinguir la expansión de la epidemia, de acuerdo con los investigadores reduce en un grado apreciable la velocidad con que avanza el virus. De este modo, en condiciones controladas y con reglas de higiene adecuadas -sin carnavales u otros eventos masivos que se sabe han catapultado el avance de la epidemia-, consideran que será posible abolir cuarentenas y darle vuelta a la hoja. Habría que mencionar, no obstante, que algunos especialistas no están de acuerdo en que la presencia de anticuerpos garantice inmunidad a la enfermedad, y en este escenario las conclusiones de Streeck y colaboradores pierden sustento.  Esperemos a los días por venir y crucemos los dedos porque no sea el caso.",
    "Un vistazo a las estadísticas de la epidemia de COVID-19 muestra que el número de infectados en Japón es relativamente bajo en comparación con países como Italia, España o los Estados Unidos. Esto a pesar de la cercanía de Japón con China y de haber reportado su primer caso el 16 de febrero, en una etapa temprana de la epidemia. Al 4 de abril, según la Universidad Johns Hopkins, el número de infectados con COVID-19 en Japón es de 2,935, con 69 muertos, muy por debajo de los países con los mayores números de casos. La velocidad de crecimiento de casos de infección en Japón ha sido, además, relativamente baja y contrasta fuertemente con la velocidad de crecimiento que se observa en algunos países occidentales.Los expertos no tienen una explicación sólida para esta diferencia, pero algunos la atribuyen, entre otros factores, al hecho de que en Japón es tradicional el uso de mascarillas para cubrir la nariz y boca como un medio para minimizar la posibilidad de contagio de enfermedades respiratorias en la temporada invernal. Uno de estos expertos es Jeremy Howard de la Universidad de San Francisco, quien es el iniciador de una campaña para el uso de mascarillas faciales para mitigar los contagios en la presente epidemia de coronavirus.  Howard hace notar que en países orientales en donde es común el uso de mascarillas faciales, lo que incluye a Japón, Hong Kong, y Singapur, la velocidad crecimiento de la epidemia de COVID 19 ha sido considerablemente menor que en países occidentales como Italia y España, en donde no es tradicional el uso de estas mascarillas. En ese sentido, George Gao, director general del Centro para la Prevención y el Control de Enfermedades de China, en una entrevista concedida a la revista “Science” el pasado 27 de marzo, afirmó que en su opinión “El gran error en los Estados Unidos es no usar mascarillas faciales”, pues “el virus es trasmitido por gotas de saliva a corta distancia”. En contra de la opinión de Gao, en algunos países occidentales se ha desincentivado el uso de mascarillas faciales por la eventualidad de que sean adquiridas por la población de manera masiva y se genere una escasez de mascarillas para uso médico. El argumento es que las mascarillas faciales no protegen al que las lleva, además de que un uso inadecuado de las mismas podría en realidad resultar contraproducente. Se argumenta, por ejemplo, que podría generar un falso sentido de seguridad que llevaría a descuidar otras medidas importantes.  A pesar de sus posibles desventajas, se sabe que las mascarillas faciales ayudan a minimizar la dispersión de virus. Aun más, un artículo aparecido el pasado viernes en la revista “Nature Medicine” encuentra que esto es posible no sólo con respecto a gotas de saliva relativamente grandes que viajan una pequeña distancia antes de caer al suelo -y de ahí el concepto de “sana distancia”-, sino también con respecto a gotas más pequeñas que permanecen suspendidas en el aire por más tiempo y que se dispersan con los virus a mayores distancias. El artículo de referencia fue publicado por un grupo de investigadores encabezado por Nancy Leung de la Universidad de Hong Kong. El trabajo reportado por Leung y colaboradores tuvo como propósito estudiar la dispersión de coronavirus, virus de la influenza y virus de la gripe común y se llevó a cabo antes de la aparición de la actual epidemia de coronavirus. No se estudió por tanto el virus del COVID-19. Consideran los investigadores, sin embargo, que sus resultados se extienden al coronavirus de la actual pandemia. Encuentran Leung y colaboradores que las mascarillas quirúrgicas ayudan a la limitar la dispersión del coronavirus y del virus de la influenza, pero no el de la gripe común. El uso extendido de mascarillas faciales -quizá no de la clase utilizada por el personal médico- ayudaría de este modo a limitar la dispersión del coronavirus, tanto por personas con síntomas como por infectados asintomáticos. Y no solamente de los virus de corto alcance, sino también de aquellos que viajan más allá de la sana distancia.Por lo demás, en cuanto se refiere a la actual epidemia, más vale que sosobre y no que fafalte.",
    "Si bien todavía está lejano el fin de la epidemia de COVID-19, cabe preguntarse cómo será la vida después del coronavirus. Ciertamente, México, dadas sus grandes limitaciones económicas, tendrá menos opciones para cambiar estilos de vida que los países con un mayor grado de desarrollo. Aun así, el rumbo que tomen los países industrializados tendrá con seguridad un impacto en nuestro país.Dado que no hay una vacuna en contra del coronavirus ni un tratamiento específico contra el mismo, con el fin de atenuar su propagación se nos pide minimizar los contactos personales. Así, y a pesar de los enormes avances médicos ocurridos a lo largo del último siglo, la situación no es demasiado diferente de la que prevalecía en 1918 durante la pandemia de la llamada gripe española. Los especialistas, no obstante, saben mucho más acerca del virus que provoca la pandemia actual que lo que en su momento se sabía del agente infeccioso responsable del brote epidémico de 1918, y esto con seguridad acelerará el desarrollo de una vacuna y de tratamientos en contra del coronavirus.Otra diferencia entre la epidemia actual y la de 1918 es la posibilidad de mantener contactos entre personas, por medio de la red Internet, a pesar del aislamiento físico. Es posible, por ejemplo y si la naturaleza de la actividad lo permite, realizar nuestro trabajo desde la casa. Es posible, también, realizar una reunión virtual entre personas, sin importar los lugares del mundo en los que se encuentren. Estas posibilidades no son nuevas y se han usado ya por un buen tiempo. La epidemia de coronavirus, sin embargo, podría catalizar su uso de manera más amplia y cotidiana.Una actividad en la que la comunicación a distancia es pertinente es la educación, en particular a nivel universitario. Con respecto a esto, y a raíz de la suspensión temporal de actividades académicas de la UASLP hace algunos días, un grupo de profesores nos vimos forzados a continuar con nuestros cursos de manera remota. En lo personal, para quien esto escribe, esta nueva forma de impartir clase constituyó una experiencia nueva. Con la ayuda pertinente, sin embargo, pudimos organizarnos y continuar con el curso de manera remota. Si bien es todavía muy temprano para evaluar el grado de éxito del arreglo forzado, no es posible negar que tiene aspectos positivos. Uno de estos es el relativo a la facilidad y naturalidad con la que los jóvenes estudiantes se han adaptado a las nuevas circunstancias. Debo confesar en una primera instancia no me fue evidente que así ocurriría. Ahora entiendo que, siendo los estudiantes nativos digitales, interactúan de manera natural con las computadoras y demás dispositivos informáticos. Se podría argüir también que, por su familiaridad con computadoras y teléfonos inteligentes, los estudiantes interactúan de manera más natural con este tipo de dispositivos que con el pizarrón tradicional del aula de clase. De hecho, un fenómeno que ya ocurre es que algunos estudiantes en lugar de anotar en un cuaderno lo que el profesor escribe en el pizarrón, de manera más conveniente le toman una fotografía con su teléfono celular. En un esquema de educación a distancia, el estudiante tendría acceso, no solamente a la imagen del pizarrón sino a todo el audio de la sesión de clase.Ciertamente, dependiendo del área que se considere, la educación presencial no puede ser sustituida por completo por la educación a distancia. Es el caso, por ejemplo, de programas educativos en los que un entrenamiento práctico es una parte integral del mismo.  La educación a distancia, sin embargo, complementada con sesiones presenciales, podría jugar un papel importante en el futuro.¿Podría la epidemia del coronavirus catalizar el uso extendido de las tecnologías informáticas para actividades tales como el trabajo en casa, las reuniones virtuales y la educación a distancia? Todo esto para minimizar el contacto entre personas y evitar la ocurrencia de una pandemia como la que nos aqueja en la actualidad. Posiblemente no nos tomará mucho tiempo averiguar qué es lo que nos depara el futuro. En los tiempos después del coronavirus.",
    "El mundo, sin duda, se nos ha hecho chico de diversas maneras. Podemos, por ejemplo, viajar en vuelo comercial desde Sidney, Australia, hasta Nueva York –casi la mitad de la circunferencia del planeta– en poco más de 19 horas. Esto hubiera sido impensable hace apenas un centenar de años. En otro orden de ideas, la emisión de gases de invernadero a la atmósfera por la quema de combustibles fósiles ha incrementado la temperatura del planeta en aproximadamente 1 grado centígrado en poco más de 150 años. Un incremento de esta magnitud no es en sí extraordinario dados los cambios en el clima de nuestro planeta ocurridos a lo largo de las últimas decenas de miles de años. Sí lo es, sin embargo, por la rapidez con la que ha ocurrido.La facilidad con la que podemos trasportarnos por miles de kilómetros a lo largo de la superficie de la Tierra y la influencia que hemos tenido sobre su clima, son dos hechos que demuestran que, efectivamente, el planeta se nos ha hecho pequeño. Podemos mencionar otras evidencias al respecto: la red Internet que permite la comunicación, prácticamente instantánea, entre dos lugares separados por miles de kilómetros, y la enorme isla de basura plástica del océano Pacífico con una extensión casi igual a la de nuestro país.El achicamiento del planeta es producto de desarrollos tecnológicos de diversa naturaleza, ya sea de comunicaciones, de transporte, de generación de energía o de manufactura, entre otras. Una tecnología capaz de hacer lucir a la Tierra con un tamaño todavía más reducido es la tecnología nuclear, desarrollada a partir de los descubrimientos científicos realizados en las primeras décadas del siglo pasado. Como bien sabemos, la tecnología nuclear es capaz de liberar energía en cantidades sin precedentes, y puede ser usada para producir bombas nucleares con un poder destructivo también sin precedentes. Las primeras bombas nucleares fueron desarrolladas por los Estados Unidos hacia el final de la Segunda Guerra Mundial y, en una acción que ha sido altamente controvertida, fueron usadas en contra de la población civil japonesa, destruyendo las ciudades de Hiroshima y Nagasaki. La Unión Soviética pronto desarrolló su propia capacidad para la fabricación de este tipo de bombas y con esto dio inicio la llamada Guerra Fría. A lo largo de la Guerra Fría, los Estados Unidos y la Unión Soviética desarrollaron bombas termonucleares con un poder miles de veces más grandes que las explotadas sobre las ciudades japonesas, y acumularon arsenales con decenas de miles de bombas nucleares.Se suponía que el tamaño del arsenal nuclear con el que contaban los Estados Unidos y la Unión Soviética, que llevaría a la destrucción de los dos países -y del mundo, por extensión- en el caso que uno atacara al otro, aseguraría que ninguno tomaría una iniciativa en este sentido. De cualquier modo, sin embargo, se desarrolló el concepto de “invierno nuclear”, que sobrevendría a una eventual guerra nuclear entre los dos países que pondría en serio peligro a los sobrevivientes, lo mismo que a otras especies en la Tierra.Con la desaparición de la Unión Soviética se dio fin a la Guerra Fría y con esto se disminuyó el riesgo de un enfrentamiento nuclear a escala global. No obstante, y dado que al menos ocho países cuentan con armas nucleares, existe la posibilidad que se den enfrentamientos nucleares a escala regional.  Particularmente, preocupa a los expertos la posibilidad de un enfrentamiento nuclear entre la India y Pakistán, ambos poseedores de arman nucleares y que mantienen un conflicto por la región de Cachemira.   Un artículo aparecido esta semana en la revista “Proceedings of the National Academy of Sciences” de los Estados Unidos hace un análisis del impacto que sobre nuestro planeta, en particular en la producción de cereales, tendría un conflicto nuclear entre la India y Pakistán. El artículo fue publicado por un grupo internacional de investigadores encabezado por Jonas Jagermeyr de la Universidad de Chicago.Jagermeyr y colaboradores consideran un enfrentamiento en el que cada país lanza 50 bombas nucleares del tamaño de la lanzada sobre Hiroshima, lo que representa un uno por ciento de arsenal nuclear a nivel mundial. Los investigadores asumen que, como resultado del bombardeo de las ciudades, los fuegos resultantes lanzan a la estratósfera 5 millones de toneladas de hollín. Éste se extenderá por toda la atmósfera, absorbiendo la luz solar y produciendo un descenso en la temperatura del planeta por un grado centígrado a lo largo de cinco años. El descenso de temperatura afectará la producción de maíz, trigo, soya y arroz, que descenderá un 11 por ciento en promedio. El efecto sobre los cultivos de cereales será más pronunciado en las regiones del medio oeste de los Estados Unidos, lo mismo que en Canadá y particularmente en Rusia. Paradójicamente, sin embargo, serán más afectados países no desarrollados localizados más al sur y que dependen de la importación de cereales de los países desarrollados. En este sentido, unos 70 países en desarrollo, con una población de 1,300 millones de habitantes, verán reducidos su disponibilidad de cereales en un 20 por ciento.De este modo, un conflicto nuclear regional con apenas el uno por ciento del arsenal nuclear mundial, ocasionaría hambrunas a nivel global. Una muestra más de la pequeñez y debilidad del planeta, y de nuestra capacidad para infringirle daños. Incluso mediante una tecnología relativamente antigua. Pero ciertamente de un gran poder destructivo.",
    "Los últimos días hemos sido sorprendidos por las compras de pánico que se han dado ante el esperado, y después confirmado, arribo del coronavirus de Wuhan a la ciudad. Es comprensible, por supuesto, que ante un peligro desconocido -y del comportamiento del coronavirus todavía hay cosas por averiguar- se produzcan alarmas que lleven a comprar y acumular víveres y productos de limpieza en previsión de que la epidemia nos obligue a permanecer encerrados en nuestras casas. Es, no obstante, difícil entender que un producto que está teniendo gran demanda sea el papel higiénico. Fenómeno que, por lo demás, ocurre a escala mundial.    Sin duda, el papel higiénico es un producto indispensable. A juzgar por las imágenes que puede uno encontrar en Internet, sin embargo, se está adquiriendo en cantidades que se antojan excesivas. Buscando en Internet se pueden encontrar explicaciones para el fenómeno, ofrecidas por especialistas en la materia. Así, las compras masivas de papel higiénico darían a quien las hace una sensación de control sobre la epidemia. No queda claro, sin embargo, por qué precisamente el papel higiénico es capaz de producir esa sensación y no, digamos, los tapetes de baño, por poner un ejemplo.Otras explicaciones van en otro sentido: ante un peligro desconocido, la gente compra lo que otros compran como mecanismo de defensa. Esto suena razonable, pero habría que explicar cómo se inició todo el proceso. Es decir, por qué a alguien en algún momento se le pudo ocurrir que el papel higiénico es una defensa en contra del coronavirus. Por lo demás, explicaciones aparte, el exceso en la compra de papel higiénico y la escasez que esto podría ocasionar no es un asunto demasiado serio y podría solucionarse de diversas maneras. Por ejemplo, la edición del pasado 5 de marzo del periódico australiano NT News apareció con ocho páginas en blanco. De manera explícita, para que fueran usadas en caso de necesidad.Mas seria es la epidemia misma que se está propagando a gran velocidad. En este respecto, algo que alarma a los expertos es la posibilidad de que una persona infectada pueda trasmitir el virus aun antes de mostrar los primeros síntomas de la enfermedad. Un artículo depositado en el sitio de preimpresos medRxiv el pasado 8 de marzo apunta a esta posibilidad. Dicho artículo fue escrito por un grupo de investigadores encabezados por Ganyani Tapiwa de la Universidad Hasselt en Bélgica.En base a un estudio de brotes de COVID-19 ocurridos en Singapur y en Tianjin, China, Tapiwa y colaboradores concluyen que el 48% de las infecciones ocurridas en Singapur se dieron en una etapa pre-sintomática, mientras que en Tianjin esta cifra se eleva hasta el 64%.  Hay que hacer notar, no obstante, que el artículo de referencia no ha sido todavía evaluado por pares, por lo que sus resultados deben tomarse con precaución.De un modo u otro, lo que sí es un hecho es que la epidemia se propaga a gran velocidad y que aquellos que somos poco versados en materia de epidemias nos preguntamos sobre cuál será la mejor defensa en contra del coronavirus. Los especialistas nos dan una respuesta: debemos lavarnos las manos con agua y jabón de manera frecuente y evitar tocarnos la cara. Debemos también limpiar las superficies de los muebles con las que estemos en contacto.La base para estas recomendaciones es simple: una persona infectada expulsa al toser o estornudar gotas microscópicas de saliva con el virus que, o se dispersan en el aire o se depositan en las superficies que encuentran a su paso. Una persona sana puede infectarse si estas gotas alcanzan su nariz o boca. O bien, si con las manos recoge virus depositados en alguna superficie u objeto y después se lleva la mano a la boca, nariz u ojos.  La efectividad de estas vías de contagio dependerá de la estabilidad del virus en el aire o en las superficies en donde se depositó. Al ser el coronavirus de Wuhan un virus nuevo, esta estabilidad no es conocida. Un artículo depositado en el archivo medRxiv el pasado viernes, sin embargo, nos ofrece una respuesta. Dicho artículo fue escrito por un grupo de investigadores adscritos a instituciones de investigación norteamericanas, encabezado por Neeltje Van Doremales del Instituto Nacional de Alergia y Enfermedades Infecciosas. De acuerdo con Doremales y colaboradores, el coronavirus de Wuhan puede persistir en el aire hasta por tres horas, mientras que depositado en una superficie de cobre lo hará hasta por cuatro horas, y hasta por 2-3 días sobre plástico o acero inoxidable. Con estos resultados, concluyen los investigadores que una vía plausible de transmisión del coronavirus de Wuhan es, efectivamente, a través de las gotas microscopias de saliva expulsadas por la persona enferma. Hay que notar, sin embargo, que el artículo referido está en una etapa de revisión por pares.Por lo demás, los resultados de Doremales y colaboradores están en línea con las recomendaciones de los expertos: evitar el contacto cercano con una persona enferma, lavarse frecuentemente las manos con agua y jabón, desinfectar las superficies y objetos con los que estemos en contacto, y evitar tocarse la cara con las manos. En cuanto a cómo evitar las infecciones de portadores asintomáticos -en el caso de que se diera el fenómeno-, los especialistas no tienen posiblemente todavía una respuesta.Para terminar, podemos quizá señalar que, en las circunstancias en las que nos encontramos, las compras de pánico que acaban con el papel sanitario en las tiendas resultan es un mal menor. Mal que no iría más allá de que, aquellos que no acumulamos papel higiénico, nos viéramos desprovistos de lo que hoy es un artículo de primera necesidad. Situación que, con seguridad, tendría una pronta solución.",
    "Si nos preguntaran por una mujer que ha hecho las más importantes contribuciones al avance del conocimiento científico, Marie Curie posiblemente sea la respuesta más frecuente. Y no sin razón, pues por su trabajo científico sobre elementos radioactivos, Marie Curie fue la primera persona en recibir dos premios Nobel, el de física en 1903 y el de química en 1911. Estos descubrimientos contribuyeron a establecer a la física como ciencia dominante durante la primera mitad del siglo XX. Marie Curie, además, llevó a cabo sus descubrimientos en condiciones desventajosas, primero por ser extranjera en Francia, en donde residía, y segundo por ser mujer. De hecho, ella no fue inicialmente nominada al premio Nobel de 1903. En dicha nominación solo aparecían su marido Pierre Curie y Henry Becquerel. No obstante, al enterarse Pierre Curie de que su esposa no estaba nominada, declaró que en su caso rechazaría el premio. Con esta presión, Marie Curie fue finalmente incluida en la nominación.Si la primera mitad del siglo XX fue una época dorada para la física, la segunda mitad lo fue para la biología. Esto, por el descubrimiento de la estructura del ADN al inicio de la década de los años cincuenta, y en el que una mujer jugó también un papel central. Es el caso de Rosalind Franklin, fisicoquímica británica cuyas mediciones de rayos X dieron la clave para el descubrimiento de la estructura de doble hélice de la molécula de ADN.     Al igual que Marie Curie, Rosalind Franklin realizó un trabajo científico de una enorme importancia. Este trabajo, sin embargo, no le ha dado una presencia pública tan notoria como Marie Curie debido a circunstancias desafortunadas por las que atravesó a lo largo de su carrera profesional. La más grave de todas: su muerte prematura a la edad de 37 años afectada de cáncer de ovario.Rosalind Franklin es también recordada por circunstancias que han sido asociadas a su condición de mujer y que se dieron alrededor del descubrimiento en 1953 de la estructura del ADN ya mencionado. En dicho descubrimiento participaron James Watson y Francis Crick de la Universidad de Cambridge y Maurice Wilkins y la propia Rosalind Franklin del King´s College de Londres. La historia es la siguiente.El 25 de marzo de 1953, Watson y Francis publicaron un artículo en la revista “Nature” en el que propusieron la estructura de doble hélice hoy aceptada por los especialistas. Dicho artículo estuvo acompañado por dos artículos más, uno de ellos con Wilkins como coautor y el otro con Franklin como coautor. En este último artículo se reportaban las mediciones con rayos X de la molécula de ADN llevadas a cabo por Rosalind y un estudiante doctoral.El proceder de Watson y Crick en el momento de reportar su descubrimiento a “Nature”, sin embargo, ha sido muy criticado. En efecto, como ellos mismos reconocieron posteriormente, para idear la estructura que propusieron para el ADN, Watson y Crick hicieron uso de los resultados de rayos X de Rosalind Franklin que les fueron mostrados por Wilkins, sin que ésta se enterara. Dicho uso, además, no fue reconocido en el artículo original, violando así principios fundamentales de la práctica científica.La controversia sobre el proceder de Watson y Crick se avivó cuando el primero publicó en 1968 el libro “La doble hélice” en la que da cuenta de los acontecimientos que llevaron al descubrimiento de la estructura del ADN. En dicho libro, Watson se refiere a Rosalind como una investigadora que no sabe interpretar sus propios resultados de rayos X, poco dispuesta a cooperar con Wilkins -que no era su jefe-. Además, según Watson, “Bastaba verla para saber que no se doblegaría con facilidad. Se abstenía deliberadamente de realzar sus cualidades femeninas. Aunque sus rasgos eran algo angulosos, no carecía de atractivo, y si hubiera prestado un poco más de interés a su modo de vestir habría resultado deslumbrante. Pero no lo hacía. Nunca había carmín en sus labios que contrastara con sus negros cabellos y, a sus treinta y un años, su atuendo no demostraba más imaginación que las de las adolescentes inglesas de medias azules”.Hoy en día, estas palabras resultan sorprendentes en boca de un investigador galardonado con un premio Nobel y coautor de unos de los mayores descubrimientos científicos del siglo XX. Hay que recordar, sin embargo, que en el tiempo en el que se descubrió la estructura del ADN, en el King´s College existía un comedor exclusivo para hombres, lo que también no deja de sorprender. Por el descubrimiento de la estructura del ADN a Watson, Crick y Wilkins les fue otorgado el premio Nobel de medicina y fisiología 1962. Para entonces, Rosalind Franklin tenía cuatro años de haber muerto y no podría haber sido candidata al premio que no puede ser otorgado a una persona ya fallecida. De haber estado viva, habría sido Rosalind Franklin distinguida con un permio Nobel. La pregunta es ociosa, por supuesto, pero habría que recordar que el premio Nobel solo puede ser otorgado a un máximo de tres personas, de modo uno de los cuatro, Watson, Crick, Wilkins o Franklin tendría que haber quedado fuera. Rosalind, por otro lado, podría quizá también haber sido nominada al premio Nobel de Química.Por lo demás, la situación ha cambiado en el curso de unas pocas décadas y hoy en día la investigación científica se considera una profesión que puede ser practicada tanto por hombres como por mujeres. De manera afortunada, pues de otro modo se desperdiciaría la mitad de la inteligencia del mundo.",
    "Como bien recordamos, el 10 de noviembre de 2019 Evo Morales renunció a la presidencia de Bolivia. Lo hizo después de protestas ciudadanas por un supuesto fraude electoral durante las elecciones presidenciales llevadas a cabo tres semanas antes. Un actor central en los acontecimientos que llevaron a la renuncia de Morales fue la Organización de los Estados Americanos (OEA). Este organismo denunció que hubo una manipulación de votos para favorecer la reelección del presidente.  De acuerdo con la legislación electoral boliviana, un candidato que obtenga más de 50% de los votos emitidos gana la elección presidencial. Lo mismo ocurre si obtiene entre el 40% y el 50% de votos, con cuando menos un 10% de diferencia con el segundo lugar. De otro modo, el primer y segundo lugares en la elección tendrán que enfrentarse en una segunda vuelta para definir al ganador.      El día de la elección, el conteo rápido de votos -no oficial- se suspendió a las 19:40 con un 84% de casillas electorales contabilizadas. Los resultados daban a Evo Morales más del 40% de los votos emitidos y un margen de aproximadamente 8% sobre su más inmediato perseguidor. Esto indicaría que el primer y segundo lugares en la elección tendrían que enfrentarse nuevamente antes de definir la contienda. Sin embargo, al resumirse el conteo rápido al día siguiente por presiones de la OEA, con datos de un 95% de actas electorales, el margen de Morales sobre su inmediato perseguidor se incrementó hasta una cifra ligeramente mayor al 10%. Los números del conteo rápido fueron confirmados posteriormente por el conteo oficial y con esto Morales resultó el ganador oficial de la elección presidencial.La OEA expresó su preocupación por el cambio en el porcentaje por el cual Morales aventajaba al segundo lugar, antes y después de la interrupción del conteo rápido. La OEA encuentra inexplicable este cambio, implicando que hubo una manipulación de votos y poniendo en duda los resultados de la elección.  En estas circunstancias, Morales aceptó ir a una segunda vuelta para definir la situación. No pasó mucho tiempo, sin embargo, para que renunciara a la presidencia presionado por los militares.Los argumentos de la OEA sobre manipulación de votos en la elección boliviana son debatidos por un artículo publicado esta semana en el periódico “The Washington Post”. Dicho artículo es firmado por John Curiel y Jack Williams del Instituto Tecnológico de Massachusetts y en el mismo se presentan y analizan datos estadísticos de la elección boliviana que desmienten los argumentos de la OEA.   En su análisis, Curiel y Williams consideran solamente datos del 84% de casillas contabilizadas entes de la interrupción del conteo rápido. Consideran, además, sólo aquellas casillas que fueron verificadas tanto antes como después de la interrupción. Estas casillas -1477- representan un 30% del total. La idea central del análisis es la siguiente: si la interrupción en el conteo rápido hubiera sido usada para instrumentar una manipulación del voto, tendría que haber una inconsistencia en la tendencia observada antes y después de la interrupción de dicho conteo.Curiel y Williams encuentran, sin embargo, que no hay tal inconsistencia, concluyendo que los resultados finales de la elección son consistentes con la tendencia observada antes de la interrupción del conteo. Concluyen que muy probablemente Evo Morales ganó la elección con un margen superior al 10%, tal como indican los resultados oficiales.   Por supuesto, para que Evo Morales pasara del margen de 8% observado con el 84% de las casillas, al margen final de 10%, fue necesario que en el último 16% de casillas se incrementara la preferencia por su candidatura. Con este respecto, Curiel y Williams hacen notar que las últimas casillas en ser contabilizadas son las rurales, que tardan más tiempo en ser contabilizadas y en las cuales es mayor la preferencia por Morales. Por supuesto, si bien con argumentos estadísticos no es posible hablar de certezas sino de probabilidades, habría que considerar que hay de probabilidades a probabilidades. Así, cuando la probabilidad de un suceso es suficientemente pequeña, podemos esperar que nunca ocurra, mientras que una probabilidad suficientemente grande se convierte en casi una certeza. A manera de ejemplo, pocos planean su vida pensando que ganaran el premio mayor de la lotería, suceso que tiene una probabilidad de ocurrir apenas de una parte en 3,000. Al mismo tiempo, muy pocos se animan a jugar a la ruleta rusa, pues la probabilidad de terminar muerto es de una parte en seis. En cuanto a los resultados de Curiel y Williams -basados en el análisis estadístico de casi 1,500 casillas- la probabilidad de que correspondan a la realidad estaría más cercana a la probabilidad de morir jugando a la ruleta rusa, que a la probabilidad de ganar el premio gordo de la lotería. Así, si bien es cierto que Evo Morales se excedió buscando un cuarto periodo presidencial, es bastante probable que haya ganado la elección y, en consecuencia, que la OEA se haya excedido en sus funciones. Por decir lo menos.",
    "De acuerdo con las estadísticas de la Organización Mundial de la Salud, el número de nuevos casos de coronavirus -bautizado COVID-19- en China estaría disminuyendo. Así, el 22 de febrero se diagnosticaron 397 nuevos casos, en comparación con los más de tres mil casos diarios que llegaron a diagnosticarse en los primeros días de febrero. En contraposición, han aparecido brotes de la enfermedad en otros lugares del mundo cuyo rápido crecimiento preocupa a los expertos, particularmente en Corea del Sur, Irán e Italia. En Corea del Sur se reportaron el 22 de febrero 142 nuevas infecciones, con lo que el número total de casos de coronavirus en ese país se elevó a 346. La mayor parte de las nuevas infecciones ocurrieron en la ciudad de Daegu, de dos y medio millones de habitantes, al sur de la capital Seúl.Según los especialistas, el coronavirus tiene su origen en fauna silvestre -posiblemente murciélagos- y de ahí habría sido trasmitido por un intermediario a los humanos. Aunque no hay una certeza, un mercado de la ciudad de Wuhan en el que se venden animales vivos, ha sido señalado como el posible punto de origen de la epidemia. Algunos medios, por otro lado, han dado a la opinión experta diversas interpretaciones. El periódico británico Daily Mail, por ejemplo, contribuyó a hacer famosa a una joven china al publicar un video en el que aparece en un restaurante sofisticado dando una mordida al ala de un murciélago -al que sostiene con un par de palillos chinos-. Según el Daily Mail, el animal consumido por la joven aparentemente era parte de un plato con sopa de murciélago colocado en el centro de la mesa. Al lado de los puntos de vista de los especialistas, se han dado también diversos rumores y “teorías de conspiración” con respecto al origen el coronavirus -o sea, explicaciones que van en contra de la explicación oficial, o la que es asumida por la mayoría-. Una teoría de conspiración sostiene que el coronavirus no tiene un origen natural, sino que fue desarrollado en un laboratorio de la Academia de Ciencias de China que tiene su asiento en la ciudad de Wuhan. Dicho laboratorio trabaja con organismos patógenos, en particular con el coronavirus de Wuhan, mismo que habría sido dispersado por un error entre la población. Dicha opinión ha sido sostenida incluso por un destacado científico español, según informó el periódico El País. Tom Cotton, senador republicano por el estado norteamericano de Arkansas, hace también eco de la posibilidad de que el coronavirus -que sería parte de un programa de desarrollo de armas químicas por parte del gobierno chino- hubiese efectivamente escapado del laboratorio de Wuhan. En palabras de Cotton, según el periódico New York Times, “No tenemos evidencia de que esa enfermedad se originara ahí, pero dada, para empezar, la duplicidad y deshonestidad de China, necesitamos cuando menos hacer la pregunta y ver que es lo nos dice la evidencia, y China en estos momentos no está proporcionando ninguna evidencia en absoluto”. El laboratorio de Wuhan, por su parte, niega cualquier participación en la epidemia de coronavirus y en este respecto es apoyado por un grupo de especialistas quienes publicaron un escrito esta semana en la prestigiada revista médica “The Lancet”, en el que condenan los rumores acerca de un origen artificial de coronavirus y expresan su apoyo a los científicos, médicos y trabajadores de la salud chinos que están luchando en contra de la epidemia. Los autores del comunicado son científicos de áreas de la salud adscritos a universidad e instituciones científicas de los Estados Unidos, Alemania, Gran Bretaña, Australia, Holanda, Malasia y Hong Kong.En su escrito, los especialistas expresan que “han observado que los científicos, profesionales de la salud pública y médicos profesionales chinos, han trabajado de manera diligente y efectiva para rápidamente identificar el patógeno que está detrás de este brote, poner en marcha medidas significativas para reducir su impacto, y compartir sus resultados de manera transparente con la comunidad global de la salud. Su esfuerzo ha sido notable”.Los especialistas también afirman que: “La rápida, abierta, y transparente comunicación de los datos de los científicos chinos sobre este brote, está ahora amenazada por los rumores y desinformación acerca de sus orígenes. Estamos todos de acuerdo en condenar enérgicamente las teorías de conspiración que sugieren que el COVID-19 no tiene un origen natural. Científicos de múltiples países han publicado y analizado genomas del agente causativo, y de manera abrumadora concluyen que este coronavirus se originó en la vida silvestre, como ha sido el caso de otros patógenos emergentes. Las teorías de conspiración no hacen sino crear miedo, rumores y prejuicios, que ponen en peligro nuestra colaboración global en la lucha en contra este virus”. Dado el consenso científico, es entonces improbable que el COVID-19 sea un virus artificial, parte de un programa secreto del gobierno chino, que haya escapado de un laboratorio. No tendrían tampoco sustento las historias acerca de los peligros de disfrutar de una sopa de murciélago, pues, según los especialistas, el coronavirus no habría pasado directamente de estos animales a los humanos, sino que lo habría hecho a través de un animal intermediario, todavía no identificado. Podría usted entonces proceder con entera confianza.Teorías de conspiración aparte, lo que sí resulta impactante es el rápido desarrollo que están teniendo algunos brotes de coronavirus, independientes de la epidemia en China, como el que está en curso en Corea del Sur. Brotes que esperemos se mantengan suficientemente alejados de nuestro país.",
    "Si bien hay todavía quien dude del calentamiento global, una gran mayoría de especialistas en la ciencia climática están de acuerdo en que dicho cambio es real y que es necesario tomar medidas urgentes para mitigarlo. Habida cuenta que dicho calentamiento es producto del uso de combustibles fósiles para la generación de energía, los expertos consideran indispensable sustituir gradualmente dichos combustibles por fuentes de energía renovables no contaminantes. Como fuente de energía renovable, el Sol es una opción muy atractiva, pues cuenta con una potencia miles de veces mayor que la necesaria para satisfacer nuestras necesidades, y es prácticamente ilimitada y no contaminante. Para el aprovechamiento de la energía solar las celdas fotovoltaicas son a su vez una opción muy atractiva. Dichas celdas convierten a la radiación solar directamente en energía eléctrica, una forma de energía muy conveniente que puede ser fácilmente manipulable.  Las celdas solares han hecho valer todas sus virtudes y, como sabemos, son cada vez más comunes como generadores de energía eléctrica, tanto al nivel de casas-habitación para disminuir las facturas eléctricas, como en instalaciones de capacidad mediana, e incluso en plantas para la generación y distribución de energía a través de la red eléctrica.Por otro lado, la energía solar tiene también aspectos negativos. Quizá el más importante es que es intermitente y sólo podemos contar con ella durante el día. Así, para una sustitución plena de los combustibles fósiles por energía solar es necesario complementar las instalaciones de paneles solares con un medio para almacenar la energía generada durante el día -por ejemplo, con baterías recargables- para ser usada en la noche. Los medios de almacenamiento de energía, sin embargo, no tienen el mismo grado de desarrollo tecnológico que los paneles solares y con su alto precio encarecen una instalación solar. En estas circunstancias, las instalaciones solares fotovoltaicas tendrían que ser consideradas solamente como complementos de las plantas generadoras de energía que emplean combustibles fósiles y no como sustitutos de éstas.En este contexto, resulta interesante un artículo aparecido el pasado mes de enero en la revista “ACS Photonics” en el que se describe la posibilidad de construir una celda solar que genere energía eléctrica durante la noche. Dicho artículo fue publicado por Tristan Deppe de la Universidad de Maryland y Jeremy Munday de la Universidad de California en Davis. Para entender la propuesta de Deppe y Munday habría que recordar que una celda solar fotovoltaica es un dispositivo que absorbe radiación solar y entrega energía eléctrica. El arreglo funciona por la gran disparidad que hay entre la temperatura de la superficie del Sol -más de 5,000 grados centígrados- y la temperatura de la celda que recibe la radiación solar. Es decir, la operación de una celda fotovoltaica se basa en la interacción entre un objeto caliente -la superficie del Sol- y un objeto más frío -la celda solar.El dispositivo analizado por Deppe y Munday genera energía eléctrica basado también en la interacción entre un objeto caliente y un objeto frío. En contraposición con una celda solar convencional, sin embargo, el objeto caliente corresponde a la celda solar mientras que el objeto frío es el espacio profundo, que está a una temperatura cercana al cero absoluto -menos 273 grados centígrados-. El dispositivo, además, funciona emitiendo radiación hacia el espacio profundo -apuntando su superficie hacia el cielo- y no absorbiéndola como en la celda convencional. Para que todo esto sea posible sin violar el principio de conservación de la energía -que es aceptado por todo el mundo, incluida la naturaleza en primera instancia- es necesario que el dispositivo absorba energía de algún lado. Por ejemplo, del medio ambiente. La propuesta de Deppe y Munday apunta a la factibilidad de construir un dispositivo que genere energía durante la noche por lo que ha sido bautizado como una anti-celda solar. Esto quizá no sea del todo correcto, pues la celda analizada por Deppe y Munday no difiere en sus aspectos básicos de funcionamiento de una celda fotovoltaica convencional, más allá de que una trabaja absorbiendo radiación y la otra emitiéndola. Además, ambas celdas toman su energía del Sol. La celda convencional de manera directa, y la celda de Deppe y Munday del medio ambiente y en último término del Sol, cuya radiación mantiene la temperatura de la superficie de nuestro planeta.¿Cuánta potencia eléctrica podría obtenerse de la celda solar de Deppe y Munday? Calculan los investigadores que en condiciones ambientales ideales podría ser una cuarta parte de la potencia obtenida a pleno sol de una celda solar convencional. No es pues una cantidad despreciable.La celda solar analizada por Deppe y Munday está apenas en una etapa conceptual y para atestiguar su fabricación y demostración de su funcionamiento tendremos que esperar por los años futuros. Por otro lado, aun demostrando su factibilidad, seguramente no sería un proceso sencillo llevar a la práctica un concepto tan novedoso. Después de todo, la industria solar fotovoltaica depende de múltiples factores, incluyendo factores económicos y políticos, y ha avanzado muy lentamente desde la invención de la celda solar fotovoltaica en 1954.Por lo demás, al margen de todas estas consideraciones, resulta fascinante la posibilidad de generar energía simplemente apuntando una celda solar hacia el ultra frío espacio profundo. Empleando al medio ambiente como una especie de batería recargable.",
    "En los últimos doscientos años la expectativa de vida al nacer, es decir el tiempo promedio que se esperaría viviese una persona en el momento de su nacimiento, ha experimentado un incremento espectacular. En los países industrializados dicha expectativa se ha duplicado desde mediados del siglo XIX hasta la fecha y en otros lugares del mundo dicho incremento ha sido incluso mayor. El incremento en la expectativa de vida es producto de los avances médicos y de salud pública que se han dado en los últimos dos siglos.El incremento en nuestra expectativa de vida, por otro lado, ha sido en buena medida producto de un abatimiento en la mortalidad infantil y no implica que hayamos doblado la longitud de nuestra vida. En respecto, sin embargo, es ilustrativo considerar la evolución histórica que ha tenido nuestra expectativa de vida una vez que hemos superado la edad infantil, misma que, aunque en una menor proporción, ha tenido también incrementos sustanciales. La población blanca de los Estados Unidos, por ejemplo, entre 1850 y 2011 ha visto incrementada su expectativa de vida a los diez años en un 40%.  Así, aun sin tomar en cuenta el abatimiento de la mortalidad infantil, sí hemos incrementado sustancialmente nuestra expectativa de vida. Sirva lo anterior como una introducción para comentar un interesante artículo publicado el pasado mes en la revista “eLife” por un grupo de investigadores de la Universidad Stanford, encabezado por Myroslava Protsiv, quienes llegan a una conclusión sorprendente: la temperatura corporal promedio de la población de los Estados Unidos ha disminuido de manera paulatina a lo largo de los últimos ciento cincuenta años.    Protsiv y colaboradores hacen notar que en 1851 el médico alemán Carl Wunderlich llevó a cabo millones de mediciones axilares de temperatura con 25,000 pacientes que establecieron un estándar de temperatura corporal de 37 grados centígrados. Las mediciones modernas de temperatura corporal, sin embargo, son sistemáticamente más bajas que el estándar de Wunderlich. En estas circunstancias, Protsiv y colaboradores se propusieron averiguar si las temperaturas corporales han efectivamente disminuido con los años, o si bien la discrepancia entre las mediciones actuales con aquellas de Wunderlich son producto de una diferencia en los termómetros empleados o en lo métodos de medición de temperatura.  Para averiguarlo, lo investigadores examinaron cerca de 700,000 mediciones de temperatura corporal con tres grupos de personas a lo largo de 160 años, incluyendo 84,000 mediciones tomadas entre 1862 y 1939 a veteranos de la guerra civil norteamericana, lo mismos que mediciones tomadas en los periodos 1971-1975 y 2007-2017, tanto a hombres como mujeres. En general, Protvis y colaboradores encontraron que las temperaturas corporales de los veteranos de la guerra civil son más altos que las de los pacientes modernos. De la misma, manera, las temperaturas medidas en el periodo 1971-1975 son mayores que aquellas medidas en 2007-2017. Para mayor precisión, la temperatura corporal promedio de los hombres norteamericanos nacidos a principio del siglo XIX era más de medio grado centígrado mayor que en la actualidad. Encontraron también que las mujeres norteamericanas han disminuido su temperatura corporal en un tercio de grado centígrado desde 1890. Tanto en hombres como en mujeres la disminución de temperatura se ha dado de manera paulatina en todos los grupos estudiados.Dada esta última circunstancia, Protvis y colaboradores consideran que es muy improbable que la disminución de temperatura corporal sea producto de fallas de medición o por posibles imprecisiones de los termómetros empleados en el siglo XIX. Descartan también que el método de medición de temperatura corporal, axilar en siglo XIX y oral en lo subsecuente, pudiera tener algún efecto, dado que las temperaturas axilares son más bajas que las orales.   De este modo, los investigadores consideran que la disminución de temperatura corporal es un fenómeno real, al que habría que dar una explicación. En este sentido, Protvis y colaboradores interpretan la disminución histórica de temperatura corporal en términos de una diminución de la actividad metabólica del cuerpo. Para esto último aventuran dos posibles explicaciones.  Una de ellas se relaciona con la expansión que han tenido en los Estados Unidos la calefacción y el aire acondicionado empleados para amortiguar los cambios de temperatura ambiental. Así, al no tener que lidiar con temperaturas muy altas o muy bajas, el cuerpo gasta menos energía para mantener su temperatura y reduce su metabolismo.  Una segunda explicación, que Protvis y colaboradores consideran es la más factible, se basa en la disminución en los niveles de inflamación de la población norteamericana que goza hoy en día de un mejor estándar de vida y sanidad. Esto redunda en una disminución de infecciones crónicas, una mayor higiene dental, una menguante exposición a la tuberculosis y la malaria, y un acceso a los antibióticos. Todo esto, arguyen los investigadores, muy probablemente ha disminuido el nivel de inflamación crónica de la población norteamericana, lo mismo que su temperatura corporal. Protvis y colaboradores concluyen que sus resultados indican que los humanos en países ricos han cambiado fisiológicamente en los últimos 200 años y que su temperatura corporal es 1.6% más baja que en la era industrial. ¿Se relaciona este cambio fisiológico con el incremento en nuestra expectativa de vida? Esta es una posibilidad fascinante que abordan brevemente Protvis y colaboradores. Solamente para señalar, sin embargo, que el papel que la “evolución” fisiológica juega en el incremento de la longevidad humana es desconocido.",
    "Tanto ha mejorado la salud del mundo por la invención de los antibióticos y por la introducción de medidas de higiene, y en general por el avance de la práctica y de la ciencia médica, que nos parecen lejanos los momentos de terror que experimentaron nuestros antecesores atrapados en medio de una epidemia y ante la cual tenían pocos medios de defensa. Es el caso, por ejemplo, de la epidemia de peste bubónica que diezmó a la población europea en el siglo XIV. Lo es también de la epidemia de viruela que casi extinguió a la población indígena en México después de la conquista española.De cuando en vez, no obstante, los microrganismos patógenos nos recuerdan de su presencia en el mundo. Así, hace apenas un siglo que la llamada gripe española mató a entre 20 y 50 millones de personas. Mas recientemente, la epidemia de VIH ha llevado a la muerte a 32 millones de personas, según la Organización Mundial de la Salud. Y todavía más recientemente, la epidemia de coronavirus originada en Wuhan, China, amenaza con expandirse por el mundo.Como ha sido ampliamente difundido por los medios de comunicación, los primeros casos de la epidemia de coronavirus ocurrieron en el mes de diciembre pasado en la ciudad de Wuhan. Esta ciudad es la capital de la provincia de Hubei en la zona central de China. Su área metropolitana cuenta con una población de 11 millones de habitantes. Es un importante centro de comunicaciones, con conexiones por aire y tierra con otras megaciudades chinas, así como con conexiones aéreas con ciudades fuera de China continental y esto posibilitó la expansión de virus.  El desarrollo de la epidemia en Wuhan está actualmente en la etapa de crecimiento exponencial y sus estadísticas cambian rápidamente día a día. Hasta el día de ayer se han confirmado un total de 14,551 personas infectadas y 304 muertos, la inmensa mayoría de estos últimos en la provincia de Hubei. Por otro lado, si bien las muertes solamente han ocurrido en China continental, la infección por coronavirus se ha extendido ya a 24 países, lo mismo que a Hong Kong y Macao. En total, 171 personas infectadas han sido reportadas fuera de China continental. Japón ha reportado 20 personas infectadas, seguido de Tailandia con 19, Singapur con 18 y Hong Kong con 14. Nuestro país no ha reportado ninguna persona infectada, lo mismo que ningún otro país latinoamericano. Los Estados Unidos, en cambio, han reportado 8 infectados, de modo que el virus está ya al otro lado de nuestra frontera norte.Un artículo aparecido en línea el pasado viernes en la revista médica “The Lancet” intenta pronosticar el curso que seguirá la epidemia. Dicho artículo fue publicado por tres investigadores de la Universidad de Hong Kong. Para hacer su pronóstico, los investigadores hacen un estimado del tamaño de la epidemia en Wuhan basados en el número de infectados que fueron exportados desde Wuhan, tanto hacia otras ciudades chinas, como hacia afuera de China continental. Hacen uso también de datos sobre transportación de pasajeros por tren, carretera o por vía aérea, tanto dentro de China continental como a otros países o territorios del mundo.Encuentran los investigadores que desde el inicio de la epidemia el 1 de diciembre hasta el 25 de enero, cada persona infectada con el coronavirus habría infectado en promedio a dos o tres personas más, y que el número de infectados se habría doblado cada 6.4 días. De este modo, en dicho periodo hasta 76,000 personas podrían haber resultado infectadas.Dado que este último número es mucho mayor que el de personas reportadas con la infección de coronavirus al 25 de enero, Gabriel Leung, uno de los autores del artículo de referencia, considera que la discrepancia puede deberse a que existe un retraso entre la infección y la aparición de los primeros síntomas de la enfermedad. De manera adicional, las personas infectadas y ya con síntomas, no acuden inmediatamente a buscar ayuda médica. El tiempo necesario para hacer un diagnóstico de laboratorio también introduce un retraso antes de que el enfermo entre a las estadísticas. Por otro lado, el estudio llega a la conclusión de que muy probablemente ya se hayan exportado a otras megaciudades chinas suficientes infectados para iniciar epidemias locales. Al mismo tiempo, los investigadores hacen notar algunas debilidades de su estudio. Estas incluyen la suposición de que los patrones de viaje no fueron modificados por el desarrollo de la epidemia y que todas las infecciones, aun las más leves, produjeron síntomas. De no ser esto último cierto, habrían subestimado el tamaño de la epidemia. En que medida los investigadores de la Universidad de Hong Kong son acertados en sus predicciones, es algo que no nos tomará mucho tiempo en averiguar. Por lo pronto, se actualizan día con día el número de personas infectadas y el número de muertos. Hay que hacer notar, sin embargo, que si bien un muerto es ya demasiados muertos, el coronavirus de Wuhan no pareciera ser particularmente letal, pues el total de fallecidos es apenas el 2.1% de total de infectados.De una u otra manera, habría que reconocer que hoy en día estamos mucho mejor armados para defendernos de las epidemias de lo que estuvieron nuestros antecesores.  Durante los episodios de peste bubónica en la Edad Media las personas morían sin tener la menor idea de la causa de la enfermedad. Hoy, en contraste, sabemos que la enfermedad de Wuhan la provoca un virus, del cual conocemos incluso su genoma. Lo cual es saber mucho.",
    "Quien haya visto la célebre película “El Graduado”, estrenada en 1967, posiblemente recuerde la escena en la cual el protagonista, Benjamín Braddock -caracterizado por Dustin Hoffman-, recibe un consejo para su futura carrera profesional. La escena transcurre durante la reunión que los acaudalados padres de Benjamín le han organizado en su residencia con motivo de su reciente graduación en una universidad de prestigio del este de los Estados Unidos.  Un amigo de la familia, el señor McGuire -no llegamos a saber su nombre de pila- es quien aconseja a Benjamín. Lo hace con una actitud paternal y de una forma teatral, apartándolo de la fiesta para espetarle enfrente de la alberca: “Te quiero decir una palabra”. “Solamente una palabra”. “¿Me escuchas?” Cuando Benjamín contesta afirmativamente, McGuire simplemente menciona: “plásticos”. Ante la confusión de Benjamín, McGuire es más explícito: “Hay un gran futuro en los plásticos”.El diálogo entre Benjamín y el señor McGuire es tan famoso como la película misma y se ha interpretado en el contexto del dislocamiento entre generaciones que se dio en los Estados Unidos en la década de los años setenta. Así, para Benjamín, que se encuentra en una gran confusión sobre que quiere hacer de su vida, resulta absurdo que McGuire, un miembro de la generación de sus padres, le de tanta importancia a los plásticos, un material que es una imitación barata de otros muy superiores, como es el caso de los metales o la madera. Por otro lado, más allá de la interpretación alegórica que podamos hacer del diálogo entre McGuire y Benjamín, si lo tomamos en su sentido literal habría que reconocer que a McGuire no le faltaba razón. Una prueba visible de esto es el problema cada vez más agudo de contaminación ambiental por desechos plásticos, resultado del desproporcionado incremento en la producción a nivel mundial de estos materiales desde que se estrenó El Graduado.Con respecto a esto último, según reporta un artículo de julio de 2017 publicado en la revista “Science Advances” por un grupo de investigadores encabezado por Roland Geyer de la Universidad de California en Santa Bárbara, entre 1967 y 2015 la producción de plásticos a nivel mundial se incrementó unas 17 veces, hasta alcanzar unos 400 millones de toneladas anuales. Según la misma fuente, la producción acumulada de plásticos entre 1950 y 2015 es aproximadamente 8,300 millones de toneladas, de las cuales 2,600 millones corresponden a materiales que aún están en uso, y 5,700 millones a plásticos ya desechados. De estos últimos, solo el 14% han sido destruidos mediante incineración. El 86% restante está acumulado en depósitos, más o menos seguros, o bien disperso sobre la superficie del planeta. Habría también que hacer notar que se ha reciclado apenas el 7% del total de plásticos producidos y que un cierto porcentaje del material reciclado ha terminado por desecharse.   Una medida del nivel global de contaminación por plásticos lo obtenemos si dividimos la masa de plásticos desechados y que no han sido destruidos, por el número de habitantes del mundo. Si hacemos esta división, a cada uno de nosotros nos corresponderían alrededor de 750 kilogramos de basura plástica.La creciente contaminación por materiales plásticos ha encendido las alarmas y esto ha llevado, entre otras iniciativas -y como bien lo sabemos-, a la prohibición de las bolsas de plástico que proporcionan los supermercados. Si bien esto en principio ayudará a combatir contaminación por plásticos, las bolsas de supermercado no son el único factor que contribuye a dicha contaminación. Así, según la organización “Ocean Conservancy”, una campaña de limpieza de zonas costeras llevada a cabo en 2018 encontró que los mayores contaminantes son, en ese orden: colillas de cigarro, envolturas de comida, popotes, cubiertos de plástico, botellas de plástico, tapas de botella, y bolsas de supermercado.    Así, si bien la prohibición de las bolsas de supermercado podría aliviar el problema de contaminación por plásticos, no podría por sí misma resolverlo al no constituir dichas bolsas la única fuente contaminante. De hecho, la medida es controvertida, y así como hay quien la apoye, también tiene detractores quienes consideran que podría incluso agravar el problema de contaminación atmosférica, habida cuenta que la fabricación de los sustitutos de las bolsas de supermercado podría conducir a un mayor consumo de combustibles fósiles.Por lo demás, la medida de prohibir las bolsas de supermercado ha ganado impulso, si bien tendremos que esperar a que los años por venir nos digan de su efectividad. Lo que sí resulta claro ahora es que el señor McGuire no se equivocó con su consejo a Benjamín Braddock en eso de que había un gran futuro en los plásticos. Futuro que, sin embargo, no fue del interés de Benjamín que renunció a la seguridad que le ofrecía la acomodada posición social de su familia, según la trama de la película. Siendo además posible que tampoco haya coincidido con los intereses del planeta.",
    "Cuando desembarcó Cristóbal Colón en la isla Guanahani el 12 de octubre de 1492 encontró nativos que le parecieron muy atractivos, “….muy bien hechos, de muy hermosos cuerpos y muy buenas caras”. Al mismo tiempo, notó que algunos de ellos mostraban heridas en sus cuerpos, mismas que le hicieron saber les habían sido infringidas por gentes de otras islas. Concluyó Colón que los atacantes habían llegado de tierra firme con el propósito de capturar a los nativos de Guanahani que, por otro lado, eran pacíficos, pues “….no traen armas ni las conocen, porque les mostré espadas y las tomaban por el filo y se cortaban”.Muy contrastante fue la opinión que Colón tuvo de los indios caribe que encontró posteriormente en la Española y que según él eran salvajes caníbales que aterrorizaban con sus incursiones a los territorios de indios pacíficos en busca de carne humana. Según Colón, los caniba -así se refería a los caribes-  “no son otra cosa sino la gente del Gran Can, que debe ser aquí muy vecino, y tendrá navíos y vendrán a cautivarlos, y como no vuelven creen que se los han comido” . Con respecto a esto último Colón cometió una equivocación ocasionada por su creencia que había llegado al continente asiático. Por lo demás, equivocaciones aparte, con Colón nació la leyenda negra de los caníbales americanos. El que los indios caribes fueran caníbales, sin embargo, no es algo universalmente aceptado en la actualidad. De acuerdo con Reg Murphy, de la Universidad de Syracuse, por ejemplo, sobre la base de un análisis de la dieta de los indios caribe, es posible concluir que éstos se alimentaban fundamentalmente de pescado y animales marinos de concha y que, si fuera el caso, su consumo de carne -animal o humana- habría sido solamente esporádico. Otro argumento en contra del canibalismo de los indios caribe de la Española, se basa en la creencia de los expertos que dichos indios, que emigraron a través de la Antillas Menores desde la costa norte de Sudamérica, nunca llegaron más allá de la isla Guadalupe. En estas condiciones, los caniba de Colon podría no corresponder a los caribe de la actualidad.Un artículo aparecido esta semana en la revista “Scientific Reports”, sin embargo, concluye que los indios caribe llegaron mucho más al norte que lo que los expertos habían considerado.  Dicho artículo fue publicado por un grupo de investigadores de universidades y centros de investigación norteamericanos, encabezado por Ann Ross, de la Universidad Estatal de Carolina del Norte en Raleigh.Con el objeto de investigar el origen de los pobladores de la islas caribeñas, Ross y colaboradores analizaron 16 características faciales, tales como el tamaño de la cavidad ocular y el largo de la nariz, de 100 cráneos excavados en dichas islas, fechados entre los años 800 a 1542 de nuestra era. A través de este estudio fue posible determinar las relaciones que existen entre las distintas poblaciones caribeñas. El análisis mostró que existen tres grupos de pobladores del Caribe. Mostró, además, las rutas de migración que dieron lugar a la formación de estos grupos. La primera migración hacia el Caribe ocurrió hace unos 7,000 años, desde la península de Yucatán hacia Cuba. En una segunda migración, los pueblos arahuacos se expandieron desde la costa de Venezuela hasta la isla de Puerto Rico, entre los años 800 y 200 antes de nuestra era. Finalmente, una tercera migración fue la de los pueblos caribe, que cruzaron el mar de Caribe y llegaron a la Española alrededor del año 800 de nuestra era, continuando con una rápida expansión hacia Jamaica y las islas Bahamas -en la que se encuentra la isla Guanahani.  Así, de acuerdo con Ross y colaboradores, los indios caribe habían llegado hasta las Bahamas y la Española en la época en que Colón arribó al Nuevo Mundo. Esto apoya su versión sobre la existencia de tribus feroces de indios antropófagos que asolaban a pueblos pacíficos en busca de mujeres para secuestrar y hombres para el matadero. No la demuestra, sin embargo, pudiendo ser únicamente un mito. Hace cinco siglos, cuando el conocimiento de la geografía del mundo era incompleto, los europeos creían que había lugares remotos en donde habitaban seres con un solo ojo, o seres sin cabeza y con los ojos y la boca en el pecho: o bien, en lugares tórridos, seres con una sola pierna pero con un pie tan grande que usaban como sombrilla.  En estas condiciones no habría sido difícil crear un mito sobre la existencia de caníbales, seres tan depravados que acostumbraban devorar a sus congéneres.Tendríamos así una versión interesada sobre los indios americanos que resultó muy conveniente para justificar moralmente la dominación europea del Nuevo Mundo. Una versión que fue, además, propagada fácilmente y con impunidad, pues si aun ahora no es claro si los caribe eran caníbales feroces, hace cinco siglos hubiera sido imposible de verificar a miles de kilómetros de distancia y mar de por medio.",
    "A punto de dar por finalizado el puente Guadalupe-Reyes correspondiente a la temporada 2019-2020, se impone hacer un recuento de los buenos momentos que nos ha proporcionado. Por más que, a la par de estos buenos momentos, en algunos casos el puente haya tenido consecuencias no del todo positivas. Así, por ejemplo, al comer y beber un poco más allá de lo habitual, le dimos oportunidad a la báscula para que nos de lecturas también más allá de lo habitual. Por otro lado, si bien el puente Guadalupe-Reyes nos proporciona una buena oportunidad para subir de peso, también es cierto que el mal comer o el comer en exceso no es exclusivo de las fiestas de fin de año. Una evidencia de esto último nos lo da la epidemia de sobrepeso y obesidad que nos asuela y con respecto a la cual México ocupa un lugar destacado. En efecto, de acuerdo con la Secretaría de Salud, 73% de los mexicanos es obeso o tiene sobrepeso. El sobrepeso se define por un índice de Masa Corporal (IMC) entre 25 y 30, mientras que un IMC mayor a 30 define a la obesidad. Para tener una idea de lo que el IMC significa, a una persona de 1.75 metros de altura y 75 kilogramos de peso le corresponde un IMC de 24.5, cerca del límite superior del rango normal. Según datos de la Organización Mundial de la Salud, en 2014 el IMC promedio de la población de México era de 28.1.La epidemia de obesidad y sobrepeso no se limita a México, por supuesto, sino que tiene alcance mundial. En efecto, según datos de la Organización Mundial de la Salud, 1,900 millones de adultos de 18 o más años tienen sobrepeso y de esos 650 millones son obesos. Traducidas a porcentajes, estas cifras significan que el 39% de la población adulta del mundo tiene sobrepeso y el 13% es obesa.   Para subir de peso es, por supuesto, necesario tener a nuestra disposición alimentos -nutritivos o chatarra- para ingerir, y en este respecto no es sorprendente que los países del mundo con menores IMC sean precisamente países pobres de África como Eritrea, Etiopía y Burundi, con un IMC entre 20 y 21 -valores, sin embargo, dentro del rango considerado normal.La epidemia mundial de obesidad tiene alarmados a los expertos por las consecuencias que tiene para la salud y ha sido motivo de múltiples estudios para encontrar las causas que la motivan y buscar acotarla. En este respecto, el pasado mes de octubre, un grupo de investigadores de universidades en los Estados Unidos, publicó un artículo en la revista “Nature Communications”, en el que reportan la identificación de un circuito cerebral entre el hipotálamo y el hipocampo que influye en el consumo impulsivo de alimentos. El grupo de investigadores fue encabezado por Emily Noble de la Universidad de California del Sur. Como apuntan Noble y colaboradores, la impulsividad, o sea la respuesta impulsiva sin considerar las consecuencias de una acción, ha sido ligada al consumo excesivo de alimentos, al sobrepeso y la obesidad, y a varios desórdenes siquiátricos, incluyendo la drogadicción y una afición excesiva a los juegos de azar. En su estudio, los investigadores se propusieron determinar la influencia que la producción de una hormona en el hipotálamo tiene sobre el comportamiento impulsivo de ratas de laboratorio en cuanto al consumo de alimentos. Para este propósito, Noble y colaboradores entrenaron ratas para que presionaran una palanca y con esto obtuvieran una pastilla rica en grasa y azúcar. Los animales fueron entrenados -sin restricción de alimentos- para que esperaran 20 segundos antes de presionar la palanca y recibir la recompensa. Si la rata presionaba la palanca antes de trascurrir 20 segundo no obtenía la pastilla y el reloj se reiniciaba para otros 20 segundos de espera.   Una vez entrenadas las ratas participantes en el experimento, los investigadores estimularon la producción de la hormona concentradora de melanina en su cerebro estableciendo un circuito neuronal entre el hipotálamo y el hipocampo. Este último, se sabe, es un área del cerebro que está relacionada con el aprendizaje y la memoria. Noble y colaboradores encontraron que al estimular la producción de la hormona se incrementó la frecuencia con la que las ratas activaban la palanca antes de los 20 segundos. Con esto las ratas no incrementaron la cantidad de alimento obtenido y por tanto la eficiencia de sus intentos fue menor, lo que indica un incremento en el comportamiento impulsivo.Concluyen Noble y colaboradores que a partir de su investigación han identificado un circuito neuronal entre el hipotálamo y el hipocampo que gobierna el comportamiento impulsivo del consumo de alimentos y que esto podría llevar al desarrollo de terapias para controlar la ingesta excesiva de comida y por tanto atajar la epidemia mundial de obesidad. De ser este el caso, podríamos quizá disfrutar de futuros puentes Guadalupe-Reyes sin preocupaciones y sin necesidad de hacer firmes propósitos de someternos a una dieta estricta a partir del 7 de enero -exceptuando, por supuesto, el día de la Candelaria-. Aunque también es posible que, con el circuito neuronal hipotálamo-hipocampo bajo control, las fiestas de fin de año no vuelvan a ser las mismas.",
    "Durante el máximo acercamiento de Marte a la Tierra ocurrido en 1894, varios observatorios a lo largo del mundo detectaron un destello de gran intensidad en la superficie marciana. El público en general se enteró del acontecimiento por una nota publicada el 2 de agosto en la revista “Nature”. En esos momentos no se sabía, pero dicho destello fue producto de una explosión y marcó el inicio de la invasión de la Tierra por los marcianos. Lo anterior es narrado por el escritor británico H.G. Wells en su novela “La guerra de los mundos” publicada en el año 1898. Quien haya leído esta novela, de la cual existen versiones para el cine y la televisión, sabrá que los marcianos llegaron a la Tierra a bordo de cápsulas lanzadas desde la superficie de Marte por medio de una especie de cañón. Una vez en nuestro planeta, los marcianos, que venían en plan de conquistadores, desplegaron enormes máquinas de tres patas que lanzaban mortíferos rayos de calor, ocasionando gran destrucción y pánico entre la población. Los marcianos y sus máquinas resultaban invencibles con las armas de que disponían los terrícolas. No obstante, y para nuestra fortuna, fueron derrotados al final por un enemigo inesperado: los microbios terrestres para los cuales no tenían defensas.Hoy en día sabemos que en caso de existir vida en Marte ésta será microbiana y de ninguna manera inteligente. Así, los hechos relatados por “La guerra de los mundos” tuvieron muy pocas probabilidades de ser reales. Sin duda, la ausencia de vida inteligente en Marte disminuye en cierto grado nuestra fascinación por el planeta. No la anula, sin embargo, y Marte sigue siendo unos de los blancos favoritos de las agencias espaciales. En efecto, como lo apunta la revista británica “Nature” -la misma que habría publicado la nota sobre las explosiones avistadas en la superficie de Marte en 1894-, en el año que se avecina se llevará a cabo una invasión a Marte por naves terrestres enviadas desde la superficie de nuestro planeta. Las naves serán lanzadas por la NASA, por la agencia espacial europea en combinación con la agencia espacial rusa, por la agencia espacial china, e incluso, por la agencia espacial de los Emiratos Árabes Unidos.  La NASA lanzará en julio-agosto de 2020 la misión “Marte 2020” que, a un costo de 2,500 millones de dólares, colocará un explorador en la superficie de Marte en febrero de 2021. El explorador tiene la misma apariencia que el anterior explorador “Curiosity” pero con instrumentos científicos enteramente diferentes. Tendrá el tamaño de un automóvil compacto y un peso de una tonelada. El propósito de la misión es llevar a cabo estudios geológicos del planeta, lo mismo que estudios de astrobiología para detectar la presencia de moléculas orgánicas y determinar la posibilidad de que Marte haya albergado vida en un pasado remoto. El explorador tomará, asimismo, muestras del material marciano que guardará en cilindros metálicos. Dicho material sería traído a la Tierra para su estudio, en un momento y por medio de una misión que todavía no está definida.  La agencia espacial europea, en conjunción con la agencia espacial rusa planean lanzar en 2020 la misión “ExoMars” que colocará también un robot explorador sobre la superficie de Marte. Dicho explorador cuenta con un brazo para perforar la superficie del planeta hasta una profundidad de 2 metros, con el propósito de obtener muestras del suelo marciano e investigar la existencia de marcadores biológicos que indiquen la presencia en el planeta de vida presente o pasada. De manera adicional, China tiene planeado lanzar a mediados de 2020 la misión “Huoxing 1”, que colocará un orbitador alrededor de Marte y hará descender a su superficie un robot explorador de unos 200 kilogramos de peso. Como objetivos científicos, la misión Huoxing 1 buscará materiales orgánicos que indiquen la presencia presente o pasada de vida en Marte. Las misiones anteriores que serán iniciadas en 2020 y pretenden colocar robots exploradores en la superficie marciana, serán complementadas por la misión “Hope” de los Emiratos Árabes Unidos, la cual busca insertar un orbitador alrededor de Marte. La misión “Hope” será lanzada desde Japón, y tendrá como objetivo estudiar la atmósfera marciana.Por todo lo anterior, podemos concluir que el año 2020 que se avecina será particularmente rico en iniciativas para estudiar al planeta Marte. Así, en el remotísimo caso de que hubiera astrónomos marcianos apuntando sus telescopios hacia la Tierra, podrían observar a lo largo de 2020 una serie de destellos en la superficie de nuestro planeta -la ignición del combustible del motor de los cohetes con destino a Marte- que serían indicativos de una invasión a su planeta. Un escritor marciano tendría así la oportunidad escribir, no una novela de ficción, sino la crónica de la invasión. La cual, para fortuna de los marcianos, y dadas las enormes dificultades para llegar hasta allá, tendría que ser pacífica por necesidad. Al menos por lo pronto. Más adelante no habría garantía.    De un modo u otro, habría que desear a todos, terrícolas y marcianos por si fuera el caso, un muy feliz año 2020.",
    "Estamos inmersos una vez más en uno de los ya tradicionales puentes Guadalupe-Reyes, en los que el país se semiparaliza por casi un mes. Durante este periodo los mexicanos disminuimos de manera considerable -o de plano interrumpimos- el ritmo de trabajo y entramos en modo de fiesta – el cual es activado en algunos casos incluso antes del inicio formal del puente-. Ya encarrerados, celebramos comidas y cenas de fin de año, una semana entera de posadas, y sendas cenas de Navidad y Año nuevo, entre otras actividades. Rematamos el 6 de enero con una rosca de reyes y con esto volvemos a nuestras actividades normales. Como corolario, sin embargo, y quizá resistiéndonos un poco a perder el ambiente festivo, aquel que resulta agraciado con un muñeco al partir la rosca de reyes debe ofrecer una tamalada el 2 de febrero.     Cabe preguntarse, sin embargo, si ralentizar la actividad de país por un mes al año sea recomendable dada nuestra situación de país subdesarrollado. En este contexto, y tomando en cuenta que la ciencia -y la tecnología que de la misma se deriva- ha sido y es un elemento central para el desarrollo de los países a lo largo del último siglo, nos propusimos averiguar qué descubrimientos científicos de gran trascendencia se han llevado a cabo alrededor de las fiestas decembrinas. Uno de estos descubrimientos fue el del transistor, cuyo funcionamiento fue demostrado por John Bardeen, Walter Brattain y William Shockley el 23 de diciembre de 1947 en los Laboratorios Bell de la compañía de telecomunicaciones AT&T en Nueva Jersey, Estados Unidos. Bardeen, Brattain y Shockley formaban un grupo de trabajo en dicho laboratorio. El transistor demostrado antes de la Navidad de 1947, sin embargo, fue desarrollado solamente por Bardeen y Brattain sin la colaboración de Shockley. Este último se consideraba a sí mismo el verdadero cerebro del grupo y, tocado en su orgullo profesional, terminó fuertemente disgustado por la situación. En estas condiciones, se encerró después de la demostración del transistor en un cuarto de hotel para trabajar intensamente por varias semanas en el desarrollo conceptual de un transistor que superara en características al inventado por Bardeen y Brattain. Con gran éxito por lo demás, al grado que las ideas desarrolladas por Shockley han sobrevivido hasta la actualidad. El transistor ha sido considerado el mayor descubrimiento tecnológico del siglo XX, que ha tenido enormes consecuencias para el desarrollo de la electrónica tal como la conocemos. Por sus descubrimientos, a Shockley, Bardeen y Brattain le fue concedido en 1958 el premio Nobel de Física.  Otro descubrimiento asociado a la Navidad es del de la desintegración del átomo llevado a cabo por Lise Meitner y Otto Frisch. Meitner fue una investigadora austriaca de origen judío que, huyendo del régimen nazi se había refugiado en Estocolmo. Ahí, poco antes de la Navidad de 1938 recibió desde Berlín una carta de Otto Hahn -con quien había colaborado en el pasado- en el que le relataba los resultados de un experimento en el que había logrado producir átomos de bario a partir de bombardear átomos de uranio con neutrones. Hahn no podía interpretar los resultados de sus experimentos por lo que consultó a Meitner al respecto. La carta de Hahn coincidió con la visita de Otto Frisch, sobrino de Meitner, a Estocolmo. Frisch había viajado a esa ciudad para visitar a Meitner con motivo de la Navidad. Meitner y Frisch discutieron sobre los resultados de los experimentos de Hahn y llegaron a la conclusión correcta: al bombardear Hahn con neutrones los átomos de uranio los había desintegrado, generando en el proceso una cantidad de energía sin precedentes. Como sabemos, este descubrimiento tuvo enormes consecuencias, desde la fabricación de las bombas atómicas que destruyeron Hiroshima y Nagasaki, hasta la construcción de reactores nucleares para la generación de energía eléctrica. Además, por supuesto, de la Guerra fría que tuvo por décadas al mundo en vilo por el peligro de una guerra atómica a nivel global.Transistores y fisión atómica son dos de los descubrimientos más trascendentes del siglo XX. Ambos fueron llevados a cabo alrededor de la Navidad por investigadores inmersos en su trabajo. No podemos esperar, por supuesto, que hubiera una conexión directa entre la Navidad y dichos descubrimientos, los cuales ocurrieron en el momento preciso dictado por el avance de las investigaciones, independientemente de una fecha particular. Lejos de esto, podríamos quizá entender la coincidencia temporal de los descubrimientos del transistor y de la fisión atómica con la Navidad, en términos de la madurez intelectual de los países que posibilitaron dicha coincidencia. Madurez que estamos obligados a perseguir como país y que, ciertamente, no tiene puntos de contacto con el puente Guadalupe-Reyes.",
    "“Por qué una especie tan exitosa como el “Homo sapiens” es tan aficionada a los relatos de ficción, contando a historias que nadie cree, ni el que las cuenta ni el que las escucha; esto a primera vista parece un misterio evolutivo”. Así empieza un artículo publicado en el mes de febrero de 2018 en la revista “WIREs Cognitive Science” por Brian Boyd de la Universidad de Auckland en Nueva Zelanda. Si bien después de pensarlo dos veces la pregunta de Boyd resulta natural, estamos tan acostumbrados a las historias y películas de ficción que dicha pregunta no deja de llamar la atención, puesta en esos términos tan descarnados.Ciertamente, tendría que haber existido una ventaja evolutiva para el desarrollo de nuestra atracción por la ficción. Y en ese sentido, Boyd especula que con la aparición del lenguaje se desarrolló la narrativa, mediante la cual nuestros ancestros expandieron su conocimiento del mundo a través de las experiencias de otros y no solo de las propias. La narrativa, sin embargo, proporcionaba información solamente de experiencias que ocurrieron en la realidad. La ficción, en cambio, habría permitido explorar el mundo en forma más amplia. Así, Boyd especula que el lenguaje condujo a la narrativa y ésta a la ficción, y con esto nuestros ancestros incrementaron sus oportunidades para explorar el mundo.En el contexto anterior, es interesante comentar un artículo aparecido esta semana en la revista “Nature”, publicado por un grupo de investigadores de Australia e Indonesia, encabezado por Maxime Aubert de la Universidad Griffit, en Queensland, Austraila. Dicho artículo se refiere al descubrimiento de un mural de pintura rupestre en la cueva Leang Bulu´ Sipong 4, localizada en el sur de la isla de Célebes en Indonesia. Dicho mural tiene una antigüedad superior a los 40,000 años e incluye seis imágenes monocromas de animales: dos jabalíes y cuatro búfalos enanos. El mural incluye también ocho teriántropos -figuras con características combinadas de humanos y animales- considerablemente más pequeños. Algunas de las figuras teriántropas parecen sostener objetos largos y delgados que los investigadores interpretan como lanzas o cuerdas. Según Aubert y colaboradores, todas las figuras tienen un mismo estilo artístico y muestran un desgaste por el tiempo equivalente. Así, a pesar de sus diferencias de tamaño, no encuentran evidencia de que las figuras teriántropas hayan sido añadidas posteriormente a las figuras de jabalíes y búfalos. Por el contrario, según su interpretación, todas las figuras están relacionadas entre sí y cuentan una historia. Si bien dicha historia no es clara, una posibilidad es que los teriántropos representen cazadores con máscaras y pieles de animales a manera de camuflaje. Un aspecto que desconcierta a los investigadores, no obstante, es su tamaño diminuto en comparación con las figuras de búfalos y jabalíes.  En cualquier caso, las figuras teriántropas indicarían que los artistas que elaboraron el mural representaron escenas ficticias que no correspondían a experiencias humanas en el mundo real. Una datación del mural descubierto por Aubert y colaboradores empleando isótopos de uranio arroja antigüedades que van desde los 36,000 hasta los 41,000 años. Estas fechas implican que el mural de la cueva Leang Bulu´ Sipong 4 es la obra de arte rupestre creada por humanos modernos más antigua que se conoce. El mural representaría, en la interpretación de Aubert y colaboradores, una escena de caza de dos especies de animales en la que participan de manera coordinada múltiples cazadores. Las presas en la escena son dirigidas por un grupo de cazadores hacia donde se encuentra un segundo grupo que está a su espera. Según Aubert y colaboradores, habría también la posibilidad de que los cazadores, armados con cuerdas, estuvieran dedicados a capturar animales vivos. Si bien reconocen que la interpretación del mural de Leang Bulu´ Sipong 4 es incierta, Aubert y colaboradores señalan que dicho mural debe ser considerado, no solamente como la obra de arte figurativo más antigua conocida, sino también como la evidencia más antigua de narrativa en el arte paleolítico. Y, se podría agregar, de una narrativa de ficción en el caso de las figuras teriántropas.  De un modo u otro, el mural de Leang Bulu´ Sipong 4 es fascinante. ¿Hay alguna relación entre este mural de 40,000 años de antigüedad y nuestro desmesurado gusto por las historias de ficción que ha creado una industria cinematográfica con ingresos anuales de 100,000 millones de dólares a nivel global? Ciertamente, la sola posibilidad resulta también fascinante.",
    "En diciembre de 1912 el paleontólogo Charles Dawson anunció en una reunión de la Sociedad Geográfica de Londres el descubrimiento de un fósil, mitad humano y mitad simio, en Piltdown, en el sur de Inglaterra.  Los restos fósiles descubiertos por Dawson consistían de fragmentos de cráneo, dos dientes y la mitad de una mandíbula. El cráneo era similar al de un humano moderno, pero con un volumen menor, mientras que la mandíbula era claramente simiesca. Estas características combinadas eran justamente lo que lo expertos esperaban del “eslabón perdido”, que habría sido un paso intermedio en el proceso de evolución de nuestra especie. Los fósiles de Piltdown mostrarían de este modo que en la evolución del homo sapiens, Inglaterra habría tenido un papel destacado.  A los británicos esto último no les resultaba de ninguna manera descabellado. Después de todo, eran la potencia que dominaba al mundo y les era natural pensar que Inglaterra tendría que haber sido la cuna de nuestra especie. Resultó, sin embargo, que los fósiles descubiertos por Dawson -bautizados como el Hombre de Piltdown- eran un fraude cuidadosamente maquinado: el cráneo era el de un humano moderno y la mandíbula perteneció a un orangután, también moderno. El engaño, sin embargo, perduró por mucho tiempo, y no fue sino hasta el año 1953 cuando fue finalmente identificado como tal de manera concluyente. Así, resultó a fin de cuentas que Inglaterra no habría tenido un papel central en la evolución de nuestra especie. Ciertamente, hoy en día nos parece sorprendente que los prejuicios británicos hayan jugado un papel en el episodio del Hombre de Piltdown -así sea un papel menor- pues unos son nuestros deseos y otra la realidad del mundo, que solo puede ser explorada empleando métodos científicos. Y, no obstante, no pareciera que nuestra actitud a prejuzgar haya desaparecido del todo.Vale lo anterior en el caso de la francesa Janne Calment, quién nació en 1875 en Arlés, Francia y murió en la misma ciudad en 1997 a la edad de 122 años y es considerada la persona más longeva de la que se tiene noticia. Esto último, sin embargo, no es aceptado de manera universal y hay quién le disputa a Calment su récord de longevidad. Particularmente, el matemático ruso Nikolai Zak asegura que muy improbable que Calment haya muerto a la edad de 122 años. Basa su afirmación en un estudio estadístico sobre la longevidad alcanzada por personas que llegaron a una edad centenaria y encuentra que con 122 años Calment constituiría una anomalía estadística altamente improbable. Se dio Zak también a la tarea de investigar detalles y acontecimientos en la vida de Calment y llega a la conclusión que en realidad ésta habría muerto en 1934, a la edad de 59 años. A su muerte habría sido suplantada por su única hija Yvonne Calment quien habría sido la que realmente murió en 1997 a la edad de 99 años. La razón para llevar a cabo esta suplantación fue la de evitar pagar los impuestos de la herencia de su madre. Como parte de la trama, Yvonne habría formalmente muerto en 1934 a la edad de 36 años. Zak publicó sus resultados en la revista “Rejuvenation Research” en enero del presente año. Las reacciones en occidente a las afirmaciones de Zak son asombrosamente negativas y en cierto modo propias de la Guerra fría. No esperaríamos, por supuesto, que en Francia, y en Arlés en particular, en donde Calment es una celebridad, las vieran con simpatía. Sorprende, sin embargo, la reacción de algunos expertos que colocan al trabajo de Zak como parte de una especie de complot ruso para desacreditar a la ciencia gerontológica de occidente. En este respecto, un artículo publicado el pasado mes de septiembre en la revista “Journals of Gerontology: Medical Science” por un grupo de investigadores de Francia, Suiza y Dinamarca, encabezados por Jean Marie Robine del Instituto de Salud e Investigación Médica de Francia escribe en sus conclusiones: “…en relación al artículo publicado por Zak, quisiéramos remarcar la  inaceptabilidad de publicar un artículo con acusaciones infundadas afirmando que las familias Calment y Billiot de manera colectiva cometieron un fraude. ¿Cómo es posible que un artículo tan lleno de aserciones insustanciales pudo sobrevivir a una revisión por pares y ser subsecuentemente publicado en “Rejuvenation Research”. Basados en la evidencia que presentamos en este artículo, solicitamos que el artículo de Zak sea retirado”.Una lectura del artículo de Zak -quién trabaja como técnico soplador de vidrio en la Universidad Estatal de Moscú y no ha publicado un artículo científico en diez años desde su graduación como matemático, como lo remarcan sus críticos a la menor oportunidad- revela que está efectivamente escrito de manera poco usual para un artículo científico, incluyendo mucha información anecdótica e interpretaciones personales de hechos del pasado de Janne Calment de sobre los cuales no existe suficiente claridad.   Sin embargo, como lo señala un artículo publicado esta semana en el periódico británico “The Guardian”, en ambos bandos hay argumentos en pro y en contra del récord de longevidad de Janne Calment. Estos argumentos  podrían ser aclarados por medio de un análisis de ADN de sus restos mortales. Opción propuesta por Zak, pero no contemplada por sus oponentes. Y con respecto a este último punto, habría que preguntarse si no estamos ante una manifestación de lo que podríamos quizá llamar el síndrome de Piltdown.",
    "¿Consideraría seriamente la posibilidad de adquirir un automóvil totalmente eléctrico en lugar de uno de gasolina? Si bien no faltarán clientes para los vehículos eléctricos -dado que de todo hay en la viña de Señor-, la mayor parte de nosotros preferiríamos uno de gasolina. Esto por cuestión de precios, pues un automóvil eléctrico de tamaño mediano es algo así como 70 por ciento más caro que uno equivalente de gasolina. Se espera, no obstante, que el costo de los vehículos eléctricos disminuya paulatinamente en los años futuros y se empareje con los de gasolina. Con esto, los automóviles eléctricos se harán más atractivos y eventualmente dominen el mercado. ¿Cuándo se anticipa que se igualen los costos de los vehículos eléctricos y de gasolina? Dados los problemas para predecir el futuro la fecha es incierta, pero un estudio llevado a cabo por el Instituto Tecnológico de Massachussets -MIT, por sus siglas en inglés- llega a la conclusión que no ocurrirá antes del año 2030. El estudio, intitulado “Insights into future mobility”, puede ser encontrado en el sitio de Internet del MIT.De acuerdo con dicho estudio, el precio de los automóviles eléctricos en los años por venir estará determinado por la evolución del costo de la batería necesaria para almacenar la carga eléctrica que los impulsa, y que representa una fracción apreciable del costo total del automóvil. Se estima que en la actualidad el costo de una batería se sitúa entre los 175 dólares y los 300 dólares por kilowatt-hora. Para poner estos números en perspectiva, consideremos que -de acuerdo con un artículo aparecido el pasado 19 de noviembre en la revista de divulgación “MIT Technology Review” firmado por James Temple- un automóvil eléctrico de rango medio está equipado con una batería de 60 kilowatt-hora que tendría un costo entre los 10,500 y los 18,000 dólares.Según el estudio del MIT, el precio de las baterías por kilowatt-hora en el año 2030 caería hasta unos 124 dólares por kilowatt-hora. Esto lo deja todavía a una distancia apreciable de los 100 dólares por kilowatt-hora, que es el precio para el que se alcanzaría la paridad de precios entre los automóviles eléctricos y los de gasolina. La conclusión del estudio de referencia es que la incursión y eventual dominio de los automóviles eléctricos en nuestra vida no será tan rápido como algunos anticipan. No obstante, ocurrirá con seguridad en algún tiempo futuro. Con relación a lo anterior, si bien existen varias alternativas para fabricar las baterías de los automóviles eléctricos, la tecnología dominante está basada en el elemento químico litio. Este elemento se convierte así en una materia prima de gran valor estratégico. Habida cuenta, además, que las baterías de litio se emplean en computadoras portátiles y teléfonos inteligentes. Como material estratégico se le equipara incluso con el petróleoEn su forma pura, el litio es un metal de color blanco plata, muy ligero y que se oxida rápidamente al contacto con el aire. El mayor productor mundial de litio es Australia, pero con mucho las mayores reservas del planeta  se encuentran en el llamado triángulo de litio, localizado en el sur de Bolivia y el norte de Chile y Argentina. El litio en este caso se encuentra asociado a grandes salares o desiertos de sal: al Salar de Atacama en el norte de Chile, a los salares de las provincias de Jujuy, Salta y Catamarca en el norte de Argentina, y al salar de Uyuni en el Departamento de Potosí en Bolivia. El salar de Uyuni, localizado en el altiplano boliviano a una altitud de 3,600 metros sobre el nivel del mar, es una espectacular planicie de sal con una extensión cercana a los 10,000 kilómetros cuadrados. Bajo esta superficie se encuentra un sexto de las reservas mundiales de litio.   Además de su visibilidad mediática como componente esencial de la emergente industria de los automóviles eléctricos, el litio ha recibido en las últimas semanas la atención de los medios de comunicación en un contexto diferente, aunque ciertamente conectado con su papel como materia prima estratégica.En efecto, como sabemos, Bolivia está pasando por grandes problemas políticos derivados de la renuncia forzada del presidente Evo Morales. Algunos analistas han asociado dicha renuncia a intereses extranjeros que intentan controlar las reservas bolivianas de litio. Bolivia, a pesar de lo enorme de estas reservas, apenas está empezando a explotarlas, en contraste con Chile y Argentina que son dos de los mayores productores mundiales de este elemento. Además, el modelo boliviano de explotación del litio es -también en contraste con Chile y Argentina- controlado fuertemente por el estado, situación que se pretende cambiar según algunas interpretaciones. De todo lo anteriormente expuesto, podríamos quizá afirmar que el litio – u oro blanco, como se le ha bautizado- es un protagonista del tiempo presente, que lo mismo es responsable de una revolución en el transporte que ocurrirá en algún tiempo futuro -todavía no definido con claridad- que de problemas políticos -según interpretaciones- ocurridas en un país subdesarrollado rico en depósitos de oro blanco. Escriturados aparentemente por el diablo.",
    "Arthur C, Clarke imaginó en su novela “2001: Una odisea del espacio” una misión de cinco astronautas a la luna Jápeto de Saturno para hacer contacto con extraterrestres quienes habrían sido responsables del desarrollo de la inteligencia humana. Como Saturno está muy lejos -a la sonda Cassini de la NASA le tomó casi siete años en llegar hasta allá- la nave espacial de la misión era tripulada solamente por dos astronautas, mientras que los otros tres viajaban en un estado de hibernación. Quien haya leído la novela o visto la película de Stanley Kubrick basada en la misma, sabrá que solamente uno de los dos tripulantes logró llegar a su destino, y que tres los astronautas en hibernación murieron sin haber recuperado nunca la consciencia.Sin bien la novela de Clarke y la película de Kubrick -que hizo época- son fascinantes, si las hemos traído aquí a colación es en referencia al estado de hibernación en el que viajaban tres de los astronautas. Esto, habida cuenta que esta semana se publicitó en los medios de comunicación el trabajo que un grupo de investigadores médicos, encabezado por Samuel Tisherman, está llevando a cabo en la Universidad de Maryland sobre el desarrollo de técnicas de hibernación para tratar casos de urgencias médicas. El trabajo en cuestión fue presentado por Tisherman el lunes pasado en un simposio llevado a cabo en la Academia de Ciencias de Nueva York, y divulgado esta semana al público en general por la revista “New Scientist”. Dicha revista cita a Tishemarn diciendo: “Quiero aclarar que no estamos tratando de enviar gente a Saturno. Estamos tratando de ganar tiempo para salvar vidas”. En este sentido, Tisherman y colaboradores están empleando técnicas de hipotermia profunda para intentar salvar la vida de personas que han sufrido lesiones traumáticas, como heridas de bala o arma blanca, que los ponen al borde de la muerte. Los candidatos para la intervención han de haber perdido más de la mitad de la sangre y su corazón dejado de latir, lo que da solamente minutos para llevar a cabo una operación e intentar salvarles la vida. Según el procedimiento de Tisherman y colaboradores, una vez que el herido arriba al hospital se reduce rápidamente su temperatura corporal hasta unos 10-15 grados centígrados reemplazando toda su sangre con una solución salina fría. Esto lo lleva a un estado de hibernación, o animación suspendida como se le conoce, en el que se suprime casi por completo la actividad cerebral y con esto la demanda de oxígeno. Esto último es esencial para la supervivencia del paciente, pues al quedar el cerebro sin flujo de oxígeno por más de cinco minutos se produce un daño neuronal irreversible. En contraste, mediante la animación suspendida, el cirujano cuenta con unas dos horas para realizar la operación. Una vez terminada ésta, la temperatura corporal del paciente se eleva lentamente hasta su valor normal y se restablece su ritmo cardiaco.Tisherman y colaboradores pretenden comparar los resultados obtenidos con diez pacientes que reciban el tratamiento de animación suspendida, con aquellos obtenidos con otros diez pacientes que sigan el tratamiento tradicional. Si bien a la fecha los investigadores no han proporcionado información sobre dicha comparación, el tratamiento de animación suspendida se ha llevado a cabo con cuando menos un paciente.Existen casos aislados de personas que accidentalmente han entrado en un estado de animación suspendida y que han regresado a la vida sin daño neurológico. Un caso notable es el de la sueca Anna Bagenholm, quien en 1999 sufrió un accidente cuando esquiaba, quedando atrapada por 80 minutos en agua helada bajo una capa de hielo. Sufrió una hipotermia severa y al ser recatada su temperatura corporal era de 13.7 grados centígrados. Tuvo, además, un paro cardiaco después de 40 minutos de quedar atrapada. La víctima despertó después de diez días del accidente, paralizada del cuello hacia abajo. No obstante, se recobró casi totalmente después de dos meses en una unidad de cuidados intensivos.De manera afortunada, no sufrió Bagenholm daño neurológico a pesar del tiempo pasado sin oxigenación del cerebro. Esto indicaría que su temperatura corporal descendió rápidamente, disminuyendo la actividad cerebral y la demanda de oxígeno, antes de que sobreviniera el paro cardiaco. Habría así regresado a la vida desde un estado de animación suspendida cercano a la muerte.¿Se desarrollarán en el futuro técnicas para que los humanos puedan entrar en un estado de animación suspendida por tiempo indefinido? Si así fuera, podrían realizarse viajes espaciales de larga duración como el imaginado por Arthur C. Clarke, con astronautas en estado de hibernación al cuidado de la computadora de a bordo. Y podrían también hacerse realidad posibilidades más inquietantes. Por ejemplo, alguien con una enfermedad incurable podría ser puesto en animación suspendida hasta que se desarrolle una cura para su mal. Si esto ocurre en, digamos, cincuenta años, encontrará al volver a la vida un mundo muy diferente. ¿Podrá adaptarse?  Y ni que decir que esta posibilidad será una opción sólo para los más ricos, exacerbando de este modo las diferencias sociales. Pero todo depende de que grupos como el de Tisherman y colaboradores tengan éxito en sus emprendimientos.  Lo que está todavía por verse.",
    "Hace unos 2,600 años en Mesopotamia, en lo que hoy es territorio de Irak y Siria, ocurrió un acontecimiento sorprendente: en apenas treinta años después de alcanzar su máximo esplendor y extensión territorial, el imperio asirio, el más poderoso de su tiempo, fue conquistado por una coalición de medos y babilonios.  Hasta su colapso, el imperio asirio -o neo-asirio- había tenido una vida de tres siglos y en su máxima extensión abarcó desde Egipto y el mar Mediterráneo, hasta el golfo Pérsico y el occidente de Irán. El colapso del imperio asirio ha desconcertado a los especialistas. Ha sido atribuido a diversas causas, incluyendo revueltas internas, inestabilidad política, problemas económicos y derrotas militares propinadas por la coalición de medos y babilonios -los cuales, sin embargo, estaban muy lejos de tener la fortaleza militar de los asirios-. Un artículo aparecido esta semana en la revista “Science Advances”, publicado por un grupo internacional de investigadores encabezado por Ashish Sinha de la Universidad Estatal de California, propone una explicación adicional.    De acuerdo con Sinha y colaboradores, el colapso del imperio asirio fue debido al cambio climático. Específicamente, a sequías severas que afectaron a la productividad agrícola y que se prolongaron por décadas. Llegan a esta conclusión después de llevar a cabo un estudio para determinar el clima que ha prevalecido en el norte de Irak a lo largo de los últimos 4,000 años.Para este propósito midieron la composición de isótopos de oxígeno y de carbón de estalagmitas encontradas en una cueva en el norte de Irak cercana al sitio en el que se localizó Nínive, la capital del imperio asirio. Como sabemos, las estalagmitas son estructuras cónicas que se forman gradualmente capa por capa en el piso de una cueva, por el agua que cae desde el techo arrastrando minerales disueltos. A partir de la composición de isótopos de una estalagmita, es posible determinar las condiciones climáticas que prevalecieron en el momento de su formación. Así, horadando una estalagmita y midiendo su composición de isótopos para diferentes profundidades, Sinha y colaboradores pudieron determinar cuál fue el clima en el pasado.   Por otro lado, si bien la medición de las composiciones de isótopos nos da información sobre el clima en tiempos pretéritos, no nos permite conocer el momento preciso en que ocurrieron tales o cuales condiciones climáticas. Para averiguarlo, Sinha y colaboradores aprovecharon que el agua filtrada por el techo de la caverna tiene trazas del elemento uranio, el cual se sabe que es radiactivo y que decae -al azar pero en un tiempo promedio bien conocido- en el elemento torio. Las diferentes capas de las estalagmitas tienen entonces atrapados uranio y torio en diferentes proporciones, más torio y menos uranio en cuanto más antigua sea la capa. Así, determinando estas proporciones, Sinha y colaboradores fijaron la antigüedad de cada capa de la estalagmita y combinando esta información con la obtenida a partir de las mediciones de isótopos de oxígeno y carbono, obtuvieron un panorama completo del clima que prevaleció en el norte de Irak en el momento de la desintegración del imperio asirio.   De acuerdo con sus resultados, el norte de Irak disfrutó condiciones climáticas excepcionales y propicias para la agricultura por un periodo de unos 200 años. Este periodo corresponde al esplendor y expansión del imperio asirio. A este periodo, sin embargo, y coincidiendo con el colapso de dicho imperio, la región sufrió de una sequía extrema por varias décadas que habría afectado gravemente a la agricultura. El clima habría de este modo jugado una doble partida para los asirios: por un lado, habría creado condiciones excepcionales para su expansión, y por el otro, de manera abrupta habría llevado las cosas al extremo opuesto, provocando su colapso como sociedad.Si bien encuentran Sinha y colaboradores una correlación entre las condiciones climáticas, y el desarrollo y colapso del imperio asirio, reconocen que no necesariamente dicha correlación implica una relación de causa-efecto. Arguyen, no obstante, que la fuente de los productos agrícolas que sostenían al imperio asirio se localizaba justamente en el norte de Irak, en donde ocurrieron los cambios climáticos extremos que reportan en su artículo.Ciertamente, no es posible evitar poner a los resultados de Sinha y colaboradores en el contexto actual de cambio climático en el que se encuentra nuestro planeta y terminaríamos citando un párrafo de Sinha y colaboradores en un artículo aparecido en el sitio en línea “The Conversation”: “El cambio climático llegó para quedarse. En el siglo XXI tenemos lo que no tuvieron los asirios: una comprensión retrospectiva y una abundancia de datos climáticos. Un crecimiento no sostenible en regiones políticamente volátiles y con problemas de agua, es una receta para el desastre, bien probada a lo largo del tiempo”.",
    "El 6 de septiembre de 1856, en el diario de Elberfeld, ciudad alemana cercana a  Dusseldorf, apareció la siguiente noticia: “En el vecino valle de Neander, en las así llamadas “Rocas”, se llevó a cabo un sorprendente descubrimiento en días recientes. Durante la demolición de los acantilados de piedra caliza, lo cual no puede ser lo suficientemente lamentado desde el punto de vista estético, se descubrió una cueva, que en el curso de los siglos había sido cubierta con sedimentos arcillosos. Después de excavar en dichos sedimentos, se descubrió una costilla humana que sin duda habría pasado sin ser notada y perdida de no haber sido, afortunadamente, por el Dr. Fuhlrott de Elberfeld quién aseguró e investigó el descubrimiento”. Todo esto según Friedmann Schrenk y Stephanie Muller en su libro del año 2005  “Los Neandertales”.El descubrimiento de Fuhlrott tuvo una enorme trascendencia científica. Ésta, sin embargo, no fue evidente para los editores del diario de Elberfeld, los cuales, según la Wikipedia, complementaron su nota con frases -no incluidas en la cita de Schrenk y Muller- que le añadieron un considerable colorido.  Así, escribió el diario de Elberfeld: “Después de examinar el esqueleto, de manera precisa, el cráneo, se descubrió que el individuo pertenecía a la tribu de los Cabezas Planas, los cuales aún viven en el Oeste Americano y de los cuales se han encontrado varios cráneos en el alto Danubio y en Sigmaringen. Quizá el descubrimiento pueda aclarar si el esqueleto pertenece a uno de los primitivos habitantes de Europa central o bien si simplemente se trata de uno de los hombres de las hordas vagabundas de Atila”. En realidad, lejos de pertenecer a un miembro de la tribu de los Cabezas Planas o de las hordas de Atila, los restos descubiertos por Fuhlrott, ahora sabemos, pertenecieron a un individuo de la especie Neandertal, el cual habría muerto hace unos 40,000 años. En comparación con nuestra especie, los Neandertales eran de corta estatura y más corpulentos, tenían un cerebro más grande, un cráneo más alargado, y arcos superciliares prominentes, rasgos que reflejaban los fósiles del valle de Neander.   Haber descubierto que en el pasado existió una especie similar a la nuestra, pero al mismo tiempo con diferencias significativas, constituyó, por supuesto, una gran sorpresa. De hecho, algunos se negaron a aceptarlo. Por ejemplo, para el anatomista y fisiólogo alemán Franz Mayer, los restos descubiertos por Fuhlrott eran los de un cosaco ruso que murió durante la guerra de liberación contra Napoleón. Pronto, sin embargo, fue evidente que se trataba de una nueva especie, la cual fue bautizada como “Homo Neanderthalensis” en 1864. Quizá, de manera esperable, en la medida en que nos enteramos de que en algún momento tuvimos un competidor como especie, la reacción natural fue la de asumir que tenía una inteligencia muy pobre en comparación a la nuestra. Después de todo, los neandertales se habían extinguido mientras que nosotros seguíamos aquí, sobre la superficie del planeta. Poco a poco, sin embargo, se ha revelado que no necesariamente fue así y que los neandertales no eran de ningún modo estúpidos. Podían incluso pensar de manera simbólica.Un artículo aparecido esta semana en la revista “Science Advances” arroja luz con respecto a esto último. Dicho artículo fue publicado por un grupo internacional de investigadores, encabezado por Antonio Rodríguez-Hidalgo del Instituto de Evolución en África, Madrid, España, y en el mismo se presenta un análisis de una falange de la pata izquierda de un águila, la cual fue recuperada del sitio conocido como Cueva Foradada en Cataluña, España. Dicha falange muestra marcas características que indican que fueron hechas para separar la garra de la pata del águila, presumiblemente para usarla como dije en un collar. Es sabido que los neandertales en Europa hicieron uso de las garras de águila para fabricar collares para usarlos como ornamentos. Los resultados de Rodríguez-Hidalgo y colaboradores muestran, por primera vez, que dicho comportamiento simbólico se extendió hasta la península Ibérica, que fue el último reducto de los neandertales en Europa antes de su extinción hace unos 40,000 años.    El artículo de Rodríguez-Hidalgo y colaboradores añade evidencia sobre las capacidades cognitivas de los neandertales, particularmente sobre su capacidad para pensar en forma simbólica, lo que los coloca en una posición elevada en el proceso evolutivo de las especies del mundo. Dicha posición, por otro lado, ha ido mejorando en la medida en que se han desarrollado técnicas cada vez más sofisticadas para estudiar el pasado. Así, sabemos ahora que tenemos un antecesor común con los neandertales, y que éstos convivieron con nuestros ancestros e incluso se cruzaron y produjeron descendientes fértiles, de modo que los genes neandertal están entre nosotros. Sabemos también que enterraban a sus muertos y que producían obras de arte.De haber sabido todo esto, los europeos del siglo XIX con seguridad se hubieran choqueado. Después de todo, aun para los especialistas era difícil desembarazarse de los prejuicios raciales propios de una época en la que Europa dominaba al mundo. Así, si para un europeo del siglo XIX era obvia la superioridad de la raza blanca por sobre las demás razas del mundo, era impensable que una especie diferente -que no raza diferente- les hubiera podido hacer sombra alguna.",
    "Un artículo aparecido esta semana en la revista “Science”, publicado por George Crabtree del Laboratorio Nacional de Argonne y de la Universidad de Illinois en Chicago, nos da una perspectiva de la penetración de los automóviles eléctricos y de los problemas tecnológicos que habrían de resolverse para que la sustitución de los vehículos de gasolina o diesel por vehículos eléctricos se de en forma plena. De acuerdo con cifras citadas por Crabtree, algunos países están tomando medidas agresivas para lograr esto último en el corto-mediano plazo. Así, para el año 2025 el 100% de los automóviles en Noruega serán eléctricos o híbridos, mientras que en ese mismo año Holanda prohibirá los automóviles de gasolina o diesel y lo mismo que hará Alemania en 2030. Un poco más adelante, en el año 2040, Francia e Inglaterra planean dejar de vender automóviles de gasolina y diesel. China, por su lado, es el mayor consumidor de automóviles eléctricos del mundo. Posee casi la mitad de los automóviles de este tipo del total global, con 1.1 millones de unidades vendidas en su territorio en 2018.  En comparación, los Estados Unidos y Europa, cuentan cada uno de ellos con aproximadamente el 20% del total de vehículos eléctricos del planeta.Una de las virtudes de los automóviles eléctricos es la de contribuir a reducir la emisión de gases de invernadero por la quema de combustibles fósiles, habida cuenta que el transporte hace uso de más de un 25% de la energía que se consume en el mundo. Esto, por supuesto, ocurrirá sí la energía eléctrica para impulsar los automóviles eléctricos del futuro se obtiene de fuentes no contaminantes. Es decir, de fuentes tales como la solar, el viento, la nuclear o la hidroeléctrica, entre otras.  Si bien, asumiendo esto último, la reducción de gases de invernadero es una de las ventajas que tendrán los automóviles eléctricos, su emergencia, como apunta Crabtree, tendrá también consecuencias geoeconómicas. En decir, las regiones en donde se encuentran concentrados los combustibles fósiles perderán relevancia, dado que el sol y el viento, dos de las supuestas fuentes de la energía que moverán a los automóviles eléctricos del futuro, están distribuidos más democráticamente a lo largo del mundo.   Dado que la industria eléctrica tiene un alto grado de desarrollo, Crabtree identifica a la batería de litio, necesaria para almacenar la energía que impulsa al automóvil eléctrico, como su componente más débil. Según Crabtree, serán los desarrollos tecnológicos futuros de esta batería los que determinarán el curso de la industria de los vehículos eléctricos. Dichos desarrollos determinarán el costo del automóvil eléctrico, su tiempo de recargas, la autonomía entre recargas, su tiempo de vida y su nivel de seguridad.  En cuanto al costo de un automóvil eléctrico, incluyendo su adquisición, mantenimiento, combustible y gastos de seguro, éste es, de acuerdo con Crabtree, sustancialmente menor que el del automóvil de gasolina para vehículos de uso intensivo, como es el caso de un taxi o automóvil de alquiler con un recorrido de 100,000 kilómetros por año. Para un automóvil particular, el costo de adquirir y operar un automóvil eléctrico es unas dos veces superior al el de automóvil de gasolina. Se estima que ambos costos se equipararían en algún momento entre los años 2022 y 2026.En el contexto anterior, se esperaría que el transporte público en automóvil en los centros urbanos será el primero en transformarse en eléctrico por razones económicas. Lo mismo sucedería en unos años más con los automóviles de transporte personal en la medida en la que se reduzcan los costos de los vehículos eléctricos y se equiparen con los vehículos de gasolina. Cuando esto ocurra, la transformación de gasolina a electricidad estará impulsada por motivos económicos más que por consideraciones medioambientales de reducción de los contaminantes atmosféricos. Y en último término, según Crabtree, por el desarrollo de la tecnología de las baterías de litio. En el contexto relatado anteriormente, no sería difícil predecir que en pocos años, en la medida en que se desarrolle todavía más la tecnología de las baterías de litio, seremos testigos de la sustitución de los automóviles de gasolina por automóviles eléctricos. Tendríamos ventajas con esta conversión. Por ejemplo, se disminuiría la contaminación urbana por la emisión de contaminantes producto de la operación de los vehículos de gasolina. También, presumiblemente, se reducirá tanto el costo de los automóviles, como los de su operación y mantenimiento. Pero, por otro lado, podría incrementarse el número de automóviles en nuestras atestadas calles y avenidas. Y esto no pareciera que lo pudieran soportar. Ni las calles ni nosotros.",
    "El premio Nobel de Física 2014 fue otorgado a los investigadores japoneses Shuji Nakamura, Isamu Akasaki y Hiroshi Amano por haber inventado una fuente luminosa compacta –o LED, por sus siglas en inglés- de color azul. ¿Cuál fue la relevancia de un LED de este color que mereciera a sus desarrolladores un permio Nobel? Esta pregunta es válida, habida cuenta que anteriormente a la aparición de los LEDs azules existían ya LEDs de luz roja -lo mismo que LEDs de luz verde, si bien no tan eficientes- cuyos inventores no recibieron la misma distinción. Resulta que si bien el color azul pudiera no ser más relevante que el rojo para nosotros los humanos -dado que nuestros ojos son sensibles a ambos colores-, los LEDs azules a diferencia de los rojos pueden ser usados para fabricar LED blancos. Estos constituyen fuentes de luz compactas y eficientes, que han provocado una revolución en el campo de la iluminación artificial, y están paulatinamente sustituyendo a los antiguos e ineficientes focos incandescentes, lo mismo que a las lámparas fluorescentes, más eficientes pero contaminantes del medio ambiente por el mercurio que contienen.Los LEDs blancos -al igual que otros desarrollos como el transistor o los láseres compactos que fueron ambos también motivo de premios Nobel- presentan numerosas ventajas y han, ciertamente, tenido un impacto masivo en nuestra vida diaria. Al mismo tiempo, sin embargo, tienen desventajas. En efecto, existe preocupación entre los especialistas por posibles daños a la salud que pudieran provocar, incluyendo daños a la retina y la afectación a nuestros patrones de sueño y al ritmo circadiano que nos regula. Para poner esto último en perspectiva, habría que considerar algunos detalles relativos a la tecnología de los LEDs blancos. Estos dispositivos están constituidos por un LED azul recubierto de un material fosforescente que emite una radiación amarilla en respuesta a la excitación de la luz azul del LED. El LED emite así una combinación de luz azul y amarilla que le da una apariencia de luz blanca. El contenido de luz azul emitida por los LEDs blancos típicos, sin embargo, es demasiado grande en comparación con la luz solar -bajo la cual hemos evolucionado como especie-, y es esto lo que los especialistas han encontrado pudiera ser dañino para nuestra salud.Con referencia a esto último, un artículo aparecido esta semana en la revista “Aging and Mechanisms of Disease” publicado por un equipo de investigadores encabezado por Trevor Nash de la Universidad de Oregon, encuentra que especímenes de mosca de la fruta expuestos a luz azul sufren una reducción sustancial de su tiempo de vida en comparación con moscas mantenidas en la oscuridad, o bajo luz blanca a la que se le removió la componente azul. Durante su estudio, Nash y colaboradores expusieron a un grupo de moscas a un régimen de 12 horas bajo luz azul y 12 horas en oscuridad, lo que resultó en un daño a las células de la retina, una degeneración neuronal y daños en el sistema de locomoción de las moscas, que afecto su habilidad para trepar paredes.   Los resultados de Nash y colaboradores son sin duda malas noticias pues apuntan a la posibilidad de que los daños neuronales y acortamiento de la vida observados en las moscas de la fruta puedan ocurrir también con los humanos, que estamos continuamente expuestos a la luz azul que generan tanto las lámparas LED para iluminación nocturna, como dispositivos tales como teléfonos, tabletas electrónicas y pantallas de computadora. En realidad, los LEDs blancos han estado entre nosotros solamente por un tiempo relativamente corto y por tanto no sabemos con certidumbre el impacto que tienen sobre nuestra salud. En este contexto, son necesarias más investigaciones para confirmar -o refutar- los resultados de Nash y colaboradores, lo mismo que los resultados de otros investigadores que, igualmente, apuntan hacia los efectos nocivos de la luz azul. Por lo pronto y en tanto averiguamos que tan real es la amenaza azul, podríamos quizás tomar algunas precauciones. Por ejemplo, podríamos hacernos de unos anteojos con cristales color ámbar que filtren la luz azul y usarlos durante la noche. Perderemos algo de luz, pero en cambio estaríamos quizá a salvo del peligro. Y en cuanto a dispositivos tales como teléfonos, tabletas electrónicas y pantallas de computadores, éstos pueden ser ajustados para mitigar la emisión de luz azul. Así, la situación no resultaría ser tan desesperada y no tendríamos que recurrir a medidas extremas como sería la de renunciar a la iluminación nocturna y regresar a épocas ya superadas en las que reinaba la oscuridad de la noche. Sin perder de vista, por supuesto, que el peligro puede ser real y que más nos vale averiguarlo a la brevedad posible",
    "El 17 de febrero de 1600, Giordano Bruno fue quemado vivo en la plaza Campo de Flores en Roma después de que la inquisición romana lo condenó por herejía. Si bien no fue la razón principal para su condena, Bruno fue acusado de sostener que el universo es infinito y que cada estrella que vemos en el cielo es en realidad un sol como el nuestro, con planetas orbitando a su alrededor. El 31 de octubre de 1995, 365 años después de su ejecución, supimos con certeza que, con respecto a este último punto, Bruno tenía razón plena. Lo supimos porque fue en esta fecha que la revista “Nature” aceptó el artículo sometido por Michel Mayor y Didier Queloz de la Universidad de Ginebra, Suiza, en el que reportaban el descubrimiento de un planeta fuera de nuestro sistema solar, orbitando alrededor de una estrella similar a nuestro sol. Dicho planeta, y más propiamente dicho, exoplaneta, fue bautizado posteriormente como Dimidio.Dimidio se encuentra a una distancia de aproximadamente 50 años luz de la Tierra -un año luz es la distancia que recorre la luz en un año- y es similar en composición al planeta Júpiter. En contraste con Júpiter, sin embargo, Dimidio orbita a su estrella a una distancia considerablemente menor y esto hace que su temperatura sea superior a los 5,000 grados centígrados. El descubrimiento de Dimidio cobró relevancia esta semana cuando a Mayor y Queloz les fue concedido el premio Nobel de Física por haberlo llevado a cabo. El impacto que dicho descubrimiento ha tenido en nuestro conocimiento del universo ha sido muy grande, y a raíz del mismo se han descubierto más de 4,000 exoplanetas girando alrededor de estrellas en nuestra galaxia. Sabemos que cuando Galileo apuntó su telescopio hacia Júpiter -que se ve como un punto brillante a simple vista- pudo observar un disco con cuatros puntos luminosos a su alrededor -los cuatro satélites galileanos-. Cualquiera de nosotros con un telescopio relativamente sencillo puede repetir la experiencia de Galileo. Si, por otro lado, dirigimos nuestro telescopio hacia una estrella, seguiremos viendo un punto brillante. Obviamente, esto es debido a que las estrellas están a distancias enormemente mayores que las distancias a las que se encuentran los planetas. Dimidio, por ejemplo, está casi un millón de veces más lejos que la Tierra de Júpiter en su máximo acercamiento.   En estas condiciones, no es difícil concluir que debe ser tremendamente difícil descubrir un exoplaneta orbitando una estrella, la cual tiene un tamaño enorme en comparación con el tamaño del planeta a detectar. Además, al no tener luz propia, la brillantez del exoplaneta es muchísimo menor y esto hace difícil distinguirlo de la estrella. Con todas estas dificultades ¿cómo se la arreglan los astrónomos para descubrir exoplanetas? Una manera para hacerlo es mediante mediciones de la intensidad de la luz emitida por la estrella, tomando en cuenta que al colocarse el exoplaneta enfrente de la estrella bloquea una parte -mínima- de la luz que emite. Así, si sus mediciones de intensidad de luz son suficientemente precisas, los astrónomos podrán detectar pequeñas variaciones periódicas en la luminosidad de la estrella que revelarán la presencia de un exoplaneta orbitando a su alrededor.Otra manera de detectar un exoplaneta -que fue la empleada por Mayor y Queloz- hace uso del llamado efecto Doppler. Este efecto es el que produce el cambio en el tono de la sirena de una ambulancia cuando cruza enfrente de nosotros. Como bien sabemos, dicho tono es más grave cuando la ambulancia se aleja a gran velocidad que cuando se acerca. Un efecto similar ocurre cuando observamos la luz que emite un objeto cuando un objeto se aleja o se acerca. En este caso los cambios se observan en el color de la luz emitida, que es más azul para el objeto que se acerca en comparación con el que se aleja.Para descubrir un exoplaneta por medio del efecto Doppler los astrónomos hacen uso del bamboleo hacia adelante y hacia atrás que sufre una estrella cuando un planeta gira alrededor de ella y que la hace acercarse y alejarse de nosotros una y otra vez. Una medición en los cambios periódicos en el color de la luz emitida por la estrella revelará entonces la presencia de un exoplaneta. Dichos cambios, por supuesto, son extremadamente pequeños, de modo que los astrónomos deben emplear instrumentos de medición con una gran sensibilidad. Dados todos estos impresionantes resultados ¿qué podríamos esperar para el futuro? Entre otras cosas, que se mantenga un ritmo acelerado de descubrimiento de nuevos exoplanetas, incluyendo mundos similares a la Tierra con condiciones adecuadas para el surgimiento de la vida. Y como siguiente paso natural, la detección de señales de vida extraterrestre. Esto último, dada la inmensa cantidad de exoplanetas en nuestra galaxia y el continuo avance en las técnicas de detección, con seguridad se logrará más temprano que tarde. Y con esto, la visión de Giordano Bruno de un universo infinito y uniforme se hará realidad con todas sus consecuencias.",
    "Cuando la activista del clima Greta Thurnberg, de 16 años, se cruzó de manera no programada con Donald Trump el pasado 23 de septiembre durante la cumbre climática de la ONU, le echó una mirada con ojos de pistola que fue ampliamente difundida por los medios internacionales. Ciertamente, manipulada o no, a la adolescente sueca no le faltaron razones para mostrarse resentida con el presidente norteamericano, que retiró en 2017 a los Estados Unidos del acuerdo climático de París que negociaron 195 países para limitar la emisión de gases de invernadero a la atmósfera. De hecho, durante su intervención en la cumbre, Greta Thurnberg había hecho un fuerte reclamo -actuado, según algunos- a los líderes del mundo por su inacción para combatir el cambio climático en perjuicio de las generaciones futuras.Por otro lado, si bien el discurso de la activista sueca ha generado reacciones encontradas -incluyendo críticas negativas de los presidentes Trump y Putin-, es difícil refutar sus afirmaciones sobre la gravedad del cambio climático en curso. Una manera de apreciar dicha gravedad es como sigue. El cambio climático es atribuido por los expertos fundamentalmente a la creciente emisión de dióxido de carbono a la atmósfera por el uso de combustibles fósiles. Esta emisión ha perturbado el llamado ciclo del carbono que regula la concentración de este elemento químico en la superficie de nuestro planeta. Como nos explica un artículo aparecido esta semana en la revista “Elements”, publicado por un grupo de investigadores encabezado por Celina Suárez de la Universidad de Arkansas, el carbono existe en la superficie y en el interior de la Tierra en diversas formas -combinado, por ejemplo, con oxígeno para formar dióxido de carbono-. En la superficie de la Tierra, el dióxido de carbono es capturado por las plantas y algas para fabricar materia orgánica mediante el proceso de fotosíntesis. Esta materia orgánica es ingerida por los animales que al morir y ser enterrados originan el crecimiento de nuevas plantas. De esta manera se cierra el ciclo biológico del carbono. Existe también un ciclo geológico del carbono mediante el cual este elemento químico es intercambiado entre la superficie de la Tierra y su interior. Así, el carbono en la forma de dióxido de carbono es emitido a la atmósfera durante las erupciones volcánicas y regresado al interior de la Tierra mediante varios procesos, que incluyen su captura por las plantas mediante el proceso de fotosíntesis, su enterramiento al morir, y su transferencia a grandes profundidades por procesos geológicos. El carbono puede ser regresado igualmente al interior de la Tierra mediante su incorporación química a rocas superficiales, las cuales son posteriormente enterradas -en una escala geológica de tiempo- a grandes profundidades.De este modo, la cantidad de carbono en la superficie de la Tierra, y por tanto su concentración en la atmósfera, está determinada por un balance entre el carbono superficial que es capturado por algún mecanismo y enviado al interior de la Tierra, y aquel que es liberado desde dicho interior hacia la superficie. En la actualidad, este balance ha sido perturbado por la extracción de carbono en la forma de combustibles fósiles y su dispersión en la atmósfera en la forma de dióxido de carbono.El punto crucial es, por supuesto, la relevancia de esta perturbación en comparación con los mecanismos naturales de dispersión de este contaminante en la atmósfera. Suarez y colaboradores, en el artículo mencionado líneas arriba nos ilustran al respecto: las emisiones de dióxido de carbono a la atmósfera por la quema de combustibles fósiles son de cuarenta a cien veces más grandes que las emisiones debidas los procesos de vulcanismo. Esto, ciertamente, refuta a aquellos que todavía argumentan que el incremento paulatino de la concentración de gases de invernadero en nuestra atmósfera obedece a causas naturales. Suarez y colaboradores señalan también que cuatro de las cinco más grandes extinciones masivas de especies que han ocurrido en la historia de la Tierra están asociadas perturbaciones mayores del ciclo del carbono. La más famosa de estas perturbaciones es la que ocurrió hace 66 millones de años por el meteorito que se estrelló cerca de Chicxulub en la costa de Yucatán y que llevó a la extinción de los dinosaurios.En la actualidad, las cantidades de dióxido de carbono que estamos emitiendo a la atmósfera por la quema de combustibles fósiles están lejos de aquellas debidas a catástrofes como la Chicxulub. Dichas emisiones, sin embargo, están creciendo paulatinamente y en estos momentos son ya muy superiores a las debidas a causas naturales, lo que, sin duda, es motivo de preocupación.En estas condiciones es difícil entender a aquellos que critican la actuación de Greta Thurnberg. Entre otras cosas por ser demasiado joven y por la sospecha de que pueda estar manipulada por intereses oscuros; pero también por el énfasis con el que pronunció su discurso en la cumbre climática de la ONU, que a algunos habría parecido demasiado teatral. En todo caso, se descalificaría al mensajero, pero difícilmente a su mensaje.",
    "Si tiene interés en viajar al espacio, la NASA le ofrece llevarlo en calidad de turista a la estación espacial internacional por 58 millones de dólares. Hay que aclarar que este costo es solo por el transporte y no cubre los costos de alojamiento, que son de unos 35,000 dólares por noche. De este modo, si planea una estancia de 30 días en el espacio añada un millón de dólares al costo del transporte.Desafortunadamente para muchos de nosotros, con estos costos el viaje al espacio está fuera de cualquier posibilidad. Esperaríamos que en el futuro dicho costo se reduzca y crezca el grupo de aquellos que podrían cubrirlo, al mismo tiempo se amplían los destinos turísticos espaciales, a la Luna y a otros lugares de nuestro Sistema solar.   Si esto llegara a suceder ¿Cuáles serían los destinos favoritos? De entrada, con seguridad descartaríamos a Mercurio y a Venus -los dos planetas más cercanos al Sol- cuyas temperaturas en su superficie están lejos de ser acogedoras. En el caso de Mercurio oscilan entre 430 grados centígrados y menos 180 grados centígrados, mientras que la temperatura superficial de Venus ronda los 500 grados centígrados.  Marte, nuestro vecino cercano, posiblemente resultase una mejor opción para el turismo. Las fotografías que nos han hecho llegar las sondas de la NASA que se han posado sobre su superficie nos muestran un lugar seco y polvoriento, pero que nos recuerda algunos parajes de nuestro planeta; el desierto de Atacama en el norte de Chile, por ejemplo. Habría, no obstante, que considerar que en Marte la temperatura de su superficie varía por más de 100 grados centígrados entre el día y la noche. Otro inconveniente es que la atmósfera marciana, además de tenue, está fundamentalmente compuesta de dióxido de carbono y por lo tanto no habría manera de respirarla. Y uno más es que la superficie de Marte está continuamente bombardeada por peligrosas radiaciones de alta energía que llegan desde el espacio.  Así, habría que tomar con las reservas del caso las fotografías de la NASA que muestran un planeta relativamente amigable, pero que en realidad es inhóspito en gran medida.Los planetas exteriores, los gigantes de gas, Júpiter, Saturno, Urano y Neptuno, posiblemente no resulten del todo atractivos para una visita turística. Esto, sin embargo, no es necesariamente cierto en cuanto a sus satélites. Titán, por ejemplo, el satélite más grande de Saturno, resulta fascinante por muchas razones. Es, por ejemplo, el único cuerpo de Sistema solar, aparte de la Tierra, en donde hay mares, lagos y ríos de líquidos que se evaporan, se condensan y se precipitan en forma de lluvia. Además , en contraposición con nuestro planeta, dichos líquidos son de metano y no de agua. Esto último es debido a que a la temperatura de la superficie de Titán -menos 180 grados centígrados- el agua está congelada, mientras que el metano, que es un gas en la Tierra, se encuentra en forma líquida.No conocemos mucho de Titán por su atmósfera brumosa que oculta los detalles de su superficie. En enero de 2005 la sonda Hyugens de la Agencia Espacial Europea descendió hacia Titán y se posó suavemente en su superficie. A unos 70 kilómetros de altura, las cámaras de la sonda pudieron ver a través de la espesa bruma, observando un paisaje con montañas de hielo, un sistema de canales por el que aparentemente fluyen ríos de metano, y lo que supuestamente es el lecho seco de un lago. Una vez que se posó en la superficie del satélite, la sonda envió una fotografía del panorama a su alrededor mostrando pequeñas piedras de hielo.Así, Titan resulta ser un mundo fascinante, con muchas similitudes con el nuestro, y en el que el papel del agua lo tiene el metano. De hecho, es posible que sea el lugar más interesante para visitar dentro de Sistema solar y posiblemente sería un lugar favorito para los hipotéticos turistas del futuro -por más que su temperatura ambiental sea sensiblemente menor que aquella a la estamos acostumbrados. No sabemos, por supuesto, si la tecnología espacial permitirá alguna vez realizar viajes turísticos a lugares tan remotos como Titán.  En todo caso, por el momento tendremos que conformarnos con explorar este exótico satélite por otros medios. Para este propósito, la NASA planea enviar la misión “Dragonfly” a Titán en el año 2026, misma que arribará a su destino en 2034.    De manera adicional, la NASA acaba de dar a conocer un proyecto de desarrollo de un robot para explorar Titán. Dicho robot, que ha sido bautizado “Shapeshifter”, tendrá la capacidad de adoptar diversas formas en función de las circunstancias con las que se encuentre en un mundo tan poco conocido. Así, “Shapeshifter” podrá rodar, nadar e incluso dividirse en dos mitades, cada una de las cuales podrá volar al estilo de un dron. La NASA pretende enviar a Titán no uno sino un grupo de robots “Shapeshifter”, que podrán actuar cada uno por su cuenta, o bien operar de manera coordinada y autónoma para realizar una tarea común.   Para la exploración del espacio, dadas las dificultades y peligros que encierra, la estrategia racional a seguir pasa por el empleo de sondas automáticas en contraposición a los viajes tripulados. El turista, sin embargo, tiene su propia lógica no utilitaria y, dadas las condiciones favorables, podría quizá estar dispuesto a pagar 59 millones de dólares -o lo que sea necesario- para visitar un mundo exótico fuera de nuestro planeta, a pesar de los riesgos que esto implicaría. Afortunadamente, para la amplia mayoría de nosotros no habría oportunidad de correr riesgo alguno. Pero sí posiblemente de conocer Titán y otros mundos a través de las sondas remotas",
    "Una exposición inusual se inauguró el pasado 13 de septiembre en la Bolsa de Valores de Nueva York con el título “Redemption of Vanity”. En dicha exposición se expone un diamante de 16.78 quilates (poco más de tres gramos de peso) con un espectacular color amarillo y un valor de 2 millones de dólares. La exposición resulta inusual, no tanto por lo espectacular de la piedra exhibida, sino por el recubrimiento que se le aplicó y por el cual perdió su color. De hecho, lo perdió por completo, pues el diamante luce en la exposición con un color negro profundo nunca antes visto. Pero vayamos por partes.Sabemos que en algunos casos podemos ver objetos por la luz que emiten, como sucede con el Sol o con las lámparas para iluminación nocturna. Sabemos también que la mayor parte de los casos los objetos no emiten luz propia y que, no obstante, podemos verlos por la luz que reflejan. Así, los objetos se ven rojos o verdes porque es fundamentalmente roja o verde la luz que reflejan. Los objetos blancos, en cambio, no muestran preferencias y reflejan a todos los colores por igual. Por su parte, los objetos negros reflejan muy poca de la luz que reciben, y entre menos reflejen más negros lucen.    Un objeto perfectamente negro no refleja luz en absoluto y cabe preguntarse por el aspecto que tendría. No es difícil llegar a una conclusión. Pensemos, por ejemplo, en la boca de una cueva profunda por la que penetra luz que difícilmente vuelve a salir, y que en consecuencia podemos pensar como un objeto virtual perfectamente negro. Un objeto negro real luciría entonces como la boca de una cueva profunda; es decir, como una sombra negra y plana definida por la silueta del objeto en cuestión, sin la menor indicación de su perfil en profundidad.    No hay, por supuesto, objetos perfectamente negros y todos reflejan luz en mayor o menor medida. Se han desarrollado, sin embargo, recubrimientos que, aplicados a un objeto, lo hacen lucir de un negro profundo, no muy lejos de la perfección. Uno de estos recubrimientos fue aplicado al diamante exhibido en la Bolsa de Valores de Nueva York. Dicho recubrimiento está formado por un “bosque” de nanotubos de carbono que atrapa la luz que recibe, de la misma manera que lo hace una cueva profunda. La capa de nanotubos de carbono absorbe el 99.995 % de la radiación que recibe, y por tanto, el diamante, colocado en un pedestal dentro de un capuchón de vidrio, luce perfectamente negro en términos prácticos; esto, desde todos los ángulos en que se le observe. La exhibición es parte de un proyecto de colaboración entre Brian Wardle, profesor de aeronáutica y astronáutica del Instituto de Tecnología de Massachusetts, y el artista Diemut Strebe. Puede ser consultada tecleando en Google “The Redemption of Vanity”.  El artista juega con varios conceptos en la exposición. En particular, hace hacer notar que tanto el diamante como los nanotubos del recubrimiento se forman a partir de los mismos elementos químicos; es decir, átomos de carbono. El aspecto visual contrastante que presentan ambos tipos de materiales es debido a la manera en que los átomos de carbono se ordenan en su interior. Así, un diamante, un objeto altamente luminoso y con un elevado valor comercial, puede ser destruido y su luminosidad llevada a cero por un “bosque” de nanotubos con los mismos elementos químicos constituyentes. En el sitio de Internet del proyecto se puede leer: “El proyecto explora los valores materiales e inmateriales asociados a los objetos y conceptos en referencia al lujo, a la sociedad y al arte. Estamos presentando la devaluación literal de un diamante, que es altamente simbólico y de un alto valor económico. Se presenta un desafío a los mecanismos del mercado del arte, al mismo tiempo que se expresan preguntas sobre el valor del arte en un contexto amplio. De este modo, se inquiere sobre el significado del valor de los objetos de arte y del mercado del arte”.El recubrimiento de nantotubos de carbono empleado para hacer “desaparecer” al diamante fue desarrollado por Kehan Cui y Brian Wardle en el Instituto de Tecnología de Massachusetts y reportado esta semana en la revista ”ACS Applied Materials and Interfaces”. La investigación de Cui y Wardle no tuvo como propósito desarrollar un recubrimiento ultra-negro para diamantes, sino la pretensión de fabricar capas de nanotubos de carbono sobre una placa de un conductor de la electricidad como el aluminio. Una vez alcanzado su objetivo, sin embargo, Cui y Wardle se dieron cuenta que, además de cumplir con las propiedades físicas que buscaban, las capas de nanotubos de carbono desarrolladas eran diez veces más negras que cualquier otro material reportado hasta la fecha. Esto abrió la posibilidad de desarrollar la colaboración entre la ciencia, la tecnología y el arte desplegada en la Bolsa de Valores de Nueva York. No está claro si el diamante recubierto de nanotubos de carbono pueda ser restaurado a su condición original y su valor recuperado, o bien si su destrucción como diamante convencional es permanente. Si es esto último el caso, mantendrá un valor comercial como diamante “invisible” que podría no ser despreciable. Al margen de estas consideraciones, el proyecto tiene beneficios de relaciones públicas para los investigadores que desarrollaron el recubrimiento, lo mismo que para el artista que concibió el proyecto. Y seguramente para la casa que proporcionó el diamante de marras.",
    "Hace unos cinco mil años floreció una cultura en la región del valle del rio Indo, en los actuales Pakistán, norte de la India y sur de Afganistán, contemporánea de las culturas de Mesopotamia y Egipto antiguo, que era notablemente avanzada para su tiempo. Con respecto a esto último, los arqueólogos saben que sus ciudades fueron objeto de una planeación urbana cuidadosa y contaban con calles rectas formando una malla rectangular, lo mismo que con un sistema de drenaje y graneros para el almacenamiento de las cosechas. Incluso contaban con un baño público. Por su lado, las viviendas, disfrutaban de baño privado y de drenaje conectado a la red pública, pues la higiene personal habría sido una parte importante de la vida diaria. Igualmente, para propósitos de intercambios comerciales, la civilización del valle del Indo desarrolló un sistema estandarizado de pesas y medidas.     Y a pesar de todo lo anterior, los expertos saben relativamente poco acerca de dicha civilización.  Esto parcialmente se explica por el hecho que no ha sido posible descifrar su escritura. Como sabemos, los jeroglíficos egipcios fueron descifrados gracias a la piedra Rosetta -encontrada en 1799 cerca de Alejandría durante la expedición de Napoleón en Egipto- que contiene un texto en tres estructuras distintas: jeroglíficos, demótica y griego antiguo. La comparación de los textos escritos en egipcio y griego permitió a Champollion descifrar la escritura egipcia. Una situación similar e igualmente afortunada permitió descifrar la escritura cuneiforme de Mesopotamia. En el caso de la civilización del valle del Indo, los expertos no han tenido la suerte de encontrar un texto escrito en los idiomas, uno de ellos conocido, que permita descifrar la escritura de la civilización del valle del Indo y conocer más acerca de la misma.  No obstante la poca fortuna, los expertos han tenido éxito en otra dirección: un artículo aparecido esta semana en la revista Cell arroja luz sobre un aspecto particular de la civilización del valle del Indo, específicamente sobre la composición genética de sus integrantes. Dicho artículo fue publicado por un grupo internacional de investigadores encabezado por Vasan Shinde del Colegio de Posgraduados e Instituto de Investigación de Deccan, en Pune, India, y Vagheesh Narasinham, de la Escuela de Medicina de Harvard, en Boston, Estados Unidos.  En dicho artículo, Shinde y colaboradores reportan los resultados de un análisis genético hecho a los restos de un individuo, posiblemente del sexo femenino, recuperado de una tumba en el norte de la India con una antigüedad cercana a los cinco mil años. Inicialmente, los investigadores intentaron determinar al ADN de 61 restos óseos, pero el tiempo trascurrido desde su entierro y las condiciones del terreno solo permitieron recuperar el ADN de uno de ellos. El ADN que pudo ser recuperado resultó ser una mezcla de ADN iraní antiguo -anterior a la aparición de la agricultura hace diez mil años- y ADN de cazadores-recolectores del sudeste asiático.  El ADN recuperado, además, muestra coincidencias con el de los pobladores actuales de sudeste asiático, lo que indica, según los investigadores, que estos últimos son descendientes directos de los antiguos pobladores del valle del Indo.Por otro lado, concluyen Shinde y colaboradores que no hay una relación genética cercana entre el individuo desenterrado en el valle del Indo y sus entonces vecinos agricultores de Irán, lo que demostraría que la agricultura en dicho valle no fue desarrollada por grandes poblaciones de inmigrantes que llegaron del oeste. Para esto habría dos posibles explicaciones: o bien la agricultura se desarrolló en el valle del Indo con independencia de Mesopotamia, o bien fue desarrollada por viajeros que visitaron Mesopotamia y conocieron la agricultura, regresando después al valle del Indo a ponerla en práctica.    No obstante, como es común con un resultado científico -y sobre todo si es con respecto a algo ocurrido hace miles de años, como es el caso- hay quién se muestra cauteloso y considera que las conclusiones de Shinde y colaboradores deben de ser tomadas como provisionales. Esto, debido a que están basadas en el análisis de ADN de un solo individuo, que podría ser no representativo de la población en general. Así, sería necesario que dichas concusiones fueran validadas con estudios adicionales.  En tanto esto sucede, es interesante especular sobre si la agricultura habría aparecido en un solo punto del planeta y de allí se habría expandido por la migración de agricultores a otras partes del mundo, o si bien fue un fenómeno que se dio de manera independiente en varios lugares. Por un lado, la agricultura ha existido solo por un pequeño periodo de tiempo desde que apareció nuestra especie, el cual se mide en cientos de miles de años, y esto podría quizá hacer dudar que el fenómeno hubiera ocurrido simultáneamente en varias partes del mundo. Por otro lado, también es cierto que no es infrecuente que un descubrimiento científico haya sido realizado por más de un científico de manera independiente.Lo que si no es una especulación es que, hace cinco mil años, existió una civilización sorprendentemente avanzada, con grandes ciudades, con calles, cañerías y baños públicos, y con habitantes preocupados por su aseo personal. Y esto es, sin duda alguna, fascinante.",
    "La noticia científica de la semana fue sin duda la publicación en la revista “Nature” de dos artículos coordinados en los que se reporta el descubrimiento de un cráneo fósil, casi completo, de un primate bípedo con una antigüedad de 3.8 millones de años. Dicho cráneo -llamado, de manera poco atractiva MRD—VP-1/1- corresponde a la especie “Australopithecus anamensis”, que es la más antigua del género australopitecus y que precedió a la especie “Australopithecus afarensis”. Esta última es la especie del famoso fósil conocido como Lucy, descubierto en Etiopía en 1974 y que tiene una antigüedad de 3.2 millones de años. Uno de los artículos, en el que se reporta el descubrimiento propiamente dicho, fue publicado por un grupo internacional de investigadores encabezado por Yohannes Haile-Selassie del Cleveland Museum of Natural History, en Cleveland, Ohio, y Stephanie Melillo del Max Planck Institute for Evolutionary  Antropology en Leipzig, Alemania. El segundo artículo reporta las técnicas empleadas para datar el cráneo y fue publicado también por un grupo internacional de investigadores, encabezado por Beverly Saylor de la Case Western Reserve University en Cleveland, Ohio.El descubrimiento de Haile-Selassie y colaboradores provocó un gran entusiasmo entre los especialistas pues ayuda a esclarecer detalles de la evolución de los primates bípedos del género austrolopitecus, particularmente, sobre la transición entre las especies “A. anamensis” y “A. afarensis”. Como sabemos, el género australopitecus dio origen al género homo al que pertenece nuestra especie hace unos dos millones de años, de modo que su evolución es parte de la evolución nuestra.  De acuerdo con algunas opiniones, habría ocurrido una evolución lineal según la cual la especie “A. afarensis” surgió de la “A. anamensis”, al mismo tiempo que ésta se extinguía. Los fósiles descubiertos por Haile-Selassie y colaboradores, sin embargo, indican que ambas, habrían coexistido por cuando menos 100,000 años. La evolución habría de este modo producido dos ramas, una de las cuales, no obstante, terminó por extinguirse.  Al margen de las interpretaciones y controversias científicas, el descubrimiento de Haile-Selassie y colaboradores del cráneo con 3.8 millones de años de antigüedad permite reconstruir el aspecto que tuvo un antecesor muy lejano nuestro, que no perteneció al género homo, no manejaba herramientas y tenía un cerebro del tamaño del de un chimpancé, pero que ya caminaba en posición erguida. De hecho, una reconstrucción artística del aspecto que habría tenido en vida MRD—VP-1/1 puede, por ejemplo, encontrarse en el número de esta semana de la revista “Science”. Por otro lado, si bien es posible saber con cierta precisión el aspecto que habría tenido el “A. anamensis”, otros detalles de su vida no son tan fáciles de deducir. Esto no es sorprendente, por supuesto, dada la enorme distancia temporal que nos separa de esta especie y los pocos fósiles de que disponemos para estudiarla. Sabemos, no obstante, que sí existió, en un remoto pasado, y que de un modo u otro es nuestro antecesor. Esto último es difícil de poner en duda hoy en día, lo que hasta hace muy poco tiempo no era necesariamente cierto. Así, había quien sostenía que nuestra especie de alguna manera ocupaba un lugar especial en el mundo, y argumentar que somos descendientes de una especie con un aspecto de simio y un cerebro del tamaño del de un chimpancé habría sido complicado. La pérdida de nuestra supuesta posición privilegiada entre las demás especies del mundo -por el avance del conocimiento paleontológico- de alguna manera evoca a la pérdida de nuestra supuesta posición privilegiada como centro de universo que ocurrió en los siglos XVI y XVII, cuando, por avances en la astronomía, quedó claro que el mundo se explica de una mejor manera si consideramos que es el Sol y no la Tierra el cuerpo celeste alrededor del cual giran los planetas, incluyendo el nuestro. Así, negar la evidencia que nos entregan los seres vivos fosilizados en tiempos remotos, para en su lugar sostener que ocupamos una posición privilegiada entre los demás seres vivos, complica en demasía el que podamos elaborar una explicación racional del mundo.En el futuro, en la medida en que avance la paleontología y todas las técnicas analíticas de las que se auxilia, y se descubran más restos fósiles, podemos esperar que tendremos una visión más clara de lo que ocurrió en el pasado remoto en cuanto a la evolución de las especies vivientes. Por lo pronto, nos conformamos con el maravilloso cráneo MRD—VP-1/1 y la fabulosa imagen del pasado remoto que nos trasmite. Lo que no es poco, a pesar del poco sexy nombre que alguien decidió imponerle.",
    "En lo que bien pasaría como una historia de terror, hace unos doscientos años un grupo de personas no identificadas exhumó el cadáver de John Barber varios años después de muerto, le cortó la cabeza y le abrió el pecho, posiblemente con la intención de extraerle el corazón. Hecho esto, lo devolvieron a su tumba, no sin antes colocarle el cráneo sobre el pecho juntamente con los fémures en cruz, al estilo de los símbolos piratas. Todo esto sucedió en el pueblo de Griswold en el estado de Connecticut en los Estados Unidos. ¿Por qué alguien habría hecho algo tan macabro? Cuando en 2012 el arqueólogo Nicholas Bellantoni exploró la tumba de John Barber, no encontró una explicación. Pronto se aventuró una hipótesis que lucía plausible: posiblemente se trataba de un episodio de vampirismo, que en esa época -inicios del siglo XIX- no era algo fuera de lo común en algunas áreas de Nueva Inglaterra. En esos momentos, sin embargo, no se obtuvo una conclusión firme al respecto y el misterio siguió sin resolverse.Recientemente, con mejores técnicas de análisis de ADN y mediante el uso de bases de datos genéticos disponibles en Internet, Bellantoni retomó el asunto y, por un lado, logró identificar a John Barber como el propietario de la tumba de referencia y, por otro lado, llegó a la conclusión que, efectivamente, la profanación de la tumba de Barber tuvo como propósito eliminarlo de este mundo por considerarlo un vampiro. De acuerdo con un artículo firmado por Michael Ruane en el diario The Washington Post, los resultados de la nueva investigación fueron presentados el pasado 23 de julio en el Museo Nacional de Salud y Medicina en Silver Spring, Maryland. Pero vayamos por partes.Durante los siglos XVIII y XIX se dio una terrible epidemia de tuberculosis en Nueva Inglaterra, tan grave que alrededor del año 1800 habría sido responsable del 25% de las muertes en el este de los Estados Unidos. Al no haber un tratamiento efectivo para curarla, un diagnóstico de tuberculosis era equivalente a una sentencia de muerte. En la medida en que progresaba la tuberculosis, los pacientes disminuían de peso y parecían consumirse, y de hecho, a la enfermedad se le conocía en ese entonces como consunción. Sufrían, además, entre otros síntomas, de ataques severos de tos, arrojando esputos con sangre y adquirían un color pálido. Siendo la tuberculosos, además, altamente contagiosa, había una alta probabilidad de adquirir la tuberculosis si algún miembro de la familia ya la padecía. Cabe destacar que el bacilo de Koch causante de la tuberculosis fue descubierto hasta finales del siglo XIX, mientras que los antibióticos necesarios para curarla no se desarrollaron sino hasta la década de los años cuarenta de siguiente siglo. En estas condiciones, sin una explicación racional sobre el origen de la enfermedad, los habitantes de Nueva Inglaterra recurrieron a la teoría del vampirismo, según la cual la consunción era producto de algunos ya fallecidos por la enfermedad y que en realidad no lo estaban del todo. Estos semi-muertos o vampiros robaban a sus víctimas su fuerza vital hasta consumirlas. Cuando se sospechaba que alguien ya muerto se hubiera convertido en vampiro, el procedimiento para remediar la situación era sacarlo de su tumba y observar si había signos que denotaran algún tipo de vida, tales como la presencia de sangre fresca en el corazón. Si tal cosa ocurriera, habría que extraerle el corazón y quemarlo. A menudo las víctimas de un vampiro eran sus propios familiares, de modo que el procedimiento de exhumar el cadáver en busca de signos de vampirismo era llevado a cabo por un miembro de la familia.Regresando al caso de John Barber, y de acuerdo con este panorama, los vecinos de Griswold habrían sospechado que era un vampiro, y la profanación de su tumba habría tenido como propósito averiguarlo con certeza. De acuerdo con las evidencias, los vecinos habrían encontrado pruebas positivas al respecto y procedido en consecuencia.Muy probablemente, sin embargo, los vecinos de Griswold se equivocaron de cabo a rabo y John Barber no era un vampiro, sino probablemente un trabajador del campo a juzgar por la artritis que padecía. Habría probablemente muerto, eso sí, de tuberculosis, como lo revelan las terribles lesiones que presenta en las costillas. Y lo más importante, al cráneo le faltan todos los dientes frontales, lo que definitivamente lo descalifica como vampiro -al menos según los estándares modernos impuestos por Hollywood.El florecimiento de los vampiros ocurrió en los siglos XVIII y XIX cuando la ciencia, tal como la conocemos hoy en día, tomaba fuerza para, con el transcurrir de los siglos, transformar al mundo. Ambos desarrollos se pueden ver como dos procesos, uno racional y otro irracional, que se mueven en paralelo y por tanto se oponen uno al otro. Por supuesto, los vecinos de Griswold no tenían información de que esto estuviese sucediendo y no pueden ser culpados en modo alguno. Por otro lado, poniéndonos en su lugar, tendría que haber sido aterrador lidiar con una enfermedad tan terrible como la tuberculosis sin demasiadas armas para combatirla. El vampirismo les proporcionaba una cierta base teórica para tratar de entenderla. Desgraciadamente, sin demasiado éxito a juzgar por los resultados.",
    "Imaginémonos en un caluroso día de verano -de los que cada vez hay más, por el bien conocido fenómeno del calentamiento global- en una habitación con grandes ventanales cerrados por los que ingresa abundante radiación solar. Es posible, a menos que dicha habitación cuente con clima artificial, que la temperatura en su interior alcance un valor demasiado alto para nuestro confort. El fenómeno físico por el cual esto ocurre -llamado efecto invernadero- es bien conocido y en términos simplificados se explica como sigue. Puesto que las ventanas son transparentes, permiten el paso de los rayos solares al interior de la habitación, en donde son absorbidos por las paredes y por otros objetos que encuentren a su paso. Esto da como resultado un incremento en la temperatura, tanto de dichos objetos como de la habitación misma. Por otro lado, es un hecho físico bien conocido que cualquier objeto, por el mero hecho de estar a una cierta temperatura, emite radiación infrarroja y con esto tiende a enfriarse. La intensidad de la radiación emitida, además, es mayor en cuanto más alta sea dicha temperatura. De este modo, la temperatura que alcanza la habitación se alcanza por un balance entre la energía solar que entra por las ventanas y aquella que se escapa a través de las mismas. Sucede, sin embargo, que el vidrio de las ventanas no es trasparente a la radiación infrarroja emitida por los objetos y esta no puede abandonar la habitación.  La temperatura de la habitación tiende entonces a elevarse.Por lo demás, ciertamente no necesitamos tener un conocimiento profundo de los fenómenos físicos que originan el efecto invernadero para poner remedio a nuestra incómoda situación en el interior de la habitación con las ventanas cerradas en medio del verano caluroso. Así, un recurso a nuestro alcance es abrir las ventanas para que de este modo pueda escapar la radiación infrarroja, al mismo tiempo que se intercambia el aire de la habitación con el aire del exterior. Suponga, no obstante, que las ventanas están selladas y no es posible abrirlas. En este caso, una solución alternativa -si bien menos efectiva que la anterior- es bajar las persianas y de este modo bloquear el ingreso de la radiación solar a la habitación.Valga lo anterior para introducir el concepto de geoingeniería solar, que es el tópico de este artículo y que surgió como una posibilidad para mitigar los efectos del calentamiento global que aquejan al planeta. Como sabemos, la creciente emisión de gases de invernadero está generando un incremento paulatino de la temperatura de la superficie de la Tierra por la acumulación de dichos gases en la atmósfera. El mecanismo por medio el cual se produce dicho incremento es análogo al efecto invernadero en nuestra habitación con las ventanas cerradas. En el caso del calentamiento global, la habitación corresponde a nuestro planeta, mientras que el vidrio de las ventanas lo constituyen los gases de invernadero en la atmósfera. Estos gases permiten el paso de la radiación solar y bloquean la radiación infrarroja emitida por la superficie de la Tierra, que de otro modo se perdería en el espacio. El incremento sostenido en la concentración de gases de invernadero ha hecho cada vez más acusado dicho bloqueo, y esto ha llevado a un incremento paulatino en la temperatura de la superficie de la Tierra.    La solución más natural para remediar el calentamiento global es, por supuesto, dejar de emitir o reducir la emisión de gases de invernadero a la atmósfera. Como esto no se ve factible en el mediano plazo, sin embargo, se ha pensado en soluciones alternativas. En cuanto a éstas, no es posible “abrir ventanas” y dejar que la radiación infrarroja emitida por la superficie de la Tierra se disipe en el espacio. En cambio, sí es posible, “bajar las persianas” para reducir el ingreso de radiación solar a nuestro planeta y esto es lo que plantea la geoingeniería solar. Una alternativa que plantea la geoingeniería solar contempla dispersar en las capas altas de la atmósfera gases o partículas que reflejen parte de la radiación solar y limitar de este modo su ingreso hasta la superficie de la Tierra. Se sabe que esto es posible por algunas erupciones volcánicas que han arrojado a la atmósfera grandes volúmenes de partículas que se han mantenido en el aire por un par de años y que han modificado el clima de la Tierra. Una de estas erupciones fue la del volcán Pinatubo en Filipinas, que en junio de 1991 arrojó a la atmósfera grandes cantidades de dióxido de azufre que produjeron, hacia finales de 1992, una reducción global de temperatura por 0.5 grados centígrados.  Las propuestas de le geoingeniería solar, sin embargo, son altamente controvertidas. Por un lado, el clima del planeta es un sistema tan complejo que por el momento no es posible anticipar con seguridad cuáles serían los posibles efectos colaterales de intentar modificar el clima a nivel global. Por otro lado, los estudios por computadora concuerdan en que el colocar una capa reflejante de la radiación solar en la atmósfera llevaría a una reducción en las precipitaciones pluviales y por consecuencia a una menor disponibilidad de agua potable a nivel global. Igualmente, se anticipa que se afectaría la capa de ozono, la producción agrícola y se incrementaría la acidificación de los océanos.    No obstante, y en medio de la controversia, hay quien está considerando de manera seria a la geoingeniería solar, si no como la solución al cambio climático, sí como un paliativo para el mismo.  A pesar de que sería equivalente a tomar píldoras para adelgazar para seguir comiendo como siempre. Sin atacar la raíz del problema, que es la creciente emisión de gases de invernadero a la atmósfera.",
    "El 2 de octubre de 2017, un laboratorio italiano, parte de la red de laboratorios europeos para el monitoreo de isótopos radiactivos, detectó la presencia del isótopo rutenio-106 en la atmósfera, aunque en concentraciones que no amenazaban la salud de la población italiana. Más tarde, ese mismo día, se reportaron hallazgos similares por otros laboratorios en Austria, Noruega y la República Checa, y en los siguientes días en otros países europeos. El rutenio-106 es un isótopo radiactivo que tiene una vida media de poco más de un año -lo que significa que en este lapso su actividad radioactiva se reduce a la mitad-.  Se produce durante la desintegración artificial del uranio-235 y su presencia en la atmósfera europea era una indicación de que en algún lugar -en esos momentos no identificado- se había liberado una cantidad considerable de dicho isótopo.Se aventuraba que la liberación de rutenio-106 podría haberse producido en la instalación nuclear rusa de Mayak, localizada en el sur de los Montes Urales. Esto fue negado por las autoridades rusas sobre la base de que no se habían medido niveles altos de contaminación alrededor de dicha instalación. En su lugar, sugirieron que la contaminación radioactiva podría obedecer a la desintegración, al reentrar a la atmósfera, de un satélite equipado con un generador de energía nuclear basado en rutenio-106. Por falta de suficientes datos, sin embargo, no se estableció de manera concluyente el origen de la nube de rutenio-106 detectada sobre los cielos europeos, misma que permaneció en el misterio.Hasta ahora, si hemos de creer a un artículo aparecido el pasado 26 de julio en la revista “Proceedings of the National Academy of Sciences” de los Estados Unidos, el cual pretende haberlo desvelado. Dicho artículo fue publicado por un equipo internacional de investigadores encabezado por Oliver Masson y Georg Steinhauser, del Instituto de Radioecología y Protección Radiológica en Francia y de la Universidad Leibniz Hannover en Alemania, de manera respectiva. En su artículo, Masson y Steinhauser hacen una recopilación de información de la red europea de monitoreo de isótopos radiactivos y de bancos de datos de diferente origen, incluyendo bancos de la Federación Rusa. En su análisis, los investigadores consideraron más de 1,100 mediciones de contaminación atmosférica y 200 mediciones de contaminación a nivel del suelo. Todo esto, en 330 localidades de muestreo y cubriendo un periodo de tiempo desde finales de septiembre de 2017 hasta mediados de octubre del mismo año.Sobre la base de su análisis, Masson y colaboradores, descartan que la contaminación de rutenio-106 pudiera haberse debido a la mezcla accidental de material radiactivo en una fundición de metales. Esto ya ha ocurrido en ocasiones anteriores, incluyendo el recordado caso de contaminación de varilla de construcción con cobalto-60 que ocurrió en México a finales de 1983. En el caso de contaminación con rutenio-106 resulta poco probable, concluyen los investigadores, pues por la extensión territorial que abarcó el fenómeno, la fundición del material radiactivo tendría que haberse dado en diferentes localidades de manera simultánea. Masson y colaboradores descartan también que la nube radioactiva proviniese de la reentrada de un satélite a la atmósfera, pues el corto tiempo de desintegración de este isótopo -poco más de una año- hacen poco atractiva la construcción de un generador de energía basado en rutenio-106. Además de que la potencia que se obtendría sería muy baja. La distribución de contaminantes que se observó a diferentes alturas, tampoco avala que haya sido producto de la desintegración de un satélite, señalando, por el contrario, que la liberación de rutenio-106 ocurrió a nivel del suelo.Basados en el análisis de los datos de dispersión de la nube de rutenio-106 que tuvieron a su disposición, Masson y colaboradores concluyen que es probable que la liberación de isótopos ocurriera durante un mal manejo del isótopo cerio-144 en una planta de reprocesamiento de combustible nuclear que estaría localizada en el sur de los Montes Urales. Los investigadores fueron incluso capaces de precisar la fecha, 26 de septiembre de 2017, cuando habría ocurrido el accidente. Los resultados apuntan a la planta de rusa de Mayak como posible fuente de la contaminación con rutenio-106.Si bien la contaminación radioactiva que se observó en Europa a finales de 2017 fue débil y no tuvo consecuencias para la población, la atención que atrajo es una medida de la peligrosidad de la radiación nuclear. Además, al no ser un suceso poco frecuente -a inicios de 2017 se detectó, por ejemplo, otra nube radiactiva, esta de de yodo-131, sobre Europa- es una evidencia de la relativa facilidad con la que puede ocurrir un accidente nuclear. Incluso con graves consecuencias, como sucedió en Chernóbil, la Isla de la Tres Millas y Fukushima.Aun así, a veces se presenta a la energía nuclear como una energía limpia, que no deja huella de carbono y por tanto no contribuye al calentamiento global. Si bien esto es verdad en cierta medida, el riesgo de contaminación del planeta con radiaciones de alta energía incompatibles con la vida, dificulta catalogar a la energía nuclear como “limpia” en un sentido estricto. Para esto, ciertamente, hay que tener una manga particularmente ancha.",
    "Según las estadísticas, casi la tercera parte de los habitantes del mundo -más de dos mil millones de personas- son adictos a los videojuegos. Lo son a tal grado que en promedio el tiempo que dedican a esa actividad es de unas seis horas a la semana, mientras que para un 7% es de más de 20 horas a la semana.    Dada la extensión que ha alcanzado el fenómeno de los videojuegos, no es sorprendente que nos haya afectado como sociedad en varios sentidos. Se ha encontrado, por ejemplo, que el tiempo que un estudiante dedica a los videojuegos está relacionado de manera inversa con sus calificaciones en la escuela -es decir, a mayor tiempo de juego más bajas son las calificaciones-. También, hay estudios que han encontrado que los jugadores de videojuegos con contenidos violentos desarrollan un comportamiento más agresivo que aquellos que juegan videojuegos no violentos. No todos los impactos que se han documentado de los videojuegos, sin embargo, son negativos. Así, aquellos que requieren de una reacción rápida en respuesta a una determinada señal o imagen visual, promueven el desarrollo de tiempos de reacción más rápidos en situaciones de la vida real. Un artículo aparecido este mes de julio en la revista “Creativity Research Journal” documenta otro ejemplo en este sentido. Dicho artículo fue publicado por un grupo de investigadores de la Universidad Estatal de Iowa, encabezado por Douglas Gentile del Departamento de Sicología de esta universidad. En su artículo, Gentile y colaboradores reportan los resultados de un estudio llevado a cabo para determinar los efectos que los videojuegos tienen sobre la creatividad de los jugadores. De manera específica, compararon las creatividades de estudiantes de licenciatura que jugaron los videojuegos “Minecraft” y “NASCAR”. Minecraft es un juego abierto, del que se han vendido 100 millones de copias, en la que no hay un objetivo determinado a perseguir, mismo que es determinado por el participante. En contraste, NASCAR es un videojuego de carreras de automóviles en el que los vehículos viran siempre en la misma dirección. Las demandas en creatividad para los jugadores son entonces considerablemente mayores en Minecraft que en NASCAR. Compararon, también, la creatividad de estudiantes que se dedicaron simplemente a ver un programa de televisión durante el experimento. Para llevar a cabo el estudio, Gentile y colaboradores reclutaron a un grupo de 352 estudiantes de licenciatura de la Universidad Estatal de Iowa, los cuales fueron divididos al azar en cuatro grupos. Dos de estos grupos participaron con Minecraft, mientras que un tercero lo hizo con NASCAR y un cuarto de manera pasiva viendo televisión. A uno de los grupos que participó con Minecraft se le dio una libertad completa para jugar, mientras que al otro se le pidió que lo hiciera de manera creativa. Los videojuegos y la sesión de televisión se prolongaron por 40 minutos.Al final del experimento se evaluó la creatividad de los participantes mediante cuatro criterios. Se les pidió, por ejemplo, como una prueba de pensamiento divergente, que en dos minutos dieran tantas maneras diferentes como pudieran de utilizar un cuchillo, más allá de su función primaria como instrumento de corte. Se les pidió lo mismo con respecto a un clip para sujetar papeles y a un periódico de papel. Como otra prueba de creatividad, en este caso convergente, se les proporcionó a los estudiantes varios grupos de tres palabras y se les pidió, para cada uno de los grupos, que encontraran una palabra que tuviera relación con las tres palabras que lo componen. Adicionalmente, para medir su producción creativa, se les pidió que dibujaran un ser que se hubiera desarrollado en un mundo muy diferente al nuestro. En este caso, la creatividad es más grande en cuanto mayores sean las diferencias morfológicas imaginadas con respecto a nosotros. Por ejemplo, uno o más de dos ojos, o ciertas asimetrías corporales.Al final de su investigación, Gentile y colaboradores encontraron que la creatividad de aquellos que jugaron Minecraft fue superior a la creatividad de los que jugaron NASCAR o sólo vieron televisión. Esto era algo que esperaban. En contraste, les sorprendió encontrar que la creatividad de los que participaron con Minecraft de manera libre fue superior a la de aquellos a los que se les pidió que fueran creativos, pues esperaban justo lo contrario.Gentile y colaboradores especulan que esto podría deberse a que los participantes a los que les pidió que fueran creativos fueron menos libres para proceder de la manera en que ellos hubieran querido y que esto les produjo un cierto grado de frustración que se prolongó hasta el momento en que hicieron las pruebas de creatividad. O bien que el esfuerzo para ser creativos durante el juego los agotó de modo que no llegaron en buena forma a la sesión final de pruebas.  Si bien los investigadores aventuran algunas otras posibles hipótesis, reconocen que no tienen una explicación satisfactoria. Sugieren, no obstante, que el resultado indicaría que la creatividad es promovida, no solamente por el tipo de videojuego, sino por la manera cómo se juega.Al igual que los teléfonos móviles y otros dispositivos de alta tecnología, los videojuegos son signos del tiempo que nos tocó vivir y con seguridad se mantendrán a nuestro alrededor de manera permanente. Dada la velocidad con la que han irrumpido en nuestra vida, no es claro si, en balance, sus efectos serán positivos o negativos. El tiempo nos lo dirá, pero, por lo pronto, y como lo señalan Gentile y colaboradores, los videojuegos ofrecen oportunidades para la educación. Así, serían elementos potencialmente positivos. Todo está que los aprovechemos de la manera adecuada.",
    "Sin bien con diferencias sustanciales entre países ricos y países pobres, gracias a los antibióticos y en general a los avances en la prevención, diagnóstico y tratamiento de enfermedades, la esperanza de vida creció de manera espectacular a lo largo del siglo pasado. En el futuro, podemos esperar que dicho crecimiento se mantenga, en la medida en que la ciencia médica progrese hacia límites todavía no determinados. En este contexto, llama la atención un artículo publicado el pasado mes de junio en la revista Population and Development Review en el que se reportan los resultados de un estudio llevado a cabo para determinar la evolución de la esperanza de vida en los Estados Unidos en las últimas décadas, específicamente en cuanto a la población de raza blanca. Dicho estudio indica que el ritmo de crecimiento de la esperanza de vida de este segmento de la población norteamericana se ha desacelerado en las últimas tres décadas, y en algunos casos incluso ha tenido un retroceso.  El estudio de referencia fue publicado por un grupo de especialistas encabezado por Irma Elo de la Universidad de Pensilvania, y para el mismo se utilizaron datos del Centro Nacional de Estadísticas de Salud y del censo de población de los Estados Unidos. Para propósitos del estudio, la población blanca norteamericana fue dividida en cuatro grupos con edades entre los 0-25 años, los 25-44 años, los 45-65 años y de más de 65 años. La población fue también dividida en 10 grupos por su lugar de residencia. Entre estas regiones se consideraron áreas ricas como Nueva York y California, y deprimidas como los Apalaches y el Medio Oeste. Los investigadores consideraron, igualmente, el carácter urbano o rural de la población bajo estudio, desagregándola en cuatro categorías: 1) zona metropolitana mayor -más de un millón de habitantes-, 2) suburbio de una zona metropolitana mayor, 3) zona metropolitana menor y 4) área rural. Además de las desagregaciones mencionadas líneas arriba, Elo y colaboradores presentan comparaciones entre las expectativas de vida observadas en los años 1990-1992 y las correspondientes expectativas para 2014-2016. Igualmente, presentan comparaciones entre los años 2009-2011 y 2014-2016. En el primer caso se evalúan los cambios en expectativas de vida a lo largo de 14 años, mientras que en el segundo caso se evalúan dichos cambios en un periodo más corto -5 años- y más reciente. Al comparar los años 1990-1992 con los años 2014.2016, Elo y colaboradores encuentran un incremento en la expectativa de vida en todos los grupos de población, rurales o urbanos, considerados, si bien con variaciones considerables entre los mismos. Así, la población masculina de las zonas metropolitanas mayores incrementó su expectativa de vida por 5.09 años, mientras que los habitantes de las zonas rurales lo hicieron solamente por 2.25 años. La población de los suburbios o de las zonas metropolitanas menores incrementaron sus expectativas de vida en 3.45 y 2.81 años, en forma respectiva. Estas cifran contrastan con las de la población femenina que incrementó su expectativa de vida por 2.98 años en las zonas metropolitanas mayores, y solamente por 0.2 años en las áreas rurales. Así, una primera conclusión del estudio de Elo y colaboradores es que la brecha de expectativa de vida entre hombres y mujeres -que viven considerablemente más que los hombres- se está cerrando rápidamente.    Por otro lado, la comparación entre los años 2009-2011 y 2014-2016 muestra, no solamente que el ritmo de crecimiento de la expectativa de vida de la población norteamericana se ha reducido significativamente en los últimos años, sino que en algunas de las categorías consideradas incluso se ha hecho negativo -es decir que la expectativa de vida ha disminuido-. Este es el caso de las poblaciones, masculinas y femeninas, de las zonas rurales y metropolitanas pequeñas.Pero quizá lo que más llama la atención son las cifras que resultan cuando se desagrega la población por edades. Particularmente, cuando se considerara la población joven, con edades entre los 25 y los 34 años, que encontraron tiene una tendencia negativa en cuanto a su expectativa de vida. Esto, para todas las poblaciones consideradas, urbanas y rurales, masculinas y femeninas. Efectos similares se encontraron con la población con edades entre los 44 y los 65 años, exceptuando aquellos que viven en las zonas metropolitanas mayores. Estos resultados dependen, además, de la región geográfica considerada. Así, son más acusados en la región de los Apalaches o del Medio Oeste que en la costa del Pacífico.Se preguntan Elo y colaboradores por las causas de esto último y llegan a la conclusión que el fenómeno obedece fundamentalmente a muertes por sobredosis de drogas recreativas en el caso de los hombres, y a desórdenes mentales y nerviosos en el caso de las mujeres. La epidemia de obesidad que sufre la población norteamericana y el consumo de tabaco podrían estar también jugando un papel, aunque menor.Si bien, como lo reconocen Elo y colaboradores, sus resultados proporcionan pistas acerca de las causas que están actuando en contra de la expectativa de vida de los norteamericanos blancos, sus conclusiones no pueden considerarse definitivas. Así, como frecuentemente sucede en el campo de la ciencia, tendrían que ser validadas -o desmentidas- por nuevos análisis. Por lo pronto, la posibilidad de que el consumo de drogas esté revirtiendo, en un país rico, una tendencia que se había mantenido por un siglo, es sin duda preocupante. Por decir lo menos.",
    "Como sabemos, el próximo sábado 20 de julio se cumplen 50 años del alunizaje del módulo “Eagle” de la misión Apollo 11 que llevó a un humano por primera vez a la superficie de la Luna, y que en palabras de Neil Armstrong representó “un gran salto para la humanidad”. Si bien hasta el momento este salto no se ha concretado, pues después del programa Apollo no ha habido más misiones tripuladas a la Luna o a destinos más distantes, el aniversario es, sin duda, digno de celebrarse.  No todo mundo piensa igual, sin embargo, y hay una minoría que duda que el viaje a la Luna hubiera realmente ocurrido y que todo fue parte de un montaje de la NASA para engañar al público. La agencia espacial norteamericana habría sido forzada a proceder de esta manera, dicen los incrédulos, por la fuerte presión a la que estaba sometida para cumplir con el ofrecimiento del presidente Kennedy, hecho público en 1961, de poner a un norteamericano en la superficie de la Luna antes de terminar la década.  Por esta razón, o bien porque hay algunos que se sienten atraídos por las llamadas teorías de conspiración, en los Estados Unidos un 6% de la población piensa que no hubo tal misión lunar y que todo fue parte de un montaje y espectáculo televisivo.  Un 12% de los británicos piensa lo mismo, al igual que un 20% de los italianos y un 57% de los rusos.  Por lo demás, las misiones lunares son terreno fértil para las teorías de conspiración por aquello de que es necesario “ver para creer”. Ciertamente, cientos de millones de personas en todo el mundo vimos el alunizaje del módulo “Eagle”. Esto, no obstante, fue a través de la televisión y no de manera presencial, lo que no es una evidencia visual para los incrédulos. De hecho, parte de los argumentos que esgrimen en contra del viaje a la Luna se basa precisamente en el análisis de fotografías y películas del alunizaje que se presumen son falsas.    Por otro lado, no todo lo que damos por cierto lo basamos en evidencias visuales experimentadas en forma directa. La mayor parte de nosotros, por ejemplo, damos por hecho que la Tierra es esférica sin haber nunca tenido una experiencia visual que así nos lo confirme. En realidad, a primera vista la Tierra nos parece plana -si bien con montañas, valles y otras irregularidades topográficas- y el que sea esférica -con antípodas parados de cabeza- no resulta una idea que inmediatamente nos venga a la mente. Así, en el siglo VI antes de nuestra era, el pensador griego Anaximandro concebía la Tierra como un cilindro, con un diámetro tres veces mayor que su altura. Nosotros habitaríamos la cara superior del cilindro, que estaría rodeado de agua y flotando en el espacio sin ningún apoyo. Hay, no obstante, numerosas evidencias indirectas que se contraponen con una Tierra plana y pronto los pensadores griegos -aun sin una evidencia visual directa como la que tienen los astronautas que viajan al espacio- llegaron a la conclusión de que ésta es esférica. Eratóstenes, en el siglo III a.C., logró incluso medir el radio de la Tierra con una precisión razonable. Para esto, midió el largo de la sombra que proyectaban al mediodía dos varas de igual longitud colocadas, una en la ciudad de Asuán y la otra en Alejandría, separadas unos 850 kilómetros en la dirección sur-norte.En la actualidad tenemos suficientes evidencias, tanto directas como indirectas, de que la Tierra es esférica y esto es casi universalmente aceptado. La mayor parte de nosotros, sin embargo, lo aceptamos basados en la opinión de los expertos -o de los pocos que han viajado al espacio. Por ejemplo, una evidencia de la redondez de la Tierra nos lo da el hecho que cuando se observa en el mar un barco que se acerca lo primero que se hace visible es la parte más alta del mismo. Esta experiencia, que posiblemente sea común para aquellos que viven en una costa, no lo es para los que vivimos en las tierras altas. En el caso de quien esto escribe, si bien tengo una firme creencia en que la Tierra es esférica, debo confesar que dicha creencia está basada en buena medida en evidencias observadas por otras personas. Esta igualmente basada en la opinión de expertos que han encontrado que la manera más simple de describir las cosas que podemos observar en nuestro entorno, es suponer que la Tierra es esférica. De otro modo entraríamos en contradicciones o necesitaríamos de una imagen del mundo demasiado complicada.Lo mismo podríamos decir con respecto a la imagen que actualmente tenemos del Sistema Solar, con el Sol en el centro y los planetas, incluyendo el nuestro, orbitando a su alrededor. Esta idea no es intuitiva, y de hecho, durante mucho tiempo se asumió -porque así nos lo parece desde la Tierra- que el Sol es el que orbita en torno nuestro. Poner a la Tierra en el centro del Sistema Solar, no obstante, llevó a demasiadas complicaciones para describir el movimiento de los demás planetas, mismas que desaparecieron una vez que se renunció a que la Tierra fuese el centro del Universo.    En el caso de los escépticos de los viajes lunares, si bien pueden aducir que no cuentan con evidencias de primera mano que los prueben, resulta más complicado negarlos que aceptarlos. En efecto, por un lado, todos los argumentos que han esgrimido han sido refutados de manera contundente por los expertos. Por otro lado, ¿es razonable pensar que un montaje con la magnitud del que se propone pudiera ser llevado a cabo sin ser descubierto? ¿No hubiera sido rápidamente denunciado por los soviéticos, competidores de los Estados Unidos en la carrera espacial?Así, lo más simple es aceptar -aun sin haber sido testigos presenciales- que los estadounidenses viajaron a la Luna hace 50 años. Y celebrar en consecuencia.",
    "Por el diario que llevó Cristóbal Colón durante su primer viaje al continente americano, nos enteramos de que, el día 16 de septiembre “comenzaron a ver muchas manadas de hierba muy verde que poco había, según le parecía, que se había despegado de tierra, por lo cual todos juzgaban se encontraban cerca de alguna isla”. No era el caso, como sabemos, y la tierra firme estaba todavía a más de tres semanas de viaje. En los días sucesivos, los exploradores continuaron viendo más de esa hierba verde.  Al amanecer del 21 de septiembre, por ejemplo, “Hallaron tanta hierba que parecía la mar cuajada de ella”.  De la misma manera, el 23 de septiembre encontraron que “Las hierbas eran muchas, y hallaron cangrejos”. Hoy sabemos que las hierbas que Colón encontró en su primer viaje a América fueron algas flotantes del llamado mar de los Sargazos, localizado en el océano Atlántico septentrional, entre las islas Azores y la costa este de los Estados Unidos. Dicho mar tiene una extensión cambiante de unos tres y medio millones de kilómetros cuadrados, y en la época de los barcos impulsados por la fuerza del viento presentaba obstáculos para la navegación.  Si bien en la actualidad los bancos de algas flotantes no son más un problema para la navegación, en las últimas semanas el sargazo ha cobrado una gran notoriedad en México, por la invasión de algas que han sufrido las playas del caribe mexicano. En particular, ha habido controversia en cuanto a la seriedad y magnitud del problema que amenaza a la industria del turismo en nuestro país. En este sentido ¿es la acumulación de sargazo en nuestras playas un fenómeno temporal que desaparecerá en los próximos años? O, por el contrario ¿es la manifestación de un fenómeno asociado a cambios atmosféricos o climáticos que será persistente en los años por venir?Un artículo aparecido esta semana en la revista Science intenta arrojar luz al respecto. Dicho artículo fue publicado por un grupo de científicos adscritos a universidades en los Estados Unidos, el cual estuvo encabezado por Mengqiu Wang, de la Universidad del Sur de Florida en Tampa. En su artículo, Wang y colaboradores reportan los resultados de un estudio llevado a cabo sobre la banda estacional de sargazo que -según imágenes satelitales- se ha formado en el océano Atlántico central entre la costa occidental de África y el Golfo de México. Dicha banda se ha formado en un área en la que el volumen de sargazo era previamente muy pequeño.Wang y colaboradores hicieron estimaciones del volumen de algas contenida en la banda de sargazo a partir de 2011, año en el que se observó un incremento abrupto y significativo en dicho volumen. Con la excepción del año 2013, en el periodo entre 2011 y 2015 hubo un incremento progresivo en el volumen de sargazo, mientras que en 2016 y 2017 se observó una disminución del mismo, para aumentar espectacularmente en 2018 hasta alcanzar 20 millones de toneladas en peso. Esta cantidad es significativamente más grande que la observada en 2015. Por otro lado, si bien en el artículo de referencia no se incluyen datos para 2019, se sabe que en estos meses la situación ha empeorado, lo que concuerda con la crisis en las playas del caribe mexicano. Con el objeto de entender las causas que originaron el aumento en la banda de sargazo en el Atlántico central en los últimos años, Wang y colaboradores desarrollaron simulaciones por computadora basadas en datos satelitales y en algunas mediciones tomadas de manera directa en los lugares de interés. Según los resultados obtenidos, el incremento en la banda de sargazo en el Atlántico central no está relacionado con intercambios con el mar de los Sargazos, sino con un incremento de nutrientes en la superficie del océano debido a dos causas. Una de estas causas es de origen natural y tiene que ver con el transporte de agua rica en nutrientes desde el fondo de océano a la superficie, frente a la costa occidental de África. Esto ocurre por variación en las condiciones climáticas. Una segunda causa está relacionada con actividades humanas: la descarga de agua del río Amazonas enriquecida con nutrientes por el incremento en la deforestación y el uso de fertilizantes. Wang y colaboradores reconocen que sus conclusiones, basadas en datos satelitales y pocos datos experimentales tomados en sitio, deben ser corroboradas con investigaciones y mediciones adicionales. Apuntan, no obstante, que los datos satelitales de los últimos 20 años muestran que a partir de 2011 ocurrieron de manera abrupta cambios sustantivos en la formación de sargazo en el Atlántico central -fuera del “tradicional”, desde tiempos de Colón, mar de los Sargazos- y que esto es indicativo de que dichos cambios serán permanentes.Con respecto a esto último, habría que cruzar los dedos, pues dado el caso, la solución al problema del sargazo en el caribe mexicano implicaría algo más que simplemente remover el volumen de algas hoy acumulado.",
    "El próximo 20 de julio se cumplen 50 años del alunizaje del módulo Eagle y de la caminata de Neil Armstrong y Edwin Aldrin en el Mar de la Tranquilidad en la superficie de la Luna. El suceso, trasmitido en directo por televisión, fue, ciertamente, espectacular. Aquellos que tenemos la suficiente edad, tuvimos la fortuna de poder atestiguar, la noche del 20 de julio de 1969, como Armstrong descendía lentamente por la escalera del módulo lunar y pisaba la superficie de nuestro satélite. Fue en esos momentos cuando Armstrong pronunció la frase que se ha hecho famosa: “Un pequeño paso para un hombre, un salto gigante para la humanidad”.El presidente Nixon habló por teléfono con Armstrong y Aldrin mientras pisaban la superficie de la Luna y, entre otras cosas, les dijo: “Por lo que ustedes han hecho, los cielos se han convertido en parte del mundo del hombre”, añadiendo, “y dado que nos hablan desde el Mar de la Tranquilidad, esto nos inspira a redoblar nuestros esfuerzos para traer paz y tranquilidad a la tierra”.Claramente, esta última declaración no fue demasiado convincente para muchos en los Estados Unidos. Particularmente, para aquellos que estaban en contra del involucramiento de este país en la Guerra de Vietnam y que arreciaron sus protestas por la invasión de Camboya. En este contexto, cuatro estudiantes de la Universidad Estatal Kent, en Kent, Ohio, murieron por disparos de la Guardia Nacional, hecho que provocó manifestaciones estudiantiles de rechazo a nivel nacional. A nivel personal, Nixon tuvo también sus momentos de intranquilidad en 1974, cuando fue forzado a renunciar a su puesto como presidente por el escándalo de Watergate.En cuanto a la afirmación que los cielos se habían convertido en parte de nuestro mundo, habría que reconocer que Nixon tampoco fue demasiado afortunado. En efecto, si bien después del alunizaje de Armstrong y Aldrin se dieron 5 misiones tripuladas a la Luna, la última en diciembre de 1972, después de esta fecha ningún humano ha viajado a nuestro satélite natural. Las posteriores misiones tripuladas al espacio se han limitado a órbitas terrestres de baja altitud, o sea a nuestra vecindad espacial inmediata.Y lo mismo podríamos decir de la famosa frase de Armstrong, pues hasta la fecha, 50 años después, no ha ocurrido el salto gigante que él daba por hecho. Un artículo aparecido en el último número de la revista MIT Technology Review, firmado por Konstantin Kakes, lo expresa en estos términos: “Cincuenta años después de que Neil Armstrong pisara la superficie de la Luna, no es difícil concluir que vio las cosas al revés. El alunizaje fue un salto gigante para un hombre -la vida de Armstrong cambió para siempre- pero solo un pequeño paso para la humanidad”.Las misiones espaciales robóticas no tripuladas, por el contrario, sí han dado un salto enorme en los últimos 50 años. Así, con anterioridad a julio de 1969, solo se habían explorado con naves o sondas espaciales dos planetas del sistema Solar: Marte y Venus. Hoy en día, la NASA ha logrado colocar varias sondas y exploradores sobre la superficie de Marte que, entre otros datos científicos relevantes, nos han hecho llegar imágenes impactantes del paisaje marciano. Igualmente, se han explorado todos los demás planetas del Sistema Solar, incluyendo los planetas exteriores gigantes, Júpiter, Saturno, Urano y Neptuno. Incluso, ahora tenemos imágenes claras de Plutón y de su luna Caronte, así como de Ultima Thule, el pequeño objeto fotografiado por la sonda New Horizons a 6,500 millones de kilómetros de la Tierra.Como un avance más en la exploración robótica del Sistema Solar, la NASA ha anunciado un proyecto de exploración de Titán, la luna más grande de Saturno. Titán constituye un mundo exótico que, no obstante, comparte algunas similitudes con nuestro planeta. Así, cuenta con una atmósfera densa que está compuesta en un 94% de nitrógeno. Cuenta, asimismo, con lagos, ciclos de lluvia y dunas de arena. La similitud con la Tierra, sin embargo, no va mucho más allá, pues la temperatura de la superficie de Titán, por su lejanía del Sol, es de aproximadamente menos 179 grados centígrados. En estas circunstancias, el agua no puede existir en forma líquida y su papel lo toma el metano -principal componente del gas natural-. Así, los lagos de Titán son de metano líquido, lo mismo que sus gotas de lluvia. Se sabe también que en Titán hay dunas de “arena”, formada ésta por materia orgánica.El proyecto de la NASA de exploración de Titán ha sido asignado al Laboratorio de Física Aplicada de la Universidad Johns Hopkins y será llevado a cabo mediante un dron de 8 hélices que ha recibido el nombre de Dragonfly -libélula-. El dron medirá 3x3 metros y aprovechará para su vuelo que la atmósfera de Titán es unas 1.5 veces más densa que la de la Tierra, mientras que su gravedad es apenas un 14% de la de nuestro planeta. El explorador será liberado en la región de dunas de Titán y avanzará por saltos de 8 kilómetros. En total recorrerá 175 kilómetros en una misión de 2.7 años. De acuerdo con la NASA, unos de los objetivos de la misión es investigar la existencia de condiciones en Titán que se piensa fueron precursoras del origen de la vida. El proyecto Dragonfly tendrá un costo de 1,000 millones de dólares e iniciará en el año 2034. Constituye un avance más en el desarrollo de sondas robóticas para la exploración del espacio, misma que posiblemente avance más en esta dirección que en la forma de misiones tripuladas. Estas misiones son mucho más costosas y sin ventajas científicas suficientes que las justifiquen. Aunque, posiblemente, con más posibilidades de manipulación política.",
    "Imagine que extravía su cartera en la calle con algo de dinero y documentos importantes, incluyendo una credencial que lo identifica ¿la daría por perdida o tendría esperanzas de que alguien la encontrara y se la hiciera llegar? Como bien se dice, la esperanza muere al último, pero habríamos de coincidir en que la probabilidad de que en nuestro país suceda algo así es más bien baja. De hecho, un artículo aparecido la semana que hoy termina en la revista “Science” la cuantifica:  apenas rebasa un 15%.Pero vayamos por partes. El artículo de referencia fue publicado por un grupo internacional de investigadores encabezado por Alain Cohn de la Universidad de Michigan, Ann Arbor, y Michel André Maréchal de la Universidad de Zurich y en el mismo se describen los resultados de un extenso estudio llevado a cabo en ciudades a lo largo de todo el mundo, para averiguar el grado de honestidad cívica de sus habitantes. Para este propósito, Cohn y colaboradores diseñaron un experimento mediante el cual “perdieron” en varios sitios públicos y privados, carteras que contenían diversas cantidades de dinero y tarjetas de negocios de su supuesto propietario con una dirección electrónica de contacto. El experimento tuvo como propósito, determinar en cuántos casos hubo un intento de devolver la cartera perdida.El estudio se llevó a cabo en 355 ciudades de 40 países, en cada uno de los cuales se escogieron entre cinco y ocho de las ciudades más grandes. Se incluyeron países de los cinco continentes, tanto desarrollados como no desarrollados. Las carteras -17,000 en total- fueron entregadas por auxiliares de la investigación en varios sitios que contaban con un área pública de recepción, incluyendo bancos, teatros y museos, oficinas postales, hoteles, lo mismo que estaciones de policía, juzgados y otras oficinas públicas. En algunas de las carteras se incluyó una pequeña cantidad de dinero, equivalente a unos 13 dólares. En otras, esta cantidad se incrementó hasta unos 90 dólares. En un tercer caso las carteras no contenían efectivo. Todas las carteras, además, contenían una llave -de valor solo para su propietario- y un recibo por compra de alimentos en una tienda de la localidad. Esto último para indicar que el propietario de la cartera era vecino del lugar. Al entregar una cartera, el auxiliar de investigación mencionaba que la había encontrado en las inmediaciones, pero que estaba con gran prisa por lo que pedían a quien la recibía que se encargara de devolverla a su dueño. Dicho esto, se alejaba sin que hubiera manera de contactarlo posteriormente. Los resultados de la investigación de Cohn y colaboradores son poco sorprendentes en algunos aspectos y sorpresivos en otros. Así, de manera esperable, encuentran que el porcentaje de carteras que intentaron ser devueltas varía grandemente entre países. Este porcentaje es considerablemente más grande en países desarrollados, como Dinamarca y Suecia, en donde alcanza un 70-80%, que en países no desarrollados -en algunos casos llamados en forma eufemística países en desarrollo-, como Perú, en donde dicho porcentaje no llega al 15%. En México el porcentaje correspondiente se sitúa entre el 18% y el 25%, con una característica particular que mencionaremos más adelante.   Dado que la honestidad ciudadana es una característica básica para el desenvolvimiento de un país, era esperable que el estudio de Cohn y colaboradores encontrara que en países avanzados se devuelven más carteras perdidas que en países que no lo son tanto. Otros resultados de la investigación, en contraste, resultaron más sorprendentes.  Es el caso de las motivaciones que tiene un ciudadano para devolver una cartera en lugar de conservarla para su provecho. Podría quizá esperarse que una cartera con dinero dentro tendría menos probabilidad de ser devuelta que una cartera vacía. Cohn y colaboradores, sin embargo, encontraron que sucede justamente lo contrario, pues una cartera con 13 dólares es más probable que sea devuelta que una vacía. Para comprobar esta tendencia, los investigadores incrementaron el contenido de la cartera hasta 90 dólares y con esto incrementaron la probabilidad de que fuera devuelta.  Lo anterior ocurre en 38 de los 40 países estudiados. La excepción a la regla es Perú, en donde no hace diferencia que la cartera esté llena o vacía, y México, en donde una cartera vacía tiene una probabilidad de alrededor del 25% de ser devuelta a su dueño, en contraste con una cartera con dinero en donde esta probabilidad disminuye hasta el 18%. De este modo, México es una singularidad entre los 40 países estudiados.¿Qué es lo que impulsa a los ciudadanos de 38 países -incluyendo a países no desarrollados como Kazajistán, Ghana y Brasil- a devolver una cartera perdida, con más determinación en cuanto mayor es la cantidad de dinero que contiene? En base a su estudio, Cohn y colaboradores concluyen que la causa principal es una de autoestima. Es decir, al devolver la cartera evitan aparecer ante sí mismos como ladrones. Así, el impulso será más grande en cuanto mayor sea el contenido de la cartera, al menos hasta llegar a las cantidades -90 dólares- empleadas en el estudio. Por lo demás, las estadísticas de Cohn y colaboradores no son como para provocar nuestro entusiasmo, pues indicarían que en México se logra inhibir el impulso de autoestima de manera efectiva. Aunque, ciertamente y vistos en ese contexto, los resultados de la investigación no resultan del todo sorpresivos.",
    "Imagine que a bordo de una máquina del tiempo viajamos cuatrocientos años hacia el pasado. Ciertamente, encontraríamos un mundo muy diferente. Y no solamente porque en 1619 no había ni automóviles, ni computadoras, ni internet, ni antibióticos, ni rayos X, por mencionar solamente algunos de los desarrollos tecnológicos de los que gozamos en la actualidad, sino también porque la concepción de mundo que se tenía hace cuatro siglos era radicalmente distinta a la actual. Con referencia a esto último, hace cuatrocientos años Katharina Guldenmann fue sometida, a sus 68 años de edad, a un juicio por brujería en una ciudad del sur de la actual Alemania, acusada por una mujer de haberla envenenado después de una disputa. El juicio duro seis años, uno de los cuales Katharina lo sufrió en prisión encadenada al piso de su celda y bajo tortura sicológica, con amenazas de someterla a tormento físico si no confesaba. Al final fue absuelta de los cargos en 1621, pero murió a los seis meses después de ser liberada por los maltratos que padeció en la cárcel.El juicio de Katharina no habría trascendido -decenas de miles de brujos y brujas fueron quemados en Europa en los siglos XVI y XVII- de no haberse tratado de la madre de Johannes Kepler, uno de los astrónomos más destacados de la historia, quien ayudó a establecer el modelo actual del Sistema Solar, según el cual el Sol ocupa la posición central, con los planetas, incluyendo a la Tierra, orbitando a su alrededor. De hecho, fue Kepler quién defendió a su madre durante el juicio y logró su absolución.Juntamente con Nicolas Copérnico, Tycho Brahe, Galileo Galilei e Isaac Newton, entre otros, Johannes Kepler fue participante destacado de la revolución científica ocurrida en la Europa de los siglos XVI y XVII, la cual dio origen a la ciencia tal como la conocemos. Antes de esta revolución, para la explicación de un determinado fenómeno físico se daba gran peso a las opiniones y puntos de vista de pensadores del pasado -algunos de un pasado muy lejano, como era el caso de algunos pensadores griegos-. En contraste, según el método desarrollado en los siglos XVI y XVII, la explicación de un fenómeno debe basarse en la experimentación y para esto es necesario desprenderse de cualquier prejuicio que se pudiera tener acerca de dicho fenómeno.La ciencia que encabezó la embestida contra las ideas prevalecientes fue la astronomía, en la que Kepler jugó un papel central. A saber: estableció que los planetas siguen órbitas elípticas y no órbitas circulares alrededor del Sol, que los alejan y acercan del centro de giro en cada revolución. El que las órbitas planetarias sean elípticas y no circulares fue en su momento un cambio más drástico de lo que parece. En efecto, según prejuicios de la época de Kepler, las órbitas de los planetas deberían ser circulares y seguir ciertas relaciones de tamaños para los diferentes planetas. De otro modo, serían imperfectas e incompatibles con la supuesta perfección del cielo.  Así, Kepler tuvo que desprenderse de ideas preconcebidas y admitir que las órbitas son elípticas y no circulares, basado en lo que le decían las mediciones de gran precisión acerca del movimiento de los planetas que había heredado de Tycho Brahe.Los enfoques de Kepler acerca de la naturaleza del mundo, sin embargo, podrían no haber sido siempre así de racionales. Al menos es lo que sugiere un artículo aparecido el pasado 26 de mayo en la revista “Talanta”, publicado por un grupo de investigadores de Israel e Italia, encabezados por Gleb ZIlberstein de la compañía Spectrophon, Rehovot, Israel. En dicho artículo, ZIlberstein y colaboradores presentan evidencia en el sentido que Kepler pudo haber sido un practicante de la alquimia, disciplina, que si bien se dice dio origen a la química moderna, fue esencialmente anticientífica. Esto último dado que los conocimientos de la alquimia eran secretos y compartidos solo por un grupo de iniciados, en contraste con la práctica científica que demanda que los resultados científicos se difundan de manera libre para que públicamente sean criticados, y aceptados o desechados según sea el consenso.    La evidencia que ZIlberstein y colaboradores ofrecen sobre las prácticas alquimistas de Kepler fueron obtenidas mediante un análisis de un manuscrito suyo que está guardado en los archivos de la Academia de Ciencias de Rusia en San Petersburgo.  Dicho manuscrito fue sometido a pruebas químicas empleando instrumentos sofisticados que revelaron la presencia de oro, plata, plomo, y mercurio, metales que están asociados a la práctica de la alquimia.  Según ZIlberstein y colaboradores, esto sugiere que, si bien Kepler pudo no haber sido un gran entusiasta de la alquimia, sí la habría practicado en alguna medida. Así, los metales encontrados en el manuscrito podrían haber sido llevados hasta ahí mediante sus dedos o las mangas de su ropa.  ¿Practicaba Kepler la alquimia? Los resultados de ZIlberstein y colaboradores apuntan en esta dirección, pero como ellos mismos lo señalan, no hay más datos que así lo indiquen. Sin embargo, no sería sorprendente que así lo hubiera hecho, pues se sabe que otros científicos de su misma estatura intelectual, como Brahe y Newton, sí estaban interesados en la alquimia.   Al margen de estos intereses, explicables por la época de transición que les tocó vivir, la contribución de Kepler, Brahe y Newton, y otros de su misma altura, en favor de una concepción racional de mundo fue decisiva. En particular, para desechar -si no en todo el mundo, al menos en buena parte del mismo- la práctica bárbara de la quema de brujas.",
    "La madrugada del 4 de febrero de 1975, la población de la ciudad china de Haicheng, que contaba en esos momentos con alrededor de un millón de habitantes, recibió de las autoridades la orden de evacuar la ciudad en prevención de un inminente terremoto. La tarde de ese mismo día, Haicheng fue alcanzada por un sismo de magnitud 7.3 que produjo alrededor de 2,000 víctimas fatales, decenas de miles de heridos y el colapso de miles de edificios. De no haberse dado la evacuación, se calcula que se habrían producido unas 150,000 muertes. El sismo de Haicheng fue predicho sobre la base de diferentes indicios ocurridos a lo largo de los meses que lo antecedieron, desde incrementos en los niveles de agua del subsuelo y variaciones en los niveles del suelo, hasta cambios en el comportamiento de los animales. Se dieron también series de pequeños temblores de tierra, los cuales se incrementaron antes de la ocurrencia del terremoto y dispararon la alerta de evacuación.El sismo de Haicheng es el primero de la historia en ser predicho. Desgraciadamente, también es el único, pues la experiencia no se ha repetido. De hecho, los expertos consideran que la ciencia de los sismos no está en estos momentos lo suficientemente desarrollada para poder predecirlos con un cierto grado de certeza.En estas condiciones, no ha quedado otra opción que buscar desarrollar sistemas para alertar a la población una vez que se producido un temblor y pueda ponerse a salvo. Como sabemos, México tiene instalado uno de estos estos sistemas, que tiene como objetivo emitir una alerta temprana sobre la ocurrencia de un sismo en las costas del Océano Pacífico. El sistema está basado en que la velocidad con que viaja la señal de alerta -a través de un medio eléctrico de comunicación- es mucho mayor que la velocidad con la que viajan las ondas sísmicas y es más efectivo en cuanto más alejado de la población esté el epicentro del fenómeno. Así, los habitantes de la Cd. de México cuentan con aproximadamente un minuto desde que reciben la señal de alerta, antes de que arribe la onda sísmica.  Para mitigar los efectos de un sismo potencialmente catastrófico, los expertos están también buscando predecir su magnitud a partir de la evolución de su amplitud durante los primeros segundos después de la ruptura de tierra que lo origina. Y es en esta dirección en la que apunta un artículo publicado el pasado 29 de mayo en la revista “Science Advances”. Dicho artículo fue publicado por Diego Melgar y Gavin Hayes de la Universidad de Oregón en los Estados Unidos y en el mismo hacen un análisis de una base de datos que incluye 3,000 sismos con magnitudes entre 6 y 9, ocurridos desde la década de los años 90. Estudiaron tanto datos tomados de estaciones sismográficas en tierra como datos tomados por satélites.En su investigación, Melgar y Hayes estudiaron la evolución de los sismos durante sus primeros instantes, con el objeto de averiguar si dicha evolución contenía alguna señal que indicara con antelación cuál fue la magnitud que finalmente alcanzaron. Encontraron que de 10 a15 segundos después de iniciado un sismo es posible predecir la magnitud que alcanzará. Esto, en el caso de sismos con magnitudes entre 7 y 9.De este modo, según los resultados de Melgar y Hayes, es posible determinar, con segundos de antelación, sí la magnitud de un sismo será moderada o potencialmente catastrófica. Esto, además, arguyen Melgar y Hayes, podría hacerse a través de mediciones vía satélite, que pueden determinar rápidamente la evolución de un sismo desde sus momentos iniciales. Así, si bien por el momento no es posible predecir un sismo antes de que se ocurra, aparentemente sí podemos determinar la magnitud que alcanzará segundos antes de que alcance su máximo desarrollo. Esto posiblemente no sea de gran ayuda en una situación de emergencia -dado que nada podemos hacer para evitar que el sismo alcance su máxima amplitud-, pero por lo menos nos indica que los sismos muestran -por fin- un flanco débil. Es decir, aunque los sismos son aparentemente todopoderosos, como parte del mundo físico que son siguen reglas que los harían predecibles si pudiéramos conocerlas. Estas reglas han sido difíciles de descubrir. El trabajo de Melgar y Hayes, no obstante, muestra que los sismos pueden predecirse, así sea solo con unos segundos de antelación, y por tanto indica que estaríamos en buen camino para descubrir qué es lo que los impulsa.  Después de todo, el objetivo de la investigación científica es entender al mundo para poder predecirlo.  Y una vez que conozcamos las reglas que lo gobiernan, podremos predecir la ocurrencia de terremotos, no por segundos, sino por horas o días de antelación. Y no por casualidad, como aparentemente fue el caso del terremoto de Haicheng.",
    "Nos enteramos en días pasados por los medios de comunicación de la ocurrencia del enésimo tiroteo masivo en los Estados Unidos, esta vez con 13 víctimas fatales. El tiroteo tuvo lugar el pasado viernes 31 de mayo en las oficinas municipales de Virginia Beach, localidad del estado de Virginia. Los tiroteos en lugares públicos en los Estados Unidos son eventos que constituyen una manifestación de un fenómeno más amplio. En efecto, tenemos que las víctimas de estos eventos son solo una fracción pequeña del total de muertos por armas de fuego en ese país, que en el año 2017 alcanzó una cifra cercana a los 40,000 según datos del Centro de Control de Enfermedades de Atlanta. Esta cifra hace que los Estados Unidos constituyan una anomalía en el mundo de los países desarrollados; habida cuenta, además, que cerca del 60% de las muertes por armas de fuego en los Estados Unidos corresponden a casos de suicidio.  De este modo, según cifras publicadas por la Asociación Médica Estadounidense, los Estados Unidos ocuparon el segundo lugar mundial en muertes por arma de fuego, superados solamente por Brasil, y por arriba de México que ocupa el tercer lugar en esta materia. Las causas para los altos índices de muerte por esta causa en Brasil y México -lo mismo que en Venezuela y Colombia, que ocupan el cuarto y quinto lugar en la nada honrosa lista- son, ciertamente, diferentes de las que prevalecen en los Estados Unidos. Estas últimas están asociadas, según una opinión extendida, a la facilidad con la que es posible adquirir y poseer legalmente un arma de fuego en un país en el que hay más armas que habitantes.En este contexto, las armas de fuego constituyen un problema de salud pública en los Estados Unidos y esto ha llevado a realizar estudios científicos para tratar de entender las causas por las que una persona normal pudiera decidir utilizar, en un determinado momento y con propósitos homicidas, un arma de fuego de las muchas que están a su alcance. Uno de estos estudios apareció el pasado 31 de mayo en la revista “JAMA Network Open”, editado por la Asociación Médica Estadounidense. El artículo correspondiente lleva como autores a Justin Chiang y Brad Bushman de la Universidad Estatal de Ohio.  En su artículo, Chiang y Bushman reportan los resultados de una investigación que tuvo como propósito determinar la influencia que los juegos de computadora en los que se despliega un cierto grado de violencia, tienen sobre el comportamiento de un niño en cuanto al manejo de un arma de fuego. Para este propósito, Chiang y Bushman, conjuntaron un grupo de 250 niños con edades entre los ocho y doce años, a los que se les dijo que la prueba en la que participarían tenía que ver con lo que a los niños les gusta hacer en su tiempo libre, como jugar con video-juegos y juguetes. Los niños fueron divididos en tres grupos y puestos a jugar por 20 minutos el juego “Minecraft”. Cada grupo jugó una versión diferente del juego, dos versiones violentas y una sin violencia. En una de las versiones violentas se matan monstruos empleando pistolas y en la otra se les da muerte por medio de sables. En la versión pacífica no existen monstruos ni armas. Los niños jugaron Minecraft por parejas. Uno de integrantes de la pareja manipuló el juego mientras que el otro sólo observó.     Al final del video-juego a cada pareja se les transfirió a una habitación contigua con juguetes y juegos de mesa, así como con dos pistolas, reales pero desactivadas, guardadas dentro de un armario. A los niños se les dijo que podían hacer uso de todo lo que se encontraba en la habitación por 20 minutos. Se les dejó a solas, pero observados por medio de una video-cámara.En el análisis de los resultados del experimento los investigadores descartaron a 30 niños por diferentes causas -algunos no encontraron las pistolas guardadas en el armario- y se evaluó su comportamiento con las pistolas. Encontraron que el 62% y el 57% de los niños que, en forma respectiva, jugaron el video-juego con pistolas y sables, manipularon las pistolas encontradas en la habitación. En contraste, solamente lo hizo el 44% de los niños que jugaron la versión no violenta. También, aquellos que jugaron las versiones violentas de Minecraft manipularon una pistola por más tiempo que aquellos que jugaron la versión no violenta. Adicionalmente, los que jugaron la versión con pistolas apretaron el gatillo y apuntaron a sí mismos o a su compañero más veces que aquellos que jugaron la versión no violenta del juego.  Chiang y Bushman concluyen que sus resultados demuestran que la exposición de un niño a un video-juego violento incrementa la probabilidad de que toque un arma de fuego en una situación dada y de que la manipule por más tiempo. Igualmente, hace más probable que la apunte hacia sí mismo o hacia otros y apriete el gatillo.  Al mismo tiempo, los investigadores reconocen que su estudio tiene limitaciones. Por ejemplo, señalan que existe la posibilidad de que los niños se inhibieran por estar en un laboratorio y no en su casa y actuaran en forma diferente.  Mencionan también que Minecraft no es un juego particularmente violento. Con relación a esto último, no obstante, hacen notar que un video-juego con escenas más crudas probablemente incrementaría los efectos que observaron.Para todos aquellos que somos legos en el tema, las conclusiones de Chiang y Bushman parecen convincentes. Como todos los resultados de una investigación científica, sin embargo, tienen que ser validados por investigaciones independientes. Por lo demás, quizá valga la pena hacer caso a las recomendaciones de Chiang y Bushman sobre la inconveniencia de exponer a los niños a los video-juegos violentos. Así como la de tener armas a su alcance.",
    "En los últimos doscientos años el mundo ha sido testigo de un avance tecnológico que se ha dado a una escala y a una velocidad sin precedentes. Por mencionar algunos de estos avances, los últimos dos siglos nos han traído: la máquina de vapor -protagonista de la Revolución Industrial-, la electricidad, los materiales sintéticos, las telecomunicaciones, las computadoras digitales, las modernas técnicas para el tratamiento y diagnóstico de enfermedades y, por supuesto, la red Internet. Y en todo esto, el conocimiento científico ha jugado el papel central.    En efecto, como sabemos, el método científico -que tal como lo conocemos tuvo sus orígenes en la Europa de los siglos XVI y XVII- ha permitido el desarrollo de tecnologías altamente sofisticadas a través de estrategias que involucran una cierta teoría científica y que van más allá de un simple procedimiento de prueba y error. Durante la Segunda Guerra Mundial, por ejemplo, el conocimiento teórico que habían alcanzado los científicos sobre física atómica y nuclear fueron centrales para el desarrollo de la energía nuclear y la bomba atómica -con la que trágicamente finalizó en 1945 la guerra entre Japón y los Estados Unidos. Si bien los esfuerzos dirigidos de científicos y tecnólogos pueden dar origen a tecnologías de gran sofisticación, el desarrollo de la ciencia básica en la que se apoyan dichas tecnologías requiere -por necesidad- que los científicos involucrados gocen de libertad para investigar y publicar los resultados de su trabajo, así como para criticar -positiva o negativamente- el trabajo de sus colegas. De no ser así se interfiere con el desarrollo de la ciencia que requiere de una discusión abierta de ideas y resultados científicos. La libertad de investigación es de este modo una premisa básica para la actividad de profesores y personal académico de las llamadas universidades de investigación, uno de cuyos objetivos es hacer avanzar el estado del conocimiento. En estas condiciones, resulta sorprendente enterarnos esta semana por la prensa que dos profesores de la Universidad Emory en Atlanta, Georgia, fueron dados de baja de sus puestos de trabajo por mantener relaciones de investigación con colegas en China. Dichos profesores responden a los nombres de Li Xiao-Jiang y Li Shiua, están casados y son de origen chino, aunque ciudadanos norteamericanos.   Los esposos Li son especialistas en genética, particularmente en el desarrollo de técnicas de edición de genes para el tratamiento de la enfermedad de Huntington. Habían estado trabajando en Emory por más de dos décadas apoyados por subvenciones de los Institutos Nacionales de Salud de los Estados Unidos. Su dimisión fue motivada por, supuestamente, no haber informado que sus investigaciones habían sido también apoyadas por agencias del gobierno de China y que en las mismas habían participado instituciones académicas de ese país. La separación de los esposos Li de la Universidad Emory les fue anunciada el 16 de mayo pasado mientras Xiao-Jiang estaba de viaje en China. Tras esto, les fue cerrado su laboratorio y la información acerca del mismo fue retirada del sitio de Internet de la universidad. Igualmente, a los cuatro investigadores postdoctorales de origen chino que trabajaban en dicho laboratorio les fue ordenado abandonar el país en 30 días -tal parece, sin embargo, que uno de ellos cuenta con un permiso “Green Card” para trabajar en los Estados Unidos.  En su defensa, los esposos Li alegan que no se les dio oportunidad de defenderse de la acusación que según ellos resulta infundada, pues habían proporcionado información a la Universidad Emory sobre sus conexiones de trabajo cuando les fue solicitada. De hecho, en un artículo que publicaron sobre su trabajo en marzo de 2018 en la revista “Cell”, declaran su adscripción, tanto a la Universidad Emory, como a la Universidad Jinan en China. Consignan, así mismo, que su trabajo estuvo apoyado por los Institutos Nacionales de Salud de Estados Unidos y por varias agencias chinas.  Es interesante mencionar también que en un blog publicado en 2017, Francis Collins, director de los Institutos Nacionales de Salud, resalta el trabajo de edición de genes realizado por los esposos Li en su búsqueda de un tratamiento para la enfermedad de Huntington.De acuerdo con información publicada esta semana en la revista “Science”, la dimisión de los esposos Li es parte de una cruzada emprendida por los Institutos Nacionales de Salud en respuesta a la preocupación del gobierno estadounidense por la posibilidad de que algunos países extranjeros, particularmente China, se estén aprovechando de manera injusta de las investigaciones apoyadas con fondos federales.Pareciera así que el episodio de los esposos Li podría inscribirse en el contexto más amplio de la ofensiva de los Estados Unidos en contra de China, la cual incluye la imposición de aranceles a las importaciones provenientes de ese país y acciones en contra de la compañía Huawei, la cual se aduce es una amenaza para la seguridad nacional.En un contexto de disputas comerciales y de seguridad nacional, pareciera que en este caso la ciencia de la genética resulta una víctima de su propio éxito. Es decir, en la medida en que es capaz de aportar soluciones a problemas de gran interés práctico -al igual que muchas otras ramas de la ciencia-, enfrenta un entorno tóxico de intereses comerciales y nacionales, lejos del ambiente de libre investigación y difusión de resultados al que aspira como ciencia.",
    "Todo es entrar a un supermercado -de los que cada vez hay más- para encontrarnos rodeados de todo tipo de alimentos, tanto frescos como procesados, y de origen lo mismo animal que vegetal. Todo esto, sin duda, nos facilita la vida. Al mismo tiempo, no obstante, los alimentos procesados ricos en grasas y carbohidratos son señalados como culpables por la epidemia de obesidad y diabetes, que asuela a muchos países de mundo, incluyendo al nuestro.Esto último tiene sentido si consideramos que somos producto de un largo proceso evolutivo y que muchos de los alimentos procesados que nos ofrecen los supermercados y que consumimos diariamente aparecieron apenas hace algunas décadas. Así, no hemos tenido suficiente tiempo para adaptarnos a la nueva alimentación.Hace unos dos millones de años los antecesores de nuestra especie sobrevivían como cazadores recolectores y en estas condiciones habrían desarrollado una capacidad para procesar la carne de los animales que cazaban. De hecho, se ha sugerido que el consumo de carne, que tiene una gran densidad energética, habría sido uno de los factores que posibilitaron el desarrollo del cerebro humano que, como sabemos, consume un 20% de la energía que emplea el cuerpo.Con el desarrollo de la agricultura hace unos 10,000 años, nuestros antecesores dieron un vuelco a sus hábitos alimenticios e incorporaron los granos a su dieta.  Habrían ido así, según la visión de algunos especialistas, en contra de la evolución a la que estuvieron expuestos a lo largo de millones de años, con las consecuentes hipotéticas enfermedades que habrían sufrido por una alimentación deficiente. De este modo, si quisiéramos mantenernos sanos, tendríamos que recurrir a una dieta similar a la que seguían los antecesores de nuestra especie hace cientos de miles, por no decir millones de añosLo cierto es que no sabemos con precisión qué es lo que los prehumanos consumían en tiempos tan remotos y, por el momento, no nos queda sino especular al respecto. Por otro lado, lo que sí sabemos es que tenemos un problema médico, el llamado síndrome metabólico -un conjunto de condiciones que incluyen una presión arterial elevada, y altos niveles de colesterol y glucosa en la sangre- que los especialistas piensan está asociado al cambio de alimentación que nos ha traído la industria de los alimentos. Así, más que especular sobre la comida de hace cientos de miles de años, resulta de mayor utilidad investigar qué es lo que estamos comiendo en la actualidad y cómo afecta esto a nuestra salud. Y en este sentido, habría que mencionar un artículo enviado el pasado 30 de abril al repositorio de artículos científicos “arXiv” mantenido por la Universidad Cornell en los Estados Unidos. En dicho artículo, un grupo de tres investigadores encabezado por Luca Maria Aiello de Nokia Bell Labs, Cambridge, Reino Unido, se reportan los resultados de una investigación llevada a cabo para averiguar qué comen los londinenses y cómo esto se relaciona con el desarrollo del síndrome metabólico.Para llevar a cabo su investigación, Aiello y colaboradores hicieron uso de la información contenida en recibos de compra de alimentos en 411 locales en la ciudad de Londres de la cadena de supermercados TESCO, la cual fue comparada con recetas médicas extendidas a pacientes en la misma área en Londres. Los recibos de la cadena TESCO -que incluyen un total de 1,600 millones de productos adquiridos durante 2015- no identifican al comprador, pero sí al área en la que vive, e incluyen una descripción de los alimentos adquiridos con sus respectivos volúmenes. Las prescripciones médicas, extendidas durante 2016, no identifican tampoco al paciente, pero sí al área en la que vive.Con esta información, los investigadores determinaron patrones de consumo de alimentos en 937 localidades a lo largo y ancho de la ciudad de Londres. Consideraron cinco categorías de alimentos: grasas, carbohidratos, azúcares, proteínas y fibra, y elaboraron mapas de su consumo por localidad. Determinaron, igualmente, niveles de consumo de calorías y de diversificación de nutrientes por área. Las recetas médicas fueron igualmente analizadas para determinar la distribución en las diferentes localidades londinenses de los fármacos que son prescritos para tratar niveles elevados de colesterol y glucosa en la sangre, y altas presiones arteriales. Con esta información, los investigadores determinaron la distribución de estas condiciones médicas, asumiendo que era la misma que la de los fármacos que se emplean para tratarlas.A través de una comparación de la información proporcionada por los recibos de supermercado y las recetas médicas, Aiello y colaboradores encontraron que el síndrome metabólico se correlaciona positivamente con el consumo de grasas, carbohidratos y azúcares, y negativamente con el consumo de fibra. Encontraron también una clara correlación positiva con el consumo de calorías, y una correlación negativa, igualmente clara, con la diversificación de nutrientes.   Concluyen Aiello y colaboradores que es posible determinar el grado de enfermedad metabólica de una población sobre la base de sus patrones de consumo de alimentos.  De manera específica, su ingesta de calorías -sobre todo de productos con alta concentración energética- y la diversidad de nutrientes de su alimentación.        Terminan Aiello y colaboradores su artículo con un consejo práctico para no terminar asociados con una enfermedad crónica “Comer menos de lo que instintivamente nos gustaría, balanceando todos los nutrientes, y evitando las grandes cantidades de comida que están a nuestra disposición”. Y, podríamos añadir, ingresando a los supermercados con tapaojos para no ver de lado.",
    "La coca -que no la cocaína- ha sido consumida desde tiempo inmemorial por las poblaciones andinas, desde Colombia hasta Bolivia. De hecho, en Perú y Bolivia su consumo es legal y la hoja de coca puede ser adquirida para este propósito sin restricciones. En Bolivia, en particular, es muy llamativo para los recién llegados lo extendido que está su uso. Lo está a tal grado que incluso han inventado el verbo “acullicar” para el acto de masticar y mantener en la boca un bolo de hojas de coca. La coca es usada lo mismo como planta medicinal -para aliviar dolores del estómago, por ejemplo- que para combatir la fatiga y mitigar las exigencias del hambre. Para aquellos no habituados a las alturas andinas de 3,500 metros en adelante, la coca se emplea también para combatir el llamado “Mal de montaña”. Tenemos así que la coca tiene múltiples usos -alejados de los que usualmente asociamos a los estupefacientes- y tradicionalmente ha formado parte de las culturas andinas. En estas circunstancias, a los pobladores de las regiones altas de Sudamérica les resultan fuera de lugar las exigencias de los países importadores de cocaína de erradicar el cultivo de la coca como un medio para combatir el consumo de la droga entre sus respectivas poblaciones. Tenemos así dos visiones encontradas sobre un mismo asunto.Ciertamente, los habitantes del altiplano boliviano pueden intentar fundamentar su posición con argumentos varios. Pueden incluso hacer uso de los resultados de un artículo aparecido esta semana en la revista “Proceedings of the National Academy of Sciences”, referente a descubrimientos arqueológicos en un sitio conocido como la “Cueva del Chileno” localizada en el sur de Bolivia, cerca de la frontera con Chile. Dicho artículo fue publicado por un grupo internacional de investigadores encabezado por Melanie Milles de la Universidad de Otago, en Nueva Zelanda, e incluye investigadores de la Universidad Mayor de San Andrés en Bolivia y de la Universidad Estatal de Pensilvania. En dicho artículo, Milles y colaboradores describen el hallazgo en la Cueva del Chileno de una bolsa de cuero, a la que le fue asignada una antigüedad de aproximadamente 1,000 años empleando técnicas de radiocarbono. Esta antigüedad corresponde, según los autores, al periodo de desintegración de la cultura Tiahuanaco, que se extendió en el altiplano boliviano desde la región del lago Titicaca, hasta el norte de Chile y Argentina. En el interior de dicha bolsa los arqueólogos encontraron varios objetos: dos tabletas de madera para inhalar sustancias -decoradas con figuras antropomorfas-, un tubo de madera finamente esculpido e igualmente para inhalar sustancias, dos espátulas de hueso, una banda tejida policromática para la cabeza, y fragmentos de plantas secas amarradas con hilos de lana y fibras. La bolsa de cuero también contenía una bolsa fabricada, de manera inusual, con tres hocicos de zorro en la que se encontraron restos de varias sustancias.Para identificar dichas sustancias, Milles y colaboradores rasparon el interior de la bolsa y obtuvieron una pequeña muestra de las mismas. Un análisis químico empleando técnicas sofisticadas encontró restos de al menos cinco sustancias psicoactivas, incluyendo cocaína y las dos componentes principales de la ayahuasca, una bebida con propiedades alucinógenas empleada en Sudamérica con propósitos ceremoniales. El hallazgo muestra que, al menos desde hace mil años, en el altiplano boliviano se usaba la coca, lo mismo que drogas alucinógenas y que los objetos encontrados en la Cueva del Chileno probablemente constituían los instrumentos de trabajo de un chamán.Igualmente, dado que las sustancias identificadas en la Cueva del Chileno se obtienen de plantas que no crecen en la región, sino en tierras más bajas en el caso de la coca y en la Amazonía en lo que se refiere a los ingredientes de la ayahuasca, el hallazgo muestra la existencia hace mil años de rutas de tráfico entre las tierras bajas y el altiplano boliviano.  El consumo de la coca en Bolivia tiene de este modo raíces ancestrales y a no dudar los bolivianos tienen argumentos de sobra para defender su producción de hoja de coca. El asunto no es tan simple, sin embargo, pues tal parece que el mercado de la cocaína en los países desarrollados ha generado un crecimiento de las áreas de cultivo de la planta de coca en Bolivia más allá de lo estrictamente necesario para el consumo de la población boliviana. De hecho, este crecimiento es uno de los motivos de controversia en el marco de las elecciones presidenciales programadas en Bolivia para el próximo mes de octubre.  Como sabemos, el actual presidente boliviano Evo Morales accedió a la presidencia de Bolivia en 2006 apoyado por los cultivadores de coca -él mismo es dueño de una parcela de cultivo-. Durante su mandato -ha tenido dos reelecciones y va por una tercera este año- se ha impulsado el cultivo de coca en la región de Chapare que coincidentemente es donde Morales tiene su base política. También, de manera coincidente, el gobierno de Morales construyó un aeropuerto internacional en esta región, el cual tiene un movimiento muy reducido. Los críticos del presidente, además, aseguran que algo así como el 90 por ciento de la coca producida en el Chapare -que no sería adecuada para acullicar- tiene como fin la producción de cocaína para exportación y que en esto el aeropuerto semivacío cumple una función.  De tener razón los críticos de Morales, el asunto de la coca boliviana es complejo y su posible defensa tendría que invocar argumentos más allá de los que podría ofrecer un resultado como el de MIlles y colaboradores. Ciertamente, en asuntos de tal complejidad la ciencia tendría poco que ofrecer.",
    "Hace apenas un par de décadas pocos hubieran anticipado lo que los teléfonos celulares han hecho posible hoy en día:  que una mayoría de personas tengan a su disposición una cámara fotográfica de buena calidad que puede usar para filmar o fotografiar en cualquier momento lo que se le pudiera ocurrir. Así, en conjunción con la red Internet, continuamente nos enteramos de manera gráfica de acontecimientos que de otro modo nos hubieran pasado desapercibidos o resultado de poca relevancia. Con la proliferación de los teléfonos inteligentes, todos estamos expuestos a ser fotografiados o filmados, y a convertirnos en habitantes del ciberespacio en contra de nuestra voluntad. Y con esto, ciertamente, perdemos algo de nuestra privacidad. Afortunadamente, las cámaras fotográficas pueden ser usadas para obtener imágenes nítidas solo hasta una cierta distancia. Las imágenes de objetos muy lejanos pierden nitidez y no permiten diferenciar detalles finos. Así, en una fotografía de una persona tomada con una cámara ordinaria a una distancia de centenares de metros sería difícil distinguir su identidad. Existen, por supuesto, cámaras especializadas con un gran número de pixeles -los elementos en el interior de la cámara que detectan la luz- capaces de producir imágenes con una mayor resolución y en la red Internet podemos encontrar ejemplos al respecto. No todo es cuestión del número de pixeles, sin embargo, pues cuando aumenta la distancia del objeto a fotografiar disminuye la cantidad de luz que refleja o emite dicho objeto y que entra por el lente de la cámara.  Como fácilmente podemos comprobar tomando una fotografía con nuestro teléfono celular en un cuarto con poca iluminación, la falta de luz compromete severamente su nitidez.Así, la cámara fotográfica de un teléfono celular podría atentar contra nuestra privacidad si se encuentra a una distancia de metros o decenas de metros. Más allá de esto no producirá imágenes lo suficientemente nítidas para revelar identidades. No obstante, dada la gran velocidad con la que avanza la tecnología, cabe preguntarse por cuánto tiempo esto último seguirá siendo válido y con este respecto traeremos a colación un artículo enviado el pasado 22 de abril al repositorio de artículos científicos arXiv.org alojado por la Universidad Cornell en el Estado de Nueva York. Dicho artículo fue escrito por un grupo de investigadores de la Universidad de Ciencia y Tecnología de Shanghai, encabezado por Zheng-Ping Li, Xiin Huang y Yuan Cao, y en el mismo se describe una cámara capaz de obtener imágenes a una distancia de 45 kilómetros. Si bien puede producir imágenes con una cierta nitidez de objetos muy lejanos, el dispositivo reportado por los investigadores chinos no es en realidad una cámara fotográfica en el sentido ordinario. Para entender su funcionamiento, consideremos primeramente que para generar una fotografía una cámara ordinaria recoge la luz -ambiental o artificial- que refleja el objeto a fotografiar y la enfoca en el elemento que produce la imagen. La cámara reportada por Zheng-Ping Li y colaboradores, en contraste, ilumina el objeto a fotografiar con un rayo láser, recoge la luz que refleja, y la enfoca en el detector que forma la imagen.De este modo, la cámara desarrollada por los ingenieros chinos puede obtener imágenes aun durante la noche, pues produce su propia luz para iluminar el objeto a fotografiar. Además, puesto que para esto último emplea un láser -que se propaga en una sola dirección-, el objeto a iluminar puede estar localizado a muchos kilómetros de distancia.  La cantidad de la luz de láser que es reflejada por el objeto fotografiado y que logra llegar a la cámara es, por supuesto, extremadamente pequeña, dada la gran distancia que tiene que viajar. De este modo, para formar una imagen es necesario emplear un detector de luz con una sensibilidad extrema, más allá de la sensibilidad característica del detector de una cámara fotográfica ordinaria. Además, Zheng-Ping Li y colaboradores tuvieron que atender un problema adicional: la luz que llega a la cámara después de ser reflejada por el objeto a fotografiar está corrompida por la luz ambiental, que no fue reflejada por dicho objeto y que, por supuesto, degradará la calidad de la imagen. Para resolver este problema, los ingenieros chinos midieron el tiempo que le tomaba a la luz del láser en ir y regresar desde el objeto fotografiado y razonaron que cualquier cantidad de luz que arriba al detector en un tiempo diferente no pude haber sido reflejada por el objeto de interés. Así, fue descartada en el momento de formar la imagen por la computadora que controlaba la cámara.Empleando todas las técnicas descritas anteriormente, Zheng-Ping Li y colaboradores pudieron obtener imágenes de la parte superior de un edificio localizado a una distancia de 45 kilómetros, con una resolución que claramente mostraba todas sus características morfológicas.De todo lo anterior podemos obtener dos conclusiones: 1) Si bien la cámara de Zheng-Ping Li y colaboradores es impresionante, por el momento resulta demasiado cara para que pudiera ser incorporada en un teléfono celular. Desde este punto de vista podemos dormir tranquilos. No obstante, aun estando a cientos de metros de un teléfono celular, quizá deberíamos tratar de mantenernos sonrientes por si a alguien se le ocurriera incorporarnos al ciberespacio empleando algún truco no considerado aquí. 2) La tecnología china avanza a pasos agigantados y con seguridad pronto alcanzará un nivel de competencia con la de los Estados Unidos. ¿Alguna lección para aprender de nuestra parte?",
    "Tuve ocasión de visitar en días pasados la ciudad de Potosí, Bolivia la cual se encuentra localizada en el sur del altiplano boliviano, al pie del Cerro Rico. Con una altitud superior a los 4,000 metros, Potosí compite con la ciudad de El Alto, también en Bolivia, como la ciudad con una población superior a los 100,000 habitantes más alta del mundo. Potosí es una ciudad con un clima frío y seco, que parece estar locada en el fin del mundo.  Como sabemos, durante la colonia española, el Cerro Rico de Potosí fue asiento de las minas de plata más ricas del planeta, las cuales produjeron una parte sustancial de este metal que circuló por el mundo. El Cerro Rico fue particularmente relevante entre los años 1545 -cuando empezó su explotación- y 1650, época en la que Potosí alcanzó una población de 160,000 habitantes que rivalizaba con la de las mayores ciudades europeas de aquellos tiempos.  Sabemos, por otro lado, que el nombre de nuestra ciudad incluye la palabra Potosí en referencia a las minas de plata del Cerro de San Pedro que se pensaba podrían alcanzar la importancia de las del Cerro Rico. Si bien no se cumplieron las expectativas en este sentido, la producción de plata de la Nueva España en su conjunto alcanzó niveles muy altos -aunque no tanto como los del Cerro Rico- con minas en Zacatecas, Guanajuato, Real del Monte y San Luis Potosí. Se sabe que una parte importante de la producción de plata en la Nueva España y en el Alto Perú -en donde se encontraba Potosí- se dirigió hacia China para la adquisición de productos como la seda y el té que tenían gran demanda. En este sentido, China era en esa época un país avanzado y autosuficiente que no estaba interesado en productos europeos o americanos otros que no fueran los metales preciosos.  Una parte más de la producción de plata se quedaba en las colonias españolas, mientras que otra, por supuesto, se dirigía a España. Además, vía el pago de la deuda adquirida por la corona española para financiar sus guerras, o bien a través del robo de galeones españoles cargados con plata por parte de los piratas ingleses, la plata americana llegó a otros países europeos.    Se ha considerado que el flujo de plata del Nuevo Mundo hacia Europa fue uno de los factores que originaron la inflación sufrida por la economía europea entre los años 1550 y 1650 y en este contexto, Anne-Marie Desaulty y Francis Albarede de la Universidad de Lyon en Francia, se propusieron estudiar con detalle el flujo de la plata de la Nueva España y del Alto Perú hacia Europa. Reportaron los resultados de su investigación en el número de febrero de 2013 de la revista Geology.  El estudio de Desaulty y Albarede se basó en la medición de la composición isotópica de plata, cobre y plomo de 15 monedas inglesas acuñadas en el periodo 1272-1649, y su comparación con las composiciones respectivas de los yacimientos de Potosí y de México, lo mismo que de monedas acuñadas en la Europa Medieval. Sabemos que los elementos químicos como la plata, el cobre y el plomo se encuentran en la naturaleza en la forma de diferentes isótopos, los cuales son variedades de un elemento químico con pesos diferentes. La utilidad de este hecho para el estudio de Desaulty y Albarede radica en que la composición de isótopos de plata, cobre y plomo del Cerro Rico es característico y diferente de la composición respectiva de las minas mexicanas. Así, la medición de la composición isotópica de las monedas inglesas permitió a los investigadores determinar el origen de la plata contenida en las monedas inglesas y a partir de éste los flujos de plata desde el Nuevo Mundo hacia Europa.     Como resultado de su investigación, Desaulty y Albarede encontraron que la composición isotópica de las monedas acuñadas antes de 1553 coincide con la composición de las monedas medievales. En contraste, la composición de las monedas acuñadas entre 1553 y 1649, con una excepción, coincide con la composición de los yacimientos mexicanos, pero no con la de Potosí. Esto prueba que hubo un flujo de plata de la Nueva España hacia Europa en los siglos XVI y XVII. Igualmente, indicaría que la plata del Cerro Rico no había alcanzado Europa al mediar el siglo XVII. Solamente hacia el final del reinado de Carlos I de Inglaterra a mediados del siglo XVII habría llegado la plata del Potosí a las monedas inglesas. De este modo, antes de 1650 el destino de la plata de Potosí habría sido China y no Europa. Dada la situación geográfica de Potosí, el metal habría sido primeramente llevado hasta la costa del Océano Pacífico, atravesando la cordillera de los Andes, y una vez allí transportado por mar hasta Acapulco. Desde Acapulco, el Galeón de Manila habría llevado los cargamentos de plata hasta China vía las Filipinas. De hecho, una ruta hacia el oeste es más natural desde Potosí que una hacia el este, lo que implicaría cruzar Brasil para alcanzar la costa atlántica. En contraste, para la plata mexicana era más natural una ruta hacia el este, llegando primeramente a Veracruz y de ahí a Europa cruzando el Océano Atlántico. Esto explicaría la tardanza de la plata del Cerro Rico en llegar a Europa.De una u otra manera, si bien en la actualidad en apariencia el Cerro Rico luce como debería haberlo hecho en su época de esplendor, internamente es un queso gruyere vacío de plata. De hecho, Bolivia ocupa actualmente apenas el sexto lugar mundial como productor de este metal, lejos del primerísimo lugar que alguna vez ocupó. Todo esto como producto de la sobreexplotación del Cerro Rico. Y con poco beneficio -por no decir maleficio- para los muchos bolivianos que perecieron en sus entrañas.",
    "Durante su primer periodo como presidente de los Estados Unidos en la década de los años 50 –específicamente, el 25 de septiembre de 1955-, Dwight Eisenhower sufrió un infarto al miocardio que lo tuvo seis semanas en el hospital. Si bien se recuperó a un grado tal que incluso fue elegido para un segundo período, la notoriedad pública de su percance médico hizo que tomaran fuerza las opiniones de algunos especialistas en contra de los alimentos ricos en grasas saturadas como causantes de enfermedades del sistema circulatorio. Así, las grasas saturadas adquirieron una mala reputación y en 1977 un comité del Senado de los Estados Unidos publicó un documento en el que se recomendaba reducir su ingesta e incrementar el consumo de carbohidratos en sustitución. De manera específica, entre otras cosas, el senado norteamericano recomendaba incrementar el consumo de frutas, vegetales y de granos integrales, y reducir el de carnes rojas, y de alimentos altos en contenido de azúcar o de sal.De alguna manera, sin embargo, algo salió mal y la dieta de los norteamericanos, reflejada en su salud, no solamente no mejoró, sino que empeoró.  Así las cosas, hoy en día las grasas saturadas han perdido buena parte de su papel de villanos de la alimentación que hoy es compartido por los alimentos ricos en azúcar.En estas circunstancias, y dadas las recomendaciones cambiantes que nos han dado los expertos a lo largo del tiempo, cabe preguntarse sobre cuál sería una dieta “sana” -en el caso de que la hubiera-. Una respuesta en este sentido nos la da un artículo aparecido el pasado 3 de abril en la revista médica “The Lancet”, publicado por un grupo internacional de investigadores encabezado por Christopher Murray y Ashkan Afshin de la Universidad de Washington en Seattle. En dicho artículo se presentan los resultados de un estudio llevado a cabo para evaluar el consumo de alimentos y nutrientes en 195 países y determinar el impacto que una ingesta subóptima de los mismos tiene en la mortalidad y morbilidad por diversas enfermedades no trasmisibles.Como parte de su investigación, Murray, Ashkan y colaboradores determinaron niveles óptimos para el consumo de grupos de alimentos que minimizan el riesgo de muerte por cualquiera de las causas consideradas, que incluyen enfermedades cardiovasculares, cánceres y diabetes tipo 2.  Entre los grupos de alimentos considerados se cuentan: frutas, vegetales, legumbres, granos enteros, nueces y semillas, leches, carnes rojas, carnes procesadas y bebidas azucaradas. Los investigadores establecieron también niveles óptimos para la ingesta de calcio, fibra, sodio, grasas trans, y ácidos grasos omega-3.En general, Murray, Ashkan y colaboradores encontraron desbalances entre el consumo real de alimentos y nutrientes y sus niveles de consumo óptimos.  En algunos casos, por debajo de estos niveles y en otros muy por arriba de los mismos. Así, la ingesta de frutas, granos enteros, nueces y semillas, y de leche, está en todas las regiones de mundo muy por debajo de lo recomendado. En contraste, es generalizado el sobreconsumo de bebidas azucaradas y en menor medida de carne procesada y de sodio. Por otro lado, son los bajos consumos de frutas y granos enteros, y la alta ingesta de sodio los que tienen una mayor influencia en la salud pública. Así, de los 11 millones de muertes ocurridas en 2017 atribuidas a una mala alimentación, más del 70% lo fueron por la ingesta inadecuada de frutas, granos enteros y sodio. Los anteriores son números globales y existen diferencias sustanciales entre diferentes regiones del mundo. Por ejemplo, entre la población acomodada de Norteamérica es particularmente alto -muy por encima de los niveles recomendados- el consumo de carnes rojas, carnes procesadas, bebidas azucaradas, grasas trans y sodio. En contraste, la ingesta de frutas, vegetales, legumbres, granos enteros y nueces y semillas está por debajo de lo recomendado. Una situación similar acontece en los países de Europa Occidental. Por su lado, en los países del este asiático se consumen carnes rojas, bebidas azucaradas, y particularmente sodio por arriba de los niveles óptimos. En referencia a México, entre los 20 países más poblados del mundo según Murray, Ashkan y colaboradores, ocupamos el primer lugar en cuanto a bajo consumo de nueces y semillas, el segundo por bajo consumo de vegetales, el tercero por bajo consumo de granos enteros, el cuarto por bajo consumo de frutas, el quinto por alto consumo de bebidas azucaradas, el sexto por bajo consumo de omega-3, el séptimo por alto consumo de grasas trans, el octavo por alto consumo de sodio, el noveno por bajo consumo de grasas poliinsaturadas y el décimo por bajo consumo de fibra.   En conclusión, según Murray, Ashkan y colaboradores, el mundo se alimenta terriblemente mal, aunque no todos por la misma razón. Nosotros, por nuestro lado y por lo que se ve, no cantamos mal las rancheras. Tendríamos así que modificar nuestros hábitos dietéticos si queremos tener una mejor salud. Antes de esto, no obstante, habríamos de asegurarnos -con estudios adicionales que lo corroboren- que Murray, Ashkan y colaboradores están en lo cierto.  So pena de encontrarnos con una sorpresa futura.",
    "Como sabemos, con la introducción de medidas de higiene la posterior aparición de los antibióticos, hoy en día esperamos vivir considerablemente más años que lo que vivieron nuestros antecesores hace apenas dos siglos. Esta expectativa tiene, ciertamente, bases objetivas, pues si echamos una vista a las tablas publicadas en varios sitios de Internet, encontraríamos que mientras que en 1800 la esperanza de vida al nacer apenas sobrepasaba los 30 años, hoy supera los 70 años. Estos números, no obstante, son cifras globales y existe una gran disparidad en esperanza de vida entre los diversos países del mundo, atendiendo a su grado de desarrollo.En la actualidad, el país con mayor esperanza de vida es Japón con 84.2 años, según la Organización Mundial de la Salud. Además de este país, el “top ten” de países con mayores esperanzas de vida está integrado por Suiza, España, Francia, Singapur, Francia, Australia, Corea del Sur, Italia y Canadá. En el otro extremo de la clasificación se encuentran países africanos como Angola y la República Centroafricana, que apenas rebasan los 50 años de esperanza de vida. En este respecto, México se encuentra arriba de la media tabla con 76.6 años. Si bien vivir años extra es deseable, estaríamos de acuerdo en que también lo es vivirlos con la mayor salud posible. Esto último, además -y al margen de cualquier deseo personal-, es relevante desde un punto de vista puramente económico, dada la incapacidad temporal o permanente que puede sufrir una persona enferma. En estas circunstancias, se han desarrollado índices para caracterizar la edad de una población atendiendo no solamente a la esperanza de vida sino también a la calidad de la misma. Uno de estos índices -DALYS, por sus siglas en inglés- mide la cantidad de días perdidos debido, tanto a una muerte prematura más allá de la esperanza de vida, como a los días perdidos por la incapacidad producto de una enfermedad.En un artículo aparecido el pasado 6 de marzo en la revista “The Lancet Public Health”, un grupo internacional de investigadores encabezado por Angela Chiang de la Universidad de Washington Seattle, reporta el desarrollo de un nuevo índice que refleja tanto la mortalidad como el estado de salud de una población. Dicho índice está basado en un grupo -determinado por ellos siguiendo un cierto criterio- de 92 enfermedades relacionadas con el envejecimiento y que son potencialmente inhabilitantes. Sobre esta base Chiang y colaboradores investigaron el costo en años de vida perdidos que representaron dichas enfermedades en 195 países en el periodo de 1990 a 2017. De manera adicional, tomando como referencia el índice que corresponde a una población promedio a nivel global de 65 años, determinaron en qué medida el índice de la población de cada uno de los países estudiados coincide o se aleja del índice promedio. De manera específica, determinaron cuál es la edad cronológica de los habitantes de cada país a la que experimentan problemas similares de salud a los que sufre la población promedio del mundo a los 65 años. Como era de esperarse, los países con mayor grado de desarrollo tendieron a salir mejor calificados en el estudio. Así, el país con menos años perdidos por enfermedad en 2017 fue Suiza con 104.9 años por cada 1,000 adultos mayores de 25 años. Le siguieron, en ese orden, Singapur, Corea del Sur, Japón e Italia. Una excepción notable entre los países ricos fueron los Estados Unidos, que con 161 años perdidos ocuparon el lugar 53, entre Argelia e Irán. En el otro extremo y ocupando el último lugar, Papúa Nueva Guinea tuvo en 2017, 500.6 años perdidos por enfermedad, seguido en orden ascendente por las Islas Marshal, Vanatu y Afganistán. En cuanto a México, ocupó el lugar 48 con 156.2 años perdidos, una posición relativamente alta e incluso por encima de la de los Estados Unidos.Con relación a la edad a la que el habitante promedio de un determinado país se siente tan enfermo o sano como lo estaría el habitante promedio del mundo a los 65 años, los resultados obtenidos por Chiang y colaboradores son vistosos -si hemos de calificarlos de algún modo.- En efecto, el país con adultos mayores más sanos es Japón, en donde a los 76.1 años el habitante promedio se siente tan sano como lo está la persona promedio a los 65 años.  Le siguen Suiza, Francia, Singapur y Kuwait en ese orden. Lejos, en el lugar 54 con 68.5 años, quedan los Estados Unidos, inmediatamente después de Irán y apenas por encima de Cuba que ocupa el lugar 57. En el otro extremo de la escala Papua Nueva Guinea es, por mucho, el peor calificado con 45.6 años. De este modo, el habitante promedio de este país está tan enfermo como un japonés promedio 30 años más viejo. Nuestro país, con 70.3 años está también relativamente bien colocado en este rubro, por encima de países como los Estados Unidos, Brasil y China, pero por debajo de Perú, con 74.3 años, lo mismo que de Panamá, Costa Rica, Colombia, Ecuador y Chile.En general, de estos últimos números se encuentra que las poblaciones de los países de la costa sudamericana del pacífico tienden a tener una buena salud en la edad avanzada, incluso mejor que la de algunos países europeos como el Reino Unido y Alemania. El artículo de Chiang y colaboradores no hace ninguna mención al respecto. En este contexto, es interesante preguntarse por la causa por la que algunos países -incluyendo al nuestro- no tan afluentes como otros, tienen sin embargo poblaciones más sanas.",
    "Posiblemente haya usted experimentado la sensación de que el tiempo corre más rápido en la medida que envejecemos. Una vez que tenemos los suficientes años, sentimos que el tiempo “vuela” y que los años se suceden a una velocidad considerablemente más grande que la que experimentamos en tiempos de juventud. Para este hecho hay dos posibles explicaciones. Una de éstas es que el tiempo, efectivamente, se ha acortado en los últimos decenios. De manera alternativa, tendríamos que asumir que la percepción de acortamiento del tiempo es solamente una ilusión. De acuerdo con la primera explicación, la Tierra estaría girando sobre su eje cada vez más rápido de modo que los días serían cada vez más cortos. Podría ser también que la velocidad de nuestro planeta alrededor del Sol esté creciendo continuamente y con esto los años durarían cada vez menos. Estaríamos quizá de acuerdo que la primera explicación no es convincente, pues la velocidad de rotación de la Tierra está en realidad decreciendo de forma continua, lo mismo que su periodo orbital alrededor del Sol en la medida en que se aleja del mismo -esto último debido a que el Sol pierde masa por la energía que emite y por tanto disminuye su fuerza de atracción sobre la Tierra-. Por lo demás, ambos efectos son extremadamente pequeños y no serían apreciables en el curso de una vida: la duración del día apenas ha cambiado 1.7 milésimas de segundo en los últimos cien años y la Tierra se aleja del Sol solamente unos 1.5 centímetros por año. Así, no habría causas astronómicas que expliquen nuestra percepción de acortamiento del tiempo, la cual debe ser por tanto solamente una ilusión. En este punto cabe preguntarse sobre el mecanismo por el cual se producía dicha ilusión. Una respuesta la encontramos en un artículo aparecido esta semana en la revista “European Review”, publicado por Adrian Bejan de la Universidad Duke en los Estados Unidos. En dicho artículo, Bejan analiza el mecanismo por el cual percibimos el transcurrir del tiempo y llega a la conclusión que, en la medida en que envejecemos, perdemos la capacidad para procesar los estímulos que recibimos del mundo a través de los sentidos y que esto conlleva una percepción de aceleración temporal. Esto último, que pareciera contraintuitivo a primera vista, es explicado por Bejan de la siguiente manera. Percibimos el tiempo por los estímulos que recibimos a través de los sentidos, tales como las imágenes del mundo que captamos a través de los ojos y que son posteriormente enviadas al cerebro por los conductos nerviosos. Por otro lado, un cambio en el tiempo es percibido por la mente cuando las imágenes registradas por el cerebro cambian. La longitud de un intervalo de tiempo que percibe la mente está así determinada por el número de imágenes que recibe el cerebro a lo largo de dicho intervalo.Los ojos adquieren la información visual mediante movimientos rápidos que fijan la vista en puntos determinados. Una vez que una imagen es captada por los ojos, es enviada al cerebro para su procesado. En la medida en que envejecemos, sin embargo, perdemos capacidad para llevar a cabo estas funciones. Por un lado, disminuye la velocidad con que los ojos pueden generar imágenes; por el otro, crece la longitud por la que tienen que viajar los impulsos nerviosos desde el órgano sensor hasta el cerebro, al mismo tiempo que disminuye la velocidad con la que viajan los impulsos por los canales nerviosos. Como resultado, a lo largo de la vida disminuye el número de imágenes percibidas por la mente en un tiempo dado.Bejan resume lo anterior como sigue. Hace notar primeramente que una mayor parte de las imágenes que recordamos deberían ser de nuestra juventud. Concluye también que la velocidad del tiempo percibida por la mente humana debería incrementarse a lo largo de la vida, pues el tiempo medido por un reloj físico entre imágenes mentales sucesivas se incrementa con la edad. La percepción de que el tiempo “vuela” en la edad avanzada es entonces explicado por el deterioro que sufrimos en la medida en que envejecemos en nuestras capacidades para adquirir y procesar la información que recibimos del mundo a través de los sentidos. Como consecuencia de dicho deterioro, existe un desacople cambiante a lo largo de la vida entre el tiempo físico -medido por los relojes- y el tiempo subjetivo percibido por nuestra mente.Desafortunadamente, poco podemos hacer por el momento para aminorar o revertir el deterioro de nuestras capacidades sensoriales. No obstante, si todo fuera cuestión del número de imágenes percibidas por el cerebro como lo afirma Bejan, lo que sí posiblemente podríamos hacer es mantener a nuestros sentidos y a nuestro cerebro en el máximo posible estado de ocupación, abrumándolos con imágenes y experiencias novedosas; tal como hicimos en nuestra juventud. Para tal fin, tendríamos que evitar en lo posible actividades repetitivas y poco novedosas que no nos dejen impresiones duraderas. Alargaríamos así nuestra percepción del tiempo, que haríamos “volar” -esperaríamos- a una velocidad menor.",
    "Entre el 31 de agosto y el 9 de noviembre de 1888 aparecieron degolladas cinco mujeres en el barrio londinense de Whitechapel. Con esto nació la leyenda de “Jack el Destripador”, producto tanto de la brutalidad de los asesinatos como de su cobertura mediática por la prensa amarillista de la Inglaterra victoriana que buscó cultivar el morbo del público. La historia es bien conocida: cinco mujeres fueron atacadas en la calle en horas de la madrugada -con la excepción de la última víctima que fue encontrada muerta en su recámara- por un asesino que nunca fue identificado. Todas las víctimas fueron encontradas degolladas; cuatro de ellas, además, con heridas en el vientre. De estas últimas a tres les fueron extraídos algunos órganos internos, incluyendo órganos sexuales. Y por encima de todo, el asesino, que se habría dado el lujo de enviar una carta retadora a la policía acompañada de un pedazo de riñón supuestamente de una de las víctimas, nunca fue descubierto. Se dieron así circunstancias favorables para que se perpetuase la leyenda de Jack el Destripador.Así las cosas, a lo largo de los últimos 130 años se han dado iniciativas por parte de diferentes actores para identificar al asesino de Whitechapel, en algunos casos alcanzando sorprendentes conclusiones. Por ejemplo, se ha identificado a Jack el Destripador con el príncipe Alberto Víctor, nieto de la reina Victoria y asiduo visitante de los bajos fondos londinenses. De manera alternativa se le ha identificado con el médico de la reina, quien habría actuado para ocultar las andanzas Alberto Víctor en los barrios bajos de Londres, durante las cuales engendró hijos con mujeres que habría que eliminar para prevenir posibles extorsiones. Se le ha identificado también con el pintor Walter Sickert e incluso se ha sugerido que el asesino no fue un hombre sino una mujer.  Otras hipótesis más aterrizadas incluyen sospechosos que lo fueron también durante las indagaciones que llevó a cabo la policía en su momento, pero que nunca pudieron se imputados por falta de pruebas. Entre éstos se incluye al pseudo médico estadounidense Francis Tumblety, que resultaría una opción atractiva por los supuestos conocimientos que habría tenido de la anatomía humana, útiles para la extracción de los órganos internos de las víctimas. Se incluye también a Aaron Kosminski, un judío polaco que experimentaba un odio patológico hacia las mujeres y que fue internado en un hospital siquiátrico al año siguiente de los acontecimientos de Whitechapel.Esta última hipótesis es respaldada por un artículo publicado esta semana en la revista “Journal of Forensic Sciences”, por Jari Lohelainen y David Miller, de las universidades de Liverpool y de Leeds, en forma respectiva. De acuerdo con Lohelainen y Miller, la suya es la primera investigación sobre la identidad de Jack el Destripador que ha sido publicada después de haber sido evaluada y aprobada por otros científicos que no participaron en la misma, que es la práctica estándar en toda investigación científica.Para alcanzar sus conclusiones, Lohelainen y Miller llevaron a cabo una investigación del ADN mitocondrial de algunas manchas de sangre y fluidos seminales observadas en un chal de seda que habría sido encontrado en la escena de crimen de Catherine Eddowes, una de las mujeres asesinadas en Whitechapel. Los investigadores presumían que la sangre y el semen provenían, en forma respectiva, de Eddowes y de su victimario, que asumían era Kosminski. Para probarlo, compararon el ADN encontrado en el chal con el de parientes vivos de ambos, Eddowes y Kosminski, encontrando que en ambos casos coincidían. De esto modo, se demostraría por un lado que el chal estuvo efectivamente en la escena de crimen y que Kosminski era el asesino.  No todo el mundo estuvo de acuerdo, sin embargo. Así, por ejemplo, un artículo de divulgación -no de investigación- aparecido en días pasados en la revista “Science” critica a Lohelainen y Miller por no publicar en forma detallada los resultados de sus determinaciones de ADN. Al respecto, dichos investigadores arguyen que la Ley de Protección de Datos del Reino Unido prohíbe publicar información detallada del ADN de individuos vivos. Otros científicos, no obstante, sostienen que las secuencias de ADN mitocondrial no representan un riesgo contra la privacidad de las personas y que Lohelainen y Miller deberían de publicar sus resultados de manera detallada. De otro modo no se podrían juzgar.  Los expertos hacen notar también que, con base en el ADN mitocondrial solamente se podrían excluir sospechosos. Así, según el artículo de “Science”, el ADN encontrado en el chal podría ser efectivamente de Kosminski, pero igualmente lo podría ser de otros miles de personas que vivieron en Londres en esa época. Todo lo anterior sumado a que no se ha demostrado fehacientemente que el chal de referencia estuvo realmente en la escena de crimen.Así, si bien los resultados de Lohelainen y Miller representan un avance significativo hacia el establecimiento de la identidad de Jack el Destripador, todavía quedan dudas por disipar. De hecho, no hay seguridad de que la conozcamos algún día. Lo cual, por otro lado, ciertamente solo tiene una importancia relativa.  Al margen de lo anterior, podemos concluir que con su trabajo Lohelainen y Miller nos demuestran las impresionantes capacidades de análisis de las que dispone la ciencia moderna, que permiten arrojar luz sobre acontecimientos que ocurrieron hace más de cien años, partiendo apenas de algunas manchas en una tela.",
    "Como sabemos, la luz del Sol es la fuente última de toda la vida sobre el planeta.  Sin radiación solar las plantas no podrían reproducirse ni generar la materia orgánica que necesitan los animales para sobrevivir. Ciertamente, sin el Sol la vida tal cual la conocemos no podría haber surgido en la superficie de nuestro planeta. Así, durante los millones de años en los que hemos evolucionado como especie hubimos de adaptarnos a las características particulares de los rayos que emite el Sol y que llegan a la superficie de la Tierra. Tenemos, por ejemplo, que los periodos de día y de noche ocurren cada 24 horas y que, en consecuencia, desarrollamos un ritmo circadiano con esta periodicidad. De haber tenido la Tierra una velocidad de rotación diferente, nuestro ritmo circadiano habría evolucionado diferente y se habría adaptado según fuera el caso. Otra característica importante de la radiación del Sol que ha demandado de una adaptación es su composición de colores que determina su tonalidad. La radiación solar tal cual la percibimos tiene un tono amarillento. Esto es debido a que su contenido de colores verde y azul-verde es relativamente grande en comparación con su contenido de azul y rojo. En estas condiciones el ojo humano evolucionó hacia una sensibilidad a la luz diurna que es máxima precisamente a los colores verde y azul-verde. Si viviéramos en otro sistema solar, con una estrella más caliente o más fría que nuestro sol -la cual emitiría radiación con una composición diferente de colores-, quizá la máxima sensibilidad de nuestro ojo correspondería a un color diferente.  Por otro lado, en fechas muy recientes en términos de la evolución -hace poco más de un siglo-, apareció una forma de luz artificial -el foco incandescente- que modificó drásticamente nuestros hábitos nocturnos. La luz que emite el foco incandescente tiene una tonalidad rojiza -en contraste con la tonalidad amarillenta de la luz solar- que revela su mayor contenido de luz roja, y que lo acerca a la tonalidad de la luz solar durante el crepúsculo. En fechas todavía más recientes -hace apenas un cuarto de siglo- apareció una nueva y revolucionaria lámpara para la iluminación ambiental: el LED, que tiene una serie de virtudes que han puesto al foco incandescente en el camino de su extinción. Al contrario de la tonalidad de la luz incandescente, sin embargo, la tonalidad de la luz emitida por los LEDs puede diferir grandemente de aquella de la luz solar por su alto contenido de luz azul, y desde este punto de vista entra en conflicto con nuestra adaptación evolutiva. En efecto, sabemos que el ojo humano, aparte de los dos tipos de receptores especializados en las visiones diurna y nocturna, tiene un tercer tipo de receptores que provoca efectos fisiológicos como respuesta a condiciones cambiantes de la iluminación ambiente. Estas respuestas incluyen la producción de melatonina, la cual prepara al cuerpo para el periodo de sueño. En este sentido, se ha encontrado que la iluminación nocturna con LEDs con un alto contenido de luz azul interfiere con la producción de melatonina y por consecuencia con el ritmo circadiano. Este resultado indica que los LEDs con un alto contenido de luz azul –luz “fría”- deben evitarse para iluminación nocturna en casas-habitación en beneficio de los LEDs de luz “cálida”.En contraste, los LEDs de luz “fría” serían útiles en las primeras horas de la mañana para despejar la somnolencia después de despertar del sueño nocturno. Esto, al menos, de acuerdo con un artículo publicado el pasado mes de enero en la revista “Scientific Reports” por un grupo de investigadores del Instituto Avanzado de Ciencia y Tecnología de Corea, encabezado por Kyungah Choi. En dicho artículo se reportan los resultados de una investigación llevada a cabo con 15 estudiantes para determinar los efectos fisiológicos y psicológicos que produce la iluminación ambiental en horas de la mañana con dos tipos de luz: luz “fría” con un alto contenido azul y luz “cálida” con una componente de color rojo dominante. Durante el estudio los estudiantes fueron expuestos durante una hora a los dos tipos de luz. Como resultado de su estudio, Choi y colaboradores encontraron una disminución significativamente más alta de los niveles de melatonina de los estudiantes después de su exposición a la luz “fría” que a la luz “cálida”. Igualmente, la exposición a la luz con un contenido mayor de color azul mejoró significativamente la sensación de somnolencia en comparación con la exposición a luz con un mayor contenido de color rojo. De acuerdo con Choi y colaboradores, los resultados de su estudio tienen implicaciones importantes para el diseño de los sistemas de iluminación de espacios interiores. En este sentido, apuntan que dicho diseño debe contemplar no solamente los aspectos relativos a los niveles de iluminación, sino también su contenido de luz azul, que debería ser bajo en horas de la noche y alto en las primeras horas de la mañana. Para lograr esto se aprovecharía tanto la flexibilidad de las fuentes LED en cuanto a la tonalidad de su emisión, como la tecnología del “Internet de la Cosas” que permitiría controlar dichas fuentes.  Se adaptaría de este modo la tecnología de la iluminación nocturna a nuestras condiciones como especie humana, que son el resultado de millones de años de evolución. Hacerlo a la inversa sería, sin duda, considerablemente más difícil.",
    "Apareció en días pasados en la prensa internacional una noticia por demás inusual: el personal de la aduana del aeropuerto de El Cairo, Egipto, al examinar una pieza de equipaje por medio de rayos X descubrió partes del cuerpo de dos momias con miles de años de antigüedad. Dichas partes -un pedazo de torso, un brazo, una mano, dos piernas y dos pies- estaban escondidas dentro de dos bocinas y se pretendía que fueran embarcados de contrabando hacia Bélgica. Los fragmentos de momia fueron confiscados por las autoridades egipcias y enviados al Museo de Antigüedades de El Cairo para su estudio y conservación.  No es, por supuesto, inusual que los objetos del antiguo Egipto sean blanco del contrabando y del mercado negro de antigüedades. Después de todo, Egipto ha fascinado a los europeos por cientos -por no decir miles- de años, sobre todo después de la invasión a este país por Napoleón a finales del siglo XVIII. Prueba de esto son los innumerables objetos de arte egipcios que se encuentran en museos y colecciones privadas del mundo desarrollado, muchos de ellos producto del robo y del saqueo. Es, sin embargo, sorprendente que esta fascinación se extienda a un pedazo de momia al grado de generar un mercado negro, por más antigua que ésta pudiera ser.  Por otro lado, hay que reconocer que las momias egipcias son fascinantes; incluso han sido tema de películas de terror de gran éxito, entre otras muestras de su gran popularidad. Ésta es debida, entre otras cosas, a que son muy antiguas y abundantes, pues la momificación en el Egipto antiguo era una práctica muy extendida por la creencia egipcia en la vida después de la muerte y la necesidad de preservar el cuerpo para que dicha vida se diera. En estas condiciones la momificación la practicaba no solamente la clase dirigente sino también la población en general, con todo y los sacrificios económicos que representara para las clases bajas. De hecho, la momificación en el Egipto antiguo estaba tan extendida que se practicaba incluso con perros y gatos. Así, con una gran sobre producción de momias, no sorprende que se hayan acumulado en grandes cantidades.Al margen de la utilidad práctica que la momificación haya tenido para que los egipcios alcanzaran la vida después de la muerte, sus creencias se hicieron obsoletas con el transcurrir del tiempo y con esto las momias perdieron su sentido original. Y dada su abundancia, fueron víctimas de prácticas que seguramente hubieran horrorizado al común de los mortales en el Egipto antiguo. En efecto, se sabe que a lo largo de la Edad media e incluso hasta las primeras décadas del siglo pasado, se pensaba que el polvo de momia egipcia tenía propiedades curativas y se usaba como medicina. Si bien esto convertía en caníbales a quien usaba dichos polvos para curar sus males, cabe suponer que en su momento las cosas no se veían de esta manera y se consideraba que, por su gran antigüedad, las momias egipcias eran objetos y no restos de personas.Igualmente, con seguridad hubieran resultado ofensivas hace algunos miles de años en Egipto las fiestas que llevaban a cabo las clases pudientes en la Inglaterra victoriana cuyo principal atractivo era “desenrollar” una momia egipcia. Y como una muestra más de su valor práctico, las momias se usaban también como pigmentos para pintura y a manera de trofeo de caza para aquellos europeos pudientes que regresaban de un viaje a Egipto en el siglo XIX. Así, una vez que se perdió la intención original con la que fueron creadas, a las momias egipcias se les dieron numerosas aplicaciones en virtud de su abundancia; abundancia que Mark Twain ilustra afirmando que se empleaban como combustible para locomotoras -lo cual era una broma, por supuesto.    Dada la sensibilidad y conocimientos científicos de nuestra época, las anteriores aplicaciones de las momias egipcias no representan en estos días una oportunidad de negocio que origine un mercado negro y operaciones de contrabando. El episodio de esta semana en el aeropuerto de El Cairo se entendería, entonces, como uno de tráfico de antigüedades egipcias. Se sabe que el mercado para el tráfico ilegal de objetos de arte y de valor cultural es floreciente y, según los expertos, es superado en tamaño solamente por el tráfico de estupefacientes y el tráfico de armas. En el caso de Egipto, dicho tráfico se ha visto incrementado desde 2009, en coincidencia con la crisis económica global. Así, si del sarcófago de una momia se tratara, no es difícil concluir que tendría un valor en el mercado negro. El episodio del aeropuerto de El Cairo nos muestra que, aparentemente, lo tienen también simples fragmentos de momia. Legalmente éstos no pueden sacarse del territorio egipcio, lo que, por supuesto, aumenta su valor en el mercado negro.",
    "Siendo los dinosaurios animales muy populares, también lo es la historia según la cual la caída de un asteroide al final del periodo Cretácico los borró de la faz de la Tierra. En efecto, basta con buscar “extinción de los dinosaurios” en Internet para que aparezcan en la pantalla numerosas imágenes de dinosaurios aterrorizados por la visión -y se sobreentiende el estruendo- de una enrome bola de fuego acercándose a gran velocidad. Y no habría sido para menos, pues el asteroide medía unos 10 diez kilómetros de diámetro y a la velocidad a la que colisionó con el suelo produjo un cráter de 180 kilómetros de diámetro, generó incendios, terremotos y tsunamis, y lanzó a la atmósfera grandes cantidades de polvo que bloquearon la luz solar y provocaron un enfriamiento drástico del planeta.Si bien en una escala de tiempo geológica los dinosaurios murieron en muy poco tiempo tras el impacto del asteroide, en realidad no lo hicieron de manera inmediata sino a través de generaciones -excepto, quizá, en el caso de aquellos que tuvieron la mala fortuna de encontrarse en el lugar incorrecto y en el momento incorrecto, justo en donde se estrelló el bólido.Se sabe que el asteroide de marras impactó en la costa de la península de Yucatán cerca del pueblo de Chicxulub hace unos 66 millones de años. La evidencia de dicho impacto la proporciona un estrato geológico a lo largo de todo el planeta con una concentración más alta de lo esperado de metal iridio -que es más abundante en los asteroides que en la Tierra-, estrato que corresponde a esa antigüedad. El iridio presente en el asteroide fue dispersado a nivel global después del impacto.No hay pues ninguna duda de que se produjo una colisión con la Tierra de un asteroide de gran tamaño hace 66 millones de años que llevó a cambios climáticos drásticos a nivel mundial. Esta colisión marca lo que se conoce como frontera KP. No han duda tampoco que de manera simultánea -en una escala de tiempo geológica- se produjo una extinción masiva de especies. No hay consenso entre los expertos, sin embargo, que la primera haya sido causa de la segunda; esto, en contra de la creencia popular. En este respecto, se sabe que al final del periodo Cretácico se produjeron grandes erupciones volcánicas en la región conocida como Escaleras del Decán en lo que hoy es el territorio de la India. Dichas erupciones dispersaron en la atmósfera grandes cantidades de dióxido de carbono y de dióxido de azufre que cambiaron el clima del planeta.Los expertos tienen así dos posibles causas para la extinción de especies en la frontera KP: el asteroide de Chicxulub o las erupciones masivas de la India. Estas posibilidades son discutidas en dos artículos aparecidos esta semana en la revista Science, publicados por dos grupos internacionales de investigadores. Uno de dichos grupos es encabezado por Courtney Sprain de la Universidad de California Berkeley y el otro por Blair Schoene de la Universidad de Princeton. En ambos artículos se discuten las causas de la extinción masiva de especies a la luz de nuevas mediciones de la antigüedad de las rocas ígneas de las Escaleras del Decán. Para fechar dichas rocas, ambos grupos de investigadores emplearon técnicas de radioisótopos -que guardan similitud con la técnica de fechado por carbono 14-. No coinciden, sin embargo, en sus resultados. Sprain y colaboradores encuentran que la mayor parte del volumen de emisiones volcánicas ocurrió después de la frontera KP. Schoene y colaboradores, en contraste, encuentran que dichas erupciones ocurrieron en pulsos, el primero de los cuales se inició decenas de miles de años antes de dicha frontera.Se sabe, por otro lado, que al final del Cretácico ocurrió un cambio climático que debe ser explicado por la emisión de contaminantes a la atmósfera. Schoene y colaboradores lo explican por este primer pulso de volcanismo. Sprain y colaboradores, en cambio, aventuran la hipótesis de que, aun sin erupciones volcánicas hay emisión de contaminantes atmosféricos que se filtran a través de grietas en la corteza terrestre. Aventuran, además, que el impacto del asteroide en Yucatán estimuló el volcanismo en la India, que tuvo su mayor actividad después dicho impacto.De este modo, para Schoene y colaboradores la extinción masiva de especies puede ser atribuida, tanto a las erupciones volcánicas de la India, como al impacto del asteroide de Chicxulub. Para Sprain y colaboradores, por el contrario, el mayor responsable habría sido el impacto de dicho asteroide.   El último número de la revista Science incluye así dos artículos científicos en los que se presentan resultados de investigaciones sobre un mismo tema. En ambos casos se fecharon rocas volcánicas empleando técnicas similares, aunque no idénticas. Si bien hay coincidencias entre los dos artículos, también hay discrepancias en sus conclusiones. Esto último no es de sorprender, dado que los acontecimientos investigados ocurrieron en un pasado inconcebiblemente remoto.Por lo demás, los dos artículos sobre la extinción de los dinosaurios publicados esta semana por Science nos ilustran sobre uno de los aspectos esenciales del método científico: la confrontación abierta y pública de puntos de vista basados en resultados de experimentos y mediciones, los cuales servirán como soporte para futuras investigaciones sobre el mismo tema. Y así “ad infinitum”, cada vez más cerca del resultado correcto.",
    "En el año 1963, Derek de Solla Price, entonces profesor de la Universidad Yale en los Estados Unidos y padre de lo que se conoce como “Cienciometría” o “Ciencia de la Ciencia”, publicó un libro con el título “Little Science, Big Science” -traducido al español como “Hacia una Ciencia de la Ciencia”- que se convirtió en un clásico. En dicho libro, entre otras cosas, de Solla Price analiza el crecimiento del número de artículos científicos publicados en los últimos 300 años, encontrando que dicho número se duplicó cada 15 años. De este modo, entre 1660 y 1960 el número de artículos científicos creció por un factor de un millón, lo que, de acuerdo con de Solla Price, explicaría las revoluciones científica e industrial.No todo es cuestión de números, sin embargo, y también debe considerarse el impacto que producen las publicaciones científicas que no es uniforme. Una característica de la actividad científica es que se apoya en trabajos hechos anteriormente por otros investigadores, lo que se revela en la bibliografía citada por el artículo en cuestión. De este modo, el éxito de un artículo científico se mide por el número de artículos que lo citan, lo que evidencia su contribución al avance de la ciencia.De acuerdo con un artículo aparecido esta semana en la revista “Nature”, sin embargo, el número de citas que recibe un artículo no es tampoco un parámetro suficiente para evaluar por completo su impacto científico y criterios más detallados deben de considerarse. Dicho artículo fue publicado por un grupo de investigadores de universidades norteamericanas encabezado por Lingfei Wu de la Universidad de Chicago.Wu y colaboradores diseñaron un índice para caracterizar el impacto novedoso -disruptivo- de un artículo científico y para tal fin consideraron un índice de novedad que cuantifica en qué medida un artículo es citado por otros artículos sin que al mismo tiempo éstos citen artículos citados por el artículo en cuestión. Es decir, si el artículo que cita no incluye citas que hayan sido referidas en el artículo citado, éste sería completamente novedoso y se le asigna un índice de novedad igual a 1. Si por el contrario, las citas a un artículo están acompañadas por citas a trabajos que hayan sido citados por el mismo, dicho artículo recibe un índice de novedad igual a menos 1.  Los investigadores llevaron a cabo un estudio bibliográfico con 65 millones de artículos, patentes y productos de software que comprenden el periodo de 1954-2014. Básicamente, demostraron que los grupos pequeños de investigadores tienden a generar avances científicos y tecnológicos que involucran ideas novedosas -con un mayor índice de novedad- más frecuentemente, que los grupos con un mayor número de integrantes. En contraste, estos últimos son más eficientes para desarrollar avances con índices de novedad negativos -lo que no tiene un significado peyorativo- basados en ideas preexistentes. Wu y colaboradores encuentran también diferencias en cuanto a las prácticas de exploración de artículos publicados en el pasado, que tiende a ser más profundas en el tiempo en el caso de los grupos pequeños.   Estas conclusiones son relevantes dada la tendencia al crecimiento en el número de integrantes de los grupos de investigación. Este crecimiento puede entenderse por la complejidad creciente de los problemas científicos y tecnológicos que demandan para su solución grupos multidisciplinarios de investigadores. La integración de estos grupos, además, está facilitada por los avances en las tecnologías de comunicación. Los resultados de Wu y colaboradores, sin embargo, apuntan a que los resultados científicos más novedosos son resultado de grupos pequeños de investigadores.Wu y colaboradores concluyen que los grupos pequeños de investigación deben de recibir apoyos para su trabajo, y que ambos, grupos chicos -que llegan a ideas novedosas más frecuentemente- y grandes -que desarrollan avances de manera más eficiente en base a ideas preexistentes- son esenciales para “una floreciente ecología de ciencia y tecnología”.   Estas conclusiones son, ciertamente relevantes para los países industrializados; que los son por sus florecientes sistemas científico-tecnológicos. El hecho mismo que en los Estados Unidos se apoyen iniciativas para estudiar a la ciencia utilizando métodos científicos, es una indicación -por si hiciera falta- del papel central que tiene la ciencia en ese país.En México éste no es el caso y la ciencia no atrae normalmente mucha atención. La excepción ha sido la semana que hoy termina cuando el CONACyT estuvo en el ojo del huracán. Sin duda, todos aquellos que tenemos algo que ver con esta institución consideramos que la misma debería ser motivo de una mayor atención a nivel nacional. Si bien no de la manera a como lo fue en los últimos días; estaríamos de acuerdo. El CONACyT, fundado en diciembre de 1970, tiene la misión de fomentar el desarrollo científico y tecnológico de México y a no dudar el país ha avanzado en esta materia a lo largo de estos años.  No lo suficiente, sin embargo, dados los recursos limitados que hemos invertido y que no han llegado al medio punto porcentual del PIB, a pesar de los reiterados anuncios de que llegaría al uno por ciento. Esperemos que las aguas se calmen y la ciencia del país llegue a buen puerto. Asumiendo que lo hace, los grupos de investigación de nuestro país, que por necesidad tienden a ser pequeños, tendrán la ventaja de ser precisamente pequeños.  Al menos si hemos de creerle a Wu y colaboradores.",
    "El pasado 1 de diciembre, a petición de los Estados Unidos, el gobierno canadiense detuvo a Meng Wanzhou, vicepresidenta y directora de finanzas de la compañía china Huawei. Se le acusa de robo de tecnología y de fraude financiero a instituciones norteamericanas. Cabe hacer notar que Huawei es el mayor fabricante de equipo para redes de comunicación a nivel mundial y que Meng es hija del fundador de la misma. Se afirma también que Huawei tiene conexiones con el gobierno chino.Huelga decir que la detención de Meng ha sido muy controvertida y criticada por China que rechaza las acusaciones que se le hacen a Huawei.  El incidente se ha puesto en el marco de la guerra comercial que libran este país y los Estados Unidos, habida cuenta que Huawei es el líder de la nueva tecnología de comunicaciones inalámbricas conocida como 5G. Habida cuenta también que el año pasado superó en ventas de teléfonos inteligentes a la compañía Apple y con esto se convirtió en el segundo fabricante de estos dispositivos a nivel mundial, superada solamente por Samsung. La tecnología 5G aumentará sustancialmente la velocidad y capacidad de la tecnología 4G actual y permitirá hacer cosas cualitativamente diferentes. En particular, posibilitará el llamado “Internet de las Cosas”, tecnología que implica la conexión inalámbrica con la red Internet de una gran cantidad de objetos de índole diversa, desde tostadores de pan y refrigeradores, hasta automóviles y monitores de pulso cardiaco.   El Internet de las Cosas permitirá, por ejemplo, monitorear sin intervención humana el nivel de gas doméstico en el tanque estacionario de una vivienda y solicitar a la compañía gasera que acuda a llenarlo en caso necesario. Permitirá, así mismo, monitorear de manera permanente la presión arterial de un paciente hipertenso y transmitirla en tiempo real a su doctor para su vigilancia. En el caso de los automóviles autónomos, las conexiones inalámbricas de alta velocidad serán fundamentales para lograr que el automóvil transite por el camino evitando a otros automóviles, sean autónomos o guiados por un humano. Ciertamente, la tecnología 5G y el Internet de las Cosas nos cambiarán la vida en no poca medida.Al mismo tiempo, una red inalámbrica con un enorme número de objetos conectados facilitará la intrusión de “hackers” y el robo de información, y esto es motivo de preocupación entre los especialistas, y particularmente del gobierno de los Estados Unidos. En este sentido, un artículo aparecido esta semana en la revista de divulgación MIT Technology Review, afirma que la verdadera causa de la detención de Meng el pasado mes de diciembre es la preocupación del gobierno norteamericano por el peligro que representa la penetración de la tecnología de Huawei en las futuras redes 5G de los Estados Unidos y de otros países del mundo. Una red 5G con una gran componente de tecnología Huawei, se argumenta, las dejaría expuestas a sufrir espionaje por parte del sistema de inteligencia chino. En estas condiciones, constituiría un riesgo demasiado grande para la seguridad nacional.  Por otro lado, aparte del problema de seguridad también está el aspecto comercial pues, según el artículo de MIT Technology Review, siendo Huawei el mayor fabricante de equipo para redes de comunicación y el segundo mayor fabricante de teléfonos inteligentes, está en buena posición para quedarse con la parte del león del mercado de tecnología 5G, que se estima podría ser de 123,000 millones de dólares en cinco años. Así, prosigue el artículo de referencia, bloqueando el ingreso de Huawei, tanto al mercado norteamericano como a otros mercados del mundo, daría tiempo a sus competidores para darle alcance.Por lo demás, los intentos del gobierno norteamericano por frenar el crecimiento de Huawei se han dado incluso antes de la detención de Meng y al margen de la misma, presionando incluso a diferentes países para que veten su tecnología. En la actualidad, aparte de los Estados Unidos, Australia, Nueva Zelanda y Japón han vetado a Huawei para el desarrollo de sus redes 5G y lo mismo podrían hacer en el futuro otros países, incluyendo Alemania y Canadá. La próxima semana, el Secretario de Estado norteamericano estará de visita en países de Europa Central y expresará su preocupación acerca de la creciente presencia de Huawei en esa región del mundo.De una u otra forma, trátese de ganancias comerciales o de asuntos de seguridad nacional, es sin duda impactante ser testigos del enfrentamiento tecnológico entre los Estados Unidos y China, mismo que hubiera sido impensable hace apenas unas pocas décadas. Y en este punto cabe la reflexión de por qué México ha avanzado tan poco en materia de tecnología. Ciertamente, no ha sido porque no haya invertido suficientes recursos en la preparación de científicos ingenieros de alta capacitación, pues el CONACyT ha operado un amplio programa de becas de posgrado desde el año 1971. La explicación quizá la encontremos en el poco empeño que ha habido para establecer líneas de desarrollo tecnológico en áreas que se consideren prioritarias.  Ciertamente, no lo ha habido ni siquiera en materia petrolera. Que ya es decir algo.",
    "Como fue ampliamente difundido por los medios de comunicación, en días pasados, la ciudad de Chicago fue invadida por una masa de aire polar que hizo descender la temperatura ambiente a niveles muy por debajo de los habituales en esta época del año. Si bien la temperatura ambiente mínima no alcanzó el récord de menos 33 grados centígrados del 20 de enero de 1985, los menos 30 grados centígrados sufridos por Chicago el pasado miércoles no es tampoco que hayan estado demasiado lejos. Hay que tomar en cuenta, además, que la velocidad del viento provoca una sensación una temperatura más baja que la que marca el termómetro y que Chicago se caracteriza por ser una ciudad ventosa.  En estas circunstancias, no fue bien recibido el tuit del presidente norteamericano del pasado lunes 28 de enero en el que se refiere a la inminente ola de frío que golpearía al norte de los Estados Unidos, pidiéndole al calentamiento global que por favor regrese pues lo necesitaban. Las bajas temperaturas que afectaron el norte de los Estados Unidos fueron debidas al desbordamiento de la masa de aire frío circulante, el llamado vórtice polar, que normalmente está contenida en una región alrededor del polo norte. Cuando las fuerzas que mantienen confinado al vórtice polar se debilitan, éste se desplaza hacia el sur. Si bien es una idea controvertida, hay científicos que piensan que el calentamiento global -que afecta de manera amplificada a la región ártica- debilita el confinamiento del vórtice polar. De ser así, el presidente norteamericano estaría invocando al ente equivocado. Aunque, a decir verdad, no es claro si genuinamente piensa que el calentamiento global es una patraña o si sus comentarios son interesados.Lo que sí está claro es la emisión de gases de invernadero a la atmósfera por las actividades industriales ha producido cambios en el medio ambiente a nivel global que, entre otros efectos, ha llevado a un incremento sostenido en la temperatura del planeta, y que ha alcanzado alrededor de un grado centígrado por arriba de sus niveles preindustriales.En este contexto es interesante comentar un artículo aparecido en el último número del presente año de la revista Quaternary Science Reviews, en el que se presenta un estudio del impacto que tuvo la conquista europea del continente americano, llegando a una sorprendente conclusión: el colapso de la población indígena después de la conquista fue de tal magnitud que afectó al clima del planeta. Pero vayamos por partes. Mencionaremos primeramente que el artículo de referencia fue publicado por un grupo de investigadores de universidades británicas encabezado por Alexander Koch de University College London y que en el mismo se analiza el abandono de tierras de cultivo después de la llegada de los europeos. Este abandono fue debido al colapso de la población indígena, que se habría reducido hasta en un 90%, por las guerras de exterminio, por la esclavitud a la que fueron sometidos o por la introducción por los conquistadores de patógenos que no existían en el continente y para los cuales los nativos americanos no tenían defensas.Al abandonarse las tierras de cultivo -que a nivel continental constituyeron un área aproximadamente igual a un cuarto del territorio de México- fueron reforestadas por la flora original que había sido en su momento desmontada para preparar dichas tierras para la agricultura. Esto provocó una mayor área verde que incrementó la velocidad de remoción de dióxido de carbono -el principal gas de invernadero- de la atmósfera. Aquí hay que recordar que durante el proceso de fotosíntesis las plantas toman dióxido de carbono para fabricar materia orgánica.Como resultado, se produjo una reducción de la concentración de dióxido de carbono en la atmósfera que Koch y colaboradores estiman fue de 3 partes por millón. Como apuntan estos investigadores, dicha concentración es aproximadamente un 30% de la reducción en dióxido de carbono que se sabe se produjo en el siglo XIV y que a su vez dió origen al periodo de bajas temperaturas conocido como Pequeña Edad del Hielo. Así, el aniquilamiento de la población indígena del continente americano contribuyó a enfriar el planeta en un periodo anterior al de la Revolución Industrial. Es decir, contribuyó a crear un efecto inverso al del actual calentamiento global.Esta conclusión es sin duda sorprendente y solo esperaríamos que no fuera tomada demasiado en serio por el presidente norteamericano, que poca simpatía ha mostrado hacia nosotros.",
    "Como sabemos, la temperatura global de la Tierra se ha incrementando en casi un grado centígrado en los últimos dos siglos. Esto no tendría nada de extraordinario de no ser porque dicho incremento se ha dado a una velocidad inusual. En efecto, sabemos que la temperatura de nuestro planeta ha experimentado cambios de varios grados centígrados aproximadamente cada 100,000 años que han producido las llamadas glaciaciones o edades del hielo. Esto es debido a que con esta periodicidad se modifica la órbita de la Tierra alrededor del Sol y con esto la cantidad de radiación solar que recibe.  Hace unos 15,000 años la Tierra estaba de un proceso de calentamiento después de la última glaciación y, según la NASA, le tomó unos 5,000 años en incrementar su temperatura global entre 4 y 7 grados centígrados. En contraste, en el último siglo dicha temperatura se ha incrementado a un ritmo casi diez veces más grande. Los científicos, por otro lado, no encuentran que este último incremento pueda explicarse por la influencia de fenómenos naturales, tales como cambios en la radiación del Sol, erupciones volcánicas o fenómenos climáticos como El Niño. Y sí encuentran, por el contrario, que hay una concordancia entre dicho incremento y la elevación de los niveles de gases de invernadero en la atmósfera que se sabe tienen el efecto de retener el calor emitido por la superficie de la Tierra que de otro modo se perdería en el espacio. Hay razones, pues, para creer que la temperatura de la Tierra se está incrementando por acciones nuestras.   Un artículo aparecido esta semana en la revista “Nature Communications” pone en una perspectiva milenaria el calentamiento que en la actualidad está experimentado la Tierra. Dicho artículo fue publicado por un grupo de investigadores de universidades en los Estados Unidos encabezados por Simon Pendleton de la Universidad de Colorado y se enfocó en el estudio de la flora que ha sido dejada al descubierto por la retracción de los hielos árticos por efecto del calentamiento global. La investigación fue llevada a cabo en la isla de Baffin en el ártico canadiense.Se sabe que los efectos del cambio climático se amplifican en la región ártica que se está calentando a una velocidad de dos a tres veces más grande que el resto del planeta. En particular, las masas de hielo polar están reduciendo rápidamente su volumen, dejando al descubierto áreas que podrían haber permanecido enterradas por miles o decenas de miles de años.  Posibilidad que Pendleton y colaboradores se propusieron investigar.Para esto, recolectaron plantas que recientemente habían emergido a la superficie al retraerse las capas de hielo que las cubrían. Los investigadores hacen notar que una vez expuestas en la intemperie, las plantas son rápidamente destruidas por la erosión del medio ambiente, de modo que una planta recolectada en la orilla de la capa de hielo presumiblemente habría emergido a la superficie por primera vez desde que fue originalmente fue cubierta por la capa de hielo. Llevaría así la información de cuándo se formó dicha capa.Dicha información se recuperó midiendo su contenido de carbono-14. Como sabemos, el tiempo que ha transcurrido desde la muerte de una planta puede ser determinado midiendo su concentración de este isótopo del carbono que tiene un tiempo de decaimiento radiactivo de 5730 años. El principio para esta datación es muy simple: mientras está viva la planta absorbe del aire una cierta cantidad de este isótopo de carbono, lo que deja de hacer al morir. Así, la determinación del contenido de carbono-14 de una planta que murió hace miles de años nos permite determinar cuando ocurrió esto último. Y en el caso de las determinaciones llevadas a cabo por Pendleton y colaboradores, les permitió determinar cuando fue cubierta por los hielos la superficie en la que crecieron las plantas investigadas. Los resultados del estudio muestran la superficie recién descubierta por el retraimiento de los hielos en la isla de Baffin permaneció enterrada por más de 40,000 años. Con el objeto de confirmar sus conclusiones, los investigadores llevaron a cabo una datación por carbono-14 de rocas igualmente descubiertas por el retraimiento de los hielos. En este caso, el cabrono-14 en generado en el cuerpo de las rocas por efecto de los rayos cósmicos. Estos rayos producen carbono-14 en rocas expuestas en la superficie, pero no en aquellas enterradas por una capa de hielo. El estudio de las rocas confirma los resultados obtenidos con la datación de las plantas.  De los resultados de Pendleton y colaboradores se desprende que, por primera vez en cuando menos 40,000 años, podemos ver paisajes del ártico canadiense que habían permanecido ocultos por el hielo. Igualmente, los investigadores hacen notar que, basados en estudios paleo-climáticos de las regiones árticas, es posible que un calentamiento global como el experimentado por el planeta a lo largo del último siglo no había ocurrido en los últimos 115,000 años. Algo que sin duda resulta preocupante. Por decir lo menos.",
    "Las lámparas incandescentes, introducidas comercialmente por Thomas Alva Edison hace poco más de un siglo, iluminaron la oscuridad de las noches y con esto nos trajeron un cambio sustancial en nuestro estilo de vida. Dichas lámparas no fueron, por supuesto, el primer medio artificial de iluminación que empleamos a lo largo de la historia. Antes de ésta, nos valimos de fogatas y, en la medida en que nuestra tecnología se sofisticó, de antorchas y de lámparas de gas, entre otras opciones. La luz eléctrica, sin embargo, representó un salto importante en cuanto a la eficiencia de iluminación que resultó ser arriba de diez veces más grande que la eficiencia de las lámparas de gas.Como sabemos, hoy en día y a cien años de su aparición, las lámparas incandescentes han perdido la batalla frente las lámparas LED que tienen numerosas ventajas, incluyendo una mayor confiabilidad y, sobre todo, una eficiencia sustancialmente mayor. Esto último lo pueden atestiguar quienes hayan sustituido en sus casas a los antiguos focos incandescentes por lámparas LED.Las ventajas de las lámparas LED, por otro lado, no se reducen solamente a una mayor confiabilidad y eficiencia. Por el contrario, las características y flexibilidades de la tecnología LED tienen el potencial de producir impactos mayores, no solamente en  una reducción en el consumo de energía eléctrica empleada para iluminación a nivel mundial -que es de suyo algo de la mayor importancia-, sino en campos tan diversos como la salud y la producción de alimentos  Esto, al menos según un interesante artículo aparecido en el número del 22 de noviembre del pasado año en la revista “Nature”. Dicho artículo fue publicado por un grupo de investigadores de diversos centros de investigación norteamericanas, encabezado por Paul Pattison de la firma de consultoría “Solid State Lighting Services”.En su artículo, Pattison y colaboradores hacen notar la importancia que la luz tiene para todos los seres vivientes sobre la faz de la Tierra. No solamente por el proceso de fotosíntesis -esencial para mantener la vida en el planeta- que emplea a la luz del Sol como uno de sus ingredientes fundamentales, sino por el hecho de que la luz proporciona mucha de la información que permite a los seres vivientes su adaptación al medio ambiente. Los investigadores hacen notar que la importancia de la luz para los humanos se refleja en tres hechos: la exquisitez del ojo humano como un instrumento óptico, la gran fracción del cerebro humano dedicada a procesar la información visual, y la dependencia extrema que tenemos en las tecnologías para mejorar nuestra visión, como es el caso de los anteojos para ver de cerca o de lejos. En contraste con las lámparas incandescentes que emiten solamente luz con un color rojo-amarillo, las lámparas LED son capaces de emitir luz en una variada gama de colores. Una lámpara LED convencional emite una luz primaria de color azul la cual es parcialmente absorbida en un material conocido como un fósforo que la convierte en luz de color amarillo. La combinación de la luz amarilla y la luz azul primaria es percibida por el ojo como luz blanca. Dependiendo de su diseño, no obstante, una lámpara LED es capaz de emitir una luz primaria con un color que va del violeta al azul verde, la que puede ser convertida por un fósforo en una luz verde, amarilla o roja. De este modo, la luz de la lampara LED, tal como es percibida por el ojo, puede tener toda una gama de tonalidades. Es posible, igualmente, combinar varias lámparas LED, sin fósforos convertidores, que emitan en colores determinados y producir de este modo el color requerido para una cierta aplicación.La flexibilidad de las lámparas LED para generar luz con diferentes colores y tonalidades abre todo un rango de aplicaciones. Al respecto, Pattison y colaboradores hacen notar que la luz azul de determinadas tonalidades afecta la producción de melatonina, el ritmo circadiano, y el estado de vigilia y desempeño. Así, la posibilidad de contar con fuentes flexibles de luz de diferentes colores permitiría, entre otras cosas, aumentar la productividad en el trabajo. La flexibilidad de las lámparas LED para emitir luz con un color determinado tiene también importancia terapéutica para ciertas enfermedades. Sería el caso, por ejemplo, de los padecimientos asociados a la perturbación del ritmo circadiano.Pattison y colaboradores consideran también el potencial que tienen las lámparas LED en la producción de alimentos. Al respecto, hacen notar que en las plantas, que cuentan un número más grande de receptores luminosos, la luz tiene una influencia mayor que la que tiene en los humanos. Así, podría ser posible controlar el crecimiento en invernaderos, de plantas con características escogidas a voluntad empleando luz de lámparas LED con una determinada gama de colores.En la visión de Pattison y colaboradores, las lámparas LED no son solo fuentes de luz más eficientes que llevarán a un importante ahorro de energía. Más allá de esto, dichas lámparas, con flexibilidad sin precedente para fabricar fuentes de luz con características a escoger a voluntad, serán promotoras de una revolución tecnológica de alcances similares a la que produjeron las lámparas incandescentes hace poco más de un siglo.",
    "Como fue ampliamente difundido por los medios de comunicación, el pasado 3 de enero Chinalogró posar suavemente una sonda espacial con un explorador de terreno a bordo sobre la cara oculta de la Luna. En el pasado, tanto los Estados Unidos como la entonces Unión Soviética llevaron a cabo alunizajes suaves, incluyendo los de los vuelos tripulados del programa Apolo. Dichos alunizajes, sin embargo, ocurrieron en la cara visible de la Luna.Sabemos que la Luna siempre presenta la misma cara vista desde la Tierra, debido a que el periodo de traslación de la Luna alrededor de la Tierra es exactamente igual asu periodo de rotación.No es difícil entender que alunizar en la cara oculta de la Luna presenta más dificultades que hacerlo en la cara visible. No es posible, por ejemplo, establecer un contacto directo entre la sonda en proceso de alunizaje con la estación de control en la Tierra para guiar el descenso dado que la Luna se interpone entre ambos. China resolvió este problema colocando previamente en órbita alrededor de la Luna un satélite de comunicaciones que sirvió como punto de enlace entre la sonda y la estación de control.El logro de China lo coloca entre los países líderes en la exploración espacial, habida cuenta, además, de que en el año 2018 fue el país con el mayor número de lanzamientos de cohetes que alcanzaron una órbita terrestre, incluso por encima de los Estados Unidos que fue segundo en este respecto:35 lanzamientos exitosos por parte de China en contra de 30 lanzamientos de los Estados Unidos.Todo lo anterior es, por supuesto, congruente con el impresionante desarrollo económico que ha tenido China en las últimas décadas, el cual lo ha llevado a ocupar el segundo lugar global en cuanto a producto interno bruto, sólo por detrás de los Estados Unidos. Se espera, además, que en los próximos años China supere a los Estados Unidos en este respecto. Hace 200 años el producto interno de China era el mayor del mundo,yen comparación, el de los Estados Unidos era una 15 veces menor. Las turbulencias por las que pasó China a lo largo de los siglos XIX y XX, juntamente con el despunte económico de los Estados Unidos en la primera mitad del siglo XX hicieron que esta disparidad se invirtiera al promediar el siglo XX. En los años que siguieron, no obstante, China creció a un ritmo tal queha hecho que ambas economíastengan en la actualidad tamaños comparables. Lo anterior, aunado a la guerra tarifaria que en los últimos meses ha desatado el presidente de los Estados Unidos, han hecho que el periódico New York Timeshable de la posibilidad de que se desate una nueva guerra fría económica, en analogía con la que sostuvieron la Unión Soviética y los Estados Unidos en la segunda mitad del siglo XX. La rivalidad actual de China con los Estados Unidos se analiza en una serie de artículos aparecidos en el número de diciembre del pasado añode la revista MIT TechnologyReview, publicada por el Instituto Tecnológico de Massachusetts. Entre otros, se analizan aspectos tecnológicos relativos a la fabricación de automóviles, microcircuitos, energía nuclear, exploración espacial, computación cuántica y comunicaciones.En particular, uno de estos artículosarguye en contra de la posibilidad de que se repita una guerra fría. En dicho artículo, intitulado “Los Estados Unidos y China no están en una guerra fría, paren de llamarla así”, seargumenta que las relaciones actuales entre China y los Estados Unidos no son similares a las que prevalecían entre los Estados Unidos y la Unión Soviética durante la guerra fría original. Así, el artículo de marrasseñala que en su momento estos dos países contaban con sistemas científicos y tecnológicos independientes en gran medida y que éste no es el caso actual entre los Estados Unidos y China que tienen una gran interdependencia tecnológica. La tienen a tal grado que, por ejemplo, dispositivos fabricados en China dependen fuertemente de componentes manufacturadas en los Estados Unidos y viceversa. No obstante, al margen de la interdependencia tecnológica entre los Estados Unidos y China, este último país está haciendo grandes esfuerzos para desarrollarse tecnológicamente. Al respecto, otro de los artículos aparecidos en MIT TechnologyReview se refiere a la fabricación de microcircuitos. Esta fabricación involucra tecnologías altamente sofisticadas que muy pocas compañías en el mundo poseen y que no han estado al alcance en sus versiones más avanzadas de las compañías chinas. Sin embargo, un cambio de paradigma en la tecnología que involucra el diseño de microcircuitosdestinados a aplicaciones de inteligencia artificial les está abriendo las puertas.De la misma manera, la irrupción de los automóviles eléctricos con menos partes móviles que sus contrapartes con motores de combustión interna, y por tanto más simples de fabricar, han abierto oportunidades para las compañías chinas de automóviles.Basados en lo que ha sucedido en las últimas décadas, podríamos quizá concluir que el siglo XXI será el siglo de China, a menos que ocurra una catástrofe. Después de todo, como apunta el editor de MIT TechnologyReview: “Al visitar China viene a la mente la impresión que habrán tenido los europeos al visitar los Estados Unidos hace un siglo -la de una tierra en donde todo es más grande y en donde todo sucede más rápido, un lugar lleno de energía e ideas”.",
    "Está ya por iniciar el 2019 y con esto no faltarán los buenos propósitos para el nuevo año, incluyendo los de bajar de peso, hacer más ejercicio, dejar de fumar o beber, y ahorrar dinero. Llevar estos propósitos a buen fin, lo que requiere de una gran fuerza de voluntad, nos acarrearía sin duda grandes beneficios. Por otro lado, es posible que la víspera del año nuevo no sea el mejor momento para enunciar nuestros propósitos de mejora, so pena de lucir poco convincentes. Por ejemplo, si nuestra intención es bajar algunos kilos de peso, la mejor muestra de que nuestro deseo es sincero es moderar la ingesta de alimentos durante el llamado puente Guadalupe-Reyes. De la misma manera, si nuestro propósito es el de ahorrar dinero, deberíamos moderar nuestros gastos durante las fiesta decembrinas. Habida cuenta de la dificultad de alcanzar metas de este tipo, se concluye que posiblemente debamos esperar algunos días para diseñar nuestros planes de mejora.Después de todo, hay que tomar en cuenta que la fecha de inicio del nuevo año es una convención que no tiene una base firme, por decir astronómica.  El ciclo anual de 365 días y algo más tiene, ciertamente, una base astronómica y corresponde al periodo de rotación de la Tierra alrededor del Sol; el inicio de dicho ciclo, en contraste, es una convención que varía de cultura a cultura. Así, reconociendo que la fecha para el inicio del año nuevo es relativa, no una hay razón de peso para esperar al año nuevo con el fin de iniciar una mejora en nuestro estilo de vida, la cual podríamos empezar en cualquier momento.Habría que reconocer, no obstante, que damos una gran importancia a nuestras convenciones, mismas que terminamos por creer poseen una base más firme que la que en realidad tienen. En los años previos al fin del primer milenio de nuestra era, por ejemplo, se pronosticó que el fin del mundo llegaría en el año 1000 d.C., lo que habría provocado una gran consternación entre la población de Europa. Si bien se considera que esto último es una exageración producto de siglo XIX –pues, con excepciones, la población de Europa no tenía noción del año en que vivía–, de un modo u otro se le adjudicó al año 1000 d.C.  una importancia que no tenía. Y lo mismo sucedió con el año 2000 d.C., para el que también se anticiparon desastres a nivel global de diferentes tipos.  Los cierto es que los desastres predichos no ocurrieron, y esto se podría haber anticipado recurriendo al sentido común –al menos desde una perspectiva moderna–. En efecto, buena parte del mundo se rige por el calendario gregoriano, impuesto por el papa  Gregorio XIII en 1582 en sustitución del calendario juliano. El calendario juliano asumía que un año constaba de 365.25 días y que el ciclo anual comenzaba el 1 de enero. Para simplificar, dicho calendario consideró años de 365 días, intercalando un año de 366 días –año bisiesto– cada cuatro años. El número de días que en realidad tiene una año, sin embargo, es ligeramente menor que el asumido por el calendario juliano y esto ocasionó que el mismo produjera un desfase de 10 días con respecto a la fecha astronómica. Para corregir este desfase, el calendario gregoriano adelantó diez días de modo que al jueves 4 de octubre de 1582 del calendario juliano le sucedió el viernes 15 de octubre de 1582 en el calendario gregoriano.  De manera adicional, y con el objetivo de prevenir futuros desfases, se eliminaron los años bisiestos que son múltiplos de 100 –por ejemplo, 1500 y 1700– con la excepción de los años múltiplos de 400 –por ejemplo, 1600 y 2000–.  Aun así, habría que hacer un ajuste de un día cada 3,300 años.De lo anterior es claro que la fecha de inicio de un año gregoriano y el año mismo depende de convenciones y de reglas artificiales que se establecieron en un determinado momento. No podríamos así esperar, pues sería demasiado pretencioso de nuestra parte, que tal o cual fecha o año tenga características especiales tan solo por decisión nuestra. Y dicho esto, podemos intentar hacer justicia al venidero año de 2019. Sucede que este año es aparentemente uno del montón, sin características dignas de hacer notar.  Es decir, no es un año con el que termine o empiece un siglo; y ni siquiera una década. No es tampoco un año con una combinación de dígitos vistosa –por ejemplo, 1999–. Igualmente, no es un año primo –un número primo es aquel que no tiene como divisores más que a sí mismo o la unidad– como lo fueron 2011 y 2017. La falta de méritos de 2019, sin embargo, es solo aparente, pues, convenciones aparte, todos los años son exactamente equivalentes.Al margen de estas profundas consideraciones, 2019 sí es para nosotros y en estos momentos un año especial, por la sencilla razón de que es el que está por llegar. Y para el cual muchos de nosotros tendremos propósitos de enmienda,  los cuales, no obstante y según las estadísticas, caerán en el olvido en un 80 por ciento más temprano que tarde.",
    "Sorprende enterarse que el color del traje de Santa Claus, que hoy no dudaríamos en afirmar es rojo brillante, en algún momento fue verde o de un bronceado pálido, entre otros colores. ¿En qué momento ocurrió la transición de color? Se ha dicho que la imagen que hoy tenemos de Santa Claus, como un personaje con un gran vientre y barba blanca, vestido con un traje y gorra de dormir en colores rojo y blanco, es debida a Haddon Sundblom, quien en el año 1931 fue contratado por la compañía Coca Cola para que desarrollara imágenes publicitarias basadas en Santa Claus. De acuerdo con esta versión, los colores del traje de Santa Claus corresponderían a los de la imagen corporativa de la Coca Cola.Es claro, sin embargo, si bien la campaña publicitaria de la Coca Cola ayudó a establecer la imagen que hoy en día tenemos de Santa Claus, ésta es en realidad anterior a dicha campaña. Así, por ejemplo, la portada del número de diciembre de 1902 de  revista “Puck” muestra a un Santa Claus que coincide prácticamente con la imagen actual del personaje, incluyendo el color rojo del traje, el gorro de dormir y la barba blanca. Al mismo tiempo y en contraste, la portada de la novela “Vida y Aventuras de Santa Claus”, publicado en 1902 por Lyman Frank Baum –el autor de “El maravilloso Mago de Oz” –, muestra a un Santa Claus con traje y gorro verdes, a punto de bajar por la chimenea de una casa con una bolsa de regalos al hombro. De la misma manera, el ilustrador Thomas Nast, publicó en la revista “Harper´s Weakly” en las segunda mitad del siglo XIX imágenes de Santa Claus vestido con trajes de color rojo pálido o bronceado. Podríamos así concluir que Santa Claus no siempre viste de rojo intenso –por mas que Coca Cola nos haya convencido de que así es. Esto, ciertamente, no es lo que hubiéramos esperado de un personaje entregado a los niños y que por lo mismo buscaría crearse una imagen que lo identifique a primera vista. Así, cabe preguntarse por los motivos que tendría Santa Claus para cambiar de apariencia, vistiendo algunas veces de rojo intenso y otras de rojo pálido o incluso de color verde. A menos que los aparentes cambios en el traje de Santa Claus fueran meras ilusiones visuales, en cuyo caso la pregunta apropiada giraría en torno a las causas de dichas ilusiones.Para buscar una respuesta hicimos una investigación en Internet, encontrando una cantidad sorprendente de entradas en las que se analiza la física de Santa Claus. En una de ellas hay una posible respuesta a la pregunta que nos interesa. La información se encuentra alojada en el sitio EurekaAlert mantenido por la “American Association for the Advancement of Science” para la difusión de noticias científicas y fue publicada por Kathy Sheen de la Universidad de Exeter en el Reino Unido.De acuerdo con esta investigadora, Santa Claus tiene que repartir regalos a unos 700 millones de niños en el mundo; y lo tiene que hacer en apenas 31 horas, tomando en cuenta los diferentes husos horarios del mundo. Para esto, tendría que viajar a una enorme velocidad. Tan grande que el fenómeno conocido como efecto Doppler tendría una influencia apreciable sobre el color aparente de Santa Claus, sus renos y su trineo. En este respecto, hay que recordar que el color de un objeto es una sensación producida por la luz que se refleja en dicho objeto y que alcanza nuestros ojos. Para mayor explicación, el efecto Doppler es aquel por medio del cual el sonido de la sirena de una ambulancia que se acerca a gran velocidad suena más agudo que cuando dicha ambulancia está en reposo. Aplicado esto al traje de Santa Claus y de acuerdo con las leyes de la física, un sonido más agudo corresponde al color verde mientras que el rojo lo es al más grave. Así, cuando Santa Claus con su trineo se acerca a gran velocidad el color rojo de su traje aparentará ser verde. Cuando se detenga, en contraste, recobrará su color rojo real. Esto explicaría la discrepancia en los colores  atribuidos a dicho traje por diferentes ilustradores.Lo explicaría, por supuesto, asumiendo que el trineo de Santa Claus pueda acelerar hasta las velocidades requeridas, lo cual no parece ser algo simple de lograr, entre otras muchas cosas por la gran cantidad de energía que se requeriría para tal efecto y que tendrían que proporcionar los renos del trineo.En estas circunstancias y al margen del efecto Doppler –que tiene una existencia real y un enorme número de aplicaciones, desde la medición de la velocidad de un automóvil hasta la velocidad de la órbita de un exoplaneta–, estaríamos tentados a admitir que una causa más probable para la diversidad de colores de los trajes atribuidos a Santa Claus es la gran imaginación de aquellos que se han dado a la tarea de representarlo en forma gráfica.   De un modo u otro, lo que sí no está a discusión es: 1) el gran éxito que tuvo la campaña publicitaria de la Coca Cola que consiguió eliminar del traje de Santa Claus aquellos colores que no reflejaran los propios y 2) que a los niños, principales beneficiarios del personaje, los tiene sin cuidado la forma cómo Santa Claus adquirió su color.",
    "Como sabemos, la Revolución Industrial, originada en Inglaterra a finales del siglo XVIII, ha tenido un impacto profundo que ha cambiado –para bien o para mal– nuestro modo de vida. La Revolución Industrial fue impulsada por los conocimientos científicos y tecnológicos acumulados en Europa a partir del siglo XVI y que, en el caso de los primeros, tuvieron su expresión más acabada en las leyes de la mecánica y la teoría de la gravitación universal de Isaac Newton.  En su primera fase, a lo largo de la primera  mitad del siglo XIX, la máquina de vapor y el desarrollo de la industria del carbón combustible, multiplicó enormemente la capacidad de producción de la industria tradicional y generó nuevos puestos de trabajo. Esto último provocó el éxodo de la población del campo hacia los centros urbanos en los que se asentó la nueva industria. La Revolución industrial ocasionó tensiones sociales entre los dueños de las fábricas y los obreros que trabajaban y vivían en condiciones deplorables, en un ambiente degradado por la contaminación y el hacinamiento por el acelerado crecimiento urbano. La vida en los centros industriales de la Inglaterra victoriana está descrita en las novelas del escritor inglés Charles Dickens. En la obra “Tiempos difíciles”, publicada en 1854, Dickens escribe a propósito de Coketown, la ciudad ficticia en la que se desarrolla la historia: “Era una ciudad de ladrillo rojo, o de ladrillo que habría sido rojo si el humo y las cenizas lo hubieran permitido; pero tal como estaba, era una ciudad de un rojo y negro poco naturales, como la cara pintada de un salvaje. Era una ciudad de máquinas y de altas chimeneas, de las cuales salían interminables serpientes de humo que nunca se disipaban. Tenía un canal oscuro y un arroyo que arrastraba sucias y malolientes aguas, y numerosos edificios con ventanas que resonaban y retemblaban todo el día, mientras el pistón de las máquinas de vapor subía y bajaba lentamente, como la cabeza de un elefante enfermo de melancolía.”En cuanto a las condiciones de vida en la época victoriana, Dickens las ejemplifica con el obrero Stephen Blackpool, quien vivía  “en la zona más industrial de Coketown, en las fortificaciones más íntimas de esa fea ciudadela, en donde la Naturaleza había quedado anulada por una atmósfera enrarecida de miasmas y gases tóxicos”. Blackpool tenía 40 años en la novela, pero lucía considerablemente más viejo por todas las desgracias y padecimientos que había tenido que soportar, incluyendo extenuantes jornadas de trabajo y el alcoholismo de su esposa. Por medio de sus novelas, Dickens nos describe de manera dramática, a través de situaciones y personajes ficticios, los contrastes sociales que se dieron en la Inglaterra de la primera mitad del siglo XIX y las penurias por las que pasaba la mayoría menos favorecida. Una descripción igualmente dramática, pero ésta sí con hechos y personajes reales, nos las da el reciente descubrimiento de esqueletos humanos llevado a cabo por  la compañía británica Wessex Archaeology en un antiguo cementerio que estuvo localizado en lo que hoy es el nuevo mercado de Convent Garden en la ciudad de Londres. Los esqueletos descubiertos corresponden al periodo de1830 a 1850, época en la que el sitio en cuestión experimentó en unos pocos años una transición desde una área rural a una fuertemente industrializada y urbanizada. Esto, de acuerdo con el sitio de Internet Wessex Archaeology en el que se relata el descubrimiento. Los esqueletos excavados muestran las duras condiciones de vida que sufrían los londinenses pobres en la época victoriana. De manera específica, Wessex Archaeology da cuenta de tres casos. El primero es el de una mujer mayor la cual, a pesar de una enfermedad crónica de toda la vida tuvo una ocupación agotadora que requirió del uso pesado de sus brazos y hombros. Los restos mostraban también signos característicos de sífilis congénita, al igual que una nariz rota y la pérdida de los dientes frontales. En cuanto a la causa de su muerte, hay la sospecha de que fue asesinada, pues el cráneo muestra una herida punzocortante detrás del oído derecho que indicaría que fue apuñalada, probablemente por la espalda.Un segundo caso es el de un hombre con una estatura de cerca de 1.80 metros con una nariz completamente aplastada y una depresión en su ceja izquierda, indicando que se vio involucrado en fuertes altercados. Una posibilidad al respecto es que fuera un boxeador a mano limpia. Sufría además de sífilis, posiblemente adquirida, y carecía de dientes frontales. Murió probablemente de una caída accidental que le rompió la espina dorsal y la cadera.El tercer caso es el de una niña de dos años de edad para la que no se tienen indicaciones sobre la causa de su muerte, aunque hay evidencias de que padecía malnutrición. En cualquier caso, Wessex Archaeology hace notar que en esa época la muerte de un infante no era algo inusual y que el 40% de los restos óseos excavados corresponden a niños menores a 12 años.Ciertamente, nadie dudaría que Charles Dickens, quién fue testigo directo de las duras condiciones de vida que sufrían los menos favorecidos en la época victoriana, estaría faltando a la verdad con sus novelas. Es de impresionar, no obstante, que los descubrimientos del cementerio de Convent Garden nos den evidencia directa de estas duras condiciones con personas y situaciones reales.",
    "El pasado lunes 26 de noviembre la NASA anunció que la sonda “InSight” había logrado posarse suavemente sobre la superficie de Marte después de un viaje de siete meses. La sonda “InSight”, con un peso de 360 kilogramos, tiene como misión estudiar el interior de Marte –de lo que se sabe poco, según la NASA–. Para este propósito, de acuerdo con el sitio de Internet de la agencia espacial norteamericana, “InSight” cuenta con un sismómetro de gran sensibilidad que le permitirá detectar vibraciones sísmicas producidas por la actividad de interior del planeta o por el impacto de meteoritos en su superficie. La sonda “InSight” está también equipada con un instrumento para estudiar la temperatura de Marte. En este respecto, los científicos de la misión están interesados en averiguar cómo fluye el calor desde el interior de Marte hacia su superficie y de esta manera determinar los materiales que lo componen. “InSight” cuenta, asimismo, con  un tercer instrumento para la medición del bamboleo que experimenta el eje de rotación de Marte en la medida en que se mueve a lo largo de su órbita alrededor del Sol. Esto último ayudará a determinar si el centro del planeta es líquido o rocoso.“InSight” es la sexta sonda –todas de la NASA– que ha logrado posarse con éxito sobre la superficie marciana. Lograr esto no es una empresa fácil. De hecho, ni siquiera ha sido fácil llevar una nave hasta las inmediaciones de Marte y de los más de medio centenar de intentos para hacerlo, más de la mitad han resultado en fracaso, según consigna la Wikipedia. Para depositar suavemente una sonda en la superficie de Marte, como primer paso es –obviamente– necesario llevarla hasta sus inmediaciones. Marte visto desde la Tierra es apenas un punto luminoso en el cielo y, ciertamente, lanzar desde la Tierra una nave que lo alcance no es una empresa sencilla. Entre muchas otras cosas, habría que tomar en cuenta que el viaje tomaría meses, de manera que tendríamos que calcular la posición que tendría Marte en el momento en el que la sonda lo alcanzara y apuntar nuestro lanzamiento hacia esa posición. Por lo demás, las trayectorias interplanetarias no se trazan en línea recta. En particular, “InSight” siguió una trayectoria curva que lo alejó progresivamente de la órbita terrestre hasta alcanzar la órbita marciana.Una vez que arribó a las inmediaciones de Marte, la sonda “InSight” se separó del módulo que la albergó en el viaje interplanetario e ingresó a la atmosfera marciana. Lo hizo a una altura de 130 kilómetros y a una velocidad de aproximadamente 20,000 kilómetros por hora. La interacción con la atmósfera frenó paulatinamente esta velocidad, alcanzando unos 1,400 kilómetros por hora a una altura de 11 kilómetros sobre la superficie marciana. En esos momentos se abrió un paracaídas para disminuir aun más la velocidad de la sonda, misma que alcanzó a unos 200 kilómetros por hora a un kilómetro de altura. Para el acercamiento final a la superficie de Marte, la sonda encendió cohetes retro-propulsores, tocando tierra a una velocidad de unos 8 kilómetros por hora. Posar suavemente una sonda en la superficie de Marte es a no dudarlo una operación en extremo difícil, y cabe preguntarse sobre las habilidades que han sido necesario desarrollar para llevarla a cabo. En primer lugar, podíamos preguntarnos sobre los conocimientos indispensables para trazar trayectorias interplanetarias y lograr que una nave espacial alcance, siete meses después de su lanzamiento, lo que apenas es un punto luminoso en el cielo. La respuesta a esta pregunta la conoce un estudiante de física elemental: parte de dichos conocimientos –si bien no todos los indispensables– están contenidos en las tres leyes de la mecánica y en la teoría de la gravitación universal que Isaac Newton descubrió en el siglo XVII. En cuanto a posar suavemente un objeto en la superficie marciana, el conocimiento de las tres leyes de Newton y la teoría de la gravitación es igualmente crucial.Por supuesto, la exploración espacial necesita, además de las leyes de la mecánica, de otros numerosos conocimientos científicos y tecnológicos. Necesitamos, por ejemplo, de conocimientos sobre electricidad, magnetismo y telecomunicaciones, de tecnología de cohetes, de materiales, de generadores de energía y de computadores, por mencionar solo algunos elementos indispensables. Sin los conocimientos desarrollados por Newton, no obstante, hubiera sido imposible posar suavemente a la sonda “InSight” sobre la superficie de Marte. Las contribuciones de Isaac Newton, por otro lado, no se reducen a haber posibilitado el envío de sondas  interplanetarias y ni siquiera al desarrollo de un enorme número de ingenios tecnológicos que han cambiado nuestra civilización. Lejos de eso, las leyes de la mecánica de Newton –que se aplican por igual a la caída de una manzana madura de una árbol que al movimiento de un planeta alrededor del Sol– pusieron en la misma canasta a los fenómenos terrestres y celestes  y con esto cambaron la percepción que se tenía del Universo, según la cual los fenómenos celestiales se regían por leyes físicas diferentes de aquellas de los fenómenos terrestres. Y tan la cambiaron, que hoy en día no causan ninguna sorpresa las imágenes que nos envían las sondas posadas sobre la superficie marciana –incluyendo a “InSight” –, que nos muestran paisajes áridos no demasiado diferentes de los que tenemos en algunos lugares de nuestro planeta.",
    "Existe alarma entre los expertos por el creciente calentamiento que está experimentando el planeta por la emisión de gases de invernadero a la atmósfera. Y el grado de alarma ha llegado a un nivel tal que para paliar dicho calentamiento se están proponiendo acciones que se antojan desesperadas. Como una de estas acciones se ha propuesto la dispersión de grandes cantidades de dióxido de azufre en la parte baja de la estratósfera para reducir la cantidad de radiación solar que alcanza la superficie de la Tierra.Como sabemos, dependemos de la radiación solar para mantener la temperatura de la superficie de nuestro planeta. Dicha temperatura está determinada por el balance  entre la radiación solar que es absorbida por la Tierra, y la radiación que es re-emitida al espacio por diferentes mecanismos. Por medio de uno de estos mecanismos la superficie del planeta emite radiación infrarroja que cruza la atmósfera y escapa hacia el espacio. Ciertamente, no resulta obvio que la Tierra emita radiación infrarroja. Esto, no obstante, es una ley física que podemos comprobar fácilmente acercando la mano a un objeto con una temperatura de algunos cientos de grados centígrados –sin tocarlo, por supuesto–. La emisión de radiación por la superficie de la Tierra no es aparente porque su temperatura es de sólo algunas decenas de grados centígrados. Dicha emisión, sin embargo, está presente, si bien es relativamente débil.El balance entre la radiación absorbida por la Tierra y la radiación emitida al espacio ha sido perturbada en los últimos doscientos años por el efecto invernadero producido por el uso acelerado de combustibles fósiles. En este respecto, sabemos que la quema de estos combustibles genera dióxido de carbono que se incorpora a la atmósfera generando una capa de aire que refleja parte de la radiación infrarroja; radiación que de otra manera escaparía al espacio. Así, en balance, más energía solar es retenida por la Tierra con el consecuente incremento de temperatura global.  De acuerdo con todo lo anterior, la solución preferente para paliar el incremento global de temperatura pasa por la disminución de la concentración de dióxido de carbono en la atmósfera, ya sea removiéndolo de la misma o controlando su emisión en el futuro.No obstante, como no es obvio que esto vaya a ocurrir al corto plazo, se han propuesto medidas emergentes como la de la dispersión de dióxido de azufre en la estratósfera mencionada con anterioridad. Dado que el dióxido de azufre tiene la propiedad de reflejar la radiación solar, dispersarlo en la estratósfera tendría el efecto de disminuir la cantidad de energía solar que llega a la superficie de la Tierra. Esto contribuiría a estabilizar e incluso revertir el calentamiento global.¿Es técnica y económicamente factible alterar artificialmente y de manera apreciable el balance energético de la Tierra? Este es el caso, al menos de acuerdo con un artículo aparecido esta semana en la revista “Environmental Research Letters” publicado por Wake Smith y Gernot Wagner de la Universidad Yale en los Estados Unidos. Wake y Smith se dieron a la tarea de evaluar la posibilidad real de dispersar en la estratósfera la cantidad de dióxido de azufre necesaria para reducir a la mitad el ritmo de crecimiento que está experimentando la Tierra por la emisión de gases de invernadero. Según Wake y Smith los gases de azufre tendrían que ser elevados y dispersados a una altura de 20 kilómetros y para este propósito analizaron todas las posibles opciones, incluyendo aviones comerciales y de investigación, aviones militares, globos aerostáticos y cohetes .Descartaron los aviones comerciales pues concluyeron no podrían llegar a la altura necesaria, ni aun modificándolos. Hay, por otro lado, aviones militares caza que podrían alcanzarla pero no tendrían la capacidad necesaria para completar la misión. Los globos aerostáticos podrían hacer el trabajo pero a un costo demasiado alto. Lo mismo sucede con los aviones para volar a grandes alturas desarrollados por la NASA y con los cohetes. Wake y Smith concluyen así que no existe por el momento el vehículo adecuado para dispersar el dióxido de azufre en la atmósfera y que tendría que ser desarrollado. Según Wake y Smith esto podría hacerse a un costo relativamente bajo de alrededor de los 2,350 millones de dólares. Wake y Smith consideraron un programa de 15 años que iniciaría en 2033. Se construirían 95 aviones a un costo total de 9,500 millones de dólares más los 2,350 del desarrollo de prototipo, los cuales realizarían alrededor de medio millón de operaciones. El costo total del programa por los 15 años, incluyendo la operación, sería de alrededor de 36,000 millones de dólares. De estar Wake y Smith en lo correcto, el costo de dispersar dióxido de azufre en la estratósfera en cantidades suficientes para tener un efecto apreciables en la temperatura de la Tierra es relativamente bajo.  El proyecto, no obstante, es altamente controvertido por los efectos secundarios que podría acarrear. Una disminución en la insolación solar, por ejemplo, tendría un impacto negativo en la agricultura y en la producción de alimentos. De la misma manera, habría menores incentivos para limitar las emisiones de gases de invernadero a la atmósfera, causante del problema que se pretende resolver.Disminuir la insolación solar para resolver el problema climático es entonces equivalente a, por ejemplo, tomar píldoras para adelgazar después de una opípara comida. Una solución en una situación desesperada, sin duda alguna.",
    "Aunque de todo hay en la viña del Señor, quizá estaríamos de acuerdo en que el clima que sufrimos en los días pasados estuvo lejos del que habríamos escogido de habérsenos dado la oportunidad. Afortunadamente, la onda gélida se ha retirado, aunque con seguridad habremos de sufrir más episodios de frío en lo que resta de la temporada invernal que apenas empieza. En esta perspectiva, tenemos la esperanza de que algo de consuelo nos traerá saber que si bien de aquí a la llegada de la primavera nos esperan algunos días fríos, posiblemente no lo serán más que otros que ocurrieron en tiempos pasados. En particular, en el año 536 de nuestra era, que es considerado por Michael McCormick, del Departamento de Historia de la Universidad de Harvard, como “el inicio de uno de los peores periodos para estar vivo, si no el peor año”. Esto, de acuerdo a un artículo de divulgación aparecido esta semana en la revista “Science”.¿Se justifica la opinión de McCormick? Juzgue usted. De acuerdo con “Science”, en el año 536 una misteriosa niebla sumergió en la oscuridad  a Europa, el Medio Oriente y partes de Asia por 18 meses. Como consecuencia, las temperaturas en el verano de ese año cayeron entre 1.5 y 2.5 grados centígrados, iniciando la década más fría en los pasados 2,300 años. Entre otras cosas, esto trajo nevadas en China durante el verano, y afectaciones a la agricultura en Europa con las hambrunas consecuentes. Y, para complementar el desastre climático, se desató en el año 541 la epidemia de peste bubónica conocida como Plaga de Justiniano, que diezmó –en realidad, estrictamente más que eso– a la población del Imperio bizantino.La niebla que oscureció a Europa en el año 536 ha sido atribuida a una erupción volcánica de gran magnitud ocurrida en Islandia. A esta conclusión llegó un equipo de investigadores encabezado por McCormick y Paul Majewski, este último de la Universidad de Maine, mediante un estudio de muestras de hielo recogidas del glaciar Colle Gnifetti en los Alpes suizos. ¿Cómo pueden los glaciares darnos cuenta de acontecimientos ocurridos siglos atrás? Es posible dado que los glaciares acumulan hielo a lo largo del tiempo, el cual atrapa partículas y sustancias químicas suspendidas en la atmósfera –generadas, por ejemplo, por una erupción volcánica–, que son llevadas hasta ahí por las corrientes atmosféricas. La presencia de dichas sustancias a una determinada profundidad en el hielo revela entonces su presencia en un tiempo pasado. Así, el estudio del hielo de los glaciares puede revelar acontecimientos ocurridos en tiempos remotos en la medida que éstos hayan generado sustancias que a su vez hayan sido expelidas a la atmósfera y transportadas hacia un glaciar. Los expertos pueden fechar un determinado acontecimiento por la profundidad en la capa de hielo en la cual aparecen sus huellas; esto, a partir de conocer la velocidad con la que dicha capa se formó a lo largo de los siglos.McCormick y colaboradores estudiaron muestras de hielo del glaciar Colle Gnifetti con una técnica de gran precisión que les permitió concluir que en los primeros meses de 536 ocurrió un erupción volcánica que dispersó cenizas hasta dicho glaciar. Dicha erupción sería entonces el principal sospechoso de causar la oscuridad que asoló a Europa por 18 meses con las catastróficas consecuencias descritas.Por otro lado, y a pesar de la magnitud del desastre climático, sus consecuencias fueron superadas en un lapso de cien años de acuerdo con McCormick y colaboradores. Esto también está escrito en los hielos del glaciar Colle Gnifetti. En este caso, no por huellas de cenizas volcánicas, sino por evidencias de contaminación atmosférica por plomo.La anterior se discute en un artículo publicado en la revista “Antiquity” por McCormick y colaboradores, en el que se reporta un estudio llevado a cabo con muestras de hielo del glaciar Colle Gnifetti.  Entre otros resultados, los investigadores encontraron que la contaminación atmosférica por plomo se incrementó sustancialmente en el año 640 de nuestra era. McCormick y colaboradores atribuyen el incremento en la contaminación atmosférica por plomo a un aumento sustancial en las actividades mineras para la extracción de plata a partir de minerales de galena que contienen plomo. La plata era destinada a la fabricación de moneda y el aumento en su producción revela un incremento en las actividades mercantiles. Indica por tanto una recuperación de la economía europea de la calamidad climática. Lo aquí relatado nos enseña dos cosas. En primer lugar, que la historia del mundo puede leerse en sitios sorprendentes y hasta hace poco tiempo insospechados. Nos enseña también que nuestros antepasados sufrieron de fríos y rigores climáticos considerablemente más severos que los que nosotros podemos padecer en esta ciudad, y que a pesar de esto sobrevivieron como civilización y como especie.   Aunque, pensándolo mejor, esto último posiblemente no nos sirva de gran consuelo en medio de una onda gélida.",
    "El 19 de octubre de 2017, Robert Weryk, un investigador posdoctoral en el observatorio astronómico Pan STARRS  de la Universidad de Hawái, avistó en el firmamento a unos 30 millones de kilómetros de distancia un objeto por demás inusual. Tanto que resultó ser el primer objeto conocido proveniente de algún lugar fuera de nuestro sistema solar. Dadas las circunstancias, fue bautizado como ´Oumuamua, un nombre de origen hawaiano que aproximadamente significa “primer mensajero distante”. Se estima que ´Oumuamua tiene una forma de cilindro unos 230 metros de largo y 35 metros de ancho. De esto, sin embargo, no hay seguridad pues no tenemos una imagen visual suya por la gran distancia a la que fue avistado. ¿Cómo sabemos que ´Oumuamua es un objeto interestelar? Lo sabemos por la gran velocidad a la que viaja que no es propia de los objetos del sistema solar. Por esta velocidad no puede ser retenido por la fuerza gravitacional del Sol, por lo que de manera inevitable nos abandonará después de su breve visita.  Sabemos que orbitando alrededor del Sol, aparte de planetas, hay asteroides y cometas. Estos últimos tienen usualmente órbitas muy excéntricas que los alejan y acercan al Sol de manera periódica.  Sabemos también que los cometas pueden llegar a ser muy vistosos durante sus acercamientos al Sol, cuando desarrollan una cola que puede abarcar buena parte del cielo. Dicha cola está formada por gases y partículas de polvo expulsados de la superficie del cometa por el calentamiento que experimenta al acercarse al Sol. Los asteroides, en contraste, no desarrollan una cola dado que están formados por materiales que no se vaporizan por el calentamiento por la radiación solar.Si bien en un primer momento se clasificó a ´Oumuamua como cometa, al no haber desarrollado una cola al acercarse al Sol se le reclasificó como asteroide. Un artículo aparecido el pasado mes de julio en la revista “Nature”, sin embargo, ha arrojado dudas al respecto. En dicho artículo, publicado por un grupo internacional de investigadores encabezado por Marco Micheli de la Agencia Espacial Europea, se reporta que ´Oumuamua sufrió una aceleración mayor a la esperada sólo por la fuerza del Sol. Para explicarlo, Michel y colaboradores postulan que, después de todo, ´Oumuamua se comporta como un cometa y que sufrió una fuerza adicional a la del Sol por la vaporización y emisión de sustancias de su superficie. Esto, por un mecanismo similar a como un cohete se impulsa por los gases que escapan de sus motores.Una explicación alternativa, que vuelve a colocar a ´Oumuamua en la categoría de asteroide, es ofrecida en un artículo aparecido esta semana en el repositorio ArXiv de manuscritos, alojado por la Universidad Cornell. Los autores de dicho artículo –que ha sido aceptado para publicación en la revista Astrophysical Journal Letters– son Samuel Bialy y Abraham Loeb de la Universidad de Harvard.De acuerdo con Bialy y Loeb, la causa de la aceleración adicional experimentada por ´Oumuamua es directamente la radiación solar, la cual, al “chocar” con el  asteroide, lo frena cuando se acerca al Sol o lo impulsa hacia delante cuando se aleja del mismo.  Se sabe que la fuerza ejercida por la radiación solar sobre un objeto es un efecto real, análogo a la fuerza ejercida por el viento en contra de la vela de un barco. Bialy y Loeb especulan que ´Oumuamua tiene la forma de una delgada lámina con espesor de una fracción de milímetro, lo que le permitió aprovechar el impulso de la radiación solar al igual que lo hacen los veleros.Bialy y Loeb, sin embargo, no se limitan a ofrecer una explicación alternativa para la aceleración adicional experimentada por ´Oumuamua y especulan –mas no aseguran– que pudiera tratarse de una estructura artificial construida por una civilización alienígena fuera de nuestro sistema solar. De acuerdo con esta hipótesis, ´Oumuamua podría corresponder a los desechos de un ingenio espacial fuera de operación, o bien podría ser una nave plenamente operativa que utiliza la radiación de las estrellas para impulsarse.Esta es una explicación exótica –a la vez que atractiva– que de manera natural ha provocado reacciones negativas de la comunidad de expertos. Así, por ejemplo,  Marco Micheli hace notar que la hipótesis sobre el impulso de la radiación solar fue considerada en su artículo de Nature, llegando a la conclusión de que es altamente improbable debido a que esto implicaría que el peso de ´Oumuamua, dado su volumen aparente, tendría que ser de mil a diez mil veces más pequeño que el de otros asteroides del sistema solar con tamaños comparables. Otros expertos son igualmente escépticos, pues no se detectaron emisiones de radio que delataran la presencia de alienígenas en la supuesta nave espacial.La discusión entre científicos sobre hipótesis y resultados, por lo demás, es algo normal y parte de su práctica profesional. En último término, para ser aceptada una hipótesis tiene que ser validada por los resultados de experimentos diseñados para tal fin.  En el caso de ´Oumuamua, hay que reconocer que esto será difícil de lograr pues hace muchos meses que ya desapareció en el firmamento. No obstante, de ocurrir la visita de otro objeto similar –esperando que reciba un nombre más fácil de recordar–, ´Oumuamua nos servirá de referencia y estaremos mejor preparados para entender su significado. Por lo pronto, habría que entender que la hipótesis sobre el origen alienígeno de ´Oumuamua es eso, una mera hipótesis sin sustento firme. Y, no obstante, tan valiosa como lo puede ser una hipótesis científica.",
    "El sitio arqueológico de Santa Ana/La Florida fue descubierto en el año 2002 por un equipo de investigadores ecuatorianos y franceses en la provincia de Zamora Chinchipe en el sur del Ecuador. Este sitio corresponde a la llamada cultura Mayo-Chinchipe que floreció en la cuenca del rio Chinchipe en la alta Amazonia hace 2,500-5,500 años. Santa Ana/La Florida constituyó un centro ceremonial con una plaza central circular de 40 metros de diámetro, delimitada por una doble pared de piedra, una necrópolis, dos plataformas elevadas y un templo de forma circular para la realización de actividades ceremoniales. Además de constructores de edificios, los pobladores de Santa Ana/La Florida fueron hábiles fabricantes de objetos de piedra y de recipientes de cerámica. Como prueba de esto, entre los objetos excavados en el sitio se encuentra una botella con asa de estribo decorada con caras humanas estilizadas de una gran perfección. Otro objeto excavado es un recipiente parcialmente cerrado con cuatro patas y una cabeza humana emergiendo de uno de sus extremos, con ojos muy grandes, una mejilla abultada y la boca torcida indicando que mascaba hojas de coca.        De acuerdo con Francisco Valdez del Instituto de Investigación para el Desarrollo en Francia, uno de sus descubridores, Santa Ana/La Florida demuestra que la Amazonia fue asiento de civilizaciones con un alto grado de sofisticación. Esto, en contra de la creencia de que las condiciones en dicha región son demasiado inhóspitas para el desarrollo de grupos humanos con culturas más allá de la de simples nómadas cazadores recolectores. El que la cultura Mayo-Chinchipe haya, además, florecido hace 5,000 años es sin duda doblemente sorprendente.No para ahí la cosa, sin embargo, pues un artículo aparecido esta semana en la revista Nature Ecology and Evolution afirma que fue la cultura Mayo-Chinchipe la que primeramente hizo uso del cacao como alimento. Esto, en contra de la creencia extendida de que fue en Mesoamérica en donde se domesticó la planta de cacao por vez primera. Dicho artículo fue publicado por un grupo internacional de investigadores encabezado por Sonia Zarrillo de la Universidad de Calgary en Canadá.Sabemos que entre los mayas y posteriormente los aztecas el cacao tenía una gran importancia. Entre estos últimos, los granos de cacao se usaban incluso como moneda y para el pago de tributo por parte de los pueblos conquistados. Hay, además, evidencias del consumo del cacao en Mesoamérica, específicamente en la costa de Chiapas, en fechas tan tempranas como 1,900 años antes de nuestra era. Había pues razones para pensar que el cacao se originó en Mesoamérica.No habría sido así, sin embargo, al menos según Zarrillo y colaboradores. Estos investigadores basan sus conclusiones en tres estudios independientes de búsqueda de restos de cacao llevados a cabo con diversos objetos encontrados en Santa Ana/La Florida. Entre estos objetos se incluyen cazuelas, tarros y botellas de cerámica, al igual que cazuelas de piedra y morteros. Fue también  del interés de Zarrillo y colaboradores estudiar los residuos encontrados en el fondo de recipientes de cerámica. Los restos de cacao se buscaron por medio de análisis de ADN, lo mismo que por el análisis químico de las sustancias características del cacao que habrían penetrado en las paredes porosas de los recipientes que las contenían. Se hizo una búsqueda también de restos de granos de almidón propios de las semillas de cacao.Como resultados de sus estudios, Zarrillo y colaboradores encontraron una sustancia característica del cacao en 25 objetos de cerámica y en 21 de piedra. Encontraron también que seis de los objetos analizados contenían granos de almidón provenientes del cacao. En cuanto a los estudios de ADN, éstos apoyan igualmente la presencia del cacao en Santa Ana/La Florida. Para reforzar estos resultados, los investigadores hacen notar que tres de los objetos analizados dieron positivo a la presencia de cacao en las tres pruebas independientes que se les aplicaron, mientras que 17 objetos lo hicieron en cuando menos dos. Se prueba así de manera sólida el consumo de cacao en la alta Amazonia.  De su estudio, Zarrillo y colaboradores concluyen, además, que los patrones de consumo de cacao en Santa Ana/La Florida se prolongaron cuando menos desde 5,300-2,100 antes de la era cristiana. Esto implica que en la alta Amazonia se consumió el cacao 1,500 años antes de que lo hicieran en Mesoamérica. De estar Zarrillo y colaboradores en lo correcto, México perdería su status como el lugar en donde se originó el cacao, producto que hoy tiene presencia en todo el mundo. Si bien esto no es algo que hubiéramos deseado, tampoco es catastrófico. Si es, por otro lado, de lamentar que un país como el nuestro, que de un modo u otro diseminó el cacao por todo el mundo, hoy no le saque provecho a su cultivo. Que no lo hace lo prueban las estadísticas de producción de cacao a nivel mundial dominadas por países africanos –que no lo tenían hace cuatro siglos–, con México ocupando un muy lejano octavo lugar. Aunque en nuestra descarga podríamos argüir que, mientras que los inventores originales de las bebidas a base de cacao declinaron como civilización hace ya mucho tiempo a pesar de todos sus avances, nosotros seguimos en la brega con todo y nuestros problemas.",
    "En estos tiempos de cambios tecnológicos sin precedentes, una revolución en puerta es la de los vehículos autónomos que se conducen sin chofer a bordo. Dada la complejidad del tráfico vehicular, esta revolución inevitablemente provocará accidentes, algunos de ellos con graves consecuencias, incluso fatales, para las personas involucradas. En esta perspectiva, han surgido interrogantes con respecto a la manera en la que los vehículos autónomos deberán ser programados para responder a una situación de emergencia.Un vehículo autónomo, por ejemplo, podría confrontar una situación en la que al circular por una avenida a gran velocidad se encuentre con una persona cruzando la calle de manera imprudente, teniendo que elegir entre dos opciones: seguir de frente y arrollarla, o virar bruscamente y estrellarse contra de un obstáculo arriesgando la vida a sus pasajeros. La máquina autónoma se encontrará así ante la disyuntiva de elegir a aquellos que tienen prioridad para salir mejor librados del accidente que podría tener consecuencias fatales. En estas circunstancias, ¿cómo deberán programarse los vehículos autónomos para manejar las situaciones de emergencia en las que se tenga inevitablemente que elegir entre salvar o condenar a tal o cual persona? ¿Tendrán prioridad los niños sobre los adultos?, ¿las mujeres sobre los hombres? ¿Tendrá la máquina autónoma la obligación de minimizar el número de víctimas? Las respuestas a estas preguntas no resultan simples y para ayudar a encontrarlas un grupo de investigadores del Instituto Tecnológico de Massachusetts encabezado por Iyad Rahwas, diseñó una plataforma en línea que bautizó con el nombre de “Máquina Moral”, por medio de la cual recabó cerca de 40 millones de opiniones al respecto de personas de 233 países y territorios en el mundo. Los resultados de estudio con la “Máquina Moral” fueron publicados esta semana en la revista Nature. Para llevar a cabo su tarea, la plataforma “Máquina Moral” presenta al usuario 13 situaciones en las que un vehículo autónomo sin frenos avanza hacia personas que cruzan una calle. En todos los casos el vehículo tiene dos posibilidades para actuar: seguir de frente o virar bruscamente, ambas con resultados fatales. En cada situación la plataforma pide al usuario su opinión sobre cuál de las opciones debe escoger el vehículo.   Así, la plataforma confronta al usuario con un dilema que involucra, por ejemplo, decidir si las mujeres tienen prioridad sobre los hombres, o si la tienen los niños o bebés sobre los adultos o personas de edad avanzada. Deben incluso dar su opinión sobre la prioridad que tendrían personas con sobrepeso en comparación con aquellas en buena condición física, o si la situación económica de una persona afecta dicha prioridad. La plataforma confronta también al usuario con el dilema de decidir si el vehículo autónomo debe actuar de un modo que minimice el número de víctimas.A manera de ejemplo, en una situación presentada por la “Maquina Moral” un vehículo autónomo avanza directamente hacia un grupo de tres mujeres que cruzan la calle, una de ellas con sobrepeso. Si el vehículo no desvía su curso arrollará a las tres mujeres con resultados fatales. La opción es virar hacia la izquierda, en cuyo caso el vehículo se estrellará contra una barrera de concreto matando a sus tres ocupantes mujeres, dos de ellas atletas –y por tanto en una excelente condición física, en contraste con la mujer pasada de peso.Los resultados del estudio de Rahwas y colaboradores fueron esperables en algunos aspectos y sorprendentes en otros. Por ejemplo, de manera esperable encuentran que los usuarios de la plataforma mostraron de manera global una fuerte tendencia a salvar personas jóvenes en perjuicio de personas de edad avanzada. Y lo mismo en cuanto a proteger a las personas por sobre la vida de los animales. Se evidenció también una fuerte inclinación a minimizar el número de víctimas. Los usuarios se inclinaron igualmente por salvar a las personas que cumplen las leyes de tráfico por sobre aquellas que no lo hacen.Otros resultados fueron, en cambio, menos esperables. Así, globalmente hubo una preferencia por las personas de un mayor estatus social, por aquellos en buena condición física por sobre aquellos con sobrepeso, y por las mujeres en perjuicio de los hombres.Rahwas y colaboradores obtuvieron también resultados diferenciados por regiones geográficas que en algunos casos difieren de los resultados globales. Así, por ejemplo, en los países del este asiático hay una preferencia menos pronunciada por salvar jóvenes en perjuicio de las personas mayores, lo mismo que por minimizar el número de víctimas.Por otro lado, a pesar de la complejidad del problema moral intrínseco a los vehículos autónomos, Rahwas y colaboradores consideran que existen bases para poder solucionarlo, pues hay puntos en los que hay concordancia a nivel global.El futuro nos dirá sobre qué bases las compañías automotrices diseñarán sus programas de control de vehículos autónomos. En tanto esto se resuelve, conviene tomar nota de algunos de los resultados del estudio de Rahwas y colaboradores que revelan los peligros a los que estamos expuestos al caminar por las calles y cómo estos peligros dependen de nuestro género, edad, condición social y hasta sobrepeso. Si bien poco o nada podernos hacer para minimizar la mayor parte de estos riesgos, cuando menos podríamos considerar el ponernos en una mejor condición física. Lo cual, por otro lado, es intrínsecamente saludable.",
    "En el marco del acuerdo climático de París de 2015, 195 países acordaron buscar limitar el incremento global de temperatura a un valor muy por debajo de los 2 grados centígrados con respecto a su nivel preindustrial, haciendo esfuerzos para que dicho incremento no superase los 1.5 grados centígrados. Esto, con el fin de evitar una catástrofe climática a nivel global.Como parte del acuerdo climático de París, el Panel Intergubernamental sobre Cambio Climático (PICC) de la Organización de las Naciones Unidas recibió una invitación para elaborar un reporte sobre los efectos que tendría un incremento global de temperatura por arriba de los 1.5 grados centígrados. El PICC aceptó dicha invitación y como parte de su respuesta difundió el pasado 8 de octubre el primero de tres documentos que ha preparado sobre este tópico. En dicho documento se comparan los dos escenarios que surgirían con incrementos de 1.5 y 2 grados centígrados de temperatura global. De acuerdo con el reporte del PICC, limitar el incremento de temperatura a 1.5 grados centígrados reduciría de manera  significativa el ritmo de crecimiento del nivel de los océanos, así como el deshielo de los casquetes polares. De la misma, si limitamos el calentamiento a 1.5 grados centígrados sobrevivirán entre un 10% y un 30% de los arrecifes de coral, los cuales prácticamente desaparecerían si el aumento llegara a los 2 grados centígrados. Así, según el PICC, es indispensable llevar a cabo una transformación rápida en materia de  “tierra, energía, infraestructura urbana y sistemas industriales” para limitar la emisión de los contaminantes atmosféricos causantes del cambio climático. Un estrategia para la transformación energética es la sustitución, en la medida de los posible, de las fuentes de generación de energía basadas en combustibles fósiles por fuentes de energía renovable. En este respecto, las energías eólica y solar fotovoltaica están actualmente jugando un papel central.La energía eólica puede ser aprovechada empleando molinos de viento que transforman la energía de viento en la energía de rotación de las aspas del molino, y a partir de ésta en energía eléctrica por medio de generadores de electricidad. Las celdas fotovoltaicas, por su lado, capturan la radiación solar y la transforman directamente en energía eléctrica. Puesto que para su operación los molinos de viento y las celdas fotovoltaicas no consumen combustibles fósiles –como el petróleo o el gas natural–, podíamos esperar que no contribuyan al cambio climático.Esto último, sin embargo, no es estrictamente cierto, al menos a nivel regional. Los paneles solares, por ejemplo, absorben una cierta cantidad de la radiación solar que incide sobre ellos y que de otro modo hubiera sido absorbida por el medio ambiente. Dicha radiación es convertida en electricidad que es enviada por medio de una línea de transmisión a otra localidad en donde es consumida generando calor que se disipa en el medio ambiente. Los paneles solares, de este modo, contribuyen a un enfriamiento regional alrededor de la instalación solar y a un calentamiento en el lugar en donde se consume la energía eléctrica que producen. No obstante, dado que en algunas regiones la temperatura se elevaría mientras que en otras disminuiría, el efecto global sería mucho menor que a nivel regional. Los molinos de viento, por su lado, frenan la velocidad del viento que incide sobre sus aspas y de este modo alteran los intercambios de calor y de humedad entre las superficie de la tierra y la atmósfera. De acuerdo con los expertos, esto produce una elevación de temperatura alrededor del área de instalación de los molinos de viento. Este efecto es el tema de un artículo aparecido en línea el pasado 4 de octubre en la revista Joule cuyos autores son Lee Miller y David Keith de la Universidad de Harvard. Miller y Keith se propusieron estudiar el efecto que, sobre la temperatura ambiente, tendría la instalación en el territorio continental de los Estados Unidos de molinos de viento en cantidad suficiente para generar toda la energía eléctrica consumida por ese país. De manera interesante, encuentran que la operación de dichos molinos incrementaría en 0.24 grados centígrados la temperatura ambiente a nivel de todo el territorio estadounidense.Miller y Keith hacen notar que este incremento de temperatura es mucho mayor que la disminución de 0.1 grados centígrados que se estima se obtendría eliminando todas las instalaciones en los Estados Unidos que generan electricidad por medio de combustibles fósiles.  Así, lejos de ayudar al cambio climático, los molinos de viento lo agravarían. Ciertamente, esta situación se haría menos desfavorable en la medida en que se eliminara la quema de combustibles fósiles y disminuyera la emisión de gases de invernadero. El punto de equilibrio a partir del cual sería ventajoso el uso de la energía eólica, sin embargo, ocurriría sólo hasta finales del presente siglo. La energía eólica no sería de este modo la solución ideal al corto plazo para resolver los problemas del clima. En contraste, Miller y Keith hacen notar que el impacto climático de la energía solar fotovoltaica es diez veces menor que el de la energía eólica, lo que, la convertiría en una opción más favorable. Por otro lado, y sin lugar a dudas, el tema es controvertido por lo complicado que resultan ser los fenómenos climáticos. Pareciera, no obstante, que la energía solar fotovoltaica le lleva ventaja a la energía eólica como solución al problema climático. Al menos si hemos de creerle a los resultados de Miller y Keith.",
    "Un artículo publicado esta semana por Paul Mitchell de la Universidad de Pensilvania en la revista en línea PLOS Biology nos hace patente lo mucho que ha cambiado, en apenas un siglo y medio, nuestra percepción sobre la diversidad racial del planeta. En dicho artículo, Mitchell discute las investigaciones llevadas a cabo por Samuel Morton, un connotado médico y antropólogo norteamericano de la primera mitad del siglo XIX, sobre el nivel de inteligencia de las diferentes razas humanas. Morton estaba interesado en determinar el volumen craneal de las razas humanas y para este propósito se dio a la tarea de reunir cráneos procedentes de todo el mundo. Sus esfuerzos cristalizaron en una colección de unos 600  cráneos susceptibles de ser estudiados. Para determinar el volumen de un cráneo, Morton vertía semillas de pimienta blanca hasta llenarlo y posteriormente media el volumen de las semillas por medio de un recipiente graduado. En experimentos posteriores, y con el objeto de obtener mas exactitud en sus determinaciones, sustituyó las semillas por municiones de plomo. Morton publicó su resultados en tres libros, el primero de fue intitulado “Crania Americana”  y apareció en 1839. Morton consideraba cinco razas: caucásica, americana, mongoloide, malaya y etíope (africana) y de sus mediciones encontró que la raza caucásica tiene en promedio el mayor volumen craneano y la etíope el menor. Una idea en boga en la época sostenía que el volumen craneal era un medida del nivel de inteligencia. Así, Morton concluyó que la raza blanca es superior a todas las demás y que la africana se encuentra en el fondo de la escala del intelecto. Las conclusiones de Morton no tienen hoy sustento y se consideran que son el resultado de sus prejuicios raciales, muy de acuerdo con la época que le tocó vivir. Entre otras cosas, se critica que Morton no haya considerado que el tamaño de cráneo depende del tamaño del cuerpo y que éste a su vez resulta de una adaptación al medio ambiente. Igualmente, no hay hoy día un sustento para la división de razas por él consideradas. En un artículo publicado en 1978 en la revista “Science”, el paleontólogo y divulgador científico norteamericano Stephen Jay Gould saca a colación el trabajo de Morton y lo acusa de haber interpretado de manera tramposa sus mediciones. Llega a esta conclusión comparando los volúmenes craneales obtenidos empleando semillas de pimienta con aquellos que arrojan las municiones de plomo que resultan ser mas grandes. Encuentra, además, que la discrepancia es mayor para los cráneos africanos que para los caucásicos y que esto pudiera deberse a que Morton haya vertido una mayor cantidad de semillas en los cráneos caucásicos –presumiblemente sacudiéndolos para que se asentaran– que en los cráneos de otras razas. Esto, especula, Gould, lo habría hecho de manera inconsciente, impulsado por su convicción en la superioridad de la raza blanca.   Paul Mitchell, sin embargo –en la referencia mencionada al inicio de este artículo–, sostiene que no hay suficientes volúmenes craneales obtenidos por medio de las semillas de pimienta que puedan ser directamente comparados con volúmenes obtenidos por municiones de plomo, de modo que no podemos llegar a una conclusión definitiva sobre los posibles errores de medición de Morton. Mitchell llega a esta conclusión analizando notas manuscritas de Morton sobre los resultados de sus mediciones y que no fueron del conocimiento de Gould.Mitchell, no obstante concluye que las investigaciones de Morton sí fueron fuertemente influidas por sus prejuicios raciales y al respecto cita al caso del anatomista y fisiólogo alemán Friedrich Tiedemann quien en 1836 publicó un artículo en el que reporta mediciones de tamaños craneales con 248 cráneos de cinco grupos raciales. Tiedemann encuentra resultados similares a los de Morton pero su interpretación es diametralmente opuesta y concluye hay tal dispersión de volúmenes craneales entre los seres humanos –de modo que un americano bien  puede tener una cabeza más grande que un europeo– que no se puede decir sobre esta base que hay diferencias de inteligencia entre razas. Sin duda Morton tenía prejuicios raciales. Para comprobarlo basta con leer lo que escribió en “Crania Americana” con respecto a la raza caucásica: “La raza caucásica se caracteriza por una piel naturalmente clara, susceptible de todo tinte. Peinado largo y rizado y de varios colores. El cráneo es grande y ovalado, y su porción anterior está llena y elevada. La cara es pequeña en proporción a la cabeza, de forma ovalada, con rasgos bien proporcionados. Los huesos están arqueados, la barbilla llena y los dientes verticales. Esta raza se distingue por la facilidad con la que obtiene los más altos logros intelectuales”. Lo anterior en contraste con su opinión con respecto a los nativos americanos: “La raza americana está marcada por un cutis marrón, cabello largo, negro y lacio, y barba deficiente. Pómulos altos, nariz grande y aguileña, boca grande y labios hinchados. Los ojos son negros y profundos, las cejas bajas. El cráneo es pequeño, ancho en las protuberancias parietales, prominente en el vértice, y plano en el occipucio. El carácter de los americanos nativos es reacio al aprendizaje y lento en adquirir conocimiento. Es inquieto, vengativo y aficionado a la guerra, y totalmente contrario a las aventuras marítimas”.Ciertamente, en 150 años mucho han cambiado nuestros prejuicios raciales; si bien aun quedan algunos resabios del pasado como bien nos consta.",
    "Durante 2,500 años la civilización maya floreció en un extensa región de unos 400,000 kilómetros cuadrados, localizada en el sureste de México, Guatemala, Belice y partes de Honduras y El Salvador. Como sabemos, y por razones que todavía no son claras para los arqueólogos, la civilización maya entró en declive hacia finales del primer milenio de nuestra era, desplazándose la población hacia el norte de la península de Yucatán. Como resultado, las ciudades de las tierras bajas fueron abandonadas y eventualmente engullidas por la selva En lo que se refiere a la región del Petén, en el centro de la zona maya y durante el llamado periodo clásico, florecieron ciudades como Tikal, Calakmul,  Naachtun y Palenque que permanecieron ocultas, sepultadas por la vegetación, hasta tiempos relativamente recientes. Algunas hasta el siglo XX, como es el caso de Calakmul y Naachtun. La situación de las ciudades mayas del Petén, sepultadas por la selva, han dificultado naturalmente la labor de los arqueólogos que buscan desvelar los elusivos secretos que ocultan, incluyendo el estilo de vida de sus constructores. Afortunadamente, la tecnología ha acudido en su ayuda. De manera precisa, lo ha hecho la técnica conocida como LIDAR –acrónimo de la expresión en inglés “Laser Imaging Detection and Ranging”– que permite ver a través del manto de árboles y descubrir edificios y construcciones ocultas por el mismo.La tecnología LIDAR permite medir la distancia a la que se encuentra un objeto por medio de un haz de láser que se apunta hacia dicho objeto. Básicamente, la técnica LIDAR mide el tiempo que le toma a la luz del láser alcanzar al objeto y regresar al punto de partida después de ser dispersado por el mismo. Es decir, mide el tiempo que tarda en producirse el “eco” del rayo luminoso. Conociendo la velocidad a la que viaja la luz, la distancia al objeto puede ser determinada a partir de dicho tiempo.  Los ecos sonoros son fenómenos que nos son familiares dado que el tiempo de retraso entre la producción del sonido y su regreso al punto de partida después de ser reflejado por un objeto es típicamente de segundos o fracciones de segundo, tiempo que fácilmente podemos percibir. Los ecos luminosos, en cambio, son imposibles de percibir dado que la velocidad con la que viaja la luz es casi un millón de veces mayor que la del sonido. Así, nos parece que la luz va y regresa de manera instantánea. No es así, por supuesto, como lo podemos comprobar por medio del uso de instrumentos especializados, uno de los cuales es precisamente el LIDAR.Un artículo aparecido esta semana en la revista Science, publicado por un equipo internacional de investigadores encabezado  por Marcello Canuto de la Universidad Tulane en Nueva Orleans, nos muestra de manera fehaciente las posibilidades que ofrece el LIDAR para la investigación arqueológica. En dicho artículo, se reportan los resultados de un estudio llevado por medio de LIDAR en 12 sitios arqueológicos en el Petén, incluyendo a Tikal y Naachtun.Canuto y colaboradores emplearon un dispositivo LIDAR montado en un avión que sobrevoló el área de estudio y dirigió el haz de un laser hacia las copas de los árboles a medida que rastreaba dicha área y medía el tiempo que le tomaba al haz de luz regresar al punto de partida. Si las copas de los árboles constituyesen una barrera infranqueable para la luz, el haz reflejado no podría revelar nada que estuviera oculto por debajo de dichas copas. Afortunadamente, por experiencia se sabe que parte de la luz logra penetrar por los huecos que dejan las hojas de los árboles y llega hasta el suelo, produciendo no solamente un eco sino varios según los obstáculos que encuentre en su camino. Un análisis de los diferentes ecos luminosos pueden así dar información sobre estructuras arqueológicas ocultas a simple vista. Y esto es precisamente lo que demuestran en su artículo Canuto y colaboradores que rastrearon un área total de 2,144 kilómetros cuadrados, identificando 61,480 estructuras antiguas que incluyen edificios de varios tipos, además de fortificaciones militares, terrazas agrícolas, caminos, canales y depósitos de agua.Una gran cantidad de información puede ser deducida a partir de estos datos. Por ejemplo, Canuto y colaboradores estiman entre 150,000 y 240,000 el número de pobladores en el área estudiada en el periodo clásico tardío. Extrapolando a toda el área maya de tierras bajas en Guatemala, Belice, Campeche y Quintana Roo, los investigadores estiman que la habitaron entre 7 y 11 millones de personas.  Y más importante quizá, los investigadores concluyen que los resultados de su estudio “apoyan sin ambigüedad la noción de que los mayas de las tierras bajas construyeron espacio variable y combativo en el cual una red de ciudades densamente pobladas y defendidas estaba sostenido por una serie de prácticas agrícolas que optimizaron la productividad de la tierra, la diversidad de recursos y la sostenibilidad en una escala mucho más grande que lo considerado hasta ahora”.  La ciencia y la tecnología moderna que de la misma resulta nos da así una muestra más de su poder para investigar el pasado. En este caso particular, facilitando la tarea a los arqueólogos proporcionándoles una herramienta para llevar a cabo un estudio detallado de los sitios arqueológicos del Petén, el cual habría significado un esfuerzo considerablemente mayor empleando métodos tradicionales. Estudio que, por lo demás, ha evidenciado el alto grado de desarrollo que alcanzaron los mayas de las tierras bajas.",
    "Los dickinsonia resultan sin duda animales  extraños, tanto por su nombre como por su apariencia física. En lo que respecta al nombre hay una explicación: éste les fue impuesto por su descubridor, el geólogo australiano Reg Sprigg, en honor a su jefe, Ben Dickinson, entonces Director de Minas del Sur de Australia. Habría que añadir que los fósiles de dickinsonia, con más de 550 millones de años de antigüedad, fueron descubiertos por Sprigg en 1946 en los montes Ediacara en el sur de Australia. Pero aparte de un nombre fuera de lo común, los dickinsonia tenían también una apariencia inusual, según lo evidencian las impresiones en roca que han dejado estos organismos y que han sido descubiertas por los paleontólogos en varios lugares del planeta. Los dickinsonia tenían un cuerpo ovalado dividido en dos partes aproximadamente simétricas y segmentadas en forma transversal. No contaban con un sistema digestivo, por lo que posiblemente absorbían su alimento a través de su cara inferior anclados en el fondo del mar en una cama de bacterias. Tenían un cuerpo blando, desprovisto de esqueleto o caparazón, y alcanzaban tamaños desde unos pocos centímetros hasta un metro y medio. Para tener una descripción más completa del extraño aspecto de los dickinsonia se pueden consultar un buen número de sitios de Internet con fotografías de las huellas que han dejado. Hace unos 540 millones de años dio inicio el periodo Cámbrico, durante el cual se produjo un explosión en la diversidad de vida en la Tierra que en los siguientes cientos de millones de años condujo a la aparición de los animales superiores. Los dickinsonia son anteriores a dicha explosión y en este contexto los paleontólogos han tenido dificultades para colocarlos en la línea evolutiva de la vida. De hecho, se ha discutido si los dickinsonia fueron animales, plantas o incluso una especie de amibas gigantes. Se ha discutido también si constituyen antecedentes directos de las formas de vida que se desarrollaron en el periodo  Cámbrico, o si fueron un experimento evolutivo que resultó fallido.Un artículo aparecido esta semana en la revista Science apoya, de manera convincente según los especialistas, a aquellos que afirman que los dickinsonia son miembros del reino animal. Dicho artículo fue publicado por un grupo internacional de investigadores de Australia, Rusia y Alemania, encabezados por Ilya Bobrovskiy de la Universidad Nacional Australiana en Camberra, Australia.  En su artículo, Bobrovskiy y colaboradores describen los resultados de una investigación llevada a cabo con especímenes fósiles de dickinsonia y de andiva –otro organismo de la biota precámbrica–, con el propósito de determinar si era posible encontrar en los restos fósiles evidencia de material orgánico asociado que demostrara su origen animal.Cabe mencionar que Bobrovskiy realizó la investigación como parte de su tesis doctoral y, según revela en una entrevista, cuando se la propuso al que sería su asesor  éste no se mostró muy convencido de que tendría éxito. Y no obstante lo animó a llevarla a cabo.  El problema básico era que en los 550 millones de años que habían transcurrido, los supuestos materiales orgánicos se habrían descompuesto sin dejar rastro. Bobrovskiy y colaboradores, sin embargo, no estaban a la caza de los compuesto orgánicos originales sino el producto de su descomposición. Es decir, estaban buscando los restos “fósiles” de dichos materiales. Y para sorpresa de todos, los encontraron.Según lo relata Bobrovskiy en una entrevista, para obtener los restos fósiles para su estudio “tuvo que viajar en helicóptero hasta una remota región del mar Blanco en el  norte de Rusia”, y una vez ahí “descender colgado de cuerda por el borde de un acantilado para desprender bloques de piedra, arrojarlos hacia abajo, lavarlos y repetir el proceso hasta encontrar los fósiles que buscaba”. Una vez que consiguió sus muestras, Bobrovskiy regresó a Australia para analizarlas. Para este propósito deprendió una muy delgada capa  de la superficie del fósil y la sometió a análisis químicos sofisticados. Estos análisis identificaron la presencia de materiales orgánicos que indicaban que los tejidos originales del organismo que produjo el fósil contenían moléculas de colesterol, lo que demostraba que dicho organismo pertenecía al reino animal. Así, apuntan Bobrovskiy y colaboradores, los dickinsonia con 558 millones de años de antigüedad se convierten en los animales más antiguos en la faz de planeta cuya existencia ha sido demostrada. Concluyen también que “la presencia de animales dickinsonianos alcanzando tamaños de 1.4 metros revela que la aparición de una biota precámbrica en el récord fósil no es un experimento independiente con animales de cuerpo grande, sino un preludio a la explosión cámbrica de vida”.Y por nuestro lado, viendo los toros desde la barrera, no nos queda sino asombrarnos de que sea posible, empleando los métodos y las técnicas de la ciencia –además de la voluntad decidida de un investigador–, obtener información acerca de la naturaleza de organismos que vivieron en una época inconcebiblemente lejana y que por su aspecto pensaríamos procedentes de otro planeta. Así lo pensaríamos de no ser porque ahora sabemos que probablemente son parte esencial de una explosión que ha repercutido hasta el presente.",
    "En agosto de 2006, durante la reunión de la Unión Astronómica Internacional (UAI) celebrada en Praga, se llegó a un acuerdo para definir lo que es un planeta. De acuerdo con dicho acuerdo, un planeta es un cuerpo celeste que: 1) orbita alrededor del Sol, 2) tiene una masa lo suficientemente grande para que, por la acción de la fuerza de gravedad que dicha masa ejerce sobre sí misma, adquiera una forma aproximadamente esférica, y 3) gire alrededor del Sol en una órbita que esté  libre de otros cuerpos celestes por la acción de su fuerza de gravedad que crece con su masa.Un planeta debe ser entonces capaz, de acuerdo con el tercer criterio, De acuerdo con el tercer criterio de despejar su órbita de otros cuerpos astronómicos. De otro modo, si una misma órbita es compartida por varios cuerpos, éstos no tienen la categoría de planetas. El acuerdo de la UAI tuvo consecuencias funestas para Plutón –que hasta entonces era el noveno planeta del Sistema solar–, pues tiene una masa pequeña y no cumple con la última condición. Por esto fue oficialmente degradado a la categoría de planeta enano. Así, de nueve planetas que teníamos sólo nos quedan ocho. El acuerdo para definir lo que es un planeta se tomó por votación entre los astrónomos asistentes a la reunión de Praga de la UAI. Dicha votación, sin embargo, se llevó a cabo en el último día del congreso durante la sesión de clausura, y contó con la participación (minoritaria) de sólo 424 astrónomos –esto último no es sorprendente pues es conocida la poca participación de congresistas en el último día de una reunión científica–. De manera entendible, la decisión tomada en estas circunstancias ha sido motivo de críticas por parte de la comunidad de astrónomos. Además de que, al margen del número de votantes en la sesión de la UAI, el procedimiento mismo que siguió para establecer una clasificación o taxonomía de planetas ha sido controvertido.Con respecto a esto último, una crítica demoledora es la que hace un grupo de astrónomos de instituciones norteamericanas encabezados por Philip Metzger de la Universidad de Florida Central, en un artículo que apareció en línea la semana pasada en el sitio de Internet de la revista Icarus. En dicho artículo, Metzger y colaboradores reportan los resultados de una investigación bibliográfica llevada a cabo en revistas de astronomía de los últimos 200 años. El propósito de dicha investigación fue clarificar lo que los astrónomos históricamente han entendido por términos tales como “planeta”, “planeta pequeño”, “planeta menor” y “asteroide”. Y en base a sus resultados, determinar hasta que grado la taxonomía de los planetas establecida por la UAI tiene un sustento histórico que refleje un cierto consenso científico. En su investigación, Metzger y colaboradores encuentran un solo artículo –escrito en el año 1802 por William Herschel descubridor del planeta Urano– en el que se emplea la capacidad de un cuerpo astronómico para despejar su órbita como criterio para su clasificación como planeta. En todos los demás casos investigados este criterio está ausente. Así, por ejemplo, Ceres, que es el cuerpo más grande del llamado cinturón asteroides localizado entre Marte y Júpiter y que por tanto comparte órbita con un enorme número de otros objetos, es referido históricamente como planeta y no como asteroide. De este modo, según Metzger y colaboradores, el tercer criterio acordado por la UAI para establecer la taxonomía planetaria no se sustenta en un consenso científico  histórico y por el contrario resulta ser el mero resultado de una votación –por un grupo minoritario de astrónomos, además–. Y esto, apuntan, está frontalmente en contra de la naturaleza misma de la ciencia cuyos procesos deben estar libres de cualquier dictado autoritario. Apuntan, además, que la UAI ha dañado la percepción pública de la ciencia con su imposición de clasificación de planetas, la cual ha llegado a los libros de texto.Como resultado, Metzger y colaboradores recomiendan a la UAI que con relación a la taxonomía planetaria debe de “abstenerse de recurrir al voto para crear la ilusión de un consenso científico”. Recomiendan igualmente que “las organizaciones educativas se preocupen por enseñar a los estudiantes que la taxonomía es una parte vital de la ciencia, al igual que lo son la observación de la naturaleza, la elaboración de hipótesis y la comprobación de las predicciones que de las mismas resultan”. En particular, señalan, “la taxonomía es empleada por los científicos para organizar sus observaciones y para pensar de manera más clara y comunicar conceptos que se unen para formar hipótesis”. Y el coscorrón a la UAI lo rematan señalando: “las definiciones tales como la de “planeta” están determinadas por este proceso y no de forma arbitraria”.  Así, concluyen Metzger y colaboradores, los criterios para clasificar planetas deben basarse en su historia geofísica y no en si o no comparte su órbita con otros objetos astronómicos. Plutón, recibe de este modo una ayuda sólida en sus aspiraciones –suponemos que las tendrá– de regresar al selecto grupo de cuerpos celestes conocidos como “planetas”. Si así ocurriera, se corregiría la injusticia de la que fue víctima, dado que Plutón, según Metzger, es después de la Tierra el segundo planeta más complejo e interesante del  Sistema solar.",
    "En 1962 se descubrieron cerca de la ciudad de Niederstotzingen en el sur de Alemania, 12 tumbas que contenían esqueletos humanos y restos de caballos y objetos de origen geográfico diverso, incluyendo armas y armaduras, joyería y equipo para montar. El cementerio funcionó como tal entre los años 580 a 630 de nuestra era, en una región que en esa época estaba ocupada por tribus alamanas. Dichas tribus se encuentran entre las tribus germánicas que entraron en conflicto con los romanos en los siglos III y IV d.C. En el año 496 d.C. los alamanes fueron derrotados por Clodoveo I, rey de los francos, e incorporados al reino franco. Y es en este contexto que los expertos interpretan el hallazgo de Niederstotzingen. Para mayor explicación y según los especialistas, bajo la influencia franca los alamanes llevaron a la práctica la construcción de tumbas ostentosas en las que se enterraba no solamente a los miembros de una familia directamente emparentados entre sí, sino también a individuos con una relación cercana pero no consanguínea. Y esto es lo que reflejan los resultados de una investigación publicados esta semana en la revista Science Advances por un grupo internacional de investigadores encabezado por Niall O’Sullivan del instituto de investigación privado EURAC Research, con sede en Bozen-Bolzano en el norte de Italia.    Según O’Sullivan y colaboradores, la serie de tumbas descubiertas en Niederstotzingen contiene un total de trece esqueletos humanos –diez adultos y tres infantes– distribuidos en 7 tumbas individuales y 2 tumbas colectivas con tres esqueletos cada una. Todas las tumbas están distribuidas en un área de aproximadamente 15x15 metros cuadrados y de esto se infiere que los restos humanos que contienen corresponden a individuos que en vida tuvieron una relación cercana. Esto sería, por supuesto, particularmente cierto de las tumbas colectivas.El análisis genético llevado a cabo por O’Sullivan y colaboradores muestra que de los ocho individuos para los cuales se pudo obtener suficiente información genética cinco estaban emparentados al menos en segundo grado. En contraste, dos individuos de una de las tumbas colectivas y un tercero de la otra no tenían ninguna relación de parentesco con el grupo. Esto implica que la posición de un individuo dentro de la familia a la que pertenecía dicho cementerio no dependía del grado de consanguinidad que guardara con los demás miembros de la familia. Esto último, y de acuerdo con O’sullivan y colaboradores, está en concordancia con la peculiar estructura de una sociedad alamana según la cual la posición social de un individuo dependía del número de seguidores que lograba reclutar, fueran o no fueran éstos familiares suyos. El concepto de familia resulta así extendido y un individuo para fortalecerse buscaba reclutar a tantos seguidores como le fuera posible. O’Sullivan y colaboradores, por otro lado, se preguntan por la manera de cómo se habrían integrado a la familia del cementerio de Niederstotzingen los miembros no emparentados y especulan que podría haberse tratado de rehenes incorporados al grupo cuando niños para ser entrenados como guerreros. Esto, sin embargo, esto es sólo una mera conjetura pues no existen suficientes evidencias que lo prueben.       Desde un punto de vista genético, los investigadores encuentran de los individuos enterrados en el cementerio de Niederstotzingen se dividen en dos grupos. Un primer grupo con un genoma que se acerca al de las poblaciones modernas del norte y este de Europa, y un segundo grupo que se acerca más a las poblaciones mediterráneas.   Por otro lado, a partir de estudios de la concentración de isótopos en los dientes O’Sullivan y colaboradores pudieron determinar el lugar en donde crecieron los individuos cuyos restos fueron encontrados en Niederstotzingen. Así, se determinó que todos crecieron en la localidad, con la excepción de dos cuya concentración de isótopos indica que crecieron en una región de mayor altitud.De manera interesante, uno de estos dos individuos fue enterrado en una de las tumbas comunes, juntamente con individuos que sí crecieron en la localidad. Es decir, el ser extranjero no era impedimento para pertenecer a una familia con todas las prerrogativas de los otros miembros, incluso la de ser enterrado en una tumba común. La apertura de las familias para incorporar miembros al margen de consideraciones de sangre y de bagaje cultural que se desprende del estudio del cementerio Niederstotzingen resulta sin duda sorprendente en estos días de cerrazón e intolerancia hacia personas culturalmente diferentes. Y en este respecto, podríamos quizá afirmar que como sociedad hemos retrocedido con respecto a la situación prevaleciente hace 1,400 años. Aunque, por supuesto, hay de males a males y con toda seguridad, de dársele la opción, nadie escogería vivir en los tiempos del cementerio de Niederstotzingen.",
    "¿Considera usted que ha cambiado el clima en la ciudad de San Luis Potosí en las últimas décadas? En particular, ¿considera que hoy en día hay más días calientes  que los que había hace veinte años? Si así piensa estaría de acuerdo con el Climate Impact Lab, una organización que agrupa a especialistas en la ciencia del clima de la Universidad de California, Berkeley, la Universidad de Chicago y la Universidad Rutgers. En efecto, de acuerdo con un análisis del Climate Impact Lab, mientras que en 1985 esperaríamos que hubiera en San Luis Potosí unos 2 días al año en los que la temperatura ambiente alcanzara o superara los 32 grados centígrados, en la actualidad debemos esperar que haya unos 12 días al año con estas temperaturas. Además, el número de días calientes en San Luis Potosí continuará en aumento por el resto del siglo,  de modo que en 80 años podemos esperar unos 40 días con temperaturas de al menos 32 grados centígrados. Toda esta información está disponible en una página interactiva de internet del periódico New York Times. El crecimiento progresivo del número de días calientes es, por supuesto, consecuencia del cambio climático que está sufriendo el planeta por la emisión de gases de invernadero a la atmósfera. Además, de crecer dichas emisiones los números podrían ser peores y todo apunta a que esto último será el caso. Así, por ejemplo, los Estados Unidos, el segundo mayor emisor de gases de invernadero después de China, se retiró el pasado año del acuerdo alcanzado por 196 países en la cumbre climática de París de 2015. El acuerdo busca limitar el incremento de la temperatura global en 1.5 grados centígrados con respecto a sus valores pre-industriales de hace doscientos años. Cabe hacer notar, no obstante, que dicho aumento hoy en día alcanza casi un grado centígrado y podría alcanzar el límite de 1.5 grados pretendido por el acuerdo de París en los próximos 5 años. Esto, de acuerdo a la oficina meteorológica del Reino Unido.La actitud del gobierno federal de los Estados Unidos con respecto al cambio climático, por otro lado, contrasta con la del estado de California que ha emitido leyes que buscan limitar de manera drástica la emisión de gases de invernadero. En este respecto, la legislatura de California aprobó el pasado martes una ley que obliga a que para el año 2045 el 100% de la electricidad que se genere en el estado sea por fuentes que no emitan contaminantes atmosféricos. En un horizonte temporal más cercano, para el año 2026 el 50% de la electricidad debe ser obtenida por este tipo de fuentes.El calentamiento global es, ciertamente, un problema cuya solución debe encontrarse con el concurso de todos los países del mundo, particularmente los industrializados. El ejemplo de California, sin embargo, es importante porque es la quinta economía del mundo –sólo superada por los Estados Unidos, China, Japón y Alemania– y es un terreno fértil para el desarrollo de tecnologías y estrategias de trascendencia global. En California la electricidad es generada en un 43% por plantas termoeléctricas que consumen gas natural, mientras que las energías renovables generan alrededor de un 30%. El porcentaje restante se genera fundamentalmente en centrales nucleares y grandes centrales hidroeléctricas. Las energías renovables incluyen las pequeñas centrales hidroeléctricas, la biomasa, la geotérmica, la eólica y la solar.La energía eólica contribuye con poco más del 6% de la electricidad generada en California. La energía solar, por su parte, si bien su contribución era prácticamente inexistente en 2008, a partir de esa fecha creció de manera sostenida hasta alcanzar un 12%.Dado que los paneles solares y las turbinas eólicas no generan gases de invernadero durante su operación, resultan fuentes de energía muy atractivas para atacar el problema climático. En particular, los paneles fotovoltaicos generan electricidad de manera directa a partir  de la energía del sol por lo que resultan ser quizá la una opción más natural.Las energías solar y eólica, sin embargo, tienen el inconveniente de su intermitencia. Así, la energía solar no está disponible durante la noche y su intensidad puede disminuir sustancialmente en días nublados, además de que varía de manera importante a lo largo del año dependiendo de la latitud. De este modo, para proporcionar un flujo estable de energía eléctrica es necesario complementar a los paneles solares y a las turbinas eólicas con medios de almacenamiento de energía. Las baterías eléctricas son el medio natural para este propósito. Dichas baterías, sin embargo, son costosas y tienen una corta vida de operación.En estas circunstancias, las plantas fotovoltaicas y eólicas tienen que ser complementadas con medios tradicionales de generación de energía eléctrica con el fin de proporcionar electricidad cuando los paneles solares o las turbinas eólicas no pueden hacerlo. De este modo, los expertos auguran que alcanzar un 100% de generación de energía eléctrica por fuentes libres de contaminación ambiental será excesivamente costoso, al menos con las tecnologías actuales de almacenamiento de energía.Por lo demás, posiblemente todas estas consideraciones serían intrascendentes en las décadas por venir de no tomar el mundo el ejemplo de California e  implementar acuerdos globales efectivos para combatir el cambio climático. Y dados los peores augurios, nuestros descendientes tendrían que acostumbrarse a vivir entre olas de calor. Lo mismo que de frío, pues el calentamiento global provoca eventos climáticos extremos en múltiples direcciones.",
    "Se dice que Napoleón Bonaparte consideraba que su esposa Josefina era su estrella de la buena suerte y que llevaba consigo una imagen suya en sus campañas militares. Sea esta historia cierta o falsa, lo que sí sabemos es que la suerte no estuvo del lado de Napoleón en la batalla de Waterloo. Dicha batalla se llevó a cabo en el mes de junio de 1815 -cuando, por cierto, ya se había divorciado de Josefina- y en la misma sufrió una severa derrota que marcó el final de sus días en Europa.    Ciertamente, la suerte no estuvo del lado de Napoleón en la batalla de Waterloo. O quizá dicho de manera más apropiada, el clima no estuvo de su lado y sí del lado de los ingleses y prusianos. En efecto, según un artículo publicado en junio de 2005 en la revista Weather por Dennis Wheeler y Gastón Demarée, el 17 de junio de 1815 y las primeras horas del día siguiente llovió intensamente en Waterloo, quedando el campo de batalla convertido en un lodazal. Esto dificultó el accionar de las tropas francesas que retrasaron el inicio de las hostilidades, dando tiempo para el arribo de las tropas prusianas y así dar una ventaja decisiva a los aliados. El mal tiempo habría de este modo influido de manera determinante en el resultado de la batalla y marcado el curso de Europa por los siguientes cien años.  Con respecto a las condiciones meteorológicas que prevalecieron la noche previa a la batalla de Waterloo -que inició a la 11:20 horas del día 18 de junio- y las consecuencias que acarrearon, Wheeler y Demarée citan a Victor Hugo quien en Los Miserables escribe: “De no haber llovido en la noche del 17-18 de junio de 1815 el futuro de Europa habría sido diferente…Un inusual cielo nublado fue suficiente para ocasionar el colapso de un mundo”. Que el inusual clima fue uno de los factores que determinaron el resultado de la batalla de Waterloo es una conclusión que es apoyada por un artículo publicado esta semana por Matthew Genge del Imperial College London en la revista Geology. Genge asocia las fuertes lluvias a la erupción del volcán Tambora en la isla Sumbawa en Indonesia -la de más potencia jamás registrada-, dos meses antes de la batalla de Waterloo. La erupción del volcán produjo cerca de 100,000 víctimas fatales y una disminución de la temperatura del planeta por la enorme cantidad de ceniza volcánica que dispersó en la atmósfera. La obstrucción de la luz solar por la ceniza flotante hizo que 1816 fuera conocido como “el año que no tuvo verano”.Según afirma Genge, de manera convencional los expertos asumen que las nubes volcánicas ascienden en la atmósfera de la misma manera como asciende una masa de aire caliente -es decir, debido a que es menos denso que el aire más frío de las capas superiores-. De este modo, de manera convencional los expertos consideran que nubes volcánicas pueden alcanzar una altura máxima de unos 50 kilómetros, dado a esa altura y de manera natural, la temperatura del aire es mayor que el de las capas inferiores de la atmosfera. Genge, no obstante, afirma en su artículo que las nubes volcánicas están cargadas de electricidad y que esto les da un impuso adicional para subir a mayores alturas. Apoya sus conclusiones por medio de cálculos de computadora que muestran que, dependiendo de su tamaño, las partículas volcánicas pueden alcanzar alturas superiores a los 100 kilómetros. La existencia de partículas cargadas de electricidad en las capas superiores de la atmósfera, según afirma Genge, provoca disturbios eléctricos en las mismas que pueden incrementar el volumen de lluvias. Y este habría sido el caso del 17-18 de junio de 1815, víspera de la batalla de Waterloo.Corrió así Napoleón con una muy mala suerte en lo que fue su última batalla -mala suerte que, estaríamos de acuerdo, no habría tenido nada que ver con su divorcio- pues de manera inoportuna para él, justamente dos meses antes de su batalla en Waterloo, ocurrió la explosión volcánica más grande de la que se tenga memoria. No esperaríamos, por supuesto, que Napoleón, pensara de manera supersticiosa que el éxito de una batalla dependiera de algo más que de las circunstancias objetivas de la misma. Después de todo no estaba en absoluto alejado del pensamiento científico y, por ejemplo, en su campaña de Egipto llevó consigo a un grupo de 167 científicos y eruditos, algunos tan notables como el físico y matemático Joseph Fourier. Además, como sabemos, en dicha campaña se descubrió la piedra de Rosetta, que a la postre dio la clave para descifrar los jeroglíficos egipcios.Aun así, no esperaríamos que Napoleón, a doscientos años de distancia, hubiera podido concebir que una erupción volcánica ocurrida dos meses antes a 10,000 kilómetros de distancia hubiera podido ocasionar su derrota en la batalla de Waterloo. Y aun hoy en día, para el común de las personas, ciertamente resulta un hecho sorprendente.",
    "Hace unos dos y medio millones de años nuestros lejanos ancestros, todavía no humanos, aprendieron a fabricar herramientas golpeando una piedra contra la otra. Como era de esperarse, dichas herramientas eran muy simples, apenas piedras con esquinas filosas. Útiles no obstante para, por ejemplo, destazar animales. Sus descendientes, hace alrededor de un millón y medio de años –éstos sí ya humanos–, sofisticaron considerablemente las técnicas de fabricación de herramientas y desarrollaron grandes habilidades manuales para trabajar la piedra. La técnica que desarrollaron, conocida como achelense, les permitió moldear por percusión piedras bifaciales –es decir con dos caras opuestas moldeadas de manera simétrica – con forma de pera, destinadas a la fabricación de hachas de mano.La tecnología achelense sobrevivió por cerca de un millón de años siendo sustituida por otras más elaboradas que permitieron un mayor control en la forma y el tamaño de las herramientas. Posibilitaron igualmente hacer un uso más eficiente de la materia prima para la fabricación de las mismas, pudiendo obtener varias herramientas a partir de una sola piedra original.La fabricación de herramientas de piedra cada vez más sofisticadas acompañó así a la evolución del género humano hasta llegar a nuestra especie. En particular, según los especialistas, la sofisticación de las herramientas avanzó asociada al desarrollo de habilidades de comunicación y de transmisión de conocimientos adquiridos. Y en la medida en que los hombres primitivos se esforzaron en desarrollar herramientas, se adaptaron mejor a su medio ambiente y avanzaron en su colonización del planeta. Y tendremos que reconocer que el esfuerzo desarrollado fue mayúsculo, si hemos de juzgar por el éxito que ha tenido nuestra especie.Tal parece, no obstante, que hubo poblaciones de humanos primitivos que no se  esforzaron tanto en el desarrollo de nuevas tecnologías y en su adaptación al medio ambiente con resultados funestos. Este al parecer fue el caso de la población de Homo erectus –una especie humana que terminó por extinguirse– que habitó el centro de la península arábiga al inicio de la Edad de piedra. Esto, de acuerdo con un artículo publicado el pasado 27 de julio en la revista Plos One por un grupo internacional de investigadores encabezado por Ceri Shipton de la Australian National University en Canberra, Australia.    Shipton y colaboradores alcanzaron esta conclusión después de llevar a cabo un estudio arqueológico con herramientas fabricadas por dicha población con tecnología achelense. De acuerdo con Shipton, los pobladores del sitio empleaban una cultura de mínimo esfuerzo cuando fabricaban sus herramientas y hay la posibilidad de que esto los haya llevado a su desaparición.De manera específica, tal parece que dichos pobladores fabricaban sus herramientas empleando los materiales que encontraban en su entorno inmediato y que tenían una calidad mediocre. Esto, a pesar de que en una colina cercana se encontraba una mina de roca a cielo abierto de buena calidad. Shipton los califica así de una población de “flojos” que no quisieron esforzarse en subir a la colina y acarrear los materiales que requerían para fabricar sus herramientas, conformándose con lo que podían encontrar a su alrededor, pues no encontraron evidencia que hubieran utilizado los materiales de la mina de piedra. Esto contrasta con el comportamiento de especies posteriores como el Neandertal y el Homo sapiens que transportaban sus materiales desde largas distancias. Shipton y colaboradores tienen también la impresión de que, además de perezosa, la población de Homo erectus era demasiado conservadora y no tomaron las medidas pertinentes, manteniendo sus mismas prácticas, cuando su hábitat empezó a perder humedad. Así, su pasividad ante el cambio climático y el poco esfuerzo que estuvieron dispuestos a realizar para mejorar su tecnología habría sellado su suerte evolutiva.     Es sin duda sorprendente la tecnología y habilidad manual que desarrollaron los hombres primitivos para fabricar herramientas hace cientos de miles y aun millones de años, particularmente hacia el final de la Edad de piedra. De hecho, muy pocos de nosotros podríamos fabricar una herramienta de piedra como las que los miembros primitivos de nuestra especia lograban producir y para convencernos habría que ver los muchos videos que podemos encontrar en la Internet en el que se muestran las técnicas desarrolladas por nuestros ancestros. Es también sorprendente enterarnos que hubo poblaciones de humanos que podrían haberse extinguido por su flojera para realizar su máximo esfuerzo para el desarrollo  tecnológico. ¿Nos deja esto algunas enseñanzas en México en donde esta actividad recibe una muy baja prioridad?",
    "¿Cuando nacemos contamos con un conocimiento innato que determina en buena manera lo que llegaremos a ser de adultos? O por el contrario ¿lo que somos es enteramente resultado de las experiencias que tenemos a lo largo de la vida?  Estas preguntas, que han sido debatidas por filósofos a lo largo de los últimos siglos, han sido motivo de discusión en los últimos meses a la luz de nuevos desarrollos en el campo de la inteligencia artificial dados a conocer por la compañía DeepMind, con sede en Londres, Inglaterra, y que actualmente es parte de Google.Los enfrentamientos de ajedrez entre computadoras y humanos ha sido un parámetro empleado para evaluar el avance de la tecnología de la inteligencia artificial. Como sabemos, un año de referencia en este sentido es 1997, cuando la computadora Deep Blue de la compañía IBM derrotó al campeón de ajedrez Gerry Kasparov. A partir de ahí, los programas de cómputo han progresado de manera continua desarrollando capacidades mucho más allá de las humanas para juegos de tablero como el ajedrez, el shogi (ajedrez japonés) y Go chino.   En ese sentido, DeepMind dio a conocer en diciembre pasado el desarrollo de un programa de computadora, llamado AlphaZero que fue capaz de derrotar a otros programas de ajedrez y shogi que hasta entones fueron campeones de su disciplina. Dicho desarrollo está descrito en un artículo que fue depositado el pasado mes de diciembre en la base de datos arXiv alojada en la Cornell University, Ithaca, Nueva York. El artículo tiene como autores a un grupo de investigadores de DeepMind encabezados por David Silver.Los programas anteriores a AlphaZero que fueron capaces de derrotar a los humanos en el ajedrez fueron alimentados con conocimientos de la estrategia del juego desarrollados por nosotros a lo largo de la historia. Al programa AlphaZero, en cambio, no le fue proporcionado conocimiento alguno sobre dicha estrategia quedando a expensas de sus propias capacidades. En estas circunstancias, AlphaZero desarrolló su propia estrategia de juego durante un periodo de entrenamiento. Lo hizo además, de una manera bastante eficiente en tiempo.En efecto, cuando fueron puestos a competir AlphaZero y Stockfish –un programa de ajedrez alternativo, ganador del campeonato de ajedrez para computadoras en 2016– el primero superó al segundo en apenas 4 horas de entrenamiento. De la misma manera, AlphaZero superó a Elmo, campeón mundial de shogi, en 2 horas, así como al programa AlphaGo Lee en 8 horas. En base a estos resultados, Silver y colaboradores concluyen que AlphaZero muestra capacidades super-humanas que le permitieron dominar los juegos de ajedrez, shogi y Go en unas pocas horas sin emplear ningún conocimiento previo de los mismos. No todo mundo está de acuerdo, sin embargo. Así, Gary Marcus de la New York University en un artículo que depositó el pasado mes de enero en arXiv, si bien reconoce las impresionantes habilidades de AlphaZero como jugador de ajedrez, shogi y Go, al mismo tiempo opina que no es realmente cierto que dichas habilidades las haya desarrollado sin la ayuda de los humanos.En apoyo a lo anterior, Marcus argumenta que en realidad la mano de los humanos está implícita en el diseño del algoritmo de AlphaZero, el cual fue llevado a cabo con un propósito específico: dominar la estrategia en los juegos de ajedrez, shogi y Go.  Así, el conocimiento humano sobre dichos juegos acumulado a lo largo de su historia está vertido en el diseño de AlphaZero, que no resulta igualmente eficiente para resolver problemas fuera del dominio para el cual fue creado. Al margen de la controversia sobre el grado de inteligencia de AlphaZero, su capacidad de aprendizaje sin lugar a dudas resulta impresionante –como lo reconoce Marcus– y muestra el elevado grado de desarrollo que ha alcanzado la tecnología de la inteligencia artificial. Y como además dicha tecnología está avanzando a pasos agigantados, aun si AlphaZero sólo puede hacer una cosa extremadamente bien y otras no tanto, esta situación con seguridad se revertirá en un futuro inmediato que verá la aparición de máquinas con capacidades de inteligencia super-humanas.Y una vez que hagan su aparición dichas máquinas se podrá quizá resolver la disyuntiva centenaria: nacemos con un cierto conocimiento básico de modo que nuestro cerebro esta de entrada “alambrado” de una determinada manera, o bien, nuestro desarrollo hasta convertirnos en adultos está determinado por el medio ambiente y las influencias a las que estamos sujetos.",
    "Si bien habrá regiones del mundo que se beneficien con el cambio climático, sus consecuencias serán por lo general negativas. Las regiones costeras, por ejemplo, podrían inundarse en la medida que aumente el nivel de los océanos por la fusión de hielos polares y la expansión del agua del mar. Igualmente, habrá regiones que enfrentarán riesgos de desertificación. Y, por supuesto, habrá un incremento en la frecuencia de fenómenos climáticos extremos –de hecho, ya ocurre–, incluyendo sequías, olas de calor y huracanes de gran intensidad.En contraste con todas estas consecuencias, que han sido profusamente publicitadas por los medios de comunicación, hay un efecto asociado al cambio climático que resulta sorprendente y poco conocido. Éste es  el relativo al incremento en la tasa de suicidios por efecto de altas temperaturas ambientales. Al menos es lo que afirma un artículo aparecido esta semana en la revista “Nature Climate Change” publicado por un grupo internacional de investigadores encabezado por Marshall Burke de la Stanford University en California. Para alcanzar sus conclusiones, Burke y colaboradores llevaron a cabo un estudio con 850,000 casos de suicido en los Estados Unidos y más de 600,000 en México, en miles de condados en los Estados Unidos y municipios en México. Básicamente, los investigadores se interesaron en determinar la relación entre la tasa de suicidios y un aumento o disminución de temperatura ambiental con respecto a su valor habitual, en una localidad dada y a lo largo de un determinado mes. Al medir cambios en la tasa de suicidios como respuesta a cambios en temperatura ambiente, los investigadores eliminaron otras causas que a nivel local o estacional pudieran también generar cambios en dicha tasa. De manera notable, Burke y colaboradores encontraron que un incremento de temperatura ambiente de un grado centígrado por arriba del promedio mensual elevó en 0.7% la tasa de suicidios en los Estados Unidos entre los años 1970 y 1990. Este porcentaje, además, mostró una tendencia moderada a la alza entre 1990 y 2004, último año para el que se reportan datos. De manera similar, en el caso de México los investigadores encontraron que la tasa de suicidios se elevó en un 2.1% por cada grado centígrado de incremento de temperatura ambiente, en el periodo 1980-2010. Podríamos quizá esperar que en los lugares con clima cálido la población estuviese mejor adaptada para soportar los incrementos de temperatura y la tasa de suicidios fuera menos sensible a las fluctuaciones de temperatura ambiental. No es sin embargo el caso, pues Burke y colaboradores encuentran que el incremento en dicha tasa es la misma en los lugares fríos y calientes. La adaptación a un clima cálido no hace así una diferencia apreciable.No la hace igualmente el nivel socioeconómico y grado de desarrollo de un país, habida cuenta que el mismo fenómeno ocurre en los Estados Unidos y en México. En particular, Burke y colaboradores encuentran que el contar con aire acondicionado no impacta al incremento en la tasa de suicidios, misma que también es insensible al sexo del suicida y al método que escogió para quitarse la vida.  ¿Por qué se incrementa la tasa de suicidios al aumentar la temperatura ambiente? Burke y colaboradores no proporcionan una respuesta definitiva al respecto pero especulan que una alta temperatura puede alterar el bienestar mental, posiblemente por efectos colaterales a la termorregulación de cuerpo; es decir, por cambios en el flujo sanguíneo en el cerebro cuando el cuerpo trata de mantener su temperatura normal.Para explorar esta posibilidad, Burke y colaboradores analizaron más de 600,000 mensajes en Tweeter localizados en los Estados Unidos –emitidos entre mayo de 2014 y julio de 2015–, en busca de patrones de lenguaje que expresaran una alteración en el bienestar mental en respuesta a una fluctuación de la temperatura ambiente. De este análisis, los investigadores encontraron una correlación entre dichas fluctuaciones y la aparición de mensajes con palabras que denotaban un estado depresivo, tales como solo, ansiedad, depresión y atrapado. Si bien, como lo reconocen Burke y colaboradores, esto no constituye una prueba definitiva de su hipótesis, sí es una indicación de que las altas temperaturas alteran el estado de bienestar mental. Las consecuencias de los anteriores resultados en el contexto del cambio climático son directas. Es decir, si la Tierra está incrementando globalmente su temperatura, la tasa de suicidios se elevará en la misma proporción. Asumiendo a un incremento global de temperatura de 2.5 grados centígrados para en año 2050 con respecto a la temperatura del año 2000, Burke y colaboradores estiman que entre los años 2000 y 2050 se habrán acumulado más de 21,000 suicidios adicionales por efecto del calentamiento global, 14,000 en los Estados Unidos y 7,500 en México.De estar Burke y colaboradores en lo cierto, el cambio climático no solamente trae huracanes, sequías, olas de calor e inviernos crudos, sino que también es capaz de influir sobre nuestro comportamiento autodestructivo. Lo que no hay que desestimar si tomamos en cuenta que, de acuerdo con datos de la Organización Mundial de la Salud, ocurren anualmente a nivel global alrededor de 800,000 muertes por suicidio. De hecho, según la misma fuente, el suicidio es la decimo séptima causa de muerte a nivel global y la segunda entre aquellos con edades entre los 15 y los 29 años.El cambio climático tiene así consecuencias que son al mismo tiempo sorprendentes y dignas de tomarse en cuenta.",
    "Una escena particularmente impactante al inicio de la película “2001: Odisea del Espacio” del director de cine estadounidense Stanley Kubrick, estrenada en 1968, muestra a dos grupos de primates, presuntos antecesores de nuestra especie, disputándose un charco de agua. La pelea se lleva a cabo a base de gritos, chillidos y gestos amenazantes sin llegar al contacto físico –lo que se sobrentiende habría constituido una norma para resolver diferencias entre grupos rivales– y termina con uno de los grupos apoderándose del charco.La manera de zanjar disputas, sin embargo, habría cambiado radicalmente después de que un grupo de primates se encontró con un extraño monolito de piedra negra, el cual inicialmente les infundió temor. No tardaron mucho, sin embargo, en superar el miedo que les inspiraba el monolito y se le acercaron e incluso lo tocaron, y con esto cambiaron el curso de la evolución de la inteligencia humana.  En efecto, el contacto con el monolito dotó a uno de los primates con la suficiente claridad mental para descubrir que haciendo uso del fémur de un animal podía aumentar considerablemente su fuerza física y esto le daba ventaja en la caza de  animales, lo mismo que en las disputas con sus enemigos primates. Así, en un nuevo enfrentamiento el primate iluminado asesta un golpe mortal a uno de sus enemigos que cae fulminado. Al comprobar  su nuevo poder, el primate lanza eufórico el hueso hacia arriba que da giros seguido por la cámara  y en ese momento se produce un salto en el tiempo por millones de años y el fémur es remplazado en la pantalla por una nave espacial. Si bien a partir de este punto la ficción de Kubrick sigue otros derroteros, es interesante reflexionar sobre la evolución de las armas de destrucción a partir del fémur primigenio, a lo largo de los millones de años que en “2001: Odisea del Espacio” duran apenas un suspiro. Esto, en el contexto de una inminente revolución en armamento letal –después de las que trajeron la invención de las armas de fuego y la energía atómica– producto de las nuevas tecnologías de inteligencia artificial.No existen, por supuesto, pruebas de la veracidad de la historia relatada por Kubrick que tiene, ciertamente, más valor poético que científico, particularmente en lo que respecta al asunto del monolito. Podríamos esperar, no obstante, que de manera accidental nuestros remotos ancestros –en un tiempo todavía no determinado– hayan efectivamente descubierto la utilidad de un garrote para poner fuera de combate a un adversario.  Tenemos, además, evidencia científica de que los garrotes se sofisticaron con el tiempo y que fueron un arma común durante el periodo neolítico y hasta fechas relativamente recientes.Una piedra arrojada a gran velocidad constituye una opción ofensiva o defensiva  que estuvo también a disposición de nuestros primitivos ancestros. La opción se vuelve además letal empleando una honda que esencialmente constituye un extensión del brazo de lanzamiento. Una revolución en la velocidad de los proyectiles empleados para poner fuera de combate a un adversario llegó con la invención de las armas de fuego que alcanzaron en su momento más de 400 kilómetros por hora y velocidades supersónicas en las versiones actuales.Ya en tiempos recientes, una segunda revolución que incrementó de manera sustancial el poder destructivo de las armas fue resultado de los avances científicos que condujeron al desarrollo de la bomba nuclear que multiplicó asombrosamente el poder de las armas convencionales, como quedó evidenciado por la destrucción de las ciudades de Hiroshima y Nagasaki al final de la Segunda Guerra Mundial.    En los momentos actuales, y como resultado de avances en las tecnologías de la inteligencia artificial, se avizora una revolución más en el desarrollo de las armas letales que, según algunos expertos, puede tener consecuencias todavía más graves que las dos revoluciones anteriores. En efecto, esta vez, está en curso el desarrollo de máquinas de guerra que podrían por sí mismas localizar y atacar blancos determinados de manera autónoma, sin intervención humana de por medio.Según sus críticos, la capacidad que tendrían las nuevas armas de decidir en un determinado momento quien debe vivir o morir está en contra de toda consideración moral y por lo tanto su desarrollo debe ser vedado por completo. En este sentido se pronunció recientemente un grupo de 2,400 especialistas en el campo de la inteligencia artificial en un desplegado promovido por la organización “Future of Life Institute”. En dicho desplegado, los especialistas, entre los que se incluyen representantes de compañías tecnológicas como Google y Tesla, se comprometen en no participar en el desarrollo de armas letales autónomas. Los firmantes del desplegado instan a “gobiernos y gobernantes a crear un futuro con normas internacionales sólidas, reglamentaciones y leyes en contra de  las armas letales autónomas”. Igualmente, solicitan a las compañías y organizaciones tecnológicas, lo mismo que a los líderes políticos, unirse a su promesa.En los años por venir podremos saber si la iniciativa promovida por el “Future of Life Institute” tiene éxito o si, por el contrario, las armas autónomas harán su aparición en el mundo con su atemorizante capacidad para decidir quién vive o muere en una determinada circunstancia. En este último caso, mucha agua habrá corrido bajo el molino desde que hace millones de años uno de nuestros ancestros descubrió que podía poner fuera de combate a su enemigo de manera más efectiva utilizando un garrote.",
    "¿Cuál es la forma de la Tierra? Si se levantara una encuesta entre el público en general para averiguar opiniones al respecto, muy probablemente una abrumadora mayoría contestaría que tiene la forma de un globo. Aun así, si la muestra fuera lo suficientemente amplia, con seguridad no faltaría quién opinara que la Tierra no es esférica sino plana. De hecho, tal parece que esta opinión está ganando adeptos en los últimos años. En efecto, según publicaciones en diversos medios de comunicación, está creciendo el número de aquellos que piensan que la Tierra tiene la forma de un disco en cuyo centro se encuentra el polo norte. La periferia de dicho círculo, que corresponde a lo que es el continente Antártico, estaría formada por una pared de hielo de 45 metros de altitud, la cual sería necesaria para contener el agua de los océanos que de otro modo se desbordaría por los bordes. Todo esto de acuerdo con la página de Internet de la Sociedad de la Tierra Plana. Si bien eventos como los eclipses de luna demuestran que la Tierra es esférica, habría que conceder que los argumentos en los que la mayor parte de nosotros basamos nuestras creencias sobre la forma de la Tierra son en buena medida de oídas y no por experiencia propia. Así, tenemos la seguridad de que la Tierra no es plana porque, por ejemplo, sabemos que Sebastián Elcano circunnavegó el planeta en el siglo XVI demostrando que es esférica. Igualmente, sabemos de la redondez de la Tierra por las fotografías tomadas desde el espacio que así lo demuestran. A primera vista, no obstante, la Tierra luce plana –aun a la altura a la que vuelan los aviones comerciales–  y esto es lo que argumentan los que rechazan que sea esférica. Viendo el asunto más a fondo, sin embargo, encontramos que la hipótesis de la Tierra plana ofrece explicaciones demasiado artificiales y forzadas para ciertos fenómenos astronómicos, los cuales, en contraste, pueden ser explicados de manera simple asumiendo que es esférica. Consideremos, por ejemplo, la sucesión de períodos de día y de noche. Para explicarlos, la Sociedad de la Tierra Plana asume que el Sol se encuentra a una altura sobre la superficie de la Tierra de unos 5,000 kilómetros –menor que la distancia que hay entre Madrid y Nueva York– y que gira en círculos sobre la misma. Así, en un determinado momento, aquellas regiones de dicha superficie que estén más cercanas al Sol recibirán una mayor iluminación como ocurre al mediodía. Para explicar las noches, por otro lado, se tiene que asumir que el Sol de alguna manera dirige la luz hacia abajo como en una lámpara de buró. Con respecto a las puestas de sol, en las que el disco solar desciende y desaparece en el horizonte, la página de la Sociedad de la Tierra Plana ofrece una explicación que luce también muy forzada. De acuerdo con ésta, tal como sucede como una parvada de pájaros que por efecto de perspectiva parece descender en la medida en que se aleja aun si no disminuye la altitud a la que vuela, el Sol al alejarse desciende y termina por desaparecer en el horizonte.  Aun más difícil de sostener son las afirmaciones de que existe una pared de hielo que rodea el extremo circular del mundo y de que las tierras más allá de dicha pared son desconocidas y muy posiblemente sufran de temperaturas cerca del cero absoluto. En efecto, sin bien el grueso de los mortales nunca hemos pisado el continente Antártico, sabemos que ha sido ya explorado y que incluso algunos países tienen bases científicas ahí. Y sabemos también que es un continente ciertamente muy frío, pero que las temperaturas que ahí se alcanzan están lejos del cero absoluto de menos 273 grados centígrados. ¿Cómo argumenta la Sociedad de la Tierra Plana en contra de estos hechos? Afirmando que son falsos, producto de una conspiración internacional que quiere, por alguna razón no clara, mantener en secreto la planitud de la Tierra.   La hipótesis de una Tierra plana ofrece así explicaciones complicadas y forzadas –en el mejor de los casos– en contraste con la suposición de una Tierra esférica que permite explicar de manera muy simple y natural fenómenos tales como la sucesión del día y la noche, la rotación de las estrellas, los eclipses de luna, las puestas y salidas de sol y las estaciones del año. Y en un mundo natural tan complicado, las explicaciones más simples son las que hay que adoptar.La confrontación de los modelos de Tierra esférica y Tierra plana se puede poner en la perspectiva de una confrontación entre ciencia y pseudociencia. Una explicación científica,  además de ser tan simple como sea posible, debe ser predictiva. Es decir, debe ser capaz de predecir la ocurrencia de un fenómeno dadas ciertas condiciones iniciales. La capacidad de la ciencia de predecir fenómenos, que no tiene la pseudociencia, es lo que ha permitido el desarrollo de la tecnología moderna, que ha llevado, entre muchas otras cosas, al desarrollo de la red Internet y los teléfonos inteligentes, por mencionar solamente algunas de las aplicaciones tecnológicas en boga.     Dado el impacto obvio que ha tenido la ciencia en nuestros días, es desconcertante comprobar que hipótesis pseudocientíficas tales como las de la Tierra plana, el creacionismo, la inutilidad de las vacunas y la negación del cambio climático tengan seguidores, incluso en números crecientes en algunos casos. En otros, que afortunadamente hacen mayoría, se atienden más a los argumentos científicos, aun con evidencias que no son necesariamente de primera mano.",
    "Si bien todos los días somos testigos del rápido crecimiento que esta experimentando la ciudad de San Luis Potosí, pocas veces nos detenemos a pensar sobre lo que esto significa desde el punto de vista de los recursos naturales necesarios para sostener dicho crecimiento. Es decir, no reflexionamos en el hecho de que para construir edificios, casas habitación y pavimentar calles, se emplean grandes cantidades de cemento y hierro, entre otros insumos, que son elaborados a partir de materias primas extraídas de subsuelo; haciendo uso, además, de combustibles fósiles también extraídos del subsuelo. Y visto a nivel global –independientemente de que no todos los países de mundo crecen a la misma velocidad– hay un uso creciente de recursos no renovables para la construcción y mantenimiento de infraestructura que plantea problemas de sostenibilidad e impacto ambiental.De manera adicional, las materias primas no renovables se emplean no solamente para construir infraestructura sino para fabricar, también en forma creciente, todo tipo de bienes de consumo; incluyendo automóviles, como igualmente nos consta a los que vivimos en muchas ciudades en México y que sufrimos el creciente tráfico de vehículos que colapsa o amenaza con colapsar nuestras vialidades.El uso no sostenible de las materias primas no renovables es, por otro lado, una de las facetas que reflejan las condiciones de salud del planeta, el cual, de acuerdo con un editorial publicado el pasado jueves en la revista Science, está en “un estado peligroso”.  Dicho editorial afirma además que “Los efectos combinados del cambio climático, la contaminación y la pérdida de biodiversidad han puesto en riesgo nuestra salud y bienestar”. El editorial de referencia, que lleva por título “La Tierra del mañana”, tuvo como propósito lanzar una serie de artículos mensuales en los que se “señalarán algunas de las opciones que todavía tenemos para darle forma a la Tierra del mañana”. En la edición del pasado jueves aparecen artículos que tratan sobre organismos genéticamente modificados, educación para el futuro, un sistema sostenible de explotación de materias primas y sistemas de generación de energía con una emisión neta cero de contaminantes atmosféricos.El artículo sobre explotación de materias primas, intitulado “Hacia un sistema sostenible de materiales”, fue escrito por un grupo de investigadores encabezado por Steven Davies de la University of California, Irvine y en el mismo se discuten diversas vías para hacer sostenible dicha explotación. Davies y colaboradores hacen primeramente notar que en el año 2017 se extrajeron del subsuelo 90,000 millones de toneladas de diferentes materiales y que se espera que este número se duplique en el año 2050. Mencionan igualmente que entre los efectos negativos de la explotación de materiales se encuentran la degradación del terreno, la pérdida de hábitats, la generación de desechos, la degradación de la calidad del agua y la contaminación de los ecosistemas. Aunado a lo anterior, los investigadores mencionan que la producción de metales representa un 8% del total de energía consumida a nivel global, porcentaje que se espera aumente en la medida en que se empobrezcan los minerales metálicos en el futuro con el potencial incremento de contaminación ambiental.Los autores hacen notar también que si bien la emisión de contaminantes atmosféricos ocurre en gran medida por el uso de combustibles durante el proceso de refinación o fabricación, la producción de cemento genera directamente dióxido de carbono por el tratamiento a altas temperaturas al que se someten los materiales de piedra caliza empleados en dicho proceso. Esto, en adición a la generación de dióxido de carbono por la quema de combustibles fósiles.Davies y colaboradores consideran varias estrategias para lograr la sostenibilidad en la extracción de materiales, incluyendo la extensión del tiempo de vida del producto, y la disminución de la cantidad de material contenido en cada producto manteniendo su funcionalidad –como ocurrió con la invención del transistor que sustituyó con ventaja a los bulbos al vacío–. Consideran igualmente como estrategia una mayor eficiencia de fabricación, el reciclaje de materiales y la sustitución de un material en un producto por otro de un menor impacto ambiental.  No obstante lo anterior, los autores del artículo de referencia señalan que si bien las estrategias discutidas no son nuevas y se han empleado en algunos casos, su implementación para lograr sustentabilidad en la extracción de materiales es compleja. Entre otras cosas, porque la puesta en operación de una medida particular que mejore una cierta parte del ciclo de uso de materiales puede producir efectos negativos en otra parte de dicho ciclo.No parece haber entonces una solución simple para lograr una sostenibilidad en la extracción de materiales del subsuelo y podríamos quizá esperar en el futuro inmediato solamente avances lentos en este respecto. En todo caso, y en lo que respecta a nuestro medio, es ilustrativo enterarnos que el crecimiento económico y urbano que está experimentando nuestra ciudad, además de sus obvios beneficios, también tiene sus aspectos no tan positivos.",
    "Como sabemos, a mediados del siglo XIX una plaga afectó severamente a los cultivos de papa en Irlanda con consecuencias devastadoras para la población campesina que dependía de manera fundamental de dicho cultivo para su subsistencia. Como resultado, entre 1841 y 1851 murieron alrededor de un millón de irlandeses y otro millón emigró en busca de mejores condiciones de vida, disminuyendo en un 25% la población de la isla.  Algunos irlandeses –que ciertamente no se encontraban entre los más pobres– pudieron comprarse un pasaje por barco y emigraron a lugares lejanos como Australia o los Estados Unidos. No tenían, por otro lado, demasiadas esperanzas de poder algún día regresar a Irlanda y se despedían de su tierra para siempre. Un pequeño puente de piedra en el condado de Donegal en el noroeste de Irlanda, llamado el Puente de lágrimas, lo evidencia.Dicho puente –que puede uno visitar por medio de Google Maps– constituía el punto hasta donde llegaban parientes y amigos de aquellos emigrantes de la localidad que se dirigían al puerto de Derry para embarcarse hacia su lugar de destino. El estado de ánimo de unos y otros está reflejado en una piedra colocada junto al puente que muestra una leyenda en irlandés que traducida dice: “La familia y los amigos del viajero que se dirige hacia tierras lejanas llegaban hasta aquí. Aquí se daba la separación. Este es el Puente de lágrimas”. La emigración de personas hacia países extraños en busca de una mejor vida no es, por supuesto, algo del pasado. Por el contrario, y como bien nos consta, es desafortunadamente un fenómeno de gran actualidad. Aun así, los medios de transporte modernos han achicado el planeta y con esto han atenuado en alguna medida los inevitables puentes de lágrimas.No sería necesariamente el caso, sin embargo, si en un futuro hipotético los emigrantes emprendieran un viaje a un destino fuera del planeta Tierra, como discuten Frédéric Marin y Camille Beluffi, de la Université de Strasbourg  y de la compañía Casca4de, en forma respectiva, en un artículo sometido esta semana a la base de datos arXiv.org alojado por la Cornell University.     En su artículo, Marin y Bellufi discuten detalles de un hipotético viaje al exoplaneta Próxima Centauri b descubierto en 2016. Dicho planeta orbita alrededor de Próxima Centauri, la estrella más cercana al Sol, que se encuentra a una distancia de 4.2 años luz de la Tierra. Los especialistas han reunido una buena cantidad de información con respecto a Próxima Centauri b. Saben, por ejemplo, que es un planeta rocoso con un tamaño muy parecido al de la Tierra y que completa una órbita alrededor de su estrella en 11.2 días. Saben también que la temperatura del planeta haría posible la existencia de agua líquida en su superficie. Próxima Centauri tiene entonces condiciones de habitabilidad,  lo que, de acuerdo con Marin y Bellufi, lo hace un candidato atractivo para una futura misión tripulada.Por la lejanía de Próxima Centauri, sin embargo, no sería una misión que pudiera llevar a cabo una sola generación de viajeros. En efecto, Próxima Centauri está a una distancia tal que a la luz le toma 4.2 años alcanzarla y puesto que no es posible acelerar un vehículo hasta esa velocidad, el tempo que tomaría el viaje sería necesariamente mayor. Por ejemplo, a la nave Apolo que llegó a la Luna le tomaría más de 100,000 años. Hoy en día hay naves espaciales más rápidas, pero aun a una velocidad de 700,000 kilómetros por hora, que es la mayor alcanzada hasta la fecha, el viaje tomaría 6,300 años y por necesidad tendría que ser multi-generacional.En este contexto, y con el objeto de estudiar la evolución del número de miembros de la tripulación a lo largo del viaje a Próxima Centauri y determinar las probabilidades de alcanzar éxito, Marin y Bellufi simularon por medio de una computadora dicha evolución bajo diferentes condiciones. Entre estas condiciones consideraron, el tamaño de la tripulación inicial y las restricciones de apareamiento entre miembros emparentados para evitar la endogamia y preservar la diversidad genética.Marin y Bellufi encuentran que con una tripulación inicial de 25 hombres y 25 mujeres y restringiendo de manera total la endogamia, hay un 50% de probabilidades de que la tripulación se extinga en algún momento durante el viaje. Para asegurar que esto no suceda, encontraron que es necesaria una población inicial mínima de 49 hombres y 49 mujeres.Ciertamente, no se prevé una misión tripulada a Próxima Centauri en un futuro ni cercano ni lejano y en ese sentido la investigación de Marin y Bellufi tiene poca utilidad práctica en lo inmediato. De hecho, en cierto modo se acerca a la ciencia ficción. Pone en perspectiva, sin embargo, los problemas que enfrentarían nuestros descendientes para preservar una tripulación genéticamente saludable en un viaje interestelar de colonización, los cuales son cualitativamente diferentes de aquellos que encontraron los colonizadores y emigrantes del pasado.Hay algunos puntos de contacto, no obstante. Así, al igual que los emigrantes irlandeses del siglo XIX, la primera tripulación de un viaje interestelar tendría que pasar por su propio puente de lágrimas.  Aunque posiblemente con motivaciones diferentes.",
    "Se sabe que en tiempos prehispánicos hubo contactos entre Mesoamérica y los pueblos que habitaron en el suroeste de los Estados Unidos y el noroeste de México. Una evidencia de dichos contactos la proporciona el descubrimiento de restos de recipientes de cerámica con residuos de cacao con una antigüedad de más de mil años en el sitio arqueológico de Pueblo Bonito en el estado de Nuevo México. Esto lo reporta un artículo publicado en el año 2009 en la revista Proceedings of the National Academy of Sciences de los Estados Unidos. En base a dicho descubrimiento y dado que el cacao no se produce en Nuevo México sino en regiones tropicales, se concluye que existieron intercambios comerciales entre Pueblo Bonito y Mesoamérica que transportaron al cacao unos 2,000 kilómetros hacia el norte. Igualmente, los restos de plumas y huesos de guacamaya, así como las campanas de cobre que se han encontrado en los sitios arqueológicos del sureste de los Estados Unidos, proporcionan evidencias de contactos prehispánicos entre esta región y Mesoamérica. Sabemos, por otro lado, que la turquesa fue una piedra muy usada en Mesoamérica para la fabricación de toda clase de objetos suntuarios y ceremoniales, incluyendo máscaras, brazaletes, collares, diademas, aretes y mangos de cuchillo. Los depósitos de turquesa en Mesoamérica, sin embargo, son escasos, y esto motiva la pregunta sobre cuál fue el origen de la turquesa empleada por los pueblos mesoamericanos. Una posible respuesta a dicha pregunta es como sigue. Los expertos saben que, en contraste con Mesoamérica,  en el suroeste de los Estados Unidos y el noroeste de México hay minas de turquesa que fueron explotadas en épocas prehispánicas. Estas minas pudieron ser la fuente de la turquesa mesoamericana que habría sido transportada por mercaderes unos 2,000 kilómetros hacia el sur,  siguiendo una ruta comercial que en sentido inverso habría llevado hacia el norte cacao, guacamayas y campanas de cobre, entre otras mercancías. El argumento ciertamente suena razonable, habida cuenta además que en esa época, si bien había fronteras políticas, no había muros entre los hoy territorios de México y los Estados Unidos que obstaculizaran el tráfico de personas y mercancías. Un artículo aparecido esta semana en la revista Science Advances, sin embargo, desmiente el argumento con datos duros. El artículo fue publicado por un grupo de investigadores de universidades en los Estados Unidos y del Instituto Nacional de Antropología e Historia de nuestro país, encabezado por Alyson Thibodeau de Dickinson College en Pensilvania. Thibodeau y colaboradores se propusieron averiguar la procedencia de la turquesa empleada por los aztecas y los mixtecos y para este fin llevaron a cabo un estudio con un total de 43 azulejos, 38 provenientes del centro ceremonial de Tenochtitlan, en su mayoría del Templo Mayor, y 5 de origen mixteco, a los que midieron las concentraciones de isótopos de estroncio y plomo. Para entender la utilidad de medir dichas concentraciones hay que notar que la concentración de estos isótopos en la turquesa depende de las características geológicas del lugar en la que se encuentra su yacimiento. De este modo, la concentración de isótopos en una muestra dada podría dar una indicación de su lugar de procedencia. De su estudio, Thibodeau y colaboradores concluyen que las concentraciones de isótopos de estroncio y plomo en los azulejos mexicas y mixtecos son significativamente diferentes que aquellas que corresponden a muestras de turquesas obtenidas de yacimientos en el suroeste de los Estados Unidos. Esto desmiente la hipótesis de que la turquesa mesoamericana proviene de dichos yacimientos.Los datos de Thibodeau y colaboradores, sin embargo, no les permiten determinar con certeza el lugar de procedencia de la turquesa empleada por mexicas y mixtecos, pues no se han localizado en el área de Mesoamérica yacimientos de turquesa para compararlos con los azulejos prehispánicos. Al respecto, no obstante, los investigadores hacen notar que el hecho de que no sea posible localizar actualmente yacimientos de turquesa en lo que fue Mesoamérica no significa que no existieran en el pasado, pues pudieron haberse agotado por sobrexplotación.Si bien Thibodeau y colaboradores desmienten una hipótesis largamente aceptada sobe el origen de la turquesa que fue empleada en el México precolombino para la fabricación de las obras de arte que nos asombran en los museos, el misterio sobre dicho origen está aun sin resolverse. Esto, por supuesto, no es de sorprender, dado el tiempo que ha pasado desde que dichas obras de arte fueron creadas. Por el contrario, lo que sí es sorprendente es que después de una guerra de conquista y destrucción de un modo de vida, podamos averiguar que la turquesa con la que fue fabricado un azulejo por un artista en Tenochtitlan hace más de 500 años no pudo provenir de una mina localizada a una distancia de 2,000 kilómetros.",
    "Es sin duda sorprendente enterarnos que del total de cráneos humanos del periodo Neolítico que han sido descubiertos, de un 5% a un 10% muestran trepanaciones llevadas a cabo con pacientes vivos. ¿Qué motivos tuvieron nuestros ancestros hace miles de años para llevar a cabo este tipo de prácticas?  Con un notable éxito, además, pues lograron que el paciente sobreviviera en un porcentaje significativo de casos.Una posible respuesta a la pregunta anterior nos la ofrece Plinio Prioreschi en su obra “A History of Medicine”. De acuerdo con Prioreschi, el hombre del Neolítico no tenía de ninguna manera los conocimientos suficientes para entender los beneficios médicos que una trepanación podría haber tenido. Además, no concebía a la muerte como parte de la condición humana sino como producto de un hecho de violencia. Así, la muerte podría sobrevenir por un flechazo en el abdomen o un garrotazo en la cabeza. A las heridas en la cabeza, sin embargo, el hombre del Neolítico las habría puesto en una categoría diferente a la de las heridas en otras partes del cuerpo. La razón para esto es que, dependiendo de su magnitud, un golpe en la cabeza podría provocar sólo una pérdida temporal del conocimiento que el hombre primitivo no distinguía de la muerte. De este modo, ante la recuperación de la conciencia del desmayado, a los ojos del hombre del Neolítico la muerte por un golpe en la cabeza podía ser reversible, que no era ciertamente el caso de otro tipo de heridas. Vista así, la cabeza ocupa un lugar especial en el cuerpo.De manera adicional, la muerte podría sobrevenir por una enfermedad que era concebida como debida a la influencia de malos espíritus o la ausencia de los benignos, mismos que habría que expulsar o admitir al cuerpo según fuera el caso. Y dada su posición especial en el cuerpo, la cabeza era el canal natural para lograrlo. De este modo, concluye Plinio Prioreschi, las trepanaciones en la cabeza en el periodo Neolítico tenían el propósito de expulsar a los malos espíritus del cuerpo –o permitir la entrada de los buenos– con el fin de curar una enfermedad.  En cualquier caso, y al margen de las especulaciones de Prioreschi, lo que sí es un hecho es que desde hace miles de años se practican trepanaciones de cráneos. Con notable éxito, además, como lo discute un artículo aparecido esta semana en la revista World Neurosurgery, publicado por un grupo de tres investigadores encabezado por David Kushner de la University of Miami. En dicho artículo, se reporta el resultado de una investigación de cráneos prehistóricos con trepanaciones, descubiertos en regiones costeras y montañosas de Perú.Kushner y colaboradores estudiaron más de 800 cráneos que corresponden a un periodo de casi 2,000 años, desde al año 400 antes de la era cristiana hasta el año 1,500 de nuestra era. Entre otros aspectos, los investigadores buscaron determinar si las operaciones de trepanación estuvieron asociadas a la existencia de traumas craneanos previos, así como determinar los porcentajes de supervivencia de los pacientes basados en la sanación de los cortes de hueso. Para este último propósito dividieron los cráneos estudiados en tres grupos. En un primer grupo se incluyeron aquellos que no mostraron ninguna evidencia de evolución del corte de hueso, indicando que el paciente murió durante la operación o pocos días después. En un segundo grupo se incluyeron los casos de recuperación moderada con una supervivencia del paciente por algunas semanas. Finalmente, en un tercer grupo se incluyeron las trepanaciones en las que los bordes de los cortes presentan una remodelación extensiva y son estos los que se consideran los casos de éxito y supervivencia a largo plazo.   Como resultado de su estudio, Kushner y colaboradores encontraron que en la costa peruana, entre los años 400-200 antes de nuestra era, el 10.3% de los casos estuvieron asociados a una fractura por traumatismo craneal y tuvieron una supervivencia a largo plazo del 40%. En las montañas centrales de Perú, entre los años 1,000-1,400 de nuestra era, el 26% de casos estuvieron asociados a fracturas craneales y la supervivencia fue del 53%. Finalmente, en la región de Cuzco durante el imperio Inca, entre los años 1,400 y mediados de 1,500, se tuvo una supervivencia del 75-83% y sólo el 11.9% de las trepanaciones se hicieron asociadas a una fractura craneal previa. De la investigación de Kushner y colaboradores se concluye que solamente un porcentaje minoritario de trepanaciones se hicieron por fracturas craneales previas y que la supervivencia de dichas operaciones alcanzó valores asombrosos para la época, dando fe de la habilidad y conocimientos anatómicos de los cirujanos incas. ºDe hecho, como lo discuten Kushner y colaboradores, los porcentajes de supervivencia que alcanzaron los incas en las operaciones de trepanación en los siglos XV y XVI es el doble de los correspondientes porcentajes durante la guerra civil norteamericana en el siglo XIX.No queda claro, sin embargo, cuáles fueron las motivaciones principales de los incas para practicar las trepanaciones. ¿Buscaban facilitar la entrada al cuerpo de los espíritus benignos o expulsar a los malignos? Si este fuera el caso, la habilidad de los cirujanos peruanos no fue empleada de la mejor manera y a más de un inca le tocó arreglárselas con una innecesaria perforación en el cráneo. Lo cual, por supuesto, no habría sido culpa de los cirujanos sino resultado de la época que les tocó vivir.",
    "La física, estaríamos de acuerdo, no es la materia más popular de la escuela preparatoria.  Lejos de esto, para muchos estudiantes la física es una disciplina árida y difícil de abordar, constituida por un conjunto de fórmulas inconexas que hay que memorizar y aplicar a ciegas;  con una utilidad práctica, además, que no es inmediatamente obvia. En estas circunstancias, no es sorprendente que, con excepciones, los cursos de física en la escuela preparatoria no tengan demasiado éxito. Y lo mismo sucede con algunos cursos introductorios de física a nivel universitario.En contra del desolador panorama anterior, sin embargo, la física está lejos de constituir un conjunto de fórmulas inconexas sin mayor utilidad práctica. Por el contrario, es una disciplina altamente estructurada con un enorme rango de aplicaciones como bien nos consta. En estas condiciones, el desarrollo de métodos pedagógicos para una enseñanza eficiente de la física elemental adquiere una gran relevancia.De suyo, la física es una disciplina con conceptos relativamente complejos y una mejora en sus métodos de enseñanza requiere el desarrollo de esquemas para facilitar al estudiante la adquisición de dichos conceptos. Hay que notar, por otro lado, que un estudiante que se enrola en un primer curso de física, lo hace con un esquema mental intuitivo sobre las causas que determinan los fenómenos físicos a su alrededor –la caída de los objetos por la gravedad, por ejemplo–. Dicho esquema, sin embargo, comúnmente está en contradicción con los conceptos aceptados de la física. Una enseñanza efectiva de esta disciplina debe entonces erradicar estos conceptos erróneos y sustituirlos por los correctos. Tradicionalmente, los estudiantes aprenden física, al igual que otras disciplinas, a través de clases impartidas por un maestro frente a pizarrón. En este esquema, el maestro lleva la parte activa mientras que el papel de los estudiantes es el de receptores pasivos de los conocimientos expuestos por el profesor. En los esquemas de aprendizaje activo, en contraste, los estudiantes adquieren por si mismos el conocimiento a través de experimentos, razonamientos propios y discusión con otros estudiantes. Todo este proceso es guiado por un instructor que, sin embargo, tiene un papel relativamente pasivo. Según los expertos que han empleado métodos de aprendizaje activo para enseñar física, dichos métodos son más efectivos que los tradicionales para erradicar los conceptos intuitivos erróneos de los estudiantes y sustituirlos por los conceptos correctos. Un artículo publicado el pasado 24 de mayo en la revista Frontiers in ICT por un grupo de investigadores encabezado por Eric Brewe de Drexel University, en Filadelfia, Pensilvania, aporta datos duros que apoyan estas conclusiones. En dicho artículo, Brewe y colaboradores reportan los resultados de un estudio llevado a cabo para determinar el efecto que tiene tomar un curso de física bajo el método de aprendizaje activo, sobre los procesos cerebrales que experimenta un estudiante al momento de resolver un problema de física. Para este propósito, los investigadores conjuntaron a un grupo de 55 estudiantes, 33 hombres y 22 mujeres, de la Florida International University. Como parte de la prueba, los estudiantes participantes, que no habían previamente tomado un curso introductorio de física a nivel universitario, se enrolaron en un curso de física que empleaba aprendizaje activo y llevaron a cabo pruebas de razonamiento físico antes y después de tomarlo.De manera concurrente con la pruebas de razonamiento físico, los estudiantes fueron sometidos a pruebas de resonancia magnética funcional. Con relación a esto, hay que recordar que la técnica de resonancia magnética funcional permite determinar qué zonas del cerebro están activas en un determinado momento y por tanto dieron a los investigadores una indicación de los procesos cerebrales asociados al razonamiento físico, antes y después del entrenamiento con aprendizaje activo. Como resultado de sus experimentos, Brewe y colaboradores encontraron que los estudiantes, como esperaban, mejoraron su desempeño en la pruebas de razonamiento físico. De manera más significativa, encontraron también que el aprendizaje activo puso a funcionar zonas del cerebro que estaban inactivas con anterioridad. Con este resultado, los investigadores aportan evidencia sólida a favor del valor de las técnicas de aprendizaje activo para la enseñanza de la física. Brewe y colaboradores concluyen que la actividad cerebral durante el razonamiento físico puede ser modificada con un curso de física con un esquema de aprendizaje activo de un semestre, quedando para una investigación futura determinar si esto es también cierto para los cursos tradicionales y para otras disciplinas.De un modo u otro los resultados de Brewe y colaboradores son, sin duda, fascinantes. ¿Implicarían que en un futuro cercano un bien diseñado y mejor impartido curso introductorio de física fascinará a una mayoría de estudiantes? Ver para creer.",
    "En un artículo publicado en 1980 en la revista Science, un grupo de investigadores de la University of California, Berkeley, encabezados por Luis Álvarez –premio Nobel de Física 1968– avanzaron una posible explicación para la extinción masiva de organismos –incluidos los dinosaurios no aviares– que se sabe ocurrió hace unos 66 millones de años en la transición del periodo Cretácico al Paleógeno. Si bien existen explicaciones alternativas para explicar dicha extinción, la hipótesis de Álvarez y colaboradores ha ganado impulso con el transcurrir de los años y es hoy ampliamente compartida por los especialistas.De acuerdo con dicha hipótesis, la extinción ocurrió por la caída de un meteorito de grandes dimensiones que habría levantado una gigantesca nube de polvo que se esparció por toda la atmósfera del planeta bloqueando la luz del sol. Con una intensidad de radiación solar reducida, las plantas perdieron capacidad para llevar a cabo el proceso de fotosíntesis por medio del cual generan la materia orgánica que sirve de sustento a las especies animales, llevando así a la extinción de aquellas más susceptibles.  ¿Cómo pudieron Álvarez y colaboradores sustentar una hipótesis sobre un suceso ocurrido hace decenas de millones de años? Lo pudieron hacer basados en el descubrimiento de estratos geológicos en varias partes del planeta ricos en metal iridio y que corresponden a la época en que ocurrió la extinción masiva de especies en la transición Cretácico-Paleógeno.  En efecto, ocurre que este metal es raro en la superficie de la Tierra pero no en el material interestelar y esto sugiere que el iridio encontrado en dichos estratos proviene del espacio exterior. Así, el impacto de un meteorito rico en iridio habría levantado un nube de polvo de dimensiones globales y dispersado dicho metal sobre toda la superficie del planeta.    Se tuvo así evidencia indirecta de la ocurrencia del impacto de un meteorito hace 66 millones de años pero faltaba localizar el sitio en donde se habría producido. Hoy se acepta que ocurrió cerca del pueblo de Chicxulub en el litoral norte de la península de Yucatán. El impacto del meteorito generó un cráter de 180 kilómetros de diámetro que está en la actualidad parcialmente cubierto por el mar.  Quedan, no obstante, muchas preguntas por contestar y el meteorito de Chicxulub se mantiene como un tópico de investigación de actualidad. Así, se conjetura que inmediatamente después del impacto se habría producido una onda de calor que convirtió al cielo en un horno y que podría haber durado apenas unos diez minutos. Dicha onda de calor se produjo por el calentamiento por fricción del material expulsado por el impacto durante su reentrada a la atmósfera. Esto habría sido seguido por un periodo invernal debido a la nube de polvo que bloqueó los rayos solares con una duración de meses a años, y por un periodo de calentamiento global por el efecto invernadero de las altas concentraciones de dióxido de carbono que se generaron por el impacto del meteorito. Esto último está apoyado por los resultados de una investigación publicada el pasado jueves 24 de mayo en la revista Science por un grupo de investigadores de los Estados Unidos y de Túnez, con Kenneth McLeod de la University of Missouri a la cabeza. El interés de los investigadores fue el de determinar la amplitud y duración del calentamiento global posterior al impacto de Chicxulub y para esto se enfocaron en el estudio de la concentración del isótopo 18 de oxígeno en restos fósiles –dientes, huesos y escamas– de peces que sufrieron dicho impacto. Dichos restos fósiles fueron localizados en la región de El Kef en el noroeste de Túnez que durante la transición Cretácico-Paleógeno estaba bajo el mar.  Para hacer sus determinaciones, McLeod y colaboradores hicieron uso del hecho que la concentración del isótopo 18 de oxígeno en el agua disminuye en la medida en que su temperatura se incrementa, y que esto se refleja en la concentración de dicho isótopo en el tejido orgánico de los peces que la habitan. De este modo, los investigadores pudieron determinar los cambios que sufrió la temperatura del planeta por efecto del meteorito de Chicxulub a partir de la determinación de la composición de oxígeno 18 en los restos fósiles de El Kef.Como resultado, McLeod y colaboradores encuentran que la temperatura del planeta se incrementó por cerca de 5 grados centígrados por efecto del impacto del meteorito, y que dicho incremento se prolongó por alrededor de 100,000 años. Es difícil, por supuesto, llegar a certidumbres sobre hechos ocurridos hace decenas de millones de años y las conclusiones de McLeod y colaboradores no son compartidas por todos los especialistas. Así, hay quien afirma que la cantidad de gases de invernadero generados durante el episodio de Chicxulub no fue lo suficientemente grande para producir un incremento de 5 grados centígrados en la temperatura del planeta y que la emisión de gases por volcanismo, que se sabe estuvo particularmente activo en la época, es la verdadera causa.Como quiera que sea, McLeod y colaboradores ofrecen datos duros –como en su momento lo hicieron Álvarez y colaboradores sobre la ocurrencia de un impacto de dimensiones planetarias– sobre un incremento sustancial de la temperatura del planeta el cual necesitó 100,000 años en disiparse. Y en este contexto, alertan sobre los efectos de muy largo plazo que puede tener el incremento global actual –que ya casi alcanza un grado centígrado con respecto a valores preindustriales– por el uso hasta hace poco indiscriminado de combustibles fósiles.",
    "Como sabemos, el plomo es un metal con propiedades físicas peculiares. Su peso específico, por ejemplo, es apreciablemente mayor que el de otros metales comunes como el hierro o el aluminio. Se caracteriza también por ser un metal suave, resistente a la corrosión y que funde a temperaturas relativamente bajas. El plomo es, además, un metal tóxico contaminante del medio ambiente. El plomo ha sido utilizado por miles de años y de hecho es uno de los primeros metales empleados por nuestra civilización.  Los romanos, en particular, hace dos milenios hicieron un uso liberal del plomo y, entre otras aplicaciones, lo emplearon para construir los tubos de la red de distribución de agua de la ciudad y en la fabricación de recipientes para elaborar edulcorantes hirviendo vino de uva. Esta última práctica habría provocado entre la elite romana enfermedades por intoxicación con plomo, que algunos han querido incluso ver como la causa del declive de Roma.Al margen de esta especulación, sin embargo, de lo que sí se tienen evidencias firmes es que los romanos contaminaron al medio ambiente con desechos de plomo con sus prácticas de extracción y fundición de este metal. Dicha evidencia proviene de estudios de contaminación por plomo de los hielos del Groenlandia, que si bien está a situada muchos miles de kilómetros del punto en donde fueron generados los contaminantes, esto no impidió que las corrientes atmosféricas los llevaran hasta ahí en donde fueron atrapados por el hielo.  De este modo, la capa de hielo de Groenlandia, que ha crecido en espesor con el tiempo a lo largo de miles de años, tiene un registro histórico detallado de la emisión de plomo a la atmósfera. Para leer este registro, basta con medir la concentración de plomo en el hielo en diferentes profundidades. A partir, por supuesto, de conocer qué profundidad corresponde a cada tiempo histórico. Por otro lado, si bien investigar la contaminación por plomo que produjo Roma a lo largo de su existencia tiene un interés científico por sí mismo, hay que hacer notar que la minería de plomo se dio de manera conjunta con la de la plata que estaba destinada a la elaboración de monedas. Las fluctuaciones de la emisión de plomo a la atmósfera por los romanos nos dan entonces cuenta del ritmo de aceleración o desaceleración de su actividad económica. Aunque en el pasado se había llevado a cabo un estudio en esta dirección, en un artículo aparecido esta semana en la revista Proceedings of the National Academy of Sciences de los Estados Unidos se reportan los resultados de una investigación que lo amplía de manera considerable. Dicho artículo fue publicado por un grupo  interdisciplinario e internacional de investigadores encabezados por Joseph McConnell del Desert Research Institute en el estado de Nevada en los Estados Unidos.McConnell y colaboradores estudiaron la concentración de plomo de la capa de hielo de Groenlandia dentro de un espesor de 400 metros que corresponde a los años 1,100 antes y 800 después de nuestra era. Este intervalo de tiempo comprende el surgimiento de la república Romana y el declive del imperio Romano de occidente. El nuevo estudio proporciona información a intervalos de tiempo de dos años y da cuenta de variaciones en la emisión de contaminantes a lo largo de tiempo que se corresponden con acontecimientos históricos de diferente naturaleza que se dieron en la región del mar Mediterráneo.Según McConnell y colaboradores, alrededor del año 1,000 antes de nuestra era, coincidiendo con la expansión fenicia hacia el occidente del mar Mediterráneo, empezó un incremento sostenido en la emisión de plomo a la atmósfera, el cual se continuó por ocho siglos en la medida en que se incrementaron las actividades mineras en regiones al norte y occidente del Mediterráneo. Entre el siglo cuarto antes de Cristo y el siglo segundo de la era cristiana, las emisiones de plomo correspondieron a actividades mineras intensivas en la península ibérica, bajo el dominio sucesivo de Cartago, la república Romana y el imperio Romano. Ocurrieron, sin embargo, fluctuaciones de corta duración que corresponden a guerras que se dieron en las regiones en la que se localizaban las minas de plomo y plata.Un declive de mayor duración en la generación de contaminantes de plomo tuvo lugar al inicio la primera guerra púnica entre Roma y Cartago durante los años 264-241 antes de nuestra era. Dicha generación, sin embargo, creció durante los últimos años del conflicto cuando posiblemente Cartago incrementó la acuñación de moneda para pagar mercenarios para la guerra, según especulan McConnell y colaboradores.  Durante la segunda guerra púnica ocurrida entre los años 218-201 antes del cristianismo, la emisión de plomo primeramente disminuyó y posteriormente se incrementó cuando los romanos despojaron a los cartagineses de las minas de plomo y plata en el sur de la península ibérica.McConnell y colaboradores muestran en su artículo muchas otras relaciones entre fluctuaciones en la emisión de plomo hacia la atmósfera por actividades mineras localizadas en la península Ibérica con acontecimientos históricos que los historiadores tienen bien localizados en el tiempo en un periodo de casi 2,000 años. Y dado que la minería de plomo estaba asociada a la de la plata destinada a la acuñación de monedas, los resultados también nos relatan la historia de los ires y venires de la economía romana.Ciertamente, mucha más información de la que hubiéramos sospechado estaba escondida en los hielos polares.",
    "De acuerdo con Jo Boaler, profesora en enseñanza de las matemáticas en Stanford University en el estado de California, existe un mito en los Estados Unidos según el cual los estudiantes están divididos en dos grupos: aquellos que están dotados para estudiar matemáticas y aquellos que no lo están. De este modo, continua Boaler, cuando un estudiante de la escuela secundaria experimenta alguna dificultad  con su asignatura de matemáticas concluye que pertenece al segundo grupo y se desinteresa por la materia.Según Boaler, este mito, aunado a la concepción de las matemáticas como un conjunto de fórmulas y procedimientos que hay que memorizar –en lugar de un tópico lleno de ideas, conceptos y creatividad–, ha impedido que la enseñanza de las matemáticas en las escuelas secundarias de los Estados Unidos ocupe un nivel destacado a nivel mundial. En las pruebas PISA de matemáticas en 2015, por ejemplo, los estudiantes de los Estados Unidos ocuparon el lugar 38 entre 68 países.En México la situación no es mejor –de hecho es peor, si hemos de atender al lugar 54 que ocupa nuestro país en las pruebas PISA de matemáticas– y bien sabemos que hay estudiantes que desisten de aprender matemáticas al toparse con alguna dificultad. Es también común que las matemáticas se enseñen como un tema a memorizar, que fácilmente se convierte en algo tedioso y aparentemente sin sentido. No obstante, según afirma Boaler, la división de la población del mundo entre matemáticos y no matemáticos es artificial, pues la capacidad para aprender matemáticas es en realidad algo común dada la plasticidad del cerebro. De este modo, el aprendizaje de las matemáticas se facilitaría con un cambio en la manera como muchos estudiantes conciben su propia capacidad para abordar el tema. Con respecto a esto, un artículo publicado el pasado 25 de abril en la revista Frontiers in Education por un grupo de investigadores encabezado por Jo Boaler, describe los resultados de una investigación llevada a cabo para, precisamente, averiguar en que medida la actitud del estudiante ante las matemáticas incide en su aprendizaje.De manera específica, Boaler y colaboradores se propusieron determinar el impacto que tiene sobre los estudiantes de matemáticas de la escuela secundaria un curso en línea desarrollado por Boaler y algunos de sus estudiantes en Stanford University. Dicho curso enfatiza, entre otras, ideas tales como que el cerebro tiene una gran plasticidad y que todos pueden aprender matemáticas en un alto nivel. Se enfatiza igualmente en que las matemáticas son una materia de gran creatividad y belleza y que están en todos lados a nuestro alrededor.El estudio se llevó a cabo con 1,090 estudiantes de 10 escuelas secundarias de California. Del total de estudiantes, 439 tomaron el curso en línea mientras que el resto no lo hizo. Los maestros participantes tuvieron en su grupo tanto a participantes que tomaron el curso como a otros que no lo hicieron. De esta manera, los investigadores pudieron contrastar el efecto que dicho curso tuvo en el aprovechamiento de los estudiantes. Como resultado de su estudio, Boaler y colaboradores encontraron un mejor desempeño en los exámenes de matemáticas de los estudiantes que tomaron el curso en línea. De manera específica, el estudiante promedio que tomó dicho curso tuvo una calificación más alta que el 63% de los estudiantes que no lo tomaron.Con el objeto de encontrar una explicación al mejor desempeño de los estudiantes, Boaler y colaboradores midieron su involucramiento con las matemáticas en base a su participación en discusiones en clase, lo mismo que en su disposición a trabajar tan duro como les fuera posible y a no rendirse ante la aparición de dificultades. En todos los rubros evaluados, los estudiantes que tomaron el curso en línea superaron a los que no lo hicieron.De manera adicional, Boaler y colaboradores se interesaron en investigar el cambio de actitud de los estudiantes hacia las matemáticas después de tomar el curso en línea. Para este propósito, entrevistaron a 156 estudiantes antes y después de llevar a cabo la investigación y encontraron que dicho curso tuvo un efecto positivo en la confianza de los estudiantes en sí mismos para aprender matemáticas, así como en su percepción de las matemáticas como un tópico interesante y creativo.Así, según la tesis de Boaler y colaboradores, el cerebro tiene una plasticidad tal que todos tenemos la capacidad de aprender matemáticas a un nivel elevado. La creencia extendida según la cual hay personas que nacen con un cerebro no apto para las matemáticas no es entonces sino un mito que debe ser desechado. Igualmente, la concepción de las matemáticas como un tema aburrido, con numerosas fórmulas y procedimientos que hay que memorizar, debe ser superada. Sin estos obstáculos, las matemáticas presumiblemente se presentarían al común de la gente como un tema, ciertamente difícil, pero interesante, creativo y de gran utilidad, y sobre todo alcanzable.Para llegar a este punto en nuestro país, sin embargo, el camino se presenta azaroso dado que habría primeramente que entrenar a los maestros en las nuevas técnicas de aprendizaje de las matemáticas. Y esto no pareciera ser una tarea libre de obstáculos.",
    "Los especialistas del clima han llegado a la conclusión de que el calentamiento global que está experimentando el planeta ha incrementado el número de episodios climáticos extremos de calor o frío. Si bien algunos niegan la veracidad de esta conclusión, para un buen porcentaje de  aquellos que no somos especialistas en los aspectos técnicos del clima pero que sí sufrimos sus consecuencias, la opinión de los expertos no nos parece descabellada. Por lo demás, de un modo u otro, lo que sí es innegable es que los golpes de frío o calor son muy desagradables y hasta peligrosos. De hecho, sabemos que durante los episodios de clima extremo se incrementa el número de fallecimientos entre la población afectada. Por ejemplo, la ola de calor que azotó a Francia en el verano de 2003, provocó cerca de 15,000 fallecimientos. Se sabe también que en Balgladesh se incrementa en 20% el número de muertes durante los episodios de calor. Los golpes de frío son igualmente mortales y en 2003 en la India más de 900 personas murieron durante una ola de frío.Los números anteriores son consignados en la introducción de un artículo publicado el pasado 2 de mayo en la revista Environmental Research Letters por un grupo internacional de investigadores encabezado por Erin Coughlan de Perez del Red Cross Red Crescent Center situado en la Haya, Holanda. El propósito de dicho artículo es el de reportar los resultados de una investigación llevada a cabo para determinar la incidencia de eventos climáticos extremos de frío y calor a nivel global, y la posibilidad de predecir su ocurrencia con días de anticipación. Esto último con el fin de diseñar estrategias para mitigar sus consecuencias.Basados en datos climáticos de la Administración Nacional Oceánica y Atmosférica de los Estados Unidos y del Centro Europeo de Previsiones Meteorológicas a Plazo Medio, Coughlan y colaboradores desarrollaron mapas a nivel global mostrando la frecuencia de eventos climáticos extremos, tanto de frío como de calor. Dichos eventos climáticos se definen como aquellos en los cuales hay un incremento o disminución sustancial de temperatura más allá de su variabilidad habitual. Los mapas desarrollados muestran que los episodios de calor extremo se dan a lo largo de todo el planeta, con la excepción de algunas áreas tropicales en Indonesia y  del oeste de América del Sur y de África. En contraste, hay áreas extensas en donde los eventos climáticos extremos son escasos. Estas áreas se localizan en la parte norte del subcontinente sudamericano, en el centro de África, en la costa oeste de la India y en Indonesia. En general, hay menos episodios extremos tanto de frío como de calor en las regiones tropicales que en las latitudes altas Un aspecto importante de la investigación llevada a cabo por Coghlan y colaboradores fue el de la determinación a nivel global de la capacidad de predecir la ocurrencia de los eventos climáticos extremos. Esta capacidad depende de la disponibilidad de datos meteorológicos, la topografía del terreno y de las condiciones tecnológicas particulares a cada localidad. En su artículo, Coghlan y colaboradores incluyen mapas que muestran a nivel global esta capacidad en varios niveles. El máximo de estos niveles corresponde a una capacidad de predicción de un evento climático con 10 días de anticipación y se presenta, tanto para ondas de frío como de calor, en países del este de Europa, Rusia, el Medio Oriente y la región central de los Estados Unidos y Canadá. La capacidad de predicción en los trópicos varía grandemente y es muy baja en países de África Central y alta en Brasil. En términos generales, los investigadores encuentran que la capacidad de predicción para ondas de calor con 3 a 10 días de anticipación es alta en los países fuera de los trópicos y varía de lugar a lugar en los países tropicales. En términos globales Coghlan y colaboradores estiman que alrededor de 5,000 millones de personas viven en regiones cuyos golpes de calor pueden ser predichos con una antelación de 3 a 10 días y por tanto son susceptibles de mitigación empleando, además, procedimientos que no requieren de tantos recursos.En los mapas de Coghlan y colaboradores nuestro país muestra una ocurrencia moderada de eventos climáticos extremos con la excepción de la costa del Océano Pacífico. Por otro lado, la capacidad de predicción de ondas de calor varía grandemente, desde un nivel alto en el centro del país, hasta una capacidad baja en regiones de la costa del Pacífico. La capacidad de predicción de ondas de frío es también variable aunque más uniforme. Somos así un país moderadamente afortunado en cuanto a la ocurrencia de eventos climáticos extremos. Por el momento no lo somos tanto, sin embargo, en cuanto a la capacidad de predecirlos.",
    "Como sabemos, en el deporte del futbol –cuyo campeonato mundial está, por cierto, ya próximo a llevarse a cabo en Rusia– no está permitido tocar el balón con la mano, de modo tal que prácticamente la única opción que tiene un jugador con una pelota en el aire es golpearla con la cabeza. Por lo demás, esta posibilidad no es despreciada por los jugadores y no son raros los goles que se meten de este modo. Así, según estadísticas oficiales, en la liga Premier de Inglaterra, en lo que va de la presente temporada aproximadamente el 17% de los goles anotados han sido de cabeza. Estos son, por supuesto, números promedio y hay equipos y jugadores que se especializan en utilizar la cabeza para meter goles y en consecuencia sus porcentajes son sustancialmente mayores. Tomando nuevamente como ejemplo a la liga Premier, que ofrece estadísticas detalladas, encontramos que del total de 30 goles que tiene el equipo West Bromwich Albion en la temporada que está por finalizar, el 33% fueron anotados con la cabeza. De la misma manera, de los 107 goles que tiene en dicha liga el jugador Peter Crouch, el 36% los ha realizado con la cabeza.El que este último jugador meta tantos goles de cabeza no es de extrañar, habida cuenta que tiene una estatura de más de dos metros. La estatura, por otro lado, no es un factor determinante para que un jugador de futbol utilice la cabeza para meter goles y al respecto podemos mencionar al Chicharito Hernández, quien, con 1.75 metros de estatura tiene 45 goles en la liga Premier de los cuales el 27% fueron de cabeza.Y en cuanto a Pelé, al que muchos catalogan como el mejor jugador de la historia y quien ciertamente hacia maravillas con los pies, con sus 1.73 metros de estatura era también un cabeceador notable y marco tantos de cabeza memorables en las finales de los mundiales de Suecia en 1958 y México en 1970.El cabecear la pelota es sin duda uno de los elementos esenciales del juego del futbol. Esta práctica, sin embargo, ha levantado voces de alarma por parte de la comunidad médica que tiene indicios de que pudiera contribuir al desarrollo de problemas neurodegenerativos por las continuas sacudidas a las que se somete al cerebro. Esto fue abordado en una entrevista que realizo en 2014 la revista Scientific American a Robert Cantu, profesor de neurocirugía de la Boston Unversity. De acuerdo con Cantu, las sacudidas continuas a la cabeza pueden llevar a problemas de memoria, ansiedad y depresión que pudieran ser permanentes, aunque afirma que con respecto a esto último no hay certidumbre. El posible daño cerebral asociado a el cabeceo de un pelota en el juego del futbol es un tópico de actualidad y es discutido por un artículo publicado esta semana en la revista Frontiers in Neurology, por un grupo de investigadores en los Estados Unidos encabezado por Michael Lipton de Albert Einsten College of Medicine en Nueva York. En dicho artículo, Lipton y colaboradores presentan los resultados de un estudio llevado a cabo con 308 futbolistas aficionados del área de Nueva York, con edades entre los 18 y los 55 años y con más de 5 años de haber empezado a practicar el futbol durante cuando menos seis meses por año. El enfoque del estudio se centró en efectos neurológicos a corto plazo.Los voluntarios participantes promediaron 45 cabeceos a la pelota durante las dos semanas previas al estudio y un tercio del ellos sufrieron colisiones no intencionales, incluyendo patadas a la cabeza, choques de la cabeza con la de otro jugador, o contra el piso o la portería. Durante los experimentos, los participantes fueron sometido a pruebas de aprendizaje verbal y de memoria en las que se les pidió que recordaran listas de palabras aprendidas con anterioridad. Se midió también su velocidad psicomotora a través del número de movimientos que necesitaron para moverse a través de un laberinto, así como su velocidad para identificar el color de una carta de baraja, y su capacidad de atención para recordar si una determinada carta era la misma que otra mostrada anteriormente.Lipton y colaboradores encontraron una asociación entre la frecuencia de cabeceo a la pelota y una reducción en la habilidad psicomotora y de atención del jugador y en menor grado con su memoria de trabajo. No encontraron, por otro lado, una relación entre colisiones no intencionales y un deterioro neurológico. En estas condiciones, los investigadores hacen notar que las medidas preventivas actuales en el futbol, encaminadas a minimizar los impactos no intencionales a la cabeza, podrían ser insuficientes para la prevención de lesiones cerebrales en el caso posible de que los cambios neurológicos transitorios por del cabeceo del balón lleven a cambios patológicos a largo plazo.Si estudios adicionales confirman las conclusiones de Lipton y colaboradores ¿cambiarían las reglas del futbol, que es el deporte más extendido en el mundo? De ser éste el caso nos perderíamos de alrededor del 20% del número de goles –de por sí escaso– que actualmente se anotan en el futbol profesional, y de los goles de cabeza de un futuro –aunque improbable– Pelé. Dados los intereses económicos que giran alrededor del futbol la respuesta a la pregunta es posiblemente, no. Sí tendríamos, en cambio, que prohibir que los niños le peguen al balón con la cabeza, como lo ha hecho ya la Federación de Futbol de los Estados Unidos para niños menores de 10 años.",
    "Como sabemos, la Revolución francesa llevó a cambios políticos y sociales profundos en Francia que llevaron a la abolición de la monarquía y a la instauración de la primera república. Sabemos también que el impacto de dichos cambios no se limitó a Francia sino que se extendió a otras partes del mundo, incluyendo a lo que es hoy la América Latina, y que contribuyó de manera importante al desarrollo  de las democracias modernas. Durante la primera fase de la Revolución francesa, ocurrida durante los años 1789-1791, el sistema de gobierno de Francia se transformó de una monarquía absoluta en una monarquía constitucional y entre otros cambios abolió el feudalismo y el pago de diezmos a la Iglesia. Dichos cambios fueron resultado de discusiones que se dieron en el seno de la Asamblea Nacional Constituyente, la cual estaba compuesta por más de mil diputados representando al clero, a la nobleza y al Tercer Estado, formado éste último por los sin privilegios –campesinos, artesanos, comerciantes y mercaderes, entre otros habitantes urbanos–. El objetivo de los trabajos de la asamblea fue la de elaborar la primera constitución francesa.Con la composición de la Asamblea Nacional Constituyente y dada la trascendencia de los temas tratados en la misma, no es difícil entender que las discusiones llevadas a cabo hayan tenido momentos difíciles.  Si tomamos en cuenta, además, la tremenda polarización política y social que vivía Francia en esa época por la crisis de la monarquía de Luis XVI a la luz de la nuevas ideas propugnadas por la Ilustración, lo mismo que por la crisis financiera por la que atravesaba el gobierno por sus aventuras militares en el extranjero y la escasez de alimentos para la población. Con el objeto de entender la dinámica del análisis y discusión de las propuestas llevadas a la Asamblea Nacional Constituyente por los diputados y su eventual adopción o rechazo, un grupo interdisciplinario de investigadores encabezado por Alexander Barron de Indiana University en los Estados Unidos llevó a cabo un estudio de 40,000 iniciativas sometidas para la consideración de la asamblea. Los investigadores aprovecharon las transcripciones de las propuestas que han sido digitalizadas electrónicamente, de modo que pudieron hacer un análisis de textos por computadora empleando técnicas sofisticadas.Barron y colaboradores estaban interesados en determinar la novedad de cada una de las propuestas con respecto a otras sometidas con anterioridad, así como su trascendencia, medida ésta por su supervivencia en discusiones posteriores. Como resultado de su estudio, los investigadores encuentran una gran dispersión en cuanto a estos dos parámetros. Así, hubo iniciativas con una gran novedad pero que no lograron convencer a la asamblea y tuvieron poco impacto. Igualmente, hubo iniciativas poco novedosas pero que lograron permear en el ánimo de los diputados. Encuentran, no obstante, que en promedio las iniciativas más novedosas tuvieron mayor probabilidad de trascender que aquellas con menos novedad y que, en promedio, los diputados tendieron a someter propuestas novedosas. Típicamente, según Barron y colaboradores, durante las discusiones los diputados de izquierda defendían propuestas novedosas, mientras que los conservadores oponían resistencia a las mismas.  Por otro lado, para maximizar el impacto de una iniciativa jugaron un papel importante las habilidades oratorias del diputado que lo presentó. Según Barron y colaboradores, entre los más exitosos en este respecto están Jerome Petión de Villeneuve y Maximilien Robespierre. En particular, en las condiciones como se llevaron a cabo las sesiones con más de mil diputados, los investigadores hacen notar que el volumen de la voz tendría que haber sido un factor importante, dado que en la época todavía no se inventaban los micrófonos eléctricos. No obstante lo anterior, aun para aquellos diputados con pocas dotes para la oratoria existió una forma de contribuir a la revolución, pues Barron y colaboradores encontraron que no todas las ideas plasmadas en la primera constitución se originaron en discusiones en el pleno de las asamblea, sino en el seno de comités especializados formados con miembros de la misma. Esto quizá no resulta sorprendente si hemos de considerar las grandes dificultades que habrían surgido para trasmitir y discutir una iniciativa ante un grupo heterogéneo, y con intereses variados, de más de mil diputados. Ciertamente, la discusión de un asunto trascendente puede llevarse a cabo de manera más racional en un comité de tamaño reducido, lo que, por otro lado, puede llevar a resultados que no necesariamente maximizan el beneficio público. Así, sería necesaria una discusión abierta. De un modo u otro, sin embargo, nada está asegurado si consideramos que Robespierre, que pertenecía al ala de izquierda más radical de la asamblea y que era muy exitoso en la discusión pública como lo demuestran Barron y colaboradores, en una etapa posterior de la revolución francesa encabezó el llamado Reinado del Terror, durante el cual fueron guillotinadas unas 40,000 personas.",
    "Una característica del desarrollo de nuestra civilización desde tiempos prehistóricos hasta la época actual es el crecimiento  sostenido del consumo de energía per cápita. En efecto, hace un millón de años los antecesores de nuestra especie consumían solamente la energía que les proporcionaban los alimentos. Una vez que descubrieron cómo hacer uso del fuego para cocinar alimentos y para aliviar el frío, sin embargo, su consumo de energía per cápita se incrementó de manera consecuente. Lo mismo sucedió con desarrollos tecnológicos posteriores, como la agricultura, y la extracción y forja de metales para la fabricación de utensilios y herramientas.El consumo per cápita de energía se incrementó igualmente con la aparición de la rueda hidráulica hace unos dos mil años y posteriormente del molino de viento, lo mismo que con la invención de la máquina de vapor hacia finales del siglo XVIII, la sustitución de la madera combustible por el carbón, y la sustitución parcial de este último combustible por el petróleo en las primeras décadas del siglo XX. Finalmente, hay que mencionar el desarrollo de la industria eléctrica a finales del siglo XIX que impulsó de manera considerable el consumo de energía per cápita. Así, sumando todos los incrementos que se han dado a lo largo del tiempo, tenemos que desde tiempos prehistóricos hasta la época actual el consumo per cápita de energía se ha elevado más de cien veces. En estas condiciones, tomando en cuenta el incremento explosivo que ha tenido la población de mundo, no es de sorprender que el uso creciente que hemos hecho de la energía –que a lo largo de la segunda mitad del siglo pasado abrumadoramente provino de la quema de combustibles fósiles, petróleo, gas natural y carbón–, haya tenido consecuencias a nivel global. De manera específica, que haya ocasionado el cambio climático que está en curso.Para mitigar dicho cambio, los especialistas consideran que es necesario sustituir a los combustibles fósiles por fuentes de energía limpia, entre  las que se incluyen la energía hidroeléctrica –que aprovecha las caídas de agua, naturales o artificiales–, los biocombustibles –por ejemplo, el bioetanol, obtenido a partir del maíz o la caña de azúcar–, la energía del viento o eólica, y la energía solar. Si bien el origen de las energías hidroeléctrica, biocombustible y eólica es en último término el Sol, no dejan de ser, por así decirlo, una especie de intermediarios en la relación con nuestra estrella. Así, resultaría en principio más natural evitarlos en primera instancia y buscar aprovechar a la energía solar de manera directa. Una manera de hacerlo es a través de los llamados paneles solares que convierten la radiación del Sol directamente energía eléctrica; con la ventaja, además, de que ésta es el tipo de energía que nos es de más utilidad. En el mercado existen varias tecnologías  para la fabricación de paneles solares. La que domina, sin embargo, es la basada en el silicio, que es también el material empleado para construir microprocesadores y memorias para computadoras. Por otro lado, aunque los paneles solares han estado disponibles ya por varias décadas, su alto costo no había permitido su uso masivo en la generación de energía. Esto último, afortunadamente, ha dejado de ser cierto y la energía fotovoltaica –como es conocida la energía obtenida con dichos paneles– ha reducido sus costos de manera drástica y está irrumpiendo fuertemente en el campo de la generación de energía eléctrica. Cabe preguntarse si la energía fotovoltaica es una energía limpia. Al respecto podemos mencionar que si bien la operación de un panel solar no contamina, durante el proceso de fabricación del panel mismo sí se generó contaminación ambiental en cierto grado. En el caso de un panel de silicio, la fabricación incluye procesos que se llevan a cabo a altas temperaturas, las cuales se alcanzan quemando combustibles contaminantes del medio ambiente. En estas condiciones, para saber si en balance el uso de paneles solares ayuda al medio ambiente, es necesario cuantificar la energía empleada en la fabricación de un panel dado y estimar el tiempo que tardaría en generar dicha energía durante su operación en condiciones normales (tiempo de recuperación de energía). Si este tiempo es mayor que el tiempo estimado de vida del panel, entonces la opción fotovoltaica no sería una opción viable para proteger al medio ambiente.Afortunadamente, el balance es altamente positivo pues el tiempo de recuperación de energía para un panel de silicio operando en una región del sur de Europa es de poco más de un año, comparado con un tiempo estimado de vida del panel de 20 años. Esto, de acuerdo con el último reporte del Fraunhofer Institute for Solar Energy Systems en Alemania. Así, un panel solar de silicio durante su tiempo de vida entregará 20 veces más energía que aquella que se empleó en fabricarlo.Después de varias décadas de estar disponible en el mercado, los paneles fotovoltáicos han hecho finalmente su arribo al campo de la generación de energía eléctrica. ¿Podrán contribuir de manera apreciable a mitigar el cambio climático? Es difícil anticiparlo, pero de no hacerlo nos habrá fallado nuestra carta más fuerte. Y en tal caso quizá tengamos que revertir la tendencia de uso creciente de energía que hemos sostenido a lo largo de los últimos cientos de miles de años.",
    "En estos días de Internet y teléfonos inteligentes muchas personas han hecho rutina el uso del servicio que proporciona el Sistema de Posicionamiento Global o GPS para localizar y llegar a una dirección determinada. Si bien la irrupción del GPS para estos propósitos  ha sido  repentina, su adopción ha resultado natural entre los nativos digitales. No ha sido el caso, en cambio, para muchos de los que no nacimos rodeados de computadoras.Y mucho menos lo sería para aquellos que pasaron por este mundo hace siglos cuando el GPS hubiera sido algo inconcebible. Pensemos en los navegantes trasatlánticos – por ejemplo, Cristóbal Colón– que mantuvieron el rumbo en el mar mediante observaciones de la posición del sol y haciendo uso de la brújula. Al respecto, un caso interesante –y sorprendente como veremos en lo que sigue– es el de los vikingos, que viajaron regularmente por el Atlántico norte en los siglos X-XII, desde Noruega hasta Islandia y Groenlandia. Llegaron incluso hasta a la isla de Terranova en el norte del Canadá 500 años antes que Colón. Por esto último, según algunos autores, los vikingos serían los verdaderos descubridores de América –con permiso, por supuesto, de los ocupantes originales del continente americano, que cruzaron desde Siberia a Alaska bastante tiempo antes y que podrían protestar con toda justicia. Un aspecto notable de los viajes de los vikingos es que los hicieron sin emplear la brújula, que no apareció en Europa sino hasta el siglo XIII. Así, se habrían orientado por medio de la posición del sol. No queda claro, sin embargo, cómo habrían procedido en los días nublados.Con respecto a esto último, una hipótesis que se ha manejado desde hace algunas décadas es que los vikingos habrían orientado sus viajes por medio de ciertas rocas o cristales que tienen una propiedad conocida como birrefringencia. Esta propiedad hace que dichos cristales –por ejemplo, la calcita, de la cual existen depósitos en Islandia y por tanto estuvo al alcance de los vikingos– produzcan una imagen doble de un objeto visto a través de los mismos. Esta imagen doble está asociada a lo que se conoce como polarización de la luz para la cual existen dos posibilidades. Para mayor explicación, debemos primero mencionar que la luz es un cierto tipo de onda, tal como la que se produce en una cuerda a la que fijamos un extremo y agitamos el otro de arriba a abajo de manera periódica. La onda generada de esta manera tiene una polarización vertical. Podríamos, igualmente, agitar el extremo de la cuerda de izquierda a derecha, de donde resultaría una onda con polarización horizontal. Las dos imágenes que produce un cristal de calcita están asociadas a las dos polarizaciones de la luz. Es decir, la calcita tiene la habilidad de separar estas dos polarizaciones y por tanto se puede emplear como un detector de polarización.¿Cómo se conecta todo esto con los viajes de los vikingos? La conexión resulta por el hecho que la luz del cielo está polarizada en forma de círculos con el sol en el centro. De este modo, midiendo la polarización de la luz del cielo es posible en principio determinar la posición del sol oculto por la nubes. De este hecho es de lo que, según la hipótesis, se habrían valido los vikingos para orientar su navegación. Para determinar la polarización de la luz del cielo habrían empleado cristales de calcita o alguno equivalente. La hipótesis, sin embargo, tiene detractores que consideran que la precisión con que los vikingos pudieron haber hecho sus mediciones de polarización no era lo suficientemente buena para que tuvieran utilidad práctica.No fue necesariamente el caso, según un artículo publicado esta semana en la revista Royal Society Open Science por Dénez Száz y Gabor Horváth de Loránd University en Hungría. Para evaluar las posibilidades reales de que los vikingos hubieran podido viajar entre Noruega y Groenlandia sin una brújula, Száz y Horváth simularon por medio de una computadora viajes entre estos dos destinos, con diferentes grados de nubosidad y durante el solsticio de verano y el equinoccio de primavera. Simularon un total de 1,000 viajes con una duración de tres semanas cada uno. Asumieron que los navegantes fijaron la dirección norte a intervalos regulares de 1 a 6 horas, determinando la posición del sol invisible por medio de varios tipos de cristales birrefringentes, incluyendo cristales de calcita. De sus simulaciones, Száz y Horváth encuentran que con determinaciones de la posición del sol a intervalos de 1 a 3 horas los vikingos habrían alcanzado éxito en el 90-100% de los casos. En otras ocasiones se desviaron hacia el sur sin alcanzar Groenlandia. Habrían, sin embargo, arribado a la isla de Terranova y en este respecto los autores especulan si no fue esta la razón por las que los vikingos llegaron a tierras americanas. Los investigadores son, sin embargo, cautos y apuntan que sus estudio debe ser complementado por otros de mayor amplitud.Por lo pronto, Száz y Horváth apoyan con sus resultados la hipótesis según la cual los vikingos usaron técnicas sofisticadas para determinar la posición del sol cuando está oculto por la nubes. Esto, ciertamente, no está nada mal para haber sido hecho hace mil años. Y resulta, sin duda, sorprendente incluso para los nativos digitales.",
    "En un artículo publicado en la revista británica Nature el 23 de enero de 1896 con el título “Sobre una nueva clase de rayos”, el físico alemán Wilhelm Roentgen describe los resultados de sus investigaciones sobre las propiedades de unos misteriosos rayos que emanaban del tubo de Crookes con el que estaba trabajando.  Este tubo era un dispositivo relativamente común en los laboratorios de investigación sobre electricidad de la época y consistía en una ampolla de vidrio de forma cónica a la que se le extraía el aire y se le aplicaba un voltaje de varias decenas de miles de volts.  De acuerdo con lo que escribió Roentgen en su artículo, un tubo de Crookes emite una radiación de naturaleza desconocida –y que por lo mismo llamó rayos X– que tiene la propiedad de penetrar y traspasar a un gran número de materiales, incluyendo “un libro de mil páginas y un bloque grueso de madera”. Hace notar que los rayos X son incluso capaces de atravesar una lámina de aluminio o de vidrio de 15 milímetros de espesor. No es el caso, por el contrario, de una lámina de vidrio plomado “que resulta ser mucho más opaca”. Roentgen también menciona que los rayos X pueden penetrar el tejido orgánico e incluye en su artículo una radiografía de la mano de su esposa que muestra de manera clara la silueta de los huesos. Y con esto capturó la imaginación popular. Como sabemos, el descubrimiento de Roentgen ha tenido un impacto creciente en el diagnóstico médico en la medida en que se han desarrollado técnicas con rayos X cada vez más sofisticadas, las cuales nos permiten ver el interior del cuerpo cada vez con mayor detalle. Todo esto, sin embargo, tiene un costo, pues si bien con el tiempo hemos averiguado que la naturaleza de los rayos X es la misma que la de la luz visible, igualmente nos hemos enterado que tienen una energía considerablemente más grande que los hace peligrosos. Así, pueden provocar daños al tejido orgánico dependiendo de la dosis recibida, la que crece en la medida en que crece el detalle con que se quiere observar el interior del cuerpo. En el contexto anterior, existen esfuerzos por disminuir la dosis de rayos X necesarias para el diagnóstico médico y en esa dirección apunta un artículo publicado esta semana en la revista Optica por un grupo de investigadores de la Academia China de Ciencias, encabezado por Ai-Xin Zhang. En dicho artículo, Zhang y colaboradores reportan el desarrollo de un método que emplea una cámara de un solo pixel para obtener una imagen de rayos X con dosis de radiación ultra bajas.Si bien para quienes nacimos antes de la época de la cámara digital el término pixel resulta novedoso, los jóvenes saben que las cámaras digitales se clasifican por su número de pixeles y que la nitidez de las fotografías que pueden tomar depende de dicho número. En efecto, en una cámara digital, el negativo –que es en donde el lente de la cámara enfoca la imagen que se quiere imprimir– es sustituido por un chip de silicio formado por una gran cantidad de pequeños elementos (pixeles) sensibles a la luz. Dichos elementos o pixeles están distribuidos en una malla cuadrada a manera de un tablero de ajedrez, y cada uno de ellos es el encargado de generar la parte de la fotografía que le corresponde. Es claro, entonces, que la finura de los detalles de la fotografía dependerá de número de pixeles del chip de la cámara. Es claro, igualmente, que la cámara de un solo pixel de Zhang y colaboradores no puede generar una imagen de rayos X de la misma manera en que lo hace una cámara normal. De hecho, la fotografía que generaría no tendría en absoluto detalles y más se asemejaría a un negativo fotográfico que hubiera sido velado de manera uniforme exponiéndolo a la luz. ¿Cómo logran Zhang y colaboradores imprimir sus fotografías de rayos X? Lo hacen empleando un ingenioso método que consiste en tomar un gran número de fotografías, cada una de ellas haciendo pasar los rayos X por un filtro de papel lija puesto a rotar. Los rayos X al cruzar por el papel lija generan un patrón cambiante de iluminación de zonas oscuras y brillantes que se modifica en la medida en que se rota el filtro, de modo que cada “fotografía” que toma el único pixel de la cámara igualmente se modifica. La imagen es entonces sintetizada por la computadora a partir de todas las imágenes obtenidas empleando un procedimiento matemático sofisticado.Si bien la técnica descrita por Zhan y colaboradores no está todavía lista para su uso en el diagnóstico médico, sí pudieron los investigadores obtener imágenes empleando dosis ultra-bajas de rayos X con una calidad muy superior a las que se obtienen con la técnica tradicional empleando dosis similares de radiación. Esto, por supuesto, la hace muy atractiva para aplicaciones futuras. El descubrimiento de los rayos X por Roentgen fue, sin duda, un acontecimiento espectacular que ciertamente capturó la imaginación popular. A más de un siglo de dicho descubrimiento, el desarrollo de técnicas cada vez más sofisticadas para ver lo oculto a simple vista no provocan quizá la misma expectación. Si bien, estaríamos de acuerdo en que la posibilidad de ver de manera indirecta el interior del cuerpo humano por medio de una cámara de un solo pixel, un papel de lija y un algoritmo matemático, escapa a todo lo que la imaginación popular podría haber anticipado.",
    "En medio del Océano Pacífico, entre Hawái y California, se encuentra la llamada isla de basura, que tiene una extensión territorial casi igual a la de nuestro país. Por supuesto, si buscamos en el mapa una isla de este tamaño entre Hawái y California no la encontraremos, dado que no existe. Lo que sí existe es una extensión de 1.6 millones de kilómetros cuadrados de basura en flotación, mayormente materiales de plástico. La isla de basura, o gran mancha de basura del pacífico como también se le conoce, es el resultado de la acumulación de materiales de desecho que son arrastrados por el giro del pacífico –una corriente marina en forma de remolino provocada por la rotación de la Tierra–. Fue descubierta en 1997 y no es, por lo demás, un caso único, ya que en otras partes del Océano Pacífico, y en otros océanos también, se generan áreas de acumulación de basura.¿De dónde proviene el plástico acumulado en la gran mancha del pacífico? Dada la gran cantidad de materiales plásticos que usamos y rápidamente desechamos todos los días, podríamos pensar que la isla de basura está formada, fundamentalmente, por las botellas, vasos, bolsas, empaques, etc. de plástico que una vez desechados alcanzan las costas del pacífico, y de ahí son transportados por las corrientes marinas hasta el punto de acumulación. Esto no es necesariamente cierto, sin embargo, de acuerdo a un estudio de la organización Clean up Foundation, publicado esta semana en la revista Scientific Reports por un grupo internacional de investigadores, encabezado Laurent Lebreton de dicha organización. Clean up Foundation es una organización no lucrativa que ha recibido fondos para llevar a cabo un proyecto que busca limpiar de basura la gran mancha del pacífico. Con el objeto de cuantificar la cantidad y los tamaños de la basura en flotación en esta zona, Lebreton y colaboradores llevaron a cabo durante los meses de julio y septiembre de 2015 una extensa operación de recolección de basura empleando redes de diferente tamaño. Para complementar esta operación, en octubre de 2016 llevaron a cabo dos vuelos para la toma de imágenes aéreas de la isla de basura.De estas dos campañas de recolección de datos, Lebreton y colaboradores pudieron cuantificar el tamaño del problema. Así, calculan que dentro de área de 1.6 millones de kilómetros cuadrados que comprende la gran mancha de pacífico, se encuentran flotando unas 80,000 toneladas de basura de plástico de todos tamaños, desde pequeñas partículas de unos pocos milímetros –micro-plásticos, de los cuales calculan hay unos dos billones flotando–, hasta objetos mayores que 50 centímetros. Naturalmente, los micro-plásticos, que resultan de la desintegración de los objetos originales, superan ampliamente en número a la cantidad objetos flotando, todavía en etapa de fragmentación. En cuanto al peso de unos y otros la situación se invierte, y los micro-plásticos representan apenas el 8% de peso del total de basura flotante.Además, Lebreton y colaboradores encuentran que, de manera inesperada, al menos la mitad de los objetos de basura recogidos en la campaña tienen un origen marino y no terrestre. En particular, las redes para pescar representan un 46% del peso total de la basura recolectada.    En algunos casos, cuando no había avanzado aún su proceso de desintegración, Lebreton y colaboradores pudieron averiguar la procedencia de un objeto. Así, de entre aquellos recolectados a lo que se les pudo identificar una leyenda escrita, en el 30% de casos el idioma de la leyenda fue el japonés, seguido cercanamente por el chino con un 29.8%. Se esperaría que el tsunami de Tohoku que asoló a la costa norte de Japón en 2011 fuera responsable del gran número de objetos de origen japonés que se encontró en la isla de basura. De hecho, Lebreton y colaboradores estiman que de un 10% a un 20% del peso total de la basura flotante tuvo su origen en dicho tsunami.  Por otro lado, que el 30% de los objetos rescatados fuera hecho en China no resulta, por supuesto, sorprendente. En algunos casos lo investigadores pudieron determinar también el año de fabricación de los objetos recogidos. Así, de 50 objetos con dicha fecha identificable, 1 objeto fue fabricado en 1977, 7 en la década de los ochenta, 17 en los noventa,  24 entre 2000 y 2009 y uno en 2010. De acuerdo con los resultados de Lebreton y colaboradores, la mitad de la basura que recolectaron tiene un origen marino, lo cual no esperaban dada la cantidad de desechos de plástico de origen terrestre que es arrojado al mar. Concluyen así que la mayor parte de esta basura no flota o bien que existen mecanismos que la fragmentan en partículas tan pequeñas que no pudieron ser detectadas en el estudio. Aun en estas circunstancias, Lebreton y colaboradores encuentran que las 80,000 toneladas de basura de plástico que calculan flota  en la gran mancha del pacífico es de 4 a 16 veces más grande que lo que previamente se había estimado, lo que sería una indicación que la contaminación del planeta por plásticos está creciendo rápidamente. Y esto pasa en cierta inadvertido, en medio de las múltiples contaminaciones a las hemos sometido al planeta. Pues ¿que le hace una mancha mas al tigre?",
    "A lo largo de la última semana, los medios de comunicación han dado cuenta de un artículo publicado recientemente por Richard Jantz de la University of Tennessee en Knoxville en  la revista Forensic Antropology. Dicho artículo es relativo a la suerte que habría corrido la aviadora estadounidense Amelia Earhart en su malhadado viaje de circunnavegación del mundo en 1937. Como sabemos, Amelia Earhart fue una piloto de avión estadounidense, famosa por haber sido en 1932 la primera mujer en cruzar el Atlántico sola en un vuelo sin escalas, y por haber sido la primera persona en volar sola entre Hawái y California en 1935. Después de estos logros, Earhart se propuso realizar un vuelo de circunnavegación y para este propósito partió el 21 de mayo de 1937 desde Oakland hacia Florida en compañía de Fred Noonan en calidad de navegante. De Florida volaron hacia Puerto Rico y de ahí a la costa atlántica sudamericana. Desde Natal, Brasil, cruzaron el Océano Atlántico hasta Senegal en África, atravesaron el continente africano y volaron directamente hasta Karachi. A partir de ahí el itinerario de vuelo incluyó Calcuta, Rangún, Bangkok, el norte de Australia y Nueva Guinea, a donde arribaron el 29 de junio. Prosiguiendo con su viaje, el 2 de julio Earhart y Noonan partieron hacia la pequeña isla de Howard en medio del Océano Pacífico, distante unos 4,000 kilómetros. Nunca arribaron, sin embargo, corriendo una suerte que es un misterio hasta la fecha.  Una posibilidad es que tuvieron dificultad para localizar la isla de Howard y que se les agotó el combustible estrellándose en el mar. Una explicación alternativa es que lograron llegar como náufragos a una isla cercana a Howard en donde habrían muerto por enfermedad o inanición.  No hay certezas del destino que tuvieron, sin embargo.Una expedición británica de 1940 descubrió en el atolón de Nikumaroro, a unos 650 kilómetros de Howard, restos óseos humanos que se pensaron podrían pertenecer a Earhart. Después de examinarlos, sin embargo, el doctor David Hoddles de la Escuela Médica Central de Fiji, determinó que pertenecían a una persona del género masculino, de origen europeo o medio europeo, y de complexión robusta, desechando de este modo que pudiera tratarse de Amelia Earhart.   El asunto no quedó ahí, sin embargo, y desde entonces las conclusiones de Hoddles han sido motivo de controversia. Así, en 1998 un grupo de investigadores encabezado por Karen Burns de North Carolina State Unversity, escribieron un reporte en el que rechazan dichas conclusiones. Entre otras cosas descalifican a Hoddles, quien consideran no tenía las formación profesional suficiente para emitir una opinión experta sobre los restos óseos encontrados en Nikumaroro.     En respuesta, en un artículo publicado en 2015 en la revista Journal of Archaeological Science: Reports, Pamela Cross de University of Bradford en el Reino Unido y Richard Wright de University of Sydney, Australia, argumentan que Hoddles sí estaba calificado para evaluar los restos óseos y defienden sus conclusiones.Así las cosas, el último episodio de la controversia es el artículo mencionado al inicio de este texto recientemente publicado por Richard Jantz –quién, por cierto, es coautor del artículo de Karen Burns–,  en el que expresa su desacuerdo con las conclusiones, tanto de Hoddles como de  Cross y Wright, acerca del sexo de los huesos de Nikumaroro. En particular, a Hoddles lo descalifica por los métodos empleados en su análisis que, ciertamente, fueron propios de un tiempo en el que la ciencia forense no había alcanzado el grado de desarrollo que tiene hoy en día.El problema para una identificación concluyente de dichos huesos es que no están ya más disponibles para su estudio, pues desaparecieron después de haber sido examinados en 1940.  De este modo, la única información de la que disponen los expertos son la mediciones del tamaño de los huesos realizadas por Hoddles y las conclusiones a la que llegó. Con una muestra de restos óseos la controversia se habría podido resolver mediante un análisis de ADN. Al no contar con esta muestra, el problema se ha complicado de manera considerable. De hecho, para intentar resolverlo Jantz tuvo que hacer uso de las mediciones realizadas por Hoddles –a pesar de las descalificaciones de las que lo hace objeto– y de estimaciones de las dimensiones de los huesos de Earhart hechas a partir de fotografías. De acuerdo con Jantz, no hay evidencias para descartar que los huesos hallados en Nikumaroro pertenezcan a Amelia Earhart; y si las hay, por el contrario, para concluir que si no son de Earhart son de alguien con un físico similar. ¿Sabremos con certidumbre algún día cómo terminó sus días Amelia Earhart? De acuerdo con los expertos posiblemente no, por la poca información de que disponemos, y con esto continuará la controversia.  En los primeros años de la aviación los logros de Amelia Earhart fueron ampliamente publicitados. Lo fueron en un grado tal que, aun hoy en día, Earhart es motivo de atención en lo medios de comunicación y de que una compañía haga negocios vendiendo una muñeca con su efigie. Esto, de poder enterarse, seguramente no le sería desagradable. Al contrario de lo que, sin duda, opinaría Hoddles.",
    "El día primero de septiembre de 1923 fue un mal día para el Japón: a las 11:58 a.m. un terremoto de magnitud 7.9 seguido de un tsunami con olas de 10 metros de altura golpearon a las ciudades de Tokio y Yokohama. Según el sitio de internet del Instituto Smithsoniano, alrededor de 140,000 personas murieron, tanto en los primeros momentos del terremoto como por los incendios y tormentas de fuego que le siguieron.Durante la confusión que provocó el terremoto de Tokio se esparcieron rumores falsos según los cuales los coreanos residentes en la ciudad estaban realizando actos de pillaje y provocando incendios de manera intencionada. Entre otras falsedades, los coreanos fueron también acusados de envenenar los pozos de agua potable que abastecían a la ciudad.  Como resultado, más de 6,000 coreanos fueron asesinados por la policía, los soldados, o por grupos organizados de civiles japoneses. Con la circunstancia, además, de que algunos japoneses corrieron con la misma suerte al ser confundidos con coreanos.  Como sabemos, la difusión de noticias falsas, por ignorancia o con propósitos definidos, ha sido una constante a lo largo de la historia de la civilización. Y lo sigue siendo en la actualidad –aunque no necesariamente con consecuencias tan dramáticas en pérdidas de vidas como las de Tokio en 1923– con la ayuda, además, de las nuevas tecnologías de comunicación vía internet que, entre otras cosas, permiten la diseminación de noticias falsas desde sitios que aparentan tener la misma seriedad que otros sitios de medios de comunicación reputados. Con el objeto de investigar cómo difiere la dinámica de la difusión de información verdadera de aquella que es falsa, tres investigadores del Instituto Tecnológico de Massachusetts encabezados por Sinan Aral, llevaron a cabo un estudio con alrededor de 126,000 mensajes retrasmitidos en Twitter por 3 millones de personas un total de 4.5 millones de veces, durante los años 2006 y 2017. Las noticias incluidas en el estudio fueron clasificadas con verdaderas o falsas empleando la información proporcionada por 6 empresas dedicadas a la comprobación de hechos. Dichas empresas estuvieron de acuerdo en sus apreciaciones en un 95-98% de casos. Los resultados de este estudio fueron publicados esta semana en la revista “Science”.   Aral y colaboradores investigaron el número de veces que un tuit es retrasmitido y el número de personas que lo reciben. De manera interesante, encontraron que las noticias falsas se diseminan con una mayor amplitud y rapidez que las noticias verdaderas.  Así, mientras que menos de un 0.1% de las noticias verdaderas alcanzaron a llegar a 1,000 usuarios, alrededor de un 1% de las noticias falsas alcanzaron el mismo número de personas. Igualmente, una noticia verdadera tardó seis veces más tiempo que una noticia falsa en alcanzar a 1,500 usuarios. Con el objeto de determinar si los bots –o sitios en los que de manera automática se reproduce un mensaje– son responsables de la velocidad de difusión y penetración de las noticias falsas, Aral y colaboradores eliminaron los tuits originados en sitios bot empleando sofisticadas técnicas de detección. Los resultados obtenidos, sin embargo, no se modificaron, lo que implica que los culpables de la mayor diseminación de noticias falsas no son las computadoras sino los humanos.Todo lo anterior es particularmente cierto de las noticias falsas con contenido político, que Aral y colaboradores encuentran viajan más rápido y alcanzan más usuarios de Twitter que cualquier otra categoría de noticias falsas. En efecto, los investigadores encuentran que las noticias políticas falsas llegan a 20,000 usuarios seis veces más rápido que lo que les toma a otras noticias falsas alcanzar  a 10,000 receptores.¿Cuál es la razón por la que las informaciones falsas viajan más rápido que las verdaderas? Aral y colaboradores descartan que la explicación resida en que los tuiteros proclives a diseminar falsedades tengan más seguidores, pues encuentran que sucede justamente lo contrario. En lugar de esto, basan su explicación en la novedad de la noticia a trasmitir,  que hacen notar es mayor para las noticias falsas que para las verdaderas, y que provoca emociones de sorpresa y disgusto que impulsan al tuitero a compartirlas. La novedad de la información y las emociones que provocan serán entonces las impulsoras de las noticias falsas, al menos según Aral y colaboradores. Por otro lado, si bien por si mismas las noticias falsas no son ninguna novedad, en los tiempos actuales de mundos virtuales, redes sociales y comunicación instantánea adquieren una nueva dimensión que apenas se está empezando a explorar. Pasará así algún tiempo antes de que se clarifique cómo se propagan las falsedades y se desarrollen métodos para combatirlas. En tanto esto sucede, lo más prudente es aplicar el sentido común y pensarlo dos veces antes de retransmitir un mensaje hacia el ciberespacio. Sobre todo si el mensaje en cuestión tiene una carga política.",
    "Si bien la cirugía plástica reconstructiva se ha practicado desde tiempos muy remotos, la cirugía cosmética, empleada para modificar la apariencia de una parte del cuerpo, apenas se inició hacia el final del siglo XIX. Hay una buena razón para que así haya sido, dado que los anestésicos usados en las operaciones quirúrgicas hicieron su aparición sólo hasta mediados del siglo XIX. Antes de esto, para que alguien decidiera someterse a una operación quirúrgica tendría que haber tenido una muy buena razón, mucho más allá del simple deseo de lucir de manera diferente.La situación cambió cuando fueron posibles las operación quirúrgicas sin dolor y con esto la cirugía estética se expandió y se convirtió en un negocio. En particular, en los Estados Unidos se habría generado un cierto mercado para la cirugía de nariz por el deseo de las minorías étnicas de lucir como el grueso de la población blanca. Por esta y por otras razones, y de acuerdo con estadísticas de la Asociación Americana de Cirugía Plástica Estética, en el año 2016 se llevaron a cabo en los Estados Unidos alrededor de 150,000 cirugías cosméticas de nariz.Para saber como lucimos y contar así con elementos de juicio para tomar o no la decisión de someternos a un cirugía cosmética, tenemos a nuestra disposición espejos, y cámaras fotográficas y de video. Estos dispositivos nos dan una imagen objetiva de nuestra apariencia física que, no obstante, corresponde a una cierta perspectiva y que subjetivamente evaluamos en función de la misma. En otras circunstancias y desde otro ángulo de visión, la evaluación subjetiva que hiciéramos de nuestra apariencia física pudiera ser diferente.Con relación a esto último, es interesante hacer notar que la práctica extendida de las auto-fotografías o “selfies” está produciendo una curiosa demanda de cirugías cosméticas de nariz, al menos en los Estados Unidos. En efecto, de acuerdo con la Academia Americana de Cirugía Facial Plástica y Reconstructiva, en 2017 el 55% de los cirujanos plásticos reporta saber recibido solicitudes de cirugía nasal de pacientes que consideran que su nariz es demasiado grande a juzgar por sus fotografías “selfie”. En 2016, sólo el 16% de los cirujanos plásticos reporta haber recibido solicitudes similares, por lo que la   demanda por cirugías nasales  está en rápido crecimiento. El tamaño aparente de la nariz en una fotografía depende, por supuesto, de la distancia a la que se tome. Esto es una simple consecuencia de que, como bien sabemos, las cosas que están más cerca del punto de observación crecen en tamaño relativo con respecto a los objetos más alejados. Así, la nariz, que está sensiblemente más cerca de la cámara que el resto de la cara en una fotografía “selfie”, aparenta tener una mayor tamaño.  Una cuantificación del efecto de agrandamiento aparente de la nariz fue publicado esta semana en la revista  “JAMA Facial Plastic Surgery” por un grupo de investigadores de la Escuela de Medicina de la Universidad Rutgers en Nueva Jersey, encabezados por Boris Paskhover. De acuerdo con Paskhover y colaboradores, al tomar una fotografía de una cara típica a una distancia de 30 centímetros, la nariz aparenta tener un tamaño 30% más grande con respecto a una fotografía tomada a una distancia de cuando menos 1.5 metros. Así, como lo espejos con distorsión, las “selfies” producen una imagen deformada de la cara que retratan y de esto el fotógrafo debe estar consciente.El efecto de las “selfies” sobre la demanda de operaciones quirúrgicas nasales resulta sin duda sorprendente, y completamente inesperado, como lo han igualmente sido otros efectos que han producido los teléfonos inteligentes. Al respecto, pensemos, por ejemplo, en el fenómeno común y sorprendente que se produce cuando un grupo de personas están al mismo tiempo juntas y aisladas viajando en el ciberespacio por medio de sus teléfonos celulares. Por otro lado, para beneficio de aquellos que estén preocupados por el tamaño de su nariz en las fotografías “selfies” –que, presumiblemente, estarían destinadas a  vivir en el ciberespacio– se les recomendaría que procuren tomarlas a una distancia más grande. Si esto último no es aceptable, queda la posibilidad de editarlas digitalmente. O, como recurso extremo, someterse a una cirugía nasal, lo que presumiblemente mejoraría su apariencia física en el mundo virtual, pero la empeoraría en el mundo real. Y, llegado a este punto, el fotógrafo/cibernauta tendría que decidir cual de los dos mundos tendría prioridad. Por otro lado, y al margen de las cavilaciones anteriores, tal parece que el desarrollo y combinación de anestésicos, técnicas quirúrgicas nasales y teléfonos inteligentes ha dado un giro inesperado.",
    "En 1856 fueron descubiertos en el Valle del Neander, en lo que hoy es territorio alemán, restos fósiles que pronto fue claro pertenecían a una especie humana hasta entonces desconocida. Diez años después, el biólogo alemán Ernst Haeckel propuso el nombre “Homo stupidus” para esta nueva especie, dando por descontada su inferioridad intelectual. Para suerte de los neandertales –como hoy conocemos a los miembros de la especie descubierta en el Valle del Neander–Haeckel no tuvo éxito con su propuesta y prevaleció en cambio el nombre “Homo neanderthalensis”, publicado dos años antes por el geólogo británico William King.Desde una perspectiva europea del siglo XIX, no sorprende demasiado que Haeckel haya prejuzgado la capacidad intelectual de la especie neandertal y haya asumido que tendría que haber sido por necesidad inferior a la de nuestra especie. De hecho, Haeckel era racista y sostenía que la raza europea era superior a todas las demás, que estaban en un estadio inferior de desarrollo. Así, si entre las razas humanas, según Haeckel, existen rangos de capacidad intelectual, entre especies las diferencias en este sentido tendrían que ser abismales.  Con el transcurrir de los años, no obstante, nuestras ideas acerca de los neandertales y sus capacidades intelectuales han evolucionado de manera considerable en su favor, y hoy en día se considera que éstas no eran muy diferentes de las nuestras. Así, los neandertales podían hablar, y fabricar herramientas y adornos corporales. También tenían la costumbre de enterrar a su muertos, lo que indica que podían pensar de manera simbólica. Y por si fuera poco, tal parece que también hubo neandertales artistas. Esto último es lo que concluye un estudio publicado esta semana en la revista “Science” por un grupo internacional de investigadores encabezado por Dirk Hoffmann del Instituto Max Planck en Leipzig, Alemania, en el que reportan el fechado de pinturas rupestres descubiertas en cuevas de España. Los investigadores encuentran que las pinturas estudiadas fueron hechas en una época sorprendentemente lejana, cuando los hombres modernos no habían aun arribado a Europa, la cual estaba habitada sólo por neandertales. La investigación de Hoffmann y colaboradores se llevó a cabo en tres cuevas: La Pasiega, en el norte de España, cerca de Bilbao, Matravieso en el occidente, y Ardales en el sur, cerca de Málaga. No se fecharon las pinturas mismas sino de los grumos de carbonatos depositados sobre las mismas, los cuales se formaron después de que fueron realizadas. De esta manera, se determinaron edades mínimas para las pinturas. El fechado se llevó a cabo mediante una sofisticada técnica que mide las composiciones de los elementos radiactivos uranio y torio en los carbonatos, las cuales varían de una manera característica a lo largo del tiempo.En La Pasiega las pinturas incluyen una figura en forma de escalera cuya antigüedad es de más de 64,800 años. En Maltravieso se puede ver la silueta de una mano realizada apoyando una mano sobre la pared de la cueva y rociando pintura por medio de la boca. Se encontró que esta figura tiene una antigüedad de más de 66,700 años. Finalmente, en Ardales, la pared de la cueva tiene depósitos minerales que le dan la apariencia de una cortina vertical y que han sido pintados de rojo. En este caso, la pintura tiene una antigüedad mínima de 65,500 años.En todos los casos, las pinturas fueron realizadas cuando menos 20,000 años antes de la irrupción del hombre moderno en la Península Ibérica y esto descarta que los autores de las mismas hayan pertenecido nuestra especie. La conclusión inevitable es que fueron elaboradas por neandertales, que se sabe eran los únicos humanos que poblaban España en la época por la cantidad de restos fósiles que se han encontrado. Así, la mano que dejó su impresión en la cueva de Maltravieso habría pertenecido a un neandertal que vivió hace más de 60,000 años –lo que no deja de ser fascinante.   De este modo, resulta que las capacidades intelectuales de los neandertales –que podrían haber llegado a ser conocidos como los “estúpidos” de haber tenido éxito la propuesta de Haeckel– fueron definitivamente menospreciadas por los prejuicios raciales europeos del siglo XIX. Dichos prejuicios fueron alimentados por el éxito que tuvo Europa en la conquista del mundo en los últimos siglos. En este contexto, el hecho de que nuestra especie haya sobrevivido y que los neandertales se hayan extinguido, fue para algunos prueba más que convincente para demostrar la inferioridad  de estos últimos.No obstante, los resultados de Hoffmann y colaboradores, al igual que los de otros investigadores, demuestran que las cosas no son tan sencillas, y que si bien es cierto que los neandertales se extinguieron hace unos 30,000 años, las causas por lo que esto ocurrió no están relacionadas con su supuesta inferioridad intelectual. Así, hay incluso quien especula que de no haber desaparecido, los neandertales podrían quizá haber llegado hasta la Luna.",
    "Como fue difundido en los medios masivos de comunicación, el pasado mes de octubre fue descubierto por astrónomos de la Universidad de Hawaii un objeto desconocido –bautizado como Oumuamua– con una forma inusualmente alargada, de unos 230 metros de largo y 35 metros de ancho, y un origen probablemente más allá del Sistema Solar. Si bien finalmente los astrónomos llegaron a la conclusión que se trataba de un objeto inanimado, compuesto posiblemente de hielo y con una cubierta exterior de material orgánico, su forma peculiar motivó especulaciones en el sentido de que se trataba de una nave interestelar de visita en nuestro sistema solar. De haber sido esto último cierto ¿cómo habríamos reaccionado? Posiblemente con preocupación –por no decir pánico– si hemos de juzgar por la alarma pública que provocó la dramatización hace ocho décadas de la novela “La guerra de los mundos” del escritor británico H.G. Wells. Como sabemos, Orson Welles transmitió por radio en la ciudad de Nueva York en octubre de 1938 una dramatización de dicha novela en la que los marcianos invaden la Tierra, y aunque al principio del programa se advirtió que se trataba de una ficción, algunos radioescuchas que no oyeron esta advertencia se tomaron el relato en serio y entraron en pánico.  En un artículo publicado el pasado mes de febrero en la revista “Frontiers in Psychology” por un grupo de investigadores de la Universidad Estatal de Arizona, encabezados Jung Yul Kwon, se reportan los resultados de un estudio llevado a cabo para responder a la pregunta anterior. En particular, para arrojar luz sobre cuál sería nuestra reacción, positiva o negativa, ante el descubrimiento de vida microbiana extraterrestre. En un primer experimento, llevado a cabo con 500 participantes, se les pidió a los mismos que imaginaran un escenario en el que se anuncia el descubrimiento de vida extraterrestre microbiana y que relataran sus reacciones al respecto. De la misma manera, se les pidió su opinión sobre la reacción que tendría la población en general. Empleando un software especializado, los investigadores analizaron las respuestas de los participantes buscando palabras que denotaran actitudes positivas y negativas ante el supuesto anuncio, encontrando en ambos casos una clara preponderancia de las primeras.  En un segundo experimento, Jung Yul Kwon y colaboradores investigaron las reacciones de 501 participantes, no a un hipotético descubrimiento de vida extraterrestre, sino a una noticia publicada por el periódico “New York Times” en 1996, en la que se anunciaba el descubrimiento de un meteorito en Antártida de origen marciano con los restos fosilizados de un microbio extraterrestre –lo que es, sin embargo, motivo de controversia–. Al igual que en el primer estudio, los investigadores encontraron de un análisis de las respuestas más actitudes positivas que negativas al descubrimiento de vida extraterrestre.Por otro lado, si bien el artículo de Jung Yul Kwon y colaboradores se centra en la vida microbiana extraterrestre, también describe un estudio piloto llevado a cabo para evaluar la respuesta social, a través de artículos publicados en los medios masivos de comunicación, que en el pasado se ha dado ante el anuncio de descubrimientos que fueron relacionados con la existencia de vida inteligente extraterrestre. Entre éstos se incluye el descubrimiento de fuentes de radiación que inicialmente se pensó podrían ser señales trasmitidas por seres extraterrestres, así como el descubrimiento de numerosos exoplanetas parecidos a la Tierra con las condiciones de albergar vida inteligente. Al igual que en los dos estudios mencionados con anterioridad, los textos periodísticos analizados en el estudio piloto, los investigadores encontraron más reacciones positivas que negativas.     Habrá, por supuesto, diferencias entre la percepción del peligro que nos representarían los microbios extraterrestres confinados en el planeta Marte o una civilización inteligente situada a miles de años luz de distancia que poco daño nos podría hacer, y la ansiedad que nos produciría la presencia en la vecindad de nuestro planeta de una raza de seres extraterrestres inteligentes con el potencial de invadirnos. Y esto, sin duda, nos despertaría más emociones negativas que positivas. Por lo demás, como la probabilidad de que seamos invadidos por los extraterrestres es prácticamente nula, poca utilidad tiene hacer consideraciones al respecto. En contraste, el descubrimiento de microbios extraterrestres en el Sistema Solar, que constituye nuestro vecindario, sí entra en la esfera de lo posible. Y de lo agradable, si hemos de creerle a Jung Yul Kwon y colaboradores.",
    "El cambio climático y calentamiento global que está sufriendo nuestro planeta es un tema de preocupación tanto para especialistas como para legos en la materia. Es el caso, por ejemplo, de Jessie Diggins, miembro de equipo de ski de los Estados Unidos que está compitiendo en los juegos olímpicos de invierno que se inauguraron el pasado viernes en Corea del Sur.  En una entrevista aparecida hace unos días en el periódico “New York Times”, Diggins se queja de que la temporada de nieve en las pistas de ski en los Estados Unidos en cada vez más corta en la medida en que avanza el cambio climático. Habiendo crecido en Afton, un pueblo del estado de Minnesota, Diggins aprendió a esquiar desde niña y está preocupada por la posibilidad de que sus hijos no tengan la misma oportunidad. Hay también expertos preocupados por el proceso de calentamiento global que podría tener, en su opinión,  efectos desastrosos sobre el clima del planeta. Como sabemos, el calentamiento global es resultado de la emisión creciente de gases de invernadero a la atmósfera por el uso de combustibles fósiles. Dada esta situación, es entonces imperativo moderar el uso de dichos combustibles e incentivar el uso de las energías renovables no contaminantes.Se ha propuesto que una vía para lograr lo anterior es la eliminación de los subsidios a los combustibles fósiles –que se aplican tanto en su producción como en su consumo– cuyo precio quedará así determinado por su valor de mercado. El razonamiento en que se apoya dicha propuesta es muy simple: si se eliminan los subsidios a los combustibles fósiles –por ejemplo, a la gasolina – se encarecerán y por tanto se reducirá su consumo.Un artículo aparecido el pasado 8 de febrero en la revista “Nature”, sin embargo, concluye que las cosas no son así de simples. Dicho artículo fue publicado por un grupo internacional de investigadores encabezados por Jessica Jewell del “International Institute for Applied System Analysis”, con sede en Laxenburg, Austria.Según Jewell y colaboradores, si bien desaparecer los subsidios a los combustibles fósiles podría resultar en una reducción de su demanda neta a nivel global, y a una consecuente reducción en la emisión de contaminantes atmosféricos, el éxito sería sólo modesto. En efecto, según los investigadores, la eliminación de los subsidios a los combustibles fósiles resultaría en una reducción a nivel global entre el 0.5% y el 2% del total de dióxido de carbono emitido a la atmósfera, y esto queda lejos de los compromisos establecidos en la cumbre climática de París de 2015.  La reducción en la demanda de energía, además, dependerá de la región del mundo que se considere. Será mayor en los países de Medio Oriente, el norte de África y América Latina, que tienen altos niveles de subsidios, pero mucho menor en Europa y los Estados Unidos. Por otro lado, habrá países, particularmente de África, en los que podría haber incluso un incremento en la emisión de contaminantes atmosféricos por la sustitución del petróleo o el gas por carbón, que actualmente goza de menores subsidios y cuya combustión produce una mayor contaminación atmosférica. De estar Jewell y colaboradores en lo correcto, la eliminación de los subsidios a los combustibles fósiles no constituye por sí sola una solución para limitar la emisión de contaminantes atmosféricos. Los investigadores apuntan, además, que en los países no desarrollados dicha eliminación debería de ir acompañada por medidas adicionales para proteger a la población de menores recursos que resultaría particularmente afectada. Por otro lado, en un artículo de opinión publicado en “Nature” juntamente con el artículo de Jewell y colaboradores, Ian Parry del Fondo Monetario Internacional argumenta que los precios de los combustibles no solamente deben atender a sus costos de producción, sino que deben también reflejar las consecuencias de su uso, incluyendo el calentamiento global y otras consideraciones medioambientales como las muertes por contaminación, las congestiones de tráfico y las muertes por accidentes en las carreteras.Ciertamente, dados los problemas que han resultado del uso que le hemos dado a los combustibles fósiles, su precio debería en principio reflejar todas las calamidades que ha causado, desde poner en peligro los campos de esquí –que por lo demás, estaríamos de acuerdo, no sería de las más graves–,  hasta  provocar caos viales en los centros urbanos y afectar gravemente el clima de planeta. En la práctica, sin embargo, no es claro cómo dar a los combustibles un precio justo en este contexto, al mismo tiempo que brindamos protección a la población de menores recursos en países como el nuestro. Asumiendo, por supuesto, que la demanda de combustibles puede ser controlada por el precio que le asignemos, suposición que debe ser evaluada a la luz de los resultados de Jewell y colaboradores.",
    "El 16 de octubre de 1846, en un anfiteatro del Hospital General de Massachusetts en Boston, se dio la primera demostración pública de una operación quirúrgica empleando anestesia general. En esa ocasión, el cirujano John Collins Warren, profesor de la Escuela de Medicina de Harvard, asistido en calidad de anestesista por el dentista William Morton, removió un tumor congénito del cuello de un paciente de nombre Gilbert Abbott. Para preparar al paciente para la operación, Morton le hizo inhalar éter empleando un dispositivo de vidrio que él mismo había diseñado. La operación fue un éxito, y si bien no fue la primera que se llevó a cabo empleando anestesia general, sí fue ampliamente publicitada y marcó la irrupción de los anestésicos en el campo de la cirugía.   El éter, por lo demás, no fue la única sustancia que en el siglo XIX se probó como anestésico; otras que lo fueron también son el cloroformo y el óxido nitroso. Este último –que es conocido como “gas hilarante” por la risa que provoca entre quienes lo inhalan– fue empleado en 1864 por un grupo de dentistas en Nueva York para extraer sin dolor –y sin risa– tres mil novecientas veintinueve piezas dentales en el curso de tres semanas. El cloroformo, por su lado, le fue administrado a la reina Victoria para aliviar los dolores del parto de sus dos últimos hijos.Con el tiempo, la ciencia de los anestésicos se hizo cada vez más sofisticada desarrollándose un gran número de sustancias de la más diversa índole para aliviar el dolor en las operaciones quirúrgicas. No obstante, aun hoy en día, a más de 150 años de la demostración de Warren y Morton y cuando los horrores de las operaciones quirúrgicas sin anestesia nos parecen algo lejano, los especialistas no se ponen de acuerdo sobre cuáles son los mecanismos responsables del accionar de los anestésicos sobre el sistema nervioso. Un interesante artículo aparecido el pasado mes de diciembre en la revista “Annals of Botany”,  pretende arrojar luz con relación a esto último. Dicho artículo fue publicado por un equipo de investigadores de Alemania, Japón, República Checa e Italia, encabezado por Ken Yokawa de la Universidad de Bonn. En el mismo, se reportan los resultados de una investigación llevada a cabo para estudiar el efecto de algunas sustancias empleadas como anestésicos en el comportamiento de ciertas plantas, entre las que se encuentran la venus atrapamoscas y la mimosa sensitiva.Como sabemos, la venus atrapamoscas es una planta carnívora que atrapa a sus presas vivas por medio de una trampa colocada al final de cada una de sus hojas. Dicha trampa consiste de dos lóbulos que se cierran –como una concha de almeja– atrapando al insecto posado sobre su superficie. La trampa se acciona cuando la presa toca al menos dos de los tres pelos sensibles al tacto que se encuentran sobre uno de los lóbulos. La mimosa sensitiva, por su lado, cierra sus hojas en repuesta al contacto con un objeto extraño.Para llevar a cabo su investigación, Yokawa y colaboradores adquirieron las plantas escogidas en un vivero local. Inicialmente, las plantas respondieron a los estímulos como se esperaba. En el caso de la mimosa sensitiva, sus hojas se cerraron al tocarlas ligeramente con un pincel, mientras que la venus atrapamoscas accionó sus trampas de insectos después de tocar dos o tres de sus pelos sensitivos con una aguja. Enseguida, colocaron a las plantas por una hora dentro de una cámara de vidrio, juntamente con un recipiente conteniendo una cierta cantidad de éter. En contraste con las pruebas iniciales, después de la exposición por una hora a la atmósfera con éter ambas plantas dejaron de responder por completo a los estímulos. Siete horas después de remover la mimosa sensitiva de la cámara de vidrio recuperó su respuesta, mientras que la venus atrapamoscas fue más rápida y sólo necesito de quince minutos para hacerlo.Yokawa y colaboradores midieron también el efecto que el éter tiene sobre la actividad eléctrica de las células de los pelos sensitivos de la venus atrapamoscas. Para provocar una reacción de la planta, dichas células deben generar un impulso eléctrico, mismo que los investigadores encontraron es inhibido por el anestésico.Las plantas de este modo reaccionan a los anestésicos de manera similar a como lo hacen los miembros del reino animal, los humanos incluidos. En este respecto, los reinos animal y vegetal no estaríamos demasiado alejados y de esto emerge una pregunta: sabemos que los anestésicos nos pueden sumir en un estado en el que perdemos la consciencia y del que regresamos cuando pasa su efecto ¿Sucede lo mismo con las plantas? Es decir, ¿tienen las plantas consciencia de sí mismas la cual pierden cuando se les anestesia? Yokawa y colaboradores no nos dicen nada al respecto y todo queda, desafortunadamente, en mera especulación. Por el contrario, según Yokawa y colaboradores, lo que sus resultados sí indican es que, dada la similitud de las respuestas de plantas y animales a los anestésicos, las plantas constituyen sujetos de estudio muy convenientes –entre otras cosas porque no pueden correr– para desvelar los misterios sobre los mecanismos de acción de los anestésicos en humanos. Algo que seguramente sería sorprendente  para los pioneros de la anestesia en el siglo XIX.",
    "Durante su visita a Lima el pasado mes de noviembre para disputar con Perú el segundo y definitivo juego de la serie de repechaje para asistir al campeonato mundial de futbol a celebrarse este año en Rusia, la selección de Nueva Zelanda se quejó que los peruanos habían recurrido a tácticas sucias para tratar de frenarlos. Entre estas tácticas se habría incluido un retraso deliverado de tres horas en el avión que los transportó desde Buenos Aires a Lima, e igualmente un retraso en su traslado desde el aeropuerto de Lima hasta su hotel que habría tomado 45 minutos. Como resultado, los jugadores neozelandeses sólo habrían podido conciliar el sueño hasta la 1:30 de la madrugada. En otro incidente, durante la madrugada previa al juego clasificatorio, un grupo de aficionados peruanos lanzó con gran estruendo y en dos ocasiones fuegos artificiales justo enfrente del hotel en donde se hospedaban los jugadores neozelandeses. Si bien la expectación en Perú por la posible calificación al campeonato mundial de Rusia era muy grande –su última participación en una justa de este tipo ocurrió en 1982– se antoja difícil que los peruanos hayan llegado el extremo de retrasar a propósito un vuelo internacional. De la misma manera, los 45 minutos que les tomó a los neozelandeses transportarse desde el aeropuerto hasta su hotel probablemente haya sido más producto del tráfico congestionado de Lima que de un acto deliberado. En cambio, el lanzamiento de fuegos artificiales enfrente del hotel de los jugadores de Nueva Zelanda sí tuvo el propósito de perturbarles el sueño y mermar su rendimiento. En otro incidente poco deportivo, un aficionado en el estadio dirigió un rayo láser verde a los ojos del portero neozelandés durante el partido con el objeto de cegarlo momentáneamente y hacerlo perder la jugada.  Esto último, por lo demás, no es caso único y el uso de láseres en los estadios deportivos para molestar a los jugadores está lejos de ser inusual por las facilidades que existen hoy en día para adquirir apuntadores láser compactos de gran potencia que son fácilmente introducidos en los recintos deportivos. Una característica de la luz láser es que viaja en línea casi recta, lo que le permite alcanzar grandes distancias con poca atenuación. Esta característica la hace peligrosa pues al penetrar en el ojo puede enfocarse en un punto pequeño y dañar a la retina. El peligro, además, se ha incrementado en la medida en lo han hecho las potencias de los apuntadores láser, que han alcanzado valores mucho más allá de lo requerido por su función como señaladores en presentaciones audiovisuales. Así, el riesgo es más grande en cuanto más alta es la potencia del láser y en este sentido los apuntadores de color rojo, que típicamente tienen potencias moderadas, son los que menos riesgo involucran. Es posible, sin embargo, adquirir fácilmente apuntadores que emiten luz de color verde con potencias sustancialmente más altas que el mínimo suficiente para producir daños permanentes en la retina. Y son precisamente estos láseres los que usan los aficionados para molestar a los jugadores en los estadios.Afortunadamente, la luz de un láser no viaja estrictamente en línea recta –lo hace en mayor o menor medida en función de su costo–, y dada la gran distancia que tiene que viajar en un estadio desde la tribuna hasta la cancha, la cantidad de energía que podría penetrar en los ojos de un jugador es sólo una fracción pequeña del total emitido por el apuntador. El riesgo que éste pudiera sufrir un daño ocular es entonces proporcionalmente reducido. Entre la comunidad de especialistas, sin embargo, hay preocupación sobre los riesgos que acarrea la disponibilidad a precios reducidos de apuntadores láser de alta potencia. Por ejemplo, en un artículo aparecido el pasado mes de diciembre en la revista “Deutsches Arzteblatt International”, publicado por un grupo de investigadores de Alemania e Inglaterra encabezados por Johannes Birtel de la Universidad de Bonn, se presenta un análisis de 48 artículos en la literatura médica en donde se describen lesiones provocadas por apuntadores láser en 111 pacientes. De este análisis, los investigadores encuentran que los apuntadores láser son capaces de provocar lesiones graves en la retina que pueden ser permanentes y resultar en una agudeza visual reducida. Birtel y colaboradores también incluyeron en su artículo los casos de siete niños y adolecentes tratados en la Universidad de Bonn, los cuales sufrieron lesiones en la retina mientras jugaban con apuntadores láser de color verde de diferentes potencias. Si bien los apuntadores láser lucen inofensivos e incluso pueden ser tomados como juguetes por los niños, en realidad son dispositivos que pueden ser peligrosos si se manejan de manera inadecuada. Un problema señalado por Birtel y colaboradores es la información errónea –o falta de la misma– que proporcionan los fabricantes de apuntadores sobre la potencia real que emiten sus dispositivos. Así, la potencia real de un número de láseres investigados en la Universidad de Bonn difieren de los valores proporcionados por los fabricantes hasta por un factor de cien. Habría, pues, que tener cuidado con el manejo de los apuntadores láser y usarlos solamente cuando la ocasión lo amerite. En particular, el apuntador que fue dirigido a la cara del portero neozelandés en el juego entre Perú y Nueva Zelanda fue claramente innecesario, pues el equipo de Perú –por las buenas– tuvo todas las de ganar por su clara superioridad futbolística.",
    "En un caso de contaminación interplanetaria, los marcianos que invadieron a la Tierra en la novela “La guerra de los mundos” del escritor británico H.G. Wells fueron aniquilados por los microbios terrestres, desconocidos por su sistema inmunológico y de los cuales, por lo mismo, no pudieron defenderse. La novela de H.G. Wells fue publicada en 1898, cuando Marte ejercía una gran atracción como posible asiento de una civilización avanzada. Al final, de manera afortunada para la civilización terrestre, si bien los marcianos en la ficción tenían una civilización más avanzada que la nuestra, no lo era tanto como para anticipar lo que podría ocurrir con una gran probabilidad: la existencia de microbios terrestres contra los cuales no tuvieran defensa.De haber tenido conocimiento de la historia de México, los marcianos de H.G. Wells hubieran quizá sido más precavidos. En efecto, como sabemos, en un episodio de contaminación intercontinental los españoles conquistadores de México trajeron consigo microbios desconocidos en el Nuevo Mundo que causaron estragos entre la población nativa. La aparición de dichos microbios fue muy temprana y de hecho ayudaron a los conquistadores en su empresa al atacar a los habitantes de Tenochtitlan justo antes de que la ciudad fuera sitiada y finalmente conquistada por los invasores.  En su libro “Visión de los vencidos: Relaciones indígenas de la conquista”, Miguel León Portilla cita testimonios indígenas acerca de dicho ataque, aparentemente por la bacteria de la viruela: “Era muy destructora enfermedad. Muchas gentes murieron en ella. Ya nadie podía andar, nomás estaban acostados. Tendidos en su cama. No podía nadie moverse, no podía mover el cuello, no podía hacer movimientos del cuerpo, no podía acostarse cara abajo, ni acostarse sobre la espada, ni moverse de un lado a otro. Y cuando se movían algo daban de gritos. A muchos dio la muerte la pegajosa, apelmazada, dura enfermedad de granos”.Tan indefensa estaba la población nativa de México ante los gérmenes de la viruela introducidos por los españoles que entre los años 1519 y 1520 murieron de 5 millones a 8 millones de indígenas, un porcentaje apreciable de los aproximadamente 20 millones de personas que se estima componían el total de la población.  La epidemia de viruela de 1519-1520 fue, sin embargo, solamente el principio de la catástrofe poblacional. Según los estudiosos, a lo largo del siglo XIV México fue afectado por una serie de epidemias de diferente naturaleza que diezmaron a la población indígena que se redujo de 20 millones en 1519 hasta unos dos millones en el año 1600. Dos epidemias, conocidas como cocoliztli por los indígenas, fueron particularmente devastadoras. La primera, ocurrida entre 1545 y 1550, se estima produjo entre 5 millones y 15 millones de muertos. Una segunda epidemia tuvo lugar entre 1576 y 1578 y dio como resultado unos dos millones de decesos adicionales. Por otro lado, más allá de su naturaleza mortífera, no existe acuerdo sobre cuáles fueron los patógenos causantes de estas epidemias.En este sentido, un artículo publicado esta semana en la revista “Nature Ecology and Evolution” arroja luz sobre la identidad de los gérmenes responsables de la epidemia cocoliztli de 1545-1550. Dicho artículo fue publicado por un grupo de investigadores de Alemania, los Estados Unidos, Suiza y México, encabezados por Ashlid Vagene del Instituto Max Planck en Jena, Alemania. De acuerdo con Vagene y colaboradores, existe una alta probabilidad de que el causante de la epidemia de 1545-1550 fue la bacteria “salmonella enterica”. Llegan a esta conclusión a través del estudio de los esqueletos de víctimas del cocoliztli provenientes de un cementerio en el estado de Oaxaca –en el que se sabe enterraron a víctimas de la epidemia– y en los que encontraron trazas del genoma de la salmonela. Para mayores precisiones, no afirman con certidumbre los investigadores que dicha bacteria haya sido la el causante del cocoliztli pero sí consideran que existe una gran probabilidad de que lo sea.De una manera u otra, al margen de la verdadera identidad de los agentes patógenos que asolaron a la población nativa de México como resultado de la conquista española, la población indígena fue mermada –literalmente diezmada– en el curso de 80 años, en una catástrofe poblacional de dimensiones comparables a las de la peste bubónica que asoló a Europa en el siglo XIV y produjo 25 millones de muertos.Los indígenas en México resultaron así extremadamente vulnerables a los patógenos del Viejo Mundo que se convirtieron en aliados de los españoles y se unieron a sus caballos, arcabuces y cañones en contra de los mexicas. ¿Podría haber sido de otro modo? Es decir, al igual que en la ficción creada por H.G. Wells, ¿podrían haber sido los españoles susceptibles a los patógenos del Nuevo Mundo y así convertirse en sus enemigos? ¿Habría esto cambiado la historia de la conquista?  Las preguntas son ociosas, por supuesto, pero posiblemente no absurdas y sí entretenidas. Después de todo, sabemos que los extranjeros que visitan nuestro país son susceptibles de sufrir diversos males estomacales e intestinales por el fenómeno conocido como “Venganza de Moctezuma”.",
    "Según una historia divulgada por los medios de comunicación en días pasados, Wang Fuman, un niño de ocho años de edad que vive en un área rural de la provincia de Yunnan en el sur de China –cerca de la frontera con Vietnam–, tiene que caminar diariamente 4.5 kilómetros para asistir a la escuela. Esto en si no habría sido motivo de mayores comentarios de no ser porque una fotografía divulgada como parte de la historia muestra al niño al llegar a la escuela con el pelo congelado y las mejillas enrojecidas por el frío. Esto, por haber tenido que caminar por una hora y media en una mañana fría con temperaturas por debajo de los cero grados centígrados.La fotografía de Wang Fuman congelado, que se hizo viral, fue tomada por su profesor, quién la circuló por una red social en China y la hizo llegar a un medio de comunicación de ese país, el cual a su vez la difundió  a nivel mundial.  Como consecuencia afortunada, a Wang Fuman le fue regalada ropa de invierno, incluyendo unos guantes habida cuenta que en la difusión de su historia se incluyó también una fotografía de sus manos con la piel agrietada y con lesiones por el frío sufrido.Por otro lado, si bien el ejercicio físico puede tener su lado positivo –aunque el que tiene que realizar diariamente Wang Fuman es, ciertamente, poco saludable, sobre todo con temperaturas por abajo del punto de congelación del agua– la vida en comunidades físicamente aisladas no es algo deseable. De hecho, existe una cierta relación entre el nivel de desarrollo y el aislamiento de una comunidad.Una cuantificación del aislamiento de las diferentes comunidades en el mundo nos la da un artículo aparecido esta semana en la revista “Nature”, publicado por un grupo internacional de investigadores encabezados Daniel Weiss de la Universidad de Oxford en el Reino Unido. En dicho artículo, Weiss y colaboradores presentan, entre otros datos, un mapa global en el que se muestran con diferentes colores los tiempos de traslado desde cualquier punto en el mundo a la ciudad más cercana con una población de al menos 50,000 habitantes. Dicho mapa tiene una resolución de un kilómetro y fue realizado con datos de Google y del proyecto “Open Street Map”, alojado en computadoras de la universidades University College London e Imperial College London. Como pudiera haberse esperado, el mapa muestra que los tiempos de traslado se miden en minutos alrededor de los grandes centros urbanos de Europa, China, Japón y los Estados Unidos. En contraste, en algunos países en desarrollo, notablemente de África al sur del Sahara, estos tiempos de traslado tienden a ser considerablemente más grandes. Un tiempo de traslado corto, sin embargo, no indica un alto grado de desarrollo. Así, los tiempos de traslado en la India, que cuenta con una gran densidad de centros urbanos, son cortos, mientras que en el norte de Noruega y Suecia, que tiene poca densidad de población, son considerablemente más largos.  En términos globales, sin embargo, si existe una relación entre conectividad con los centros urbanos y grado de desarrollo. En efecto, Weiss y colaboradores encuentran que si bien un 80% de la población de mundo vive a menos de una hora de un centro urbano, la distribución a lo largo del mundo no es uniforme. Así, mientras que el 90% de los habitantes de los países ricos viven a menos de una hora de un centro urbano, en algunos países de bajos ingresos, aquellos concentrados en la región al sur de Sahara, dicha cifra es de apenas un 50%. Dado que las ciudades son el asiento de servicios tales como la educación y la salud que son básicos para el bienestar de la población, una mayor conectividad de las áreas no urbanas con las ciudades sería esencial para incrementar este bienestar. En concordancia con esto, empleando datos de encuestas sobre demografía y salud financiadas por la Agencia de los Estados Unidos para el Desarrollo Internacional, Weiss y colaboradores encuentran una correlación clara entre el tiempo de traslado a la ciudad y el nivel de ingresos, el nivel de educación alcanzado y el uso de los servicios de salud.Weiss y colaboradores hacen notar que las áreas de mayor accesibilidad incluyen aquellas con abundante infraestructura de transporte y/o con muchas ciudades dispersas, y de esto sugieren como estrategia que se debe mejorar la infraestructura de transporte y promover un desarrollo urbano poli céntrico.De acuerdo con el INEGI, el 22% de la población del país vivía en comunidades rurales. En estas condiciones México tendría, de acuerdo a la sugerencia de Weiss y colaboradores, que construir infraestructura urbana y de transporte para atender las necesidades para cuando menos 20 millones de personas. Dadas las condiciones de México y China, sin embargo, es muy probable que Wang Fuman resuelva sus problemas de transporte antes de que hagan lo propio nuestros compatriotas.",
    "Como ha sido ampliamente difundido por los medios de comunicación, la costa este de los Estados Unidos esta siendo afectada por una severa tormenta invernal que ha llevado nevadas incluso hasta el estado sureño de Florida.  Para la noche del sábado y la mañana de hoy domingo, la oficina del clima de los Estados Unidos ha emitido una alerta de bajas temperaturas que, combinadas con el efecto del viento, alcanzarán en algunos lugares temperaturas aparentes de menos varias decenas de grados centígrados. Y todo esto, como nos consta, nos afecta aun a nosotros, que estamos colocados al sur del Trópico de Cáncer a considerable distancia de los fríos del ártico. El clima extremo que está afectando a la costa este de los Estados Unidos pareciera contradecir al proceso de calentamiento global que está afectando al planeta. De hecho, en un tuit del pasado 28 de diciembre, el presidente de los Estados Unidos sugirió que, para enfrentar el frío, la costa este de los Estados Unidos pudiera hacer uso de un poco del buen calentamiento global por el que “nuestro país, pero no otros, va a pagar trillones de dólares como protección”.    En realidad, de acuerdo con los expertos, el calentamiento global y los inviernos crudos que se han dado en los últimos decenios, tanto en el norte del continente americano como en Europa y Asia, están relacionados entre si, aunque no se entiende en su totalidad el mecanismo que los liga, mismo que es motivo de investigaciones por parte de los meteorólogos. En efecto, a manera de ejemplo, se puede mencionar que dicho mecanismo se discute en un artículo publicado el pasado mes de septiembre en la revista “Bulletin of the American Meteorological Society” por un grupo internacional de investigadores encabezado por Marlene Kretschmer del “Potsdam Institute for Climate Impact Research”, Alemania.Como sabemos, los descensos de temperatura en las zonas templadas del hemisferio norte están provocados por el flujo de aire polar hacia dichas zonas. De acuerdo con Kretschmer y colaboradores, por otro lado, la corriente polar de chorro que circula en la parte norte de nuestro hemisferio sirve como una barrera de contención para el flujo de aire ártico hacia el sur. Dicha corriente, sin embargo, se debilita en la medida en que la temperatura del Polo Norte se eleva, que es precisamente lo está provocando el calentamiento global a una velocidad acelerada.  Podemos mencionar, además, que si bien dicho calentamiento ha sido el resultado de la emisión descontrolada de gases de invernadero, las regiones polares mismas están contribuyendo activamente a incrementar la cantidad de radiación solar retenida por la Tierra; esto, en la medida en que se ha reducido la fracción de su superficie cubierta por el hielo. Es decir, dado que una superficie con hielo refleja la mayor parte de la radiación solar que incide sobre ella, a medida que disminuyen los hielos polares se disminuye la cantidad de radiación que es remitida al espacio y en consecuencia la cantidad que es absorbida por la Tierra. El estudio de Kretschmer y colaboradores, sin embargo, se refiere a los descensos de temperatura que se han observado en las partes norte de Asia y Europa y más investigaciones son necesarias para averiguar si el mecanismo que han encontrado para explicar los inviernos fríos en dichos continentes es igualmente adecuado para explicar la onda de frío que actualmente azota al norte del continente americano.     De un modo u otro, sin  embargo, a través de los resultados de  Kretschmer y colaboradores es posible entender algo que a los profanos nos parece un contrasentido. Es decir, que un proceso de calentamiento, que es indiscutible está ocurriendo a nivel global y de manera más marcada en la regiones polares, puede originar una onda de frío polar. Aprendemos, también, que el clima de la Tierra es algo extremadamente complicado de entender y que para este propósito nuestro sentido común tiene severas limitaciones. Para alguien no experto en cuestiones climáticas es entonces riesgoso emitir opiniones al respecto. Ciertamente, lo es aun si estas opiniones fueran interesadas, como es el caso de aquellas que apuntan a la conveniencia de hacer uso de un poco del calentamiento global para calentar el clima de la costa este de los Estados Unidos. Que esto último es así lo prueba el hecho de haber escogido para comentar las condiciones climáticas por las que está atravesando la costa este de los Estados Unidos. Es decir, de haber escogido el clima actual de la costa oeste de ese país, que esta experimentando temperaturas por arriba de lo usual, se habría llegado justo a la conclusión opuesta.",
    "Está ya aquí la época navideña y con esto el consabido aumento en el consumo de bebidas alcohólicas. Esto es cierto tanto para México como para muchos otros países en el mundo, sin bien cada país tiene sus bebidas de preferencia para dar rienda suelta a su alegría. En México, como nos consta, la bebida alcohólica más popular es con mucho la cerveza, seguida por el tequila.  Con menores índices de consumo encontramos al whiskey, al brandy y al vodka. En términos de porcentajes, los índices de consumo de alcohol en México son elevados, y no solamente en épocas navideñas. En efecto, de acuerdo con una encuesta del Instituto Nacional de Salud Pública, en 2012 el 25 por ciento de los adolescentes mexicanos con edades entre los 10 y los 19 años consumieron alcohol de manera diaria u ocasional a lo largo del año previo a la fecha en la que se levantó la encuesta. No se observó, por otro lado, un cambio estadísticamente significativo en dicho porcentaje entre los años 2000 y 2012. En contraste, el porcentaje de adultos consumidores de alcohol en México es sensiblemente mayor y se incrementó significativamente entre estos dos años, pasando del 40% en año 2000 al 54% en 2012.Una bebida alcohólica muy popular en algunos países –si bien no en el nuestro, de tradición cervecera– es el vino de uva en sus diferentes versiones. Entre los países líderes en el consumo de vino se encuentran Francia y Portugal con un consumo anual per cápita de aproximadamente 56 botellas de 0.75 litros. Otros países europeos, incluyendo a Italia y Alemania son también grandes consumidores de vino aunque en menor proporción.Por otro lado, si bien en Inglaterra el consumo per cápita de vino es menor que el de los países líderes en el campo, es interesante traer a colación un artículo publicado en la edición de Navidad de la revista BMJ por un grupo de investigadores de la Universidad de Cambridge, Reino Unido, encabezado por Theresa Marteau, en que se analizan algunos aspectos del consumo de vino en ese país. La revista BMJ es una revista de la Asociación Médica Británica que publica artículos de investigación médica con una estricta evaluación editorial. En el número de Navidad, sin embargo, dicha revista acostumbra publicar también artículos más ligeros, los cuales, no obstante, somete igualmente a una estricta evaluación editorial. Este es el caso del artículo de Marteau y colaboradores en el que se reportan los resultados de una investigación llevada a cabo con el objeto de averiguar la evolución que ha tenido el tamaño de las copas de vino en Inglaterra a lo largo de los últimos tres siglos, y cómo esta evolución ha influido en el consumo per cápita de vino en ese país.   De acuerdo con Marteau y colaboradores, el consumo de vino en Inglaterra se cuadruplicó entre 1960 y 1980 y después de esto se duplicó entre 1980 y 2004, y se preguntan si estos incrementos están asociados a un aumento en el tamaño de las copas en las que se acostumbra beberlo. Esto lo sería, de manera análoga a como el tamaño de los platos de mesa han contribuido a un mayor consumo de alimentos como se ha sido reportado por otros investigadores. Para su estudio, Marteau y colaboradores obtuvieron datos del tamaño de las copas de vino a lo largo de los últimos tres siglos de fuentes tales como el Museo de Arte y Arqueología de la Universidad de Oxford, la Casa Real del Reino Unido, eBay –subastas de copas antiguas–, el catálogo de un fabricante de cristalería, y el sitio de internet de la tienda de departamentos con el más grande inventario de copas de vino.De su estudio, los investigadores concluyen que en el año 1700 las copas de vino tenían un volumen de 66 mililitros, el cual se incrementó a 417 mililitros en los años 2000 y a 447 mililitros en 2016-2017. Además, mientras que hasta los años 90 del siglo pasado el incremento fue gradual, éste se aceleró notablemente a partir de ese momento.Reconocen Marteau y colaboradores que, si bien el incremento en el consumo de vino en Inglaterra se ha dado concurrentemente con un aumento en el volumen de las copas, no es posible inferir de sus datos una relación de causa-efecto entre ambos fenómenos. Argumentan, sin embargo, que una copa más grande aloja una mayor cantidad de vino y que esto lleva, a un mayor consumo. Es decir, que una misma cantidad de vino es percibida como menor en una copa grande en comparación con una pequeña. Para apoyar lo anterior, Marteau y colaboradores traen a colación una investigación anterior llevada a cabo en un bar y en la que se encontró que al servir el vino en copas más grandes se incrementaron las ventas en un 10%. Uno de los factores que indujeron el crecimiento en el tamaño de las copas tendría de este modo un origen comercial.Al margen de que estudios posteriores confirmen o desmientan las conjeturas de Marteau y colaboradores, ciertamente resulta sorprendente enterarnos que en el curso de 300 años el tamaño de las copas de vino se ha multiplicado por un factor de siete y que en el siglo XVIII éstas apenas eran más grandes que una copa tequilera tradicional. Por otro lado, es posible que la misma tendencia de crecimiento se haya dado con otras copas. Las de tequila, por ejemplo, de las que existen ahora versiones gigantes que han aparecido en apenas unas décadas. La explosión de tamaños podría de este modo ser generalizada. En este contexto, lo más recomendable es actuar con la mayor prudencia en la celebración de las próximas fiestas navideñas.",
    "En un artículo aparecido esta semana en el periódico español El País, Lita Nelsen, quién dirigió por 25 años la oficina de transferencia tecnológica del Instituto de Tecnología de Massachusetts (MIT, por sus siglas en inglés), explica que esta universidad –una de las instituciones educativas referentes a nivel mundial en el área de la ingeniería– obtuvo de las patentes que ha producido apenas un 4% de sus ingresos totales. Esto, en contra de lo que hubiera podido esperarse. De hecho, el MIT obtiene la mayor parte de su presupuesto de subvenciones públicas para proyectos de investigación, seguida de las colegiaturas que cobra a sus estudiantes y las donaciones de dinero privado. Nelsen explica, además, que esta circunstancia es compartida por muchas otras universidades en los Estados Unidos.Lo anterior es una indicación de que el esquema de generación de recursos mediante la transferencia de tecnología es complicado, aun en instituciones con una sólida infraestructura científica y tecnológica. En efecto, Walter Valdivia, de la Institución Brookings en Washington, D.C., reporta en un estudio publicado en noviembre de 2013 que si bien un 13% de universidades en los Estados Unidos genera recursos a través de sus patentes, el 87% restante no genera ni aquellos recursos necesarios para operar su oficina de transferencia de tecnología. Adicionalmente, vista como un negocio, la generación de tecnología en las universidades es ineficiente en términos de los recursos invertidos y las ganancias generadas. A manera de ejemplo, según datos de la “Association of University Technolgy Managers” citados por Forbes, la Universidad de Nueva York invirtió en 2006, 210 millones de dólares en investigación que le redituaron 157 millones de dólares de ganancias –una “perdida” de 25%–, lo que la colocó en el primer lugar entre las universidades norteamericanas. El segundo lugar lo ocupó la Universidad Wake Forest con cifras de 146.3 millones de dólares de inversión, 60.5 millones de dólares de ganancia y una “pérdida” del 59%. Las siguientes 13 universidades consideradas por Forbes no sobrepasaron en ganancias un 16% de lo invertido, El sistema de la Universidad de California, por ejemplo, invirtió en 2006, 3,040 millones de dólares en investigación y obtuvo 193.4 millones  de ganancia, un 6.4% de lo invertido. Un balance deficitario entre inversión en investigación y ganancias por generación de tecnología no debe, por supuesto, ser considerada una pérdida de recursos pues las universidades no son empresas con fines de lucro. Por el contrario, las universidades son instituciones que entre sus funciones sustantivas se encuentran la educación y la generación de nuevos conocimientos y en ese sentido la investigación es un elemento básico para el entrenamiento de nuevos profesionales, particularmente a nivel de posgrado.Lo anterior es relevante para México dada la presión que existe en  nuestro país para que las universidades públicas, que dependen de manera crítica del dinero público, generen por si mismas parte de los recursos que necesitan para su operación. En este contexto, no sorprenden los planteamientos en el sentido de que dichas universidades deben procurar allegarse recursos mediante la generación y transferencia de tecnología al sector industrial. Este planteamiento se refuerza por el hecho de que la investigación científica en nuestras universidades se lleva a cabo casi exclusivamente con fondos públicos y por tanto resulta imperativo que reditúe en un beneficio público. Si  bien no es posible argumentar en lo general en contra de estas opiniones, desde un punto de vista práctico hay que poner por delante la dificultad para implantar en México un esquema de generación de recursos mediante la transferencia de tecnología, que de suyo es complicado aun en los países avanzados que cuentan con una infraestructura educativa considerablemente más sólida que la nuestra.  En efecto, a partir de que la ciencia tuvo una entrada tardía en nuestro país, hubo un lento desarrollo de nuestras universidades como instituciones de investigación. En el último medio siglo, con la creación del CONACyT en 1970, se dieron avances significativos y se han creado centros de investigación y grupos de investigación en nuestras universidades que están a un nivel de competencia internacional. Falta mucho por hacer, sin embargo, para que nuestro sistema de ciencia y tecnología alcance el tamaño y la madurez que le permitan tener un impacto significativo en los procesos de transferencia de tecnología.En estas condiciones, es preocupante que el gobierno federal no dedique mayores recursos a la ciencia y a la tecnología en México, y que estemos todavía lejos de alcanzar el 1% del producto interno bruto como inversión en ciencia y tecnología, meta que se ha fijado cuando menos en dos ocasiones. Como producto del programa de formación de recursos humanos que el CONACyT implantó en 1971, se han formado una gran cantidad de doctores y maestros en ciencias que están teniendo dificultades para encontrar empleo en México; y aquellos que han logrado hacerse de una plaza en alguna de nuestras universidades, están igualmente teniendo dificultades para conseguir recursos para montar los laboratorios que necesitan para arrancar con sus proyectos de investigación.Así, muchos esfuerzos nos falta por hacer para lograr que fluyan los recursos hacia nuestras universidades como producto de la transferencia de tecnología.  Aun si estos recursos representaran sólo un pequeño porcentaje de lo invertido en investigación.",
    "En su libro “Rosalind Franklin: la dama oscura del ADN”, la escritora norteamericana Brenda Maddox hace mención a la atmósfera sexista que prevalecía en el “King`s College” de Londres en la década de los años cincuenta del siglo pasado. Al respecto, hace notar que, por ejemplo, en dicha institución existían dos comedores, uno exclusivamente para hombres y el otro para uso mixto. Si bien, según Maddox, la asistencia al comedor para hombres no era necesariamente algo habitual entre todos los profesores e investigadores de la Universidad, la historia de Rosalind Franklin, quién fungió como investigadora del “Kings´s College” por dos años, es entendida por algunos en un contexto de discriminación profesional hacia el sexo femenino; discriminación que, por lo demás, en esa época no era exclusiva del “King´s College”. Rosalind Franklin nació en Londres, Inglaterra, en 1920 y se educó como fisicoquímica en la Universidad de Cambridge. En 1951, después de una estancia de investigación en París, durante la cual se especializó en el uso de los rayos X para el estudio de los materiales, se incorporó al “King´s College” en donde llevó a cabo investigaciones que tenían el propósito de dilucidar la estructura molecular del ADN. Su trabajo de investigación  con rayos X fue clave para que Francis Crick y James Watson, entonces en la Universidad de Cambridge, concluyeran que la molécula de ADN tiene la forma de una doble hélice. La historia del descubrimiento de la estructura del ADN ha estado envuelta en controversia pues la contribución de Franklin no fue reconocida en su momento de manera justa. De acuerdo con dicha historia, el descubrimiento de la estructura del ADN se basó en una fotografía de rayos X tomada por Franklin de la molécula de ADN. Violando una norma ética básica, Maurice Wilkins, compañero de trabajo de Franklin en “King`s College”, mostró dicha fotografía a Crick y Watson quienes inmediatamente comprendieron su importancia. Lo hizo, sin embargo, antes de que la fotografía se hiciera pública y sin el consentimiento de Rosalind, con quién no congeniaba.  Por el descubrimiento de la doble hélice de la molécula de ADN, Cricks, Watson y Wilkins recibieron el premio Nobel en Fisiología o Medicina en 1962. Ciertamente, si bien Rosalind Franklin no podría haber sido beneficiaria de dicho premio pues para la fecha hacía cuatro años que había muerto víctima de cáncer de ovario, el episodio ha sido considerado por algunos como un ejemplo de discriminación de género en el campo de la ciencia. De hecho, en 1968 James Watson publicó el libro “La doble hélice”, en la que da una versión personal de los acontecimientos que llevaron al descubrimiento de la estructura del ADN y en donde se refiere a Franklin de forma poco elogiosa y sexista. Como respuesta, han aparecido libros, incluyendo el de Brenda Maddox, que tratan de equilibrar puntos de vista sobre Rosalind Franklin, como persona y como científica.Los prejuicios sexistas sobre el tipo de ocupación propio de la mujeres, por supuesto, no se han limitado a la actividad científica ni a una época en particular. Así, durante la época victoriana –de  la cual Rosalind Franklin no estuvo muy alejada en el tiempo– a las mujeres en buena medida se les limitaba a un papel de esposas y de alguna manera la reina Victoria, primero como esposa y luego como viuda, ponía el ejemplo. Con respecto a las actividades desarrolladas por las mujeres a lo largo de la historia, cabe mencionar un artículo aparecido esta semana en la revista “Science Advances”, publicado por un grupo internacional de investigadores encabezado por Alison Macintosh de la Universidad de Cambridge en el Reino Unido. En dicho artículo se reportan los resultados de una investigación llevada a cabo con húmeros –hueso superior del brazo–  de 94 mujeres que vivieron a lo largo del periodo que va de 5,300 años a.C. a 850 años d.C., que cubre desde el Neolítico hasta la Edad media. El objetivo del estudio fue averiguar cómo la aparición de la agricultura y el sedentarismo modificó el grosor del húmero por el cambio en el tipo de actividad y como dicho grosos se compara con el de mujeres contemporáneas, tanto atletas del equipo de remo de la Universidad de Cambridge, como de otros atletas de alto rendimiento y de personas sedentarias con actividad normal.Encontraron Macintosh y colaboradores que los húmeros de mujeres que vivieron entre 5,300 años a.C. y 100 años d.C. tenían húmeros más gruesos que las mujeres contemporáneas, aun en el caso de las remeras de alto rendimiento. Posteriormente, el grosor del húmero empezó a disminuir hasta que en la Edad media alcanzó un tamaño equivalente al actual. Esto es un indicativo de que, con el advenimiento de la agricultura las mujeres desarrollaron una actividad intensiva que involucraba una gran fuerza en los brazos, posiblemente la molienda de granos por medio de un metate de piedra. En la medida en que progresó la tecnología de los molinos, la intensidad de esta actividad disminuyó y con esto el grosor del hueso superior del brazo.Lo anterior muestra que en tiempos remotos las mujeres desarrollaron actividades que requirieron de una gran fuerza muscular, lo que, en cierta medida se contrapone con la concepción victoriana del papel de la mujer en la sociedad. Actividades que son, además, complemento de la actividad intelectual ejemplificada por Rosalind Franklin, igualmente opuesta a las ideas victorianas. No hay espacio, pues, para la discriminación por género.",
    "En el verano de 1955, el entonces presidente de los Estados Unidos, Dwight Eisenhower, sufrió un ataque cardiaco mientras dormía. A pesar de que dicho ataque fue mal diagnosticado por su médico personal y transcurrieron 12 horas antes de que fuera internado de emergencia en un hospital, Eisenhower se recuperó del problema. Y lo hizo a tal grado que el siguiente año se reeligió para un segundo periodo de cuatro años en el cargo, mismo que llevó a término, si bien no antes de sufrir un segundo ataque. Los problemas cardiacos que aquejaron al presidente Eisenhower durante su presidencia, dieron visibilidad nacional a las ideas de Ancel Keys, fisiólogo de la Universidad de Minnesota, quién sostenía que un factor de riesgo para un ataque cardiaco es el alto nivel de colesterol en la sangre inducido por la ingesta de alimentos ricos en grasas saturadas. No todo mundo estuvo, sin embargo, de acuerdo con esta opinión. En particular, John Yudkin, nutriólogo del “Queen Elizabeth College” de la Universidad de Londres, creía que la ingestión de azúcar era más peligrosa para la salud que la ingestión de grasas saturadas: Esto, no solamente para el desarrollo de problemas cardiacos, sino también como causante de obesidad y diabetes.Con el tiempo, los puntos de vista de Keys prevalecieron sobre los de Yudkin y las grasas saturadas se convirtieron en el villano que habría que evitar, so pena de sufrir problemas de salud. Así, se desarrolló una próspera industria de alimentos bajos en grasas y, para compensar, rica en contenido de carbohidratos. Con el cambio de alimentación, sin embargo, algo resultó mal si hemos de juzgar por la epidemia de obesidad y diabetes que se ha desarrollado en diversas regiones del mundo. En efecto, si, por ejemplo, echamos un vistazo a las estadísticas de obesidad en los Estados Unidos notaremos que entre  1960 y 1980 el número de personas obesas se incrementó a una tasa moderada. En contraste, a partir de 1980, cuando el gobierno norteamericano hizo pública una guía para una alimentación sana, dicha tasa se incrementó de manera considerable y el número de obesos alcanzó números apabullantes. En efecto, según estadísticas del Centro de Control y Prevención de Enfermedades de los Estados Unidos, entre los años 2011-2014, dos de cada tres norteamericanos adultos estuvieron clasificados como obesos.  Como consecuencia de la catástrofe alimentaria de la últimas décadas, las ideas de Yudkin han resurgido. Así, Robert Lustig de la Universidad de California, San Francisco, acusa al  azúcar de ser el verdadero culpable de esta catástrofe. Habría que notar que aun para el lego esto tiene sentido, pues los humanos hemos evolucionado consumiendo grasas animales y sólo ha sido en los últimos siglos –un tiempo despreciable en términos evolutivos– que hemos añadido el azúcar a nuestra dieta. Como culpable de la catástrofe alimentaria, el azúcar es de este modo más sospechoso que las grasas saturadas. Sin embargo, probar de manera sólida el efecto que tiene un determinado alimento en nuestra salud es en extremo difícil por la necesidad de llevar a cabo experimentos controlados a lo largo de muchos años. Existe, además, la interferencia de las diferentes industrias de alimentos  interesadas en promover o satanizar determinados productos en su beneficio.Con relación a esto último, la semana que hoy termina apareció un artículo en la revista Plos One Biology en el que se analizan investigaciones llevadas a cabo en la década de los años sesenta del siglo pasado por encargo de la “Sugar Research Foundation”. Estas investigaciones estudiaron el efecto que tiene el consumo de azúcar en nuestra salud. El artículo de referencia fue publicado por un grupo de investigadores de la Universidad de California, San Francisco encabezados por Christin Kearns.   De acuerdo con la investigación llevada a cabo por Kearns y colaboradores, la “Sugar Research Foundation” –con ligas con la industria azucarera– financió una investigación llevada a cabo en la Universidad de Birmingham  con ratas de laboratorio para determinar el efecto que una alimentación rica en sacarosa tiene en los niveles de triglicéridos en la sangre. Dicha investigación encontró que la alimentación con sacarosa genera niveles  de triglicéridos más altos que aquella rica en almidones. Encontraron, además, indicios de que la dieta a base de sacarosa puede producir cáncer de vejiga. Cuando la “Sugar Research Foundation” se enteró de los resultados de la investigación, todavía en proceso, la suspendió y nunca publicó los resultados.Según Kearns y colaboradores, la “Sugar Research Foundation” manipuló los resultados científicos obtenidos por los investigadores de la Universidad de Birmingham para favorecer los intereses comerciales de la industria azucarera.  Y con esto habría contribuido a la epidemia de obesidad de proporciones mayúsculas que asuela al mundo. Con amargos resultados.",
    "El pasado domingo en este mismo espacio comentamos sobre el descubrimiento de los rayos X por el físico alemán Wilhelm Roentgen en el mes de noviembre de 1895 y del impacto que tuvo en el diagnóstico y tratamiento médico. En contraste con la luz visible, los rayos X tienen la capacidad de penetrar en los tejidos orgánicos –en mayor o menor medida dependiendo del tipo de tejido– y producir imágenes del interior del cuerpo. Comentábamos también sobre el uso los muones –partículas subatómicas que tienen una gran capacidad de penetración en los cuerpos materiales– para investigar la posible presencia de oquedades en el interior de una mole de piedra tan inmensa como la pirámide mayor de Guiza. Así, rayos X y muones pueden ser usados como extensiones de nuestros sentidos para ver lo que está oculto a simple vista.Como era de esperarse, los muones y los rayos X se descubrieron en países líderes en materia científica y tecnológica y en este respecto no hay nada de que sorprenderse. Es interesante notar, sin embargo, que muy poco tiempo después de su descubrimiento, ambos rayos X y muones hicieron presencia en nuestro país,  particularmente en la ciudad de San Luis Potosí.En efecto, tenemos que a los pocos meses del anuncio del descubrimiento de los rayos X y de la difusión de la radiografía de la mano de la esposa de Roentgen, el ingeniero Luis Espinosa y Cuevas –hermano de José Espinosa y Cuevas que fuera gobernador de San Luis Potosí– adquirió y trajo a San Luis Potosí una máquina de rayos X. Si bien sorprende la rapidez con la que los rayos X arribaron a San Luis Potosí, esta rapidez parcialmente se explica por la presencia afortunada del ingeniero Espinosa y Cuevas en Alemania en el momento en que se hizo público el descubrimiento de Roentgen. No la explica por completo, sin embargo, y aquí hay que añadir su interés personal en el descubrimiento y el deseo que éste tuviera una influencia en su país natal.Da acuerdo con José Refugio Martínez Mendoza, amplio conocedor de la historia científica de San Luis Potosí,  “Los hermanos José Espinosa y Cuevas y Luis Espinosa y Cuevas, serían de los primeros ingenieros graduados en el Instituto Científico y Literario; estos personajes serían igualmente de los primeros alumnos en cursar la cátedra de física, al final de la primera década de impartirse en el Instituto después de su reapertura”. Luis Espinosa y Cuevas tenía de este modo conocimientos de física, y en este contexto no resulta sorprendente que se interesara en los rayos X. Por otro lado, su entrenamiento en el área de la física no garantizaba que tuviera un interés concurrente en emplear a los rayos X para resolver problemas de índole médico. En estas circunstancias y a falta de mayor información, el suyo resultaría un caso excepcional.  De un modo u otro, la iniciativa de Luis Espinosa y Cuevas de traer a San Luis Potosí una máquina de rayos X en época tan temprana colocan al estado como pionero de los rayos X, no solamente en México sino en toda la América Latina. San Luis Potosí sería así y el  lugar en el que probablemente se realizaron las primeras radiografías.  San Luis Potosí tuvo también un contacto relativamente temprano con los muones –que se producen por el choque de los rayos cósmicos que provienen del espacio profundo con los átomos de la atmósfera– que habían sido descubiertos en 1936. Este contacto ocurrió en la segunda mitad de la década de los años cincuenta y fue responsabilidad de Gustavo del Castillo y Gama, quién, juntamente con Candelario Pérez Rosales, fue fundador de la Escuela de Física en la UASLP. Gustavo del Castillo llegó a la UASLP en 1955, después de graduarse como doctor en física en “Purdue University” en el estado de Indiana, e inició un proyecto para construir una “cámara de niebla” para detectar muones. Dicho proyecto básicamente buscaba reproducir la cámara que había construido como parte de su tesis doctoral. Gustavo del Castillo tuvo éxito en su empresa y en tiempo récord construyó la cámara y la puso a operar. Si uno toma en cuenta las condiciones que encontró en el país en lo general y en la universidad en lo particular, resulta sorprendente el éxito de del Castillo y esto definitivamente lo revela como una persona fuera de serie. Muones y rayos X han sido sin duda relevantes para el desarrollo científico de San Luis Potosí. Por un lado, la rápida adopción de los rayos X y su uso como herramienta de diagnóstico es indicativa del alto nivel de desarrollo que tuvo la medicina potosina en los inicios del siglo XX y explica su nivel actual. Por otro lado, si bien la física en San Luis Potosí ha pasado por momentos difíciles a lo largo de su historia, la brillante labor de Gustavo del Castillo como constructor de instrumentos científicos fue indicativo de su gran potencial. Hoy, a 60 años de su fundación, este potencial ha cristalizado en buena medida y la física potosina es una de las de mayor desarrollo en el país. ¿Cuál será el futuro de la investigación científica en San Luis Potosí? Es difícil precisarlo pero dependerá en buena medida de las políticas de apoyo a la ciencia y la tecnología que se implanten a nivel federal para mantener y superar su nivel actual. En particular, no podríamos esperar un desarrollo saludable de mantenerse la actual escasez de recursos destinados a la investigación. De ser el caso, no haríamos justicia a los pioneros de la investigación en el estado.",
    "Fue en noviembre de 1895 cuando el físico alemán Wilhelm Roentgen descubrió una misteriosa forma de radiación –a la que dio el nombre de los rayos X pues su naturaleza le era desconocida– con la propiedad de atravesar los objetos, incluso algunos que son opacos a la luz visible. La facilidad que tienen los rayos X de penetrar en la materia depende, por otro lado, de la naturaleza de la misma. Así, la imagen con rayos X que Roentgen tomó de la mano de su esposa, en la que claramente se revelan los huesos y el anillo que llevaba en uno de sus dedos, demostró que los rayos X penetran mucho más fácilmente en el tejido blando de la mano que a través de los huesos: Y lo hacen todavía mucho menos a través del metal del anillo.Los rayos X permiten ver de este modo el interior del cuerpo, que se encuentra oculto a nuestros ojos. Fue evidente así el enorme potencial para el diagnóstico médico que ofrecían estos rayos y no es difícil entender que hayan atraído un enorme e instantáneo interés, no sólo entre los especialistas sino entre el público en general. Roentgen incluso hizo una demostración de los rayos x ante el káiser Guillermo II apenas dos meses después de su descubrimiento.Hoy sabemos que, aparte de los rayos X, hay otras formas de radiación que nos permiten ver más allá de nuestro ojos. En algunos aeropuertos, por ejemplo, se hace uso de equipo de seguridad que emplea rayos con energías muy pequeñas –al contrario de los rayos X– para descubrir armas u otros objetos escondidos bajo la ropa. En este caso la radiación de baja energía puede penetrar por la tela de la ropa y descubrir lo que está por debajo de la misma.Otra forma de radiación empleada para descubrir lo oculto a la vista son los llamados muones, que se producen cuando los rayos cósmicos provenientes del espacio profundo penetran en la atmósfera y chocan con las moléculas de aire. Los muones tienen la particularidad de penetrar distancias relativamente grandes en materiales tales como la piedra –en contraste con la luz visible– y han sido empleados para investigar oquedades ocultas en estructuras naturales o artificiales.Una aplicación muy interesante de los muones se publicó de manera preliminar la semana que hoy termina en la revista “Nature” por un grupo internacional de investigadores encabezado por Kunihiro Morishima de la Universidad de Nagoya en Japón, y se refiere al estudio del interior de la pirámide de Keops en Egipto que  no ha sido todavía explorada por completo. Como sabemos, la pirámide de Keops es la mayor de las que componen el complejo de Guiza cerca de El Cairo. Fue construida hace 4,500 años, presumiblemente para que sirviera de tumba al faraón Keops, aunque su momia no ha sido descubierta hasta ahora. Se piensa que en su momento, la pirámide tuvo una altura de 146 metros y durante casi 4,000 años ostentó el récord de la estructura artificial de mayor altura en el planeta. Se sabe que su interior alberga tres cámaras, una cámara subterránea y dos cámaras por encima del nivel del suelo, la del rey y la de la reina, todas conectadas por un conjunto de pasillos. Para acceder a la cámara del rey hay que atravesar un pasillo de 1-2 metros de ancho, 8 metros de altura y casi 50 metros de longitud llamado la Gran Galería.Morishima y colaboradores se dieron a la tarea de explorar la pirámide en busca de cámaras adicionales. Para esto, colocaron una serie de detectores de muones en la cámara de la reina y midieron la intensidad del flujo de muones que arriban desde varias direcciones. La idea del experimento es muy simple: si existe una oquedad en un cierta dirección –medida desde el punto en donde está colocado el detector–, la intensidad del flujo de muones que arriban a dicho detector a lo largo de esa dirección será más grande que si la oquedad no existiera. Esto es, los muones viajando en una oquedad sufrirán una menor atenuación que aquellos que viajan a través de un volumen similar de piedra.De este modo, midiendo el flujo de fotones para varias direcciones, Morishima y colaboradores llegaron  a la conclusión que existe una oquedad justo arriba de la Gran Galería, con aproximadamente el mismo volumen. La resolución de sus mediciones, sin embargo, no les permite concluir si se trata realmente de una nueva cámara o si es una oquedad colocada por los constructores de la pirámide para disminuir el peso de la piedra colocada encima de la Gran Galería y prevenir su colapso. Tienen la seguridad, sin embargo, que tal oquedad existe pues es detectada por tres mediciones de muones llevadas a cabo de manera independiente.Curiosamente, sin embargo, el artículo y las conclusiones de Morishima y colaboradores no fue del agrado de la autoridades egipcias, las cuales afirmaron que los investigadores no han descubierto nada,  que la publicación es prematura y que no debió salir a la luz pública, y que, en todo caso, la existencia de la oquedad era ya conocida. Han amenazado, incluso, con retirar el permiso a los investigadores para seguir explorando la pirámide.  La existencia de una nueva oquedad en la pirámide de Keops, por supuesto, tendrá que ser discutida con argumentos científicos en el marco de nuevas investigaciones y no con descalificaciones y amenazas de retirar licencias para seguir investigado.   Al margen de esta discusión, por otro lado, no podemos sino asombrarnos de las técnicas desarrolladas empleando el conocimiento científico que nos permiten ver lo invisible.",
    "Para un maestro que no creció en la era digital, le resulta chocante descubrir durante la clase a un alumno –con seguridad un nativo digital– consultando su teléfono inteligente por asuntos ajenos al curso en lugar de atender a sus explicaciones. En esta circunstancia, el maestro, sin duda, llegará a la conclusión de que dicha falta de atención con seguridad tendrá un impacto negativo en sus calificaciones al final del curso. No es difícil coincidir con esto último que, por lo demás, resulta de aplicar el sentido común. El asunto, sin embargo, tiene sus complejidades y matices como se discute en un artículo aparecido en línea el pasado mes de agosto en la revista Computers in Human Behavior, publicado por Daniel le Roux y Douglas Parry de la Stellenbosch University en Sudáfrica, en el que se describen los resultados de estudio para determinar cómo afecta al  aprovechamiento de los estudiantes el uso de los dispositivo multimedia en el salón de clase.Los investigadores hacen notar que la presencia de dichos dispositivos en el aula impulsan al estudiante a una actividad multitarea que le hace cambiar continuamente de una actividad a otra, mediática o no mediática. En este contexto, le Roux y Parry estaban interesados en averiguar si la actividad multitarea, en la que el estudiante cambia continuamente de foco de atención, tiene un efecto negativo sobre su desempeño académico. Y si tal fuera el caso, si este efecto es el mismo para todas las áreas académicas. Participaron en el estudio un total de 1678 estudiantes de las facultades de Ciencia Agrícola, Artes y Ciencias Sociales, Medicina y Ciencias de la Salud, Economía, Ingeniería y Ciencias Naturales, el 83% de los cuales tenían al momento de realizar el estudio entre 20 y 23 años de edad. Se realizó una encuesta en línea entre los participantes para averiguar sus actividades multimedia en línea, tanto en el salón de clase, como en lo general. Entre las actividades en línea que le Roux y Parry consideraron para su estudio se incluyen, las redes sociales, los microblogs, la búsqueda de información en bases de datos enciclopédicos y la mensajería instantánea. Los resultados del estudio indican que hay una afectación negativa en el desempeño académico de los estudiantes de artes y ciencias sociales por el uso, tanto dentro como fuera del aula, de todos los canales multimedia considerados. Lo mismo es cierto para los estudiantes de ciencias naturales en cuanto al uso de las redes sociales dentro del salón de clase. Los estudiantes de ingeniería, y medicina fueron también afectados negativamente. En este caso, por el uso general de los canales multimedia. En contraste, para los estudiantes de economía no se encontró una correlación clara entre su desempeño académico y el uso de los canales multimedia. Del estudio  resultó que los estudiantes más afectados negativamente por el uso de los canales multimedia son los de área de artes y ciencias sociales.No dan le Roux y Parry una explicación conclusiva sobre qué ocasiona las diferencias entre disciplinas pero sugieren tres posibilidades. En una primera línea de razonamiento, arguyen que dichas diferencias pudieran ser debidas a  los diferentes estilos de pensar entre disciplinas: lineal, en oposición a no lineal, y concreto en oposición a abstracto.  Las ciencias sociales estarían caracterizadas por un estilo de pensamiento no lineal y abstracto que, por alguna razón no discutida, sería particularmente susceptible de ser afectado por las interrupciones que conlleva la actividad multitarea. Esto, en contraste con las ciencias duras como la física, cuyo estilo de pensamiento tendería ser lineal y concreto y más resistente a dichas interrupciones.Como una segunda explicación de sus resultados, le Roux y Parry atribuyen la menor resistencia a las interrupciones en clase, no al estudiante, sino a la naturaleza de cada disciplina: cuantitativa en el caso de las ciencias duras, y más sujeta a interpretación en el caso de las ciencias sociales. Los investigadores arguyen que sus resultados mostrarían que la enseñanza de las ciencias sociales es menos resistente a las interrupciones que la enseñanza de las ciencias duras.Una última explicación tiene que ver con el estilo de evaluación del aprovechamiento del estudiante. En el caso de las ciencias duras, la respuesta a un problema de examen tiene que ser precisa e independiente del criterio de quien lo revisa. En contraste, en el caso de las ciencias sociales, más interpretativas, la evaluación es en cierta medida subjetiva y dependiente del criterio de maestro. De este modo, si un estudiante de ciencias sociales pasa el tiempo de clase distrayendo su atención con su teléfono de manera continua, se perderá de captar las opiniones del maestro que son relevantes para conseguir buenas calificaciones al final del curso. Al margen de explicaciones, sin embargo, nativos o no nativos digitales deberíamos estar de acuerdo en que el tiempo de clase es aquel en el que los estudiantes están obligados a seguir las explicaciones del maestro. Por más que en un momento dado, por aburrimiento o complejidad del tópico expuesto, el estudiante tenga que hacer esfuerzos para no ausentarse de clase refugiándose en su teléfono.",
    "Según nos enseñaba el texto de geografía universal de la escuela primaria que cursamos hace más de medio siglo, las razas humanas se distinguen fundamentalmente por el color de la piel. Así, había razas blanca, negra, amarilla y roja, estas últimas en referencia a los pueblos del lejano oriente y del continente americano, en forma respectiva. No era difícil entender la diferencia entre las razas negra y blanca si se atendía a las figuras que acompañaban al texto en las que, por una lado, se mostraba a una persona de tez clara, con la barba bien cuidada y vestido a la usanza europea y, por el otro lado, a una persona de tez muy oscura y semidesnudo. En contraste, con todo y el correspondiente dibujo ilustrativo, era más difícil imaginar a un individuo con la cara amarilla. Y no se diga roja, dado que todos en el salón de clase éramos  nativos del continente americano.  Todo lo anterior, por supuesto, respondía una visión eurocéntrica del mundo, según la cual la raza blanca, nativa del continente europeo, era superior a todas las demás que estaban caracterizadas por un color de piel diferente al suyo. Habría que reconocer que la indudable superioridad tecnológica de Europa  con respecto al resto del mundo en el siglo XIX dieron argumentos para proclamar que la raza blanca era superior a todas las demás. En la circunstancias actuales, sin embargo, en la que estamos siendo testigos de ascenso económico y tecnológico de los países del lejano oriente, dichos argumentos han perdido por completo su eficacia.  Ciertamente, desde un punto de vista tecnológico y económico no hay argumentos que sustenten la supuesta superioridad blanca. Y tal pareciera que no los hay tampoco desde el punto de vista de la genética, si hemos de creerle a un artículo publicado esta semana en la revista “Science” por un grupo internacional de investigadores encabezado por Sarah Tishkoff de la Universidad de Pensilvania. En su artículo, Tishkoff y colaboradores reportan los resultados de un estudio genético llevado a cabo con nativos de Etiopía, Tanzania y Botsuana pertenecientes a diversas etnias. El objetivo del estudio fue el de identificar genes que afectan la pigmentación de la piel.  Como sabemos, el color de la piel está determinado por la presencia del pigmento melanina en la epidermis, de modo que a más alto nivel de melanina más oscuro es el color de la piel. La piel oscura de los pueblos africanos se ha explicado en función del nivel de protección que dicha piel proporciona en contra de la radiación ultravioleta del Sol. En África este nivel es alto dada su situación geográfica y en consecuencia la piel de los africanos evolucionó hacia altos niveles de melanina. En contraste, las poblaciones que emigraron de África hacia el continente europeo encontraron allí menores niveles de radiación ultravioleta perdiendo la piel su color oscuro original en favor de un color pálido, ventajoso para la producción de vitamina D.   No obstante, Tishkoff y colaboradores consideran que dicha explicación es insuficiente pues entre los africanos dicho color varía grandemente, siendo marcadamente oscuro entre los pueblos del área que comprende Sudán y Chad, y mucho más claro entre los San que habitan en el sur de África. Así, los investigadores razonan que tiene que haber un efecto genético adicional.Para determinar el color de la piel y de éste el nivel de melanina, Tishkoff y colaboradores midieron la cantidad de luz reflejada por la piel de un grupo de 2094 voluntarios africanos con una amplia diversidad genética y étnica. De la misma manera, llevaron a cabo un estudio genético con 1570 personas escogidas de entre aquellas a las que le fue cuantificado el nivel de melanina. Como resultado de su estudio, los investigadores identificaron 8 variantes en el genoma africano que influencian fuertemente el color de la piel, algunas oscureciéndola y otras aclarándola. Tishkoff y colaboradores encontraron también que algunas variantes genéticas que determinan el color de la piel están presente en poblaciones fuera de África. Y, con relación a esto, y de manera sorprendente, encontraron dos genes que afectan el color de la piel, del pelo y de los ojos en la población europea, que están también presentes y activos en la población San del sur de África, de piel relativamente clara. Dichas variantes genéticas se habrían desarrollado en África hace cerca de un millón de años, cuando la aparición de nuestra especie estaba aun muy lejana en el tiempo. En base a sus resultados Tishkoff  arguye que usar el color de la piel para clasificar a los humanos no tiene más validez que usar para este propósito otra característica como sería la altura. En particular, la raza africana, caracterizada por un determinado color de la piel, no existe como concepto, dada la gran la variedad de tonalidades de piel que es posible encontrar en África. Los racistas y supremacistas blancos han estado y están de este modo en un craso error. Después de todo, la piel blanca en último término resulta de una adaptación al medio. En efecto, como apunta Sarah Tishkoff, si rasuramos a un chimpancé –que se separó de nuestra línea evolutiva hace unos seis millones de años– encontraremos que tiene la piel pálida, pues no necesita de melanina para protegerse de los rayos solares.",
    "En un artículo publicado en 1864 en la revista The Quarterly Journal of Science, William King, profesor de Queens University en Irlanda, describe las características de los restos fósiles encontrados en 1857 en una cueva del Valle de Neander en Alemania, y concluye que el individuo al que pertenecieron tenía un cráneo, más cercano al de los simios que al de los humanos. Basado en esto sugiere que probablemente la capacidad intelectual del individuo en cuestión era similar a la de los simios. En particular, afirma que no habría tenido el intelecto suficiente para concebir la existencia de Dios ni para asumir obligaciones morales. Para remachar sus argumentos, King apunta que sería difícil concebir humanos con una capacidad inferior al de los aborígenes de la islas Andamán, que clasifican todavía como humanos por la forma de su cráneo pero que en cuanto a intelecto están apenas por encima de los animales. No habría de este modo ninguna posibilidad de que los neandertales y su cráneo deforme alcancen la categoría humana. Hoy, a 150 años de distancia, sabemos mucho más que King acerca de los fósiles del Valle de Neander. Para empezar, sabemos que pertenecen al llamado Hombre de Neandertal,  que  divergió de nuestra especie hace un medio millón de años y que hace unos 300,000 años emigró desde África hacia Europa y Asia. Físicamente, los neandertales eran de menor estatura, más fornidos y fuertes, y con extremidades más cortas en comparación con nuestra especie. Así, mismo, tenían un mentón reducido, una frente huidiza y huesos de las cejas prominentes.    Sin lugar a dudas los neandertales tenían un aspecto físico que los distinguía de nuestra especie –y seguramente escogeríamos no encontrarnos con alguno de ellos por la noche en un callejón sin salida–. No está claro, sin embargo, si en inteligencia se acercaban más a los simios a los humanos como sostenía King, pues su volumen craneal era igual o superior al nuestro –lo que seguramente lo habría sorprendido–. De hecho, los neandertales usaban herramientas, empleaban el fuego, eran cazadores y se cubrían con ropa. Incluso, algunos especialistas sostienen que enterraban a sus muertos y de que eran capaces de pensar en forma simbólica.Los neandertales subsistieron en Europa hasta hace unos 30,000 años cuando se extinguieron como especie por razones que no son claras. Una hipótesis que se ha aventurado al respecto es que los neandertales no sobrevivieron al contacto con poblaciones de humanos, que llegaron de África a Europa hace unos 100,000 años, por su supuesta inferioridad intelectual. Una hipótesis alternativa, sin embargo, es que los neandertales no habrían podido sobrevivir al cambio climático que afectó a Europa durante la glaciación.Cualquiera que haya sido la causa de la extinción de los neandertales, los especialistas tienen ahora pruebas de que, cuando menos en algunos casos, los encuentros entre éstos y los humanos fueron amistosos –o bien no con una intención puramente destructiva–. En efecto, sabemos ahora que las dos especies se cruzaron durante el tiempo en que coincidieron y que de esta cruza, el genoma de aquellos con ascendencia europea contiene un 2% de genes neandertales.   Aún más, un artículo publicado esta semana en la revista The American Journal of  Human Genetics, por Michael Dannemann y Janet Kelso del Instituto Max Planck en Leipzig, Alemania, encuentra que hay rasgos humanos en la actualidad que están influenciados por ADN neandertal. Para su estudio Dannemann y Kelso emplearon datos de más de 100,000 individuos proporcionados por el UK Biobank  Dicho banco de datos fue configurado mediante análisis genéticos y un cuestionario aplicado a voluntarios acerca de ellos mismos y que han llevado registro de su salud por tiempo largo.  Entre los rasgos que los investigadores encontraron asociados al ADN neandertal se encuentran el color de la piel, el color del pelo, los patrones de sueño, los estados de desánimo y el tabaquismo. El color de la piel y los patrones de sueño de los neandertales habrían estado determinados por su adaptación al clima de Europa con grandes cambios de iluminación solar a lo largo del año. En otros casos como el tabaquismo, dado que los neandertales presumiblemente no fumaban, la asociación es más difícil de explicar. Dannemann y Kelso, por otro lado, no encontraron una asociación entre el  ADN  neandertal y el color rojo del pelo, indicando que no habría habido neandertales pelirrojos.De resucitar William King encontraría un mundo completamente diferente de aquel en el que le tocó vivir. Esto no solamente por el apabullante progreso tecnológico que ha alcanzado la civilización occidental –y ahora también la civilización oriental no europea– desde su muerte ocurrida en 1886, sino también por lo fuera de época que resultan los comentarios, supuestamente científicos pero marcadamente prejuiciados, expresados en su artículo sobre los fósiles neandertales. Y, por supuesto, habría que ver su reacción de sorpresa al enterarse que él mismo es 2%  neandertal.",
    "El próximo miércoles 4 de octubre se cumplirán 60 años de que fue puesto en órbita por la Unión Soviética el Sputnik 1, el primer satélite artificial de la historia, con lo que se inició la llamada era espacial. La Unión Soviética se adelantó así por casi cuatro meses a los Estados Unidos que, tras varios intentos fallidos, logró por fin poner en órbita su primer satélite, el Explorer 1, el 31 de enero de 1958. El éxito soviético produjo un efecto de shock en el público norteamericano que asumía que la tecnología de los Estados Unidos era superior a la de la Unión Soviética. Entre otras cosas, los norteamericanos habían logrado desarrollar la bomba atómica como parte del esfuerzo bélico durante la Segunda Guerra Mundial, con un despliegue de tecnología sin precedentes, y esto alimentaba la creencia en su superioridad tecnológica. La puesta en órbita del Sputnik 1, sin embargo, puso en claro que la tecnología soviética en materia espacial era en esos momentos superior a la norteamericana.Con relación a esto último, aun antes de que los norteamericanos lograran colocar en órbita al Explorer 1, los soviéticos ya habían lanzado con éxito su segundo satélite. En esta segunda ocasión incluso con un tripulante: la perra Laika, que se convirtió de esta manera en el primer ser vivo en visitar el espacio –un honor poco envidiable, por cierto, pues los soviéticos no contaban en esos momentos con la tecnología necesaria para regresarla a la Tierra, de modo que Laika se convirtió también en el primer ser vivo en morir en el espacio.En el marco de la Guerra Fría, el que los soviéticos llevaran la delantera en materia espacial produjo preocupación en el público norteamericano pues los cohetes que se empleaban para colocar satélites en órbita podían ser igualmente usados para lanzar bombas contra territorio norteamericano. Por otro lado, el relativo atraso tecnológico de los Estados Unidos fue atribuido a un deficiente sistema de educación en ciencias y matemáticas, y esto impulsó el desarrollo de nuevos enfoques y textos de enseñanza, tanto al nivel de la escuela preparatoria como universitario.   Se formaron así comités que incluían a científicos de universidades norteamericanas para el desarrollo de textos de enseñanza de la física con enfoques innovadores. Uno de estos textos, destinado a la escuela preparatoria, fue desarrollado por el comité “Physical Science Study Committee”, conocido como PSSC por sus siglas en inglés, que fue encabezado por Jerrold Zacharias y Francis Friedman del Instituto de Tecnología de Massachusetts, e incluyó la participación de maestros de física de nivel preparatoria.     El PSSC fue creado en 1956 y después de que el Sputnik 1 fue puesto en órbita recibió un incremento sustancial de apoyo por parte de la “National Science Foundation” de los Estados Unidos y  se dio a la tarea de desarrollar un nuevo texto para la enseñanza de la física, apoyado por material de laboratorio y material fílmico.El texto desarrollado por el PSSC  se publicó en 1960 y quizá le sea familiar a aquellos que cursaron la escuela preparatoria en la década de los años 60. No tuvo el éxito esperado, sin embargo, por ser un texto de difícil lectura y comprensión para el estudiante de preparatoria promedio. Se desarrollaron, igualmente, textos de física para el nivel universitario con una gran  calidad y enfoque innovador, pero que adolecen del mismo problema y contienen materiales y enfoques demasiado avanzados para el nivel al que están dirigidos.No obstante, y al margen del éxito relativo de los esfuerzos para innovar la enseñanza de las ciencias, como bien sabemos los Estados Unidos superaron a la Unión Soviética en la carrera para llevar una misión tripulada a la superficie de la Luna, demostrando que a fin de cuentas su desventaja tecnológica con respecto a los soviéticos en la primera etapa de la carrera espacial era sólo moderada, a pesar de los problemas educativos que padecía.Es interesante notar que aun hoy en día el nivel de física y matemáticas en la escuela secundaria en los Estados Unidos no es particularmente destacado a nivel mundial, y en las pruebas PISA de la OCDE los estudiantes norteamericanos ocupan lugares sólo de media tabla. Al mismo tiempo, hay una presencia notable de estudiantes extranjeros, fundamentalmente de China y la India, en las universidades norteamericanas en áreas de ciencia, tecnología, ingeniería y matemáticas, y en algunos casos los estudiantes extranjeros son incluso mayoría. En efecto, según estadísticas del Pew Research Center, en los años 2012-2013, un 57% de los estudiantes que recibieron un grado doctoral en un área de ingeniería fueron extranjeros. En áreas de computación, matemáticas y física, los números correspondientes son: 53%, 50% y 40%.El lanzamiento de Sputnik 1 hace sesenta años propinó un golpe al orgullo norteamericano al mismo tiempo que generó preocupación por la amenaza que representaba que la Unión Soviética en plena Guerra Fría dispusiera de cohetes con características superiores a los propios. Con el tiempo, sin embargo, los logros norteamericanos en materia espacial han superado a los de cualquier otro país del mundo. En contraste, el bajo nivel de la enseñanza en ciencias y matemáticas, que se diagnosticó como la causa de la superioridad soviética al inicio de la era espacial, aparentemente no ha tenido una evolución equivalente, al grado que los norteamericanos hoy en día no están aparentemente interesados en una preparación de alto nivel en tópicos científicos y tecnológicos.",
    "La tarde del domingo 22 de mayo de 1960 se produjo en el sur de Chile el mayor sismo del que se tiene registro y que ha llegado a ser conocido como el terremoto de Valdivia. Este sismo alcanzó una magnitud de 9.5 y, en conjunción con el maremoto que le siguió, destruyó gran parte del sur de Chile. De manera afortunada, a pesar de su magnitud, el terremoto de Valdivia sólo produjo alrededor de 2,000 personas fallecidas. ¿Qué tan grande es un sismo de magnitud 9.5. Para responder a esta pregunta hay que notar primeramente que la escala en la que se mide la magnitud de un terremoto puede resultar engañosa, pues dicha escala no es directamente proporcional a la energía liberada por el sismo. Lejos de esto, tenemos que un terremoto  de magnitud 9 libera 32 veces más energía que un sismo de magnitud 8 y 1,000 veces más que uno de magnitud 7. Así, el terremoto de Valdivia fue unas 130 veces más potente que el sismo de magnitud 8.1 que devastó la Ciudad de México el 19 de septiembre de 1985 con un saldo de más de 10,000 personas muertas.De este modo, la devastación que puede producir un terremoto depende no solamente de su magnitud sino de situaciones y circunstancias particulares. En el caso del terremoto de Valdivia, por ejemplo, éste fue precedido minutos antes por otro de menor magnitud, lo que alertó a la población que se puso a salvo fuera de las edificaciones que terminaron por colapsar. En el caso de la Ciudad de México, además de la gran concentración urbana que padece, el subsuelo de la ciudad, que alguna vez fue parte de un lago, amplificó las ondas sísmicas y provocó el colapso selectivo de edificios con alturas alrededor de los 20 pisos.Otro ejemplo más cercano lo constituyen los sismos del 7 y el 19 de septiembre pasados, con magnitudes respectivas de 8.1 y 7.1. Como sabemos, el impacto que tuvo sobre la Ciudad de México el sismo de 7 de septiembre fue mucho menor que el del 19 de mismo mes, a pesar de ser 30 veces más potente. Esto fue producto de la diferencia de distancias entre la ciudad y los epicentros de ambos terremotos. En efecto, el sismo del 19 de septiembre fue particularmente destructivo debido a que su epicentro se localizó apenas a 100 kilómetros de Ciudad de México,  en contraste con el sismo de 7 de septiembre que tuvo su epicentro en el Océano Pacífico, cerca de la frontera de Chiapas y Oaxaca. Al margen de lo anterior, el hecho es han ocurrido dos sismos de gran magnitud en un lapso de apenas doce días y esto ha llamado la atención de algunos expertos que se preguntan si habría habido alguna una relación entre ambos. Un estudio preliminar llevado a cabo por científicos de la compañía Temblor Inc., que puede consultarse en Internet, sin embargo, concluye que la distancia entre los epicentros de los sismos de los pasados 7 y 19 de septiembre es demasiado grande para que se hubiera dado esta relación. Fueron, pues, dos eventos independientes, si bien como resultado de la interacción de las mismas placas tectónicas.Hay que recordar, por otro lado, que los sismos se producen al liberarse los esfuerzos que se generan por la interacción entre dos placas tectónicas. En el sureste de México esta interacción ocurre a lo largo de la costa del Océano Pacífico, desde Michoacán hasta Chiapas, entre la placa continental de Norteamérica y la placa de Cocos en el Océano Pacífico. Esta última está moviéndose hacia el este a razón de unos 6 centímetros por año, penetrando por debajo de la placa continental. Los expertos saben, además, que  la placa de Cocos, después de un descenso inicial, penetra horizontalmente hacia el continente por unos 300 kilómetros y a partir de ahí desciende rápidamente.  Una característica que distingue al sismo del pasado 19 de septiembre es que su epicentro se localizó alejado de la costa del Océano Pacífico, en el punto en donde la placa de Cocos inicia su descenso rápido a una profundidad de 50 kilómetros, según los especialistas.  Es decir, a cientos de kilómetros del punto en el Océano Pacífico en donde se encuentran las placas de Cocos y de Norteamérica en el Océano Pacífico, y a tiro de piedra del mayor centro urbano del país.La Ciudad de México es así una ciudad atípica que concentra a una de las poblaciones más grandes del mundo y buena parte de la población de México. Una ciudad que se encuentra localizada a 2,000 metros de altura lejos del mar, y en un valle rodeado de montañas que agravan el problema de contaminación ambiental. Y, por si fuera poco,  en un área de gran sismicidad y con un subsuelo que amplifica las ondas sísmicas, peligro del que la ciudad aparentemente no se hace cargo con el suficiente rigor. De otro modo, como es posible que después de la experiencia del sismo de 1985, después del pasado 19 de septiembre encontremos edificios completamente colapsados y rodeados de construcciones que sí lograron permanecer en pie.  Por cierto, después del sismo de Valdivia estuvo en peligro la organización de la copa mundial de futbol programada para llevarse a cabo en Chile dos años después. Afortunadamente, y de manera sorprendente, Chile logró superponerse a sus desventuras y organizó una copa memorable en la que incluso alcanzó el tercer lugar.",
    "Cuando la noche del 7 de enero de 1610 Galileo apuntó su telescopio hacia Júpiter, descubrió cerca del planeta tres pequeños puntos luminosos que inicialmente pensó eran estrellas. No tardó mucho en convencerse, sin embargo, que los tres puntos luminosos –cuatro, como después averiguó– eran satélites  que giraban alrededor de Júpiter, de manera similar a como los planetas giran alrededor del Sol.Galileo dirigió también su telescopio hacia Saturno, que es después de Júpiter el segundo planeta más grande del Sistema Solar. Si bien el brillo de Saturno es relativamente pequeño por su lejanía y no es especialmente llamativo a simple vista, es el planeta más espectacular del Sistema Solar por estar rodeado de anillos. Desafortunadamente para Galileo, su telescopio –avanzado para la época pero con un poder de resolución muy pobre según estándares actuales–, no le permitió identificar a los anillos de Saturno como tales y sólo los vio como dos protuberancias colocadas en lados opuestos del planeta. Mejores telescopios produjeron imágenes más claras y en 1655 el astrónomo holandés Christiaan Huygens describió a lo anillos de Saturno como un disco delgado y plano que rodea al planeta. El astrónomo italiano-francés Giovanni Cassini, por su lado, descubrió que este disco está dividido en dos anillos concéntricos por una brecha oscura conocida como la división de Cassini. Hoy sabemos que los anillos que rodean a Saturno tienen una estructura muy complicada, con múltiples anillos planos concéntricos que reflejan la luz del Sol en grados diversos.El conocimiento que tenemos de Saturno proviene tanto de observaciones hechas desde la Tierra como de las naves interplanetarias que se han enviado a explorarlo. La última de ellas es la nave Cassini –nombrada en honor a Giovanni Cassini–, lanzada por la NASA y la Agencia Espacial Europea y que acaba de terminar su misión de 20 años en el espacio el pasado 15 de septiembre.La nave Cassini partió con rumbo a Saturno en octubre de 1997 y después de un viaje de siete años arribó a su destino. Una vez ahí entró en órbita alrededor de Saturno para empezar la exploración del planeta y sus alrededores, la cual se prolongó por trece años. Dicha exploración produjo, entre otras muchas cosas, imágenes espectaculares de los anillos de Saturno vistos desde diferentes perspectivas, que sólo son posibles con una cámara fotográfica colocada en la inmediaciones del planeta. La nave envió, igualmente, imágenes con gran detalle de la capa gaseosa exterior de Saturno, incluyendo la curiosa formación hexagonal localizada en su polo norte y que gira sobre sí misma cada once horas. Dedicó la nave Cassini también tiempo a la exploración de los satélites de Saturno y encontró, por ejemplo, que en Encélado, el sexto satélite más grande, existe un océano de agua líquida de 10 kilómetros de profundidad debajo de una capa superficie de hielo de 30-40 kilómetros de espesor. Este descubrimiento coloca a Encélado como uno de los posibles lugares en el Sistema Solar que albergue vida microbiana.A bordo de la nave Cassini viajó la sonda Huygens –nombrada en honor a Christiaan Huygens– que fue lanzada hacia la superficie de Titán –el mayor satélite de Saturno–. Después de un descenso de dos horas y media a través de la atmósfera de Titán, la sonda Hyugens, frenando su velocidad con un paracaídas, logró posarse suavemente sobre la superficie del satélite desde donde trasmitió imágenes por más de una hora.Todo, sin embargo, tiene un principio y un final, y el pasado día 15 de septiembre, después de agotar todo su combustible, la nave Cassini fue lanzada en caída libre hacia Saturno, desintegrándose por el rozamiento con la atmósfera del planeta. Según la NASA, esta maniobra tuvo el propósito de eliminar la posibilidad de contaminar Encélado o Titán con microbios terrestres y así preservarlos prístinos para futuras investigaciones sobre el origen de la vida. Terminó así una de las exploraciones espaciales más exitosas, que, de acuerdo a la NASA, produjo una enorme cantidad de datos científicos cuyo análisis tomará un buen número de años.   Hace apenas cuatro siglos que Galileo apuntó su telescopio al cielo y descubrió que Júpiter es un sistema solar en miniatura con satélites girando a su alrededor. Esto, por supuesto, sabemos que es correcto. De hecho, contribuyó decisivamente a cambiar nuestra concepción antropocéntrica del mundo, pues demostró que no todo tiene necesariamente que girar alrededor de la Tierra. Apuntó Galileo también su telescopio hacia Saturno y en esta ocasión no tuvo tanta suerte, pues no pudo apreciar al planeta en toda su espectacularidad. La tecnología de su tiempo no se lo permitió. En ese sentido, después de ver las espectaculares imágenes enviadas por Cassini desde Saturno, podemos sentirnos afortunados por haber nacido cuatro siglos  después.",
    "Entre los metales comunes, el oro, sin duda, tiene un lugar especial entre nosotros; entre otras cosas, por su color y brillo que no pierde con el tiempo al contrario de otros metales como la plata o el cobre. El oro se conoce desde tiempos muy antiguos dado que es posible encontrarlo en una forma casi pura en la naturaleza. Es, por otro lado, un metal muy blando que tuvo usos limitados  en tiempos prehistóricos más allá de los ornamentales.El oro no fue el único metal conocido al inicio de la civilización. También lo fueron otros metales como la plata y el cobre que se encuentran igualmente en forma nativa en la naturaleza y que al igual que el oro, tuvieron usos decorativos y ornamentales. Este último, dada su relativa dureza, se empleó también en la fabricación de armas y diversos utensilios.El uso del cobre, sin embargo, fue limitado en la medida en que se dispuso solamente del cobre encontrado en forma nativa y su empleo más extendido tuvo que esperar al desarrollo de métodos para extraerlo de minerales ricos en el mismo. Una vez que se descubrió que calentando a alta temperatura minerales como la malaquita y la azurita es posible obtener cobre metálico, se produjo una revolución tecnológica que dio origen a lo que se llama la Edad del Cobre. De esta edad se derivó la Edad del Bronce hace unos 4,500 años, cuando se descubrió que añadiendo estaño al cobre se incrementa considerablemente su dureza y por tanto su rango de aplicaciones. Se derivó también, hace unos 3,000 años, la Edad del Hierro que produjo una revolución tecnológica todavía mayor. La invención de la metalurgia del cobre dio de este modo origen a una cadena de desarrollos tecnológicos que han sido clave para el desarrollo de la civilización y cabe preguntarse por la cadena de acontecimientos que dieron origen a dicha metalurgia.Dado que en la época en que se produjo el desarrollo de la metalurgia del cobre no se contaba con ningún conocimiento teórico sobre la naturaleza de los materiales involucrados, los descubrimientos que llevaron a dicho desarrollo tendrían  que haber ocurrido por azar. Es posible, por ejemplo, que alguien hubiese arrojado por accidente minerales de malaquita a un fuego con la temperatura suficiente para que se liberara el cobre atrapado dentro de dichos minerales. Esto y otras observaciones similares habrían dado pautas para reproducir la observación por medio de procedimientos de prueba y error, lo que podría haber ocurrido a lo largo de muchas generaciones. Hay que hacer notar, sin embargo, que para liberar el cobre es necesario alcanzar una temperatura que ronda los 1,100 grados centígrados, lo cual no es sencillo de alcanzar en un fuego abierto.De un modo u otro, es un hecho que nuestros ancestros descubrieron como extraer cobre de minerales como la malaquita y la azurita en tiempos prehistóricos y en relación a esto resulta interesante preguntarse sobre sobre el lugar y la época en que esto habría ocurrido.En base a evidencia arqueológica sobre la metalurgia del cobre, la Edad del Cobre se habría iniciado hace unos 7,000 años en una región que abarca desde los Balcanes hasta Irán. Esta fecha, sin embargo, fue puesta en duda por el descubrimiento en el sitio neolítico de Catalhoyuk en Turquía de lo que aparenta ser la escoria resultante del proceso de fundición de minerales de cobre. Dicha escoria tiene una antigüedad de 8,500 años y movería la fecha del inicio de la Edad del Cobre 1,500 años hacia el pasado. Catalhoyuk sería así la cuna de la metalurgia del cobre, desde donde se habría dispersado a otros sitios en Europa y el Asia Central.Con el objeto de comprobar o desechar esta posibilidad, un grupo de investigadores en el Reino Unido y Alemania, encabezados por Miljana Radivojevic de la Universidad de Cambridge en el Reino Unido, llevó a cabo un estudio analítico de los materiales encontrados en Catalhoyuk. Los resultados de este estudio aparecieron esta semana en la revista “Journal of Archaeological Science”. En base a sus investigaciones, Radivojevic y colaboradores no encuentran evidencia que los materiales estudiados correspondan a escoria de un proceso de fundición de cobre. Por el contrario, concluyen que resultaron de una cierta cantidad de mineral de cobre depositado en una tumba que fue posteriormente afectada por un fuego intenso que incluso carbonizó el cadáver. Resultaría así que la metalurgia del cobre se habría iniciado hace unos 7,000 años y no hace 8,500 años como la evidencia de Catalhoyuk sugería. Además, según Radivojevic y colaboradores, queda abierta la posibilidad de dicha metalurgia se hubiera desarrollado de manera simultánea en varios sitios de Europa y el Asia Central.Hoy en día tenemos un conocimiento teórico profundo acerca de los metales -entendemos, por ejemplo, por qué el oro tiene ese color y por qué nunca pierde su brillo-, lo mismo que de un gran número de materiales de todo tipo. Basados en este conocimiento, entendemos cuales son los principios que gobiernan la metalurgia de los metales y en general la tecnología de un gran número de materiales. Esto nos ha llevado a fabricar incluso materiales que no existen en la naturaleza. Y todo esto lo hemos logrado en apenas en 7,000 años, tiempo muy breve en términos del tiempo de evolución de nuestra especie.",
    "La semana que hoy termina la prensa dio cuenta del transporte de una carga de gas natural por el navío ruso Christopher Margerie de 300 metros de eslora entre los puertos de Hammerfest en Noruega y Boryeong en Corea del Sur. Dicho acontecimiento no tendría nada en particular si no fuera porque el viaje del navío ruso se llevó a cabo por el llamado Paso del Noreste, que corre desde el norte de Europa a lo largo de la costa ártica rusa y cruza hacia el Océano Pacífico a través del estrecho de Bering. El viaje, además, se realizó sin la ayuda de barcos rompehielos, pues el Christopher Margerie es un barco de construcción especial para la navegación en mares helados .La ruta por el Paso del Noreste representa un ahorro de 30% en la duración del viaje entre Europa y Asia en comparación con la ruta a través del canal de Suez y por tanto resulta de gran atractivo comercial. Con anterioridad el Paso del Noreste no era practicable por los hielos árticos que lo obstruían. Esta situación está cambiando, sin embargo, por efecto del calentamiento global, que ha reducido los hielos árticos, tanto en volumen como en extensión, permitiendo la navegación durante los meses de mayor deshielo.  En una situación similar –si bien menos favorable en términos de navegabilidad– está el Paso del Noroeste que une a Europa con el océano Pacífico a través del ártico canadiense. Así, por ejemplo, el pasado año nos enteramos que el crucero turístico “Crystal Serenity” pudo realizar un viaje entre Alaska y Nueva York navegando a través del Paso del Noroeste aprovechando el deshielo de verano. El Paso del Noroeste fue afanosamente buscado por los europeos desde el siglo XVI. De hecho, para algunos países como Inglaterra dicho paso era esencial en virtud de que las rutas por el sur, ya sea rodeando África o América del Sur, estaban monopolizadas por portugueses y españoles. El Paso del Noroeste fue finalmente descubierto en el siglo XIX, mucho más al norte de lo inicialmente esperado y bloqueado por los hielos árticos. Una expedición de búsqueda de una vía navegable por el Paso del Noroeste que es históricamente notable fue la comandada por el inglés John Franklin que terminó en desastre. La expedición al mando de Franklin, que contaba con dos barcos, el HSM Erebus y el HSM Terror, y 105 hombres al mando de 24 oficiales, partió de la costa oeste de Groenlandia en el verano de 1845 y enfiló hacia el oeste con rumbo al estrecho de Lancaster. Las dificultades para avanzar, no obstante, forzaron a los expedicionarios a invernar en la isla de Beechey en donde murieron tres hombres de la tripulación. Al fin del invierno la expedición enfiló hacia el sur. Con muy poca suerte, sin embargo, pues en septiembre de 1846 el Erebus y el Terror fueron atrapados por el hielo cerca de la isla Príncipe Guillermo, forzando a la tripulación a invernar por segunda ocasión. A partir de aquí no es claro cual fue el curso que siguieron los expedicionarios que habrían tenido que abandonar los barcos atrapados en el hielo y caminar hacia el sur para buscar salvarse. Desafortunadamente, no tuvieron éxito y todos murieron en el intento. En septiembre del año pasado, un equipo de la fundación privada “Artic Research Foundation” descubrió los restos de “HMS Terror” sumergidos unos 24 metros en el agua, a unos 100 kilómetros al sur de donde se pensaba había quedado atrapado en el hielo. Dos años antes se habían encontrado también los restos del “HMS Erebus” sumergidos 11 metros en una posición todavía más al sur. Estos descubrimientos han puesto en duda la versión del abandono de los barcos una vez que fueron atrapados por el hielo y abre la posibilidad de que los tripulantes o parte de ellos hubieran intentado dirigirse hacia el sur a bordo de los mismos. La historia de la desafortunada expedición de John Franklin pone en perspectiva la situación del Paso del Noroeste –y por extensión, el Paso del Noreste– hace apenas  150 años cuando era, si no inexpugnable, si con mucho comercialmente inviable para las condiciones ambientales y posibilidades tecnológicas de la época. Hoy en día la situación ha cambiado en forma marcada, no solamente porque la tecnología actual permite el tránsito de navíos en el ártico en condiciones ambientales que eran intransitables en el siglo XIX, sino porque las mismas condiciones ambientales se han modificado por el calentamiento global.De este modo, en la medida en que se incremente  la temperatura del planeta, las vías árticas del transporte se harán cada vez más atractivas y por tanto más congestionadas. Los expertos anticipan también que conforme se libere el Océano Ártico de los hielos y se expandan las vías de transporte marítimo se abrirán a la explotación recursos minerales de la región que hoy no son comercialmente viables. Se generarán así condiciones para incrementar la actividad industrial y en consecuencia generar  una mayor contaminación ambiental y deshielo ártico.  Así, desde el punto de vista de la calidad de vida, el siglo XXI tiene ciertamente  muchas ventajas sobre el siglo XIX. El libre tránsito por los pasos del noroeste y del noreste, sin embargo, no es una de ellas.",
    "¿Qué tanto recuerda de sus cursos de trigonometría y geometría de la escuela secundaria? Con seguridad recuerda que, entre otras muchas cosas, nos enseñaron a distinguir entre un ángulo recto y uno que no lo es, y que en el primer caso el ángulo está definido por dos líneas perpendiculares entre sí que se intersectan en un punto. Aprendimos también que un ángulo recto mide 90 grados y que un ángulo agudo mide menos que eso. La escuela secundaria nos capacitó, igualmente, para identificar a un triángulo rectángulo, uno de cuyos ángulos es recto y los otros dos agudos.Todo lo anterior no fue difícil de comprender y recordar. Un poco más complejos fueron otros tópicos y conceptos que forman parte de los cursos de trigonometría, incluyendo a las funciones trigonométricas –seno, coseno, tangente, entre otras–. Sin olvidar, por supuesto, al célebre teorema de Pitágoras que establece una relación entre la longitud de los lados –catetos– de un triángulo rectángulo con la longitud de su hipotenusa. Tanto las funciones trigonométricas como el teorema de Pitágoras nos permiten conocer el valor de una de las longitudes de un triángulo rectángulo –la hipotenusa o cualquiera de los dos catetos– dadas las otras dos longitudes y tienen por tanto una enorme utilidad práctica. En el trazado, por ejemplo, de un templo o una pirámide.  Por otro lado, es muy posible que si al terminar la escuela secundaria usted no continuó usando las funciones trigonométricas las haya ya olvidado en buena medida. Por el contrario, seguramente no tendrá ningún problema para evocar de manera clara a un ángulo rectángulo o a un ángulo agudo, pues el concepto abstracto de ángulo –generado por la intersección de dos rectas– es ya parte de nuestra cultura.No ha sido siempre así, sin embargo, y en este respecto es interesante traer a colación un artículo aparecido esta semana en la revista “Historia Mathematica”, cuyos autores son David Mansfield y Norman Wildberger, matemáticos de la Universidad de Nueva Gales del Sur en Australia. En dicho artículo se reporta un estudio llevado a cabo con la tablilla de arcilla conocida como Plimpton 322, la cual fue descubierta en el sur de Irak hace casi 100 años. Dicha tablilla tiene una antigüedad de alrededor de 3,800 años y tuvo su origen en la antigua ciudad sumeria de Larsa cerca del Golfo Pérsico.     La tablilla Plimpton 322 mide 12.7 cm por 8.8 cm y muestra claramente que sufrió una fractura, posiblemente en tiempos recientes dado que contiene restos de un pegamento moderno. Muestra cuatro columnas y quince renglones igualmente espaciados de signos cuneiformes. Estos signos  corresponden a números representados en un sistema sexagesimal, que es el que empleaban los sumerios.  Algo que ha llamado la atención de los expertos desde que la tablilla fue estudiada en la década de los años cuarenta es que de los números contenidos en la misma se derivan tripletes de números consistentes con el teorema de Pitágoras. Como sabemos, este teorema establece que si elevamos al cuadrado los dos catetos  de un triángulo rectángulo y los sumamos el resultado es igual al cuadrado de la hipotenusa. Un ejemplo de un triplete de esta clase son los números 3, 4 y 5.El anterior es un resultado sorprendente y sugiere que la tablilla Plimpton 322 tuvo una utilidad matemática. Con relación a esto, se ha sugerido que fue empleada como una ayuda para la enseñanza de las matemáticas. Mansfield y Wildberger, por el contrario, concluyen que la Plimpton 322 es en realidad una tabla trigonométrica que, al igual que las tablas equivalentes contemporáneas, permite calcular una de la longitudes de un triángulo rectángulo dadas sus otras dos longitudes. Por otro lado, a diferencia de lo que sucede hoy en día, el concepto de ángulo no existía entre los matemáticos sumerios y por lo mismo está ausente de la tablilla Plimpton 322, cuyos números y procedimientos se refieren solamente a las longitudes de los lados de un triángulo rectángulo. Además, a diferencia de una tabla trigonométrica contemporánea, que es aproximada hasta un cierto grado, los números consignados en la tablilla Plimpton 322 son exactos.Así, si bien son los griegos a los que se acredita haber inventado la trigonometría, de estar Mansfield y Wildberger en lo cierto, los sumerios se les habrían adelantado por mil años, mostrando un sorprendente nivel de sofisticación matemática, si bien poca capacidad para transmitir sus conocimientos a la posteridad. Esto, en fuerte contraste con los griegos. De todo lo anterior, y asumiendo que Mansfield y Wildberger tienen efectivamente la razón, resultaría que la trigonometría, un tópico que a muchos nos provocó dolores de cabeza en la escuela secundaria, es en realidad algo que fue inventado por los sumerios, para nuestra consternación, hace ya miles de años. Igualmente, resultaría que el concepto de ángulo, que nos parece tan natural, no lo es tanto en realidad. De hecho, su ausencia no fue obstáculo para que los sumerios desarrollaran la trigonometría y la aplicaran en la construcción de templos, palacios y canales.",
    "Según un artículo publicado en el año 2003 en la revista “Journal of Astronomical History and Heritage” por Ciyuan Liu, Xueshun Liu y Liping Ma de la Academia de Ciencias de China, en la obra literaria “Libro de documentos” de la China antigua se puede encontrar el siguiente pasaje: “En el primer día del último mes de otoño el Sol y la Luna no tuvieron un encuentro armonioso en Fang. Los músicos ciegos tocaron sus tambores; los funcionarios inferiores y la gente común se agitaron y salieron corriendo. He y Ho, sin embargo, como si fueran meros representantes de los muertos en sus oficinas, no supieron ni oyeron nada; -se apartaron de manera tan estúpida de sus obligaciones en materia de apariciones celestiales, que se hicieron acreedores a la pena de muerte fijada por el rey”. No es difícil concluir que el episodio anterior corresponde a la ocurrencia de un eclipse de sol el cual habría tenido lugar hace unos cuatro mil años; dicho eclipse sería el primero cuya descripción ha llegado hasta nosotros. No cuesta tampoco mucho trabajo concluir que He y Ho eran los astrónomos reales y que entre sus obligaciones estaba la de predecir la ocurrencia de los eclipses. Esto, con el fin de que el emperador se preparara adecuadamente para enfrentarlos, pues el sol era el símbolo del emperador que se veía amenazado por cualquier alteración que aquel sufriera. Así, según Ciyuan Liu, para combatir a un eclipse de sol el emperador normalmente adoptaba una dieta vegetariana, evitaba el palacio imperial y practicaba rituales para salvar al sol. En esta circunstancia, He y Ho cometieron una muy grave falta y, ciertamente, a ojos del emperador su castigo estuvo más que merecido.Es entendible que la repentina desaparición del Sol durante un eclipse resultara amenazadora en tiempos antiguos ante la ausencia de una explicación convincente. Con el transcurrir de los siglos, esta situación ha cambiado drásticamente y hoy en día no solamente podemos predecir con gran exactitud la ocurrencia de un eclipse, sino que entendemos plenamente la mecánica por la cual ocurren. En efecto, sabemos que un eclipse de sol  se produce cuando la Luna se interpone entre el Sol y nuestro planeta bloqueando la radiación solar. Sabemos también que si la órbita de la Luna alrededor de la Tierra y de la Tierra alrededor del Sol estuvieran en el mismo plano, tendríamos un eclipse solar cada vez que la Luna se interpusiera entre el Sol y la Tierra; es decir, cada luna nueva. No ocurre esto, sin embargo, porque la órbita de la Luna está inclinada por más de 5 grados con respecto a la órbita terrestre, de modo que la mayor parte de las veces que la Luna se sitúa entre el Sol y La Tierra lo hace en una posición por arriba o por abajo del plano de la órbita terrestre. No bloquea así la radiación del Sol. La inclinación de la órbita de la Luna hace relativamente poco frecuentes los eclipses de sol, que repiten en ciclos de aproximadamente 18 años con cerca de 42 eclipses por ciclo. Este periodo es largo para que un solo astrónomo en la antigüedad pudiera haberlo descubierto. No lo fue, sin embargo, para que lo lograran varias generaciones de astrónomos y, de hecho, la regularidad de los eclipses se conoce desde hace miles de años. Por otro lado, si bien con el descubrimiento de esta regularidad tendría que haber disminuido el nivel de ansiedad que los eclipses de sol provocaban, como eventos relativamente poco frecuentes, estos eclipses han sido tradicionalmente asociados de manera supersticiosa con desastres y malos augurios. El profundo entendimiento de la mecánica de los eclipses, por ejemplo, permite a los especialistas predecir que el día de mañana, lunes 21 de agosto,  ocurrirá un eclipse total de sol que atravesará el territorio de los Estados Unidos de costa a costa, desde Oregón en el oeste hasta Carolina del Sur en el este. Nos permite también predecir que en México el eclipse será sólo parcial y que en San Luis Potosí alcanzará un máximo de 44%, aproximadamente a las 13:10 –según el sitio timeanddate en Internet. Así, de haber nacido He y Ho cuatro mil años después y de haber contado con una conexión de Internet con seguridad no habrían tenido el triste fin al que los condenó el emperador: Por supuesto, es posible también que el emperador –de haber sobrevivido como tal– no les hubiera dado trabajo como astrónomos reales por no representarle en estos tiempos utilidad alguna. Por lo demás, la autenticidad de la colorida historia de He y Ho no ha sido demostrada de manera sólida según Cyuan Liu y colaboradores, si bien historias similares aparecen en fuentes chinas diversas. De ser cierta la historia de He y Ho probaría que los eclipses podían ser predichos desde hace más de cuatro mil de años. Probaría también una de dos posibilidades: 1) que He y Ho eran incompetentes como astrónomos o 2) que eran en extremo irresponsables y temerarios.",
    "El pasado 29 de julio se cumplieron 127 años desde la muerte del pintor holandés Vincent van Gogh, quien se suicidó en 1890 de un balazo en el pecho a los 37 años. A pesar de su relativamente breve existencia, van Gogh pintó del orden de 900 cuadros. Los pintó, además, durante sus últimos diez años de vida, pues antes de dedicarse a la pintura, lo hizo a una serie de actividades de lo más variado que, sin duda, reflejan lo inestable de su carácter. Trabajó así para la sucursal de La Haya de una galería de arte parisina, también como ayudante en una librería en la ciudad holandesa de Dordrecht, como ayudante de un pastor protestante en Inglaterra, como estudiante de teología en Amsterdam y como misionero en unas minas en la región de Mons en Bélgica. Solo después de establecerse en Bruselas en 1880 fue que resolvió dedicarse a la pintura.Aun para los que no somos conocedores en materia de pintura, es evidente que la pintura de van Gogh evolucionó grandemente en un tiempo corto. Baste comparar los colores oscuros de “Los comedores de patatas” de 1885, con los colores vívidos, con abundante amarillo, de “El café de noche” de 1888.  Por otro lado, aunque no hay un diagnóstico certero, se sabe que van Gogh sufrió de padecimientos mentales que incluso lo llevaron a ser internado en sanatorios hacia el final de su vida, y al respecto es ilustrativa la anécdota según la cual se habría cortado una oreja con una navaja después de una disputa con el pintor Paul Gauguin. Dadas estas circunstancias, hay quienes se preguntan si las drogas que le fueron administradas para tratar sus padecimientos pudieran haber influido en el estilo de van Gogh como pintor; de manera específica, en su percepción de los colores. Si bien no se sabe con certeza con qué drogas fue medicado van Gogh, se ha especulado que pudiera haber sido tratado con un derivado de la planta conocida como dedalera, que era usada en la época para tratar, además de padecimientos cardiacos, diversas enfermedades neurológicas y psiquiátricas. Se sabe, por otro lado, que una sobredosis de dedalera produce un cambio en la percepción de los colores que adquieren un tinte amarillo, lo que estaría relacionado con el uso generoso de este color en algunas pinturas de van Gogh. Otro efecto de la intoxicación con dedalera es la percepción de círculos alrededor de un punto luminoso, efecto que es también visible en algunas pinturas de van Gogh. En particular, este efecto es notable en “La noche estrellada”, que presenta una vista nocturna desde la ventana de su cuarto en el manicomio de Saint-Remi-de-Provence en el sur de Francia, en donde voluntariamente se recluyó hacia el final de sus días.La hipótesis sobre el uso de la dedalera por van Gogh está apoyada por el cuadro “El retrato del doctor Gachet”, en donde el pintor retrató al médico que lo atendió en sus últimos meses de vida. En dicho cuadro, el doctor Gachet aparece sentado en una mesa pensativo, destacándose sobre la mesa en la parte inferior del cuadro una rama de dedalera. La aparición de una rama de dedalera en un cuadro de van Gogh, si bien no prueba su consumo por el pintor, sí establece su cercanía con la planta, a la que, curiosamente pintó con un color que no corresponde a ningún color natural. Hay que hacer notar, sin embargo, que el “Retrato de doctor Cachet” fue hecho en 1890, a pocos meses de la muerte de van Gogh, y que éste usaba el amarillo con profusión aun antes de conocer al doctor Cachet. Esto, por supuesto, no descarta que van Gogh hubiera empleado previamente la droga recetada por sus anteriores médicos.Además de la dedalera, se ha aventurado que van Gogh podría haber tenido una alteración en la percepción del color por el abuso en el consumo de licor de ajenjo que era muy popular en Paris entre los pintores de la época. Según otras opiniones, sin embargo, esto es poco probable por las grandes cantidades de ajenjo que tendría que haber consumido antes de tener un efecto apreciable. A pesar de lo atractiva que resulta la hipótesis de la intoxicación de van Gogh por dedalera y el impacto de ésta en su pintura, no tenemos suficiente evidencia para probarla o refutarla. Hay incluso quien considera que la hipótesis más probable es que el pintor haya simplemente decidido usar profusamente el amarillo con plena conciencia y por motivos artísticos, y no bajo el influjo de una droga.De un modo u otro, en el 127 aniversario de su trágica muerte, habría que agradecer a Vincent van Gogh por los cuadros que heredó a la posteridad –algunos, fantásticos, con profusión de  amarillo– y  lamentar que, en pago, la vida lo haya tratado tan mal.",
    "La luz eléctrica inició su expansión por el mundo hace poco más de cien años. Para ser precisos, desde que Edison comercializó las primeras lámparas incandescentes en el año 1880. Ciertamente, Edison no fue el primero en fabricar una lámpara incandescente. Sí, en cambio, fue el primero en desarrollar una lámpara con un tiempo de vida suficientemente largo para ser comercialmente viable.    Resulta notable que el diseño de las lámparas incandescentes que todavía se pueden adquirir comercialmente sea básicamente el mismo que el de la lámpara desarrollada por Edison hace más de un siglo. En efecto, dicha lámpara consistía de un filamento de bambú carbonizado que se calentaba por medio de una corriente eléctrica hasta el punto de generar luz visible.  El filamento se colocaba en el interior un bulbo de vidrio en cuyo interior se hacía vacío para evitar que el filamento caliente se consumiera por el contacto con el oxígeno del aire. La corriente eléctrica del filamento se introducía por medio de un socket metálico roscado en la base del bulbo de vidrio.Si sustituimos al filamento de bambú por uno de tungsteno –desarrollado en 1906– y al vacío en el interior del bulbo de vidrio por un gas inerte, la descripción anterior corresponde a la de una lámpara incandescente moderna. Las coincidencias entre las lámparas de Edison y las modernas demuestran que las primeras constituyen dispositivos extraordinariamente bien concebidos, que sobrevivieron a lo largo del siglo XX a la competencia con otras tecnologías de iluminación, incluyendo a las lámparas fluorescentes.  Con el correr de los años, sin embargo, las lámparas incandescentes se han encontrado con un adversario que les ha resultado difícil de vencer: los lámparas LED. Y es que las lámparas incandescentes, además de virtudes tienen también defectos. Uno de los mayores es su baja eficiencia para la producción de luz, pues de la energía eléctrica que consumen un 90 por ciento se convierte en calor  y en este respecto son ampliamente superadas por las lámparas LED.En un inicio la baja eficiencia de las lámparas incandescentes tenía una relevancia sólo relativa. Con las crisis energéticas que se han dado a partir de la década de los años 70 de siglo pasado y los problemas de contaminación ambiental y cambio climático que nos aquejan, dicha eficiencia ha adquirido una gran relevancia, habida cuenta que el 20 por ciento de la energía eléctrica que se genera a nivel global se utiliza para iluminación. En estas circunstancias, las fuentes luminosas LED están teniendo una cada vez mayor penetración en el mercado a costa, en parte, de las lámparas incandescentes.Además de una mayor eficiencia, las lámparas LED tienen otras virtudes, incluyendo su robustez y larga duración. No hay nada perfecto, sin embargo, y estas lámparas tienen también sus desventajas. Una de ellas tiene que ver con su alto contenido de radiación azul. En efecto, en su versión más simple, una fuente LED de luz blanca consiste de un emisor primario de luz azul que es cubierto con un capa de un material que al ser excitado por la luz azul emite luz amarilla. La combinación de la luz azul primaria y la luz amarilla secundaria es percibida por nosotros como luz blanca. La radiación azul emitida por las lámparas LED es motivo de preocupación por parte de los especialistas pues se le ha asociado a diversos problemas de salud. En particular, existe evidencia de que el uso de una iluminación nocturna con alto contenido de luz azul retrasa la producción de melatonina que es la sustancia que le indica al cuerpo que ha llegado la hora de dormir. De este modo, la exposición nocturna a la luz azul  interfiere con el ritmo circadiano y el ciclo del sueño.    Un estudio que evidencia esto último fue publicado en el número de julio del presente año en la revista “Ophthalmic and Physiological Optics” por un grupo de investigadores de la Universidad de Houston, encabezado por Lisa Ostrin. El estudio de referencia fue llevado a cabo con 21 voluntarios con edades entre los 17 y los 42 años,  a los que se les pidió que usaran lentes de color amarillo que bloquean la luz azul y parte de la luz verde, tres horas antes de ir a dormir y durante dos semanas. Los voluntarios afirmaron haber concebido más rápidamente el sueño y haber tenido una mejor calidad del mismo después de usar los lentes amarillos. Además, experimentaron un incremento del nivel de melatonina de 58 por ciento y tuvieron 24 minutos más de sueño.Los humanos hemos evolucionado con la luz del Sol, que ciertamente tiene un gran contenido de luz azul, pero a la que hemos adaptado nuestro ritmo circadiano. En contraste, hemos estado bajo la influencia de la luz eléctrica  solamente por poco más de cien años y apenas estamos entendiendo en que grado somos afectados por la misma. En comparación con los tiempos del foco incandescente  –que tiene un contenido de luz azul relativamente pequeño– ahora estamos expuestos a mayores niveles de luz azul nocturna, no solamente por la lámparas LED empleadas para iluminación, sino también por la multitud de dispositivos que emplean dichas lámparas, incluyendo a las pantallas de computadora, las tablets y los teléfonos celulares que empleamos con profusión.  ¿Resultará la lámpara LED, tal como la conocemos ahora y dados los problemas que encara, un dispositivo tan exitoso como lámpara incandescente que vivió entre nosotros por más de cien años? Esto, por supuesto, sólo el tiempo lo dirá.",
    "Suponga que un habitante de la Europa medieval realiza un viaje hasta la segunda mitad del siglo XIX, –por medio, quizá, de la máquina del tiempo de H. G. Wells– y ahí se entera que el 23 de septiembre de 1846 el astrónomo alemán Johann Galle descubrió un octavo planeta después de seguir las indicaciones  del matemático francés Urban Le Verrier sobre su posible ubicación en el cielo. Las predicciones de Le Verrier se basaron en las perturbaciones observadas en el planeta Urano que sugerían la presencia de un planeta más allá de su órbita.Ante estos resultados, sin duda nuestro viajero del pasado estaría convencido que tanto Galle como Le Verrier eran magos: Galle por usar un instrumento–el telescopio– que le permitía ver cosas invisibles y Le Verrier por su poder de predicción. Dicho convencimiento no habría sido sorprendente, pues el telescopio no se inventó sino hasta finales el siglo XVI y la teoría de gravitación de Isaac Newton, en la que Le Verrier basó su predicción, no fue ideada por Newton sino hasta el siglo XVII; esto es, varios siglos después de la época de la que procedía nuestro supuesto visitante.  En la actualidad el telescopio ya no sorprende y de hecho es posible adquirir uno de estos instrumentos aun en un centro comercial. El poder de predicción del que hizo gala Le Verrier, por el contrario, resulta pasmoso aun hoy en día. El astrónomo francés Francisco Arago comentó que Le Verrier –que era su colaborador– “había descubierto un nuevo planeta con la punta de su pluma” y esto, que en su momento resultó asombroso, lo sigue siendo 150 años después.No obstante su éxito, sin embargo, no pensamos que Le Verrier fuera un mago. Al menos no una persona con poderes de adivinación o poderes ocultos que puedan alterar el curso de los fenómenos naturales y colocar a voluntad a un cuerpo celeste en una posición dada del cielo. De hecho, sin bien no es una creencia generalizada, ahora pensamos que no hay manera de alterar dicho curso ni de ver más allá de lo que nuestros cinco sentidos nos revelan. Por lo demás, tal parece que Le Verrier, aun con su evidente dominio de la teoría de la gravitación de Newton, necesitó de una cierta dosis de suerte para predecir la existencia del octavo planeta del Sistema Solar, que ahora conocemos como Neptuno. No tuvo esta misma suerte cuando, animado por su éxito anterior, en conjunto con Arago predijo la existencia de un nuevo planeta entre el Sol y Mercurio  que explicaría  las perturbaciones observadas en la órbita de este último. Esta hipótesis, sin embargo, se desechó cuando se descubrió que la teoría de la gravitación de Newton no es adecuada para describir con precisión la órbita de Mercurio.De la misma manera, tampoco han tenido suerte las predicciones de la existencia de un noveno planeta más allá de la órbita de Neptuno. En efecto, a principios del siglo XX el astrónomo aficionado Percival Lowell predijo dicha existencia basado en perturbaciones observadas en la órbita de Urano que no podían ser explicadas únicamente por la influencia de Neptuno –en contraposición con la hipótesis de Le Verrier–  y se dio a la tarea de encontrarlo. Infructuosamente, sin embargo. No obstante, después de su muerte el astrónomo Clyde Tombaugh prosiguió con la búsqueda del noveno planeta, ahora sí con éxito, al descubrir en 1930 a Plutón. Con el tiempo, sin embargo, quedó claro que Plutón es demasiado pequeño para ser el planeta predicho por Lowell. De hecho, Plutón fue degradado en 2006 por la Unión Astronómica Internacional a la categoría de planeta enano.Si bien hasta ahora no se ha probado la existencia de un noveno planeta en el Sistema Solar, algunos astrónomos están convencidos de su existencia. No basados en perturbaciones de la órbita de Urano que se descubrió en realidad no existen, sino en las perturbaciones de las órbitas de algunos planetoides más allá de la órbita de Neptuno. Con relación a esto, un artículo publicado en la revista “Monthly Notices of the Royal Astronomical Society: Letters” el pasado 27 de junio por astrónomos de la Universidad Complutense de Madrid, presenta nuevas pruebas en favor de la existencia de un noveno planeta. Las conclusiones de los astrónomos de la Universidad Complutense se basaron en un estudio de 28 cuerpos celestes con órbitas muy excéntricas, que los mantienen a una distancia promedio al Sol de 150 unidades astronómicas –una unidad astronómica es la distancia que hay entre el Sol y nuestro planeta–, y encontraron que se agrupan  de una manera tal que puede se explicada asumiendo la existencia de un noveno planeta situado a unas 300-400 unidades astronómicas del Sol. Las historias del descubrimiento del octavo planeta y de la búsqueda infructuosa del noveno, nos ilustra sobre algunas características del método científico. Por un lado nos demuestra que este método es extremadamente poderoso, al grado de posibilitar el descubrimiento “con la punta de la pluma”, de un planeta invisible a simple vista. Por otro lado, nos ilustra también que un descubrimiento científico depende, no solamente de lo preciso de una determinada teoría, sino que también requiere de una cierta dosis de suerte.En balance, sin embargo, la utilidad y el poder predictivo del método científico está años luz por delante de cualquier práctica mágica o supersticiosa que, en contraste, poco han demostrado a lo largo de miles de años.",
    "En un inicio, en la medida en que eran pocos, los automóviles mantuvieron entre nosotros una discreta presencia. Conforme creció su número, sin embargo, dicha presencia, lejos de discreta se ha vuelto avasalladora. Particularmente, aunque no de manera exclusiva, en países como México y en ciudades como la nuestra, en donde el número de vehículos ha crecido de manera espectacular en las últimas décadas. A manera de ejemplo, en la ciudad de San Luis Potosí, que no constituye caso único, el número de vehículos automotores se duplica cada ocho años y ha pasado de poco menos de cincuenta mil en 1980 a casi medio millón en  2015. Esto, de acuerdo a datos del INEGI.Los automóviles, que nos han traído tanto beneficios como problemas, tienen un papel protagónico y una constante presencia en nuestra vida. La semana que hoy termina, por ejemplo, hicieron noticia de primera plana debido a la iniciativa del gobierno de la Ciudad de México para poner una cota máxima al área destinada a estacionamientos en la construcción de nuevos inmuebles. El objetivo de esta reforma es desincentivar el uso del automóvil.Esta medida ha causado polémica. Ciertamente, el número de automóviles circulando en el área metropolitana de la Ciudad de México es excesivo y genera multitud de problemas. Quienes han criticado la reforma, sin embargo, aducen que es inadecuada en función del deficiente sistema de transporte público de la ciudad que no constituye una alternativa al automóvil particular. Se puede igualmente argumentar que, en el mejor de los casos la reforma es insuficiente. En efecto, siguiendo una misma línea de razonamiento, se podría aducir que, además de limitar estacionamientos, se debería limitar la construcción de nuevas vialidades. Para apoyar esto último se puede hacer notar –con cifras del INEGI– que con la construcción de los segundos pisos en el periférico de la Ciudad de México se observó en los años 2004-2008 un aumento significativo en el ritmo de crecimiento del número de automotores circulando en la ciudad, mismo que retomó su ritmo normal a partir de 2008. Esto indicaría que dicho crecimiento está limitado en cierta medida por la dificultad para circular por las calles de la ciudad. Esto estaría de acuerdo con el hecho que el ritmo de crecimiento del número de automóviles en la Ciudad de México es sensiblemente menor al de otras ciudades del país.Por lo demás, los problemas de tráfico no son exclusivos de países como el nuestro sino que también existen en países con mayores grados de desarrollo. En este respecto, son legendarios los problemas de tráfico que sufre la ciudad de Los Ángeles en el estado de California. Una medida que se ha implantado para mitigar dichos problemas son los carriles exclusivos para automóviles en los que viaja más de un pasajero y que buscan incentivar el uso compartido del automóvil. Esta medida ha sido también blanco de críticos que argumentan que los carriles exclusivos terminan siendo subutilizados, dejando de este modo menos carriles para el tráfico normal.Por otro lado, críticos aparte, un artículo publicado el pasado 7 de julio en la revista “Science” ofrece argumentos sólidos que apoyan la efectividad de los carriles exclusivos para automóviles compartidos. Dicho artículo fue publicado por un grupo de investigadores encabezado por Benjamin Olken del Instituto de Tecnología de Massachusetts.  En el mismo se demuestran las bondades de las políticas para  incentivar el uso compartido del automóvil, al menos para el caso particular que ellos estudiaron: la ciudad de Yakarta, capital de Indonesia.Yakarta tiene uno de los mayores problemas de tráfico del mundo, y para intentar mitigarlos el gobierno estableció en 1992 normas para incentivar el uso compartido del automóvil en una versión extrema: prohibió la circulación en algunas de las principales arterias de la ciudad en las horas pico de automóviles con menos de tres pasajeros. Posteriormente, en marzo de 2016 y de manera repentina, el gobierno levantó la prohibición. Esto dio oportunidad a Olken y colaboradores de estudiar en un caso real el efecto de imponer una regla para el uso compartido del automóvil.En su estudio, los investigadores hicieron uso de datos de Google Maps para determinar en tiempo real, cada diez minutos, las velocidades de circulación por las arterias de la ciudad, antes y después de la remoción de la norma. Encontraron que, después de esta remoción, los tiempos de traslado aumentaron 46 por ciento en las horas pico de la mañana y 87 por ciento en las de la tarde. Encontraron, además, algo sorprendente: la circulación en avenidas secundarias  para las cuales no se aplicaba la norma también empeoró. Olken y colaboradores consideran que este último efecto debe ser motivo de un estudio posterior pero adelantan que la afectación de las calles secundarias puede simplemente reflejar el aumento repentino en el número de automóviles en circulación. Podría ser también resultado de un congestionamiento de las arterias principales que desvió el trafico hacia las secundarias.En todo caso, los resultados de Olken y colaboradores son sólidos y demuestran que incentivar el uso compartido del automóvil podría constituir una vía –una medicina amarga– para aliviar el congestionamiento de tráfico de nuestras ciudades, que al paso que va no tardará en sufrir una crisis mayor. Esta vía tendría un sustento más sólido que el que parece tener desincentivar la construcción de estacionamientos; o bien la de construir más arterias de circulación que no deja de ser una solución de fuerza bruta.",
    "Como sabemos, el saludo de mano, como expresión corporal que es, juega un papel social relevante en ciertas partes del mundo. Prueba de esto es la atención desmedida que se ha dado al presidente de los Estados Unidos y a sus saludos de mano en sus encuentros con líderes del mundo. En efecto, en una búsqueda rápida en Internet encontramos abundante información al respecto, incluyendo colecciones de videos que muestran al presidente Trump saludando a diferentes dirigentes de países en varios continentes. En dichos videos podemos observar que el saludo de manos del presidente norteamericano puede incluir apretones, jalones y sacudidas. Además, en algunos casos el tiempo de contacto rebasa los límites de lo convencional, ya sea por ser los apretones de mano demasiado largos, o bien por tener una duración igual a cero –lo que significa que no hubo ningún apretón.El interés por los saludos del presidente del país más poderoso del mundo, por supuesto, radica en lo que pueden revelar. El periódico británico “The Guardian”, por ejemplo, se pregunta si la forma de saludar del presidente Trump refleja su política exterior. Así, a un perplejo primer ministro de Japón le habría secuestrado la mano por 19 segundos, presumiblemente como un gesto para demostrar que es él el que manda. El primer ministro japonés, por su parte, después de liberarse del saludo hizo más que evidente su incomodidad, haciendo un gesto que incluyó una vuelta de ojos. En el extremo opuesto, durante una conferencia de prensa en la Casa Blanca, el presidente Trump aparentemente habría ignorado una sugerencia de la canciller alemana con quién sostiene serias diferencias, para darse un saludo de manos. La canciller simplemente volteó la cara e ignoró el episodio.Dada esta situación, algunos dirigentes han optado por contraatacar. El presidente francés Emmanuel Macron, por ejemplo, sostuvo con Trump el pasado mes de mayo en Bruselas un duelo de apretones de mano en el que resultó victorioso, al lograr que el estadounidense fuera el primero retirar el saludo. Incluso, de acuerdo a observaciones de la prensa, el apretón de Macron fue tan fuerte que la mano de Trump se habría puesto pálida. Ciertamente, el saludo de mano tiene un gran simbolismo social. El papel que juega, sin embargo, no tiene la misma importancia en todas las culturas y en este sentido un grupo de investigadores de universidades norteamericanas se propusieron llevar a cabo un estudio sistemático para determinar la relevancia que el saludo de mano tiene en grupos de origen caucásico y del este de Asia. Se propusieron, igualmente, averiguar si en este respecto existen diferencias de género. El grupo de investigación fue encabezado por Yuta Katsumi de la Universidad de Illinois en Urbana-Champaign.El estudio –publicado en la revista “Journal of Nonverbal Behavior” el pasado 29 de junio–  se llevó a cabo con 44 voluntarios de origen caucásico y 44 originarios del este de Asia, todos residiendo en los Estados Unidos. Ambos grupos estuvieron compuestos por 22 hombres y 22 mujeres. Durante el experimento se les presentaron a los participantes una serie de videos en los que, en un ambiente de negocios, se veía a dos personas a punto de dar inicio a una reunión. Una de las personas actuaba como anfitrión y la otra como huésped. El anfitrión y el huésped podrían tanto saludarse como no saludarse de mano antes de iniciar la junta. Ambos, el anfitrión y el huésped, reflejaban el origen étnico y el género de los participantes; es decir, podrían ser tanto caucásicos como del este de Asía, y lo mismo hombres que mujeres. En cada caso se pidió a los voluntarios que dieran una evaluación de la interacción anfitrión huésped, y si el anfitrión les parecía competente y estarían dispuestos a iniciar un negocio con él. Tal como esperaban, los investigadores encontraron que las personas de origen caucásico, en comparación con aquellos originarios de este de Asia, evaluaron más positivamente la interacción cuando medió un saludo de manos. Encontraron, igualmente, que los participantes hombres evaluaron más positivamente la interacción cuando el anfitrión era hombre que cuando era mujer; esto, en contraste con las participantes mujeres a quienes no importó el género de anfitrión. De acuerdo con Katsumi, esto reflejaría el hecho de que el saludo de manos es inherentemente una costumbre occidental, practicada históricamente por hombres en ambientes de negocios.   El estudio de Katsumi tiene importancia para un país como los Estados Unidos en el que conviven múltiples culturas y en el que las mujeres se han incorporado plenamente a la economía. Tiene importancia pues la percepción de la relevancia del saludo de mano no sería la misma ni entre los diferentes grupos étnicos –al menos entre los dos estudiados– ni entre los dos géneros. Volviendo a la arena internacional, hay que recordar que Japón es un país en donde el saludo de mano no se practica tan frecuentemente como en los Estados Unidos y de ahí que no sorprenda el desconcierto del primer ministro japonés ante el vigoroso saludo del presidente norteamericano. No es el caso, por supuesto, de Francia y del presidente francés. Cabría incluso preguntarse si, a la luz de estudio de Katsumi y colaboradores, los presidentes Trump y Macron saludarían con el mismo vigor de haber nacido mujeres.  Al margen de las anécdotas, sin embargo, y si bien el saludo de mano es tan relevante socialmente que incluso inspira estudios científicos, sorprende la manera como en los últimos meses ha saltado a la palestra, por no decir circo.",
    "Según predicciones de la compañía CISCO de San José, California, en el año 2020 habrá 5.5 miles de millones de usuarios de teléfonos móviles, lo que representa el 70% de la población del mundo. Este dato, de por si impresionante, adquiere una dimensión adicional si lo contrastamos con el número de personas que tendrán acceso a otros servicios básicos, algunos sin duda más básicos que el de las telecomunicaciones.  En efecto, de acuerdo con CISCO, el número de personas que en el año 2020 tendrán acceso a la energía eléctrica y al agua corriente será de 5.3 miles de millones y de 3.5 miles de millones, de manera respectiva. Es decir, en 2020 habrá más propietarios de teléfonos móviles que usuarios de la energía eléctrica. Y lo más llamativo: dos mil millones de personas tendrán un teléfono móvil pero no agua corriente en sus casas.Hemos sido testigos en las últimas décadas de la irrupción de los teléfonos celulares en nuestras vidas. Hoy en día, utilizamos a los teléfonos inteligentes en una gran variedad de aplicaciones. Según el “Pew Research Center”, la actividad más común entre los usuarios de estos teléfonos en los Estados Unidos es el envío de mensajes de texto, seguida de las comunicaciones por voz o video. Actividades comunes lo son también la navegación en Internet, la consulta del correo electrónico, la participación en redes sociales, la grabación de videos y fotografías –con los resultados de todos conocidos–, los juegos y la consulta de mapas, entre otras. Los usos de los teléfonos inteligentes han de este modo sobrepasado por mucho el papel para el cual fueron originalmente concebidos y de ahí resulta el un gran impacto que han tenido en nuestro estilo de vida. Los teléfonos inteligentes han incluso modificado nuestro comportamiento. En este sentido, basta considerar que algunas veces los usamos para aislarnos e ignorar a las personas a nuestro alrededor. Otra consecuencia inesperada de la irrupción de los teléfonos inteligentes se convirtió en el tópico de un artículo publicado esta semana en la revista en línea “Plos One” por un grupo de investigadores de instituciones británicas encabezado por Matthew Timmis de la Universidad Anglia Rusking. En dicho artículo, Timmis y colaboradores reportan los resultados de un estudio dirigido a averiguar cómo se las arreglaría una persona que camina por la calle para sortear obstáculos que encuentra y no tener un accidente, si al mismo tiempo hace uso de un teléfono inteligente,.Según Timmis en parte la idea para llevar a cabo dicho estudio le surgió cuando una mañana observó que una persona que caminaba enfrente de él lo hacía demasiado despacio y haciendo eses. En un principio pensó que se encontraba en un estado inconveniente –a pesar de la hora temprana–, hasta que se cruzaron y se dio cuenta que estaba haciendo uso de un teléfono inteligente. Para llevar a cabo su estudio, Timmins y colaboradores contaron con la ayuda de 21 voluntarios, 16 hombres y 5 mujeres con edades alrededor de los 25 años y sin ninguna enfermedad motora. A los voluntarios se les pidió caminar a lo largo de un pasillo de 5.3 metros de largo. Algunas veces el pasillo estuvo libre de obstáculos, mientras que en otras ocasiones se colocó un obstáculo hecho de cartón y madera y un escalón de 7.5 centímetros de altura. Se hicieron pruebas tanto sin teléfono móvil, como con los voluntarios hablando, leyendo un texto o escribiendo un texto con un teléfono inteligente en las manos.  A los participantes se les colocaron sensores para determinar la dirección hacia donde dirigían la vista a lo largo de la prueba y se instalaron sensores para grabar sus movimientos.Encontraron los investigadores que al hacer uso los participantes de un teléfono para leer o editar un texto, fijaban la vista en los obstáculos menos frecuentemente y por menos tiempo que cuando caminaban sin usar un teléfono. El efecto fue más marcado al escribir un texto que al leerlo. Además, aun aquellos que sólo hablaban por teléfono, no necesariamente dirigían la vista hacia el obstáculo sino que a veces lo hacían hacia cualquier otro lugar.De este modo, al usar un teléfono móvil al caminar nos hacemos más dependientes de la visión periférica –rabillo del ojo– para la evaluación de los obstáculos. Esto, sin embargo, y de acuerdo con Timmins y colaboradores, no hace más inseguro nuestro paso, pues automáticamente tomamos una estrategia cautelosa haciendo movimientos más pausados y seguros. Al acometer un escalón, por ejemplo, lo hacemos de forma más lenta y elevando el pie más allá de lo estrictamente necesario.Esto, sin embargo, no significa, advierten los investigadores, que el uso de un teléfono celular mientras caminamos por la calle esté libre de peligros, pues para evitar una amenaza que aparece ante nosotros de manera rápida e inesperada bien pudiéramos necesitar de toda nuestra atención para evitarla.De un modo u otro, la investigación de Timmins pone en el escenario –por si lo necesitara– al teléfono móvil, posiblemente el dispositivo que más rápida y radicalmente ha transformado nuestro estilo de vida. Tanto, que hasta nos ha cambiado el modo de andar.",
    "Hace unos días el ministerio de educación de Turquía anunció que la enseñanza de la evolución será suprimido de los planes de estudio de la escuela secundaria. El argumento esgrimido es que los estudiantes de ese nivel no tienen todavía la capacidad suficiente para entender un  tópico “tan controvertido”.  La iniciativa ha sido criticada por aquellos que ven en la misma un intento para erosionar el carácter secular del estado turco.  La teoría según la cual las especies no son inmutables sino que están en continua transformación tiene una fuerte carga emocional y ha sido para muchos difícil de aceptar. Así, una caricatura publicada en 1871 por una revista satírica mostraba a Charles Darwin, con su característica calva y espesa barba ¡y con un cuerpo de simio! Aun hoy en día, a 150 años de la publicación de El origen de la especies, la evolución sigue siendo “controversial”. Y lo es, ciertamente, no sólo en Turquía. El concepto según el cual las especies no son inmutables sino que están en continua transformación nos permite encontrar explicaciones razonables para la existencia de las especies que vemos a nuestro alrededor. De otro modo, si asumimos que todas éstas han sido inmutables desde el momento en que fueron creadas, tendríamos que recurrir a explicaciones forzadas y poco consistentes. La evolución nos proporciona un marco para entender la diversidad de especies animales y vegetales que habitan en la Tierra, lo mismo que el origen de los restos fósiles de animales y humanos con características diferentes a las de las especies actuales. Consideremos el problema, aparentemente menor, de encontrar una explicación a la diversidad de formas de los huevos de las distintas especies de aves. Dichos huevos pueden tener una forma esférica, como es el caso de los búhos, o una forma alargada y simétrica similar a la de un dirigible zepelín. Pueden tener también una forma alargada pero asimétrica, como sucede con el arao, que es una ave marítima que habita en el norte de los continentes europeo y americano. En contraste, no hay aves que pongan huevos con formas asimétricas y cortas que asemejen a un globo de aire caliente. Si todos los huevos tienen la misma función de proteger al polluelo en su gestación ¿por qué varían tanto en su forma? Asumiendo una posición anti-evolución podríamos contestar que tienen tal o cual forma porque así fueron creados y cerrar de este modo la discusión. La respuesta posiblemente será satisfactoria para algunos pero no para otros que considerarán que ésta no es en realidad una respuesta. En un contexto evolucionista se han dado diferentes explicaciones. Se ha postulado, por ejemplo, que una forma alargada y asimétrica hace que huevo ruede en círculo y resulta así beneficiosa en aquellos casos de aves –como los araos– que ponen sus huevos en acantilados en donde están expuestos a rodar y caer. Se ha postulado también que la forma de los huevos lo determina la manera como se acomodan en el nido. Un artículo aparecido esta semana en la revista “Science” refuta estas hipótesis y adelanta una explicación de más consistencia. Dicho artículo fue publicado por un grupo internacional de investigadores encabezado por Mary Caswell Stoddard de la Universidad de Princeton. Stoddard y colaboradores llevaron a cabo un estudio con cerca de 50,000 huevos que representan 1,400 especies de aves. En su estudio los investigadores primeramente asumieron que la forma del huevo  no está determinada por el cascarón, sino por su membrana interna cuya forma es modelada por la fuerzas a las que está sometida en el oviducto antes de la formación de dicho cascarón. Asumiendo, además, que la resistencia y grosor de la membrana puede variar a lo largo de la misma, pudieron reproducir matemáticamente las formas de huevo observadas en la naturaleza. Para obtener una forma alargada y asimétrica, por ejemplo, variaron el grosor y la resistencia de la membrana a lo largo de su eje. Una forma corta y asimétrica, por otro lado, no ocurre en la naturaleza debido a que la resistencia longitudinal de la membrana tendría variar de manera demasiado brusca.Una vez que entendieron la mecánica de formación de un huevo, Stoddard y colaboradores se preguntaron por la fuerza evolutiva que impulsa el proceso y encontraron que la forma del huevo está relacionada con la habilidad del ave para volar. Así, las aves migratorias que se pasan una buena cantidad de tiempo en el aire, ponen huevos alargados o asimétricos, mientras que los huevos de las aves no migratorias tienden a ser más esféricos. Quizá, las aves migratorias requieran de pelvis estrechas y cuerpos esbeltos para facilitar su vuelo y esto lleva al desarrollo de huevos alargados.Las hipótesis manejadas por Stoddard y colaboradores son claramente superiores –en grado de sofisticación, razonamiento y metodología– a las que podrían elaborarse negando la evolución. Ciertamente, los investigadores no han dicho la última palabra y sus conclusiones pueden ser cuestionadas por estudios posteriores. Estarán, pues, como en una vitrina sujetos al juicio permanente de sus colegas.Esto último es la característica del método científico que nunca pretende llegar a la verdad absoluta. No es el caso, en contraste, de aquellos que niegan la evolución, incluyendo posiblemente al presidente turco, que según sus detractores pretende atrasar 150 años la educación de su país.",
    "Un artículo publicado el pasado año en la revista “Environmental Science: Water Research and Technology” por un grupo de investigadores encabezados por Ioannis Andrea Ieropoulos de la Universidad del Oeste de Inglaterra, propuso un uso novedoso para la orina humana: la generación de energía eléctrica. Ieropoulos y colaboradores concibieron esta posibilidad y para investigar su factibilidad se dieron a la tarea de diseñar y construir un dispositivo –conocido como celda de combustible microbiana– que hace uso del metabolismo bacteriano para convertir la orina en energía aprovechable. Para probar su diseño, los investigadores llevaron a cabo dos pruebas de campo. En una de estas pruebas instalaron un total de 288 celdas microbianas –cada una de ellas contenida dentro de un cilindro de cerámica de 15 centímetros de largo y 4.8 centímetros de diámetro–  en un área de baños del campus Frenchay de la Universidad del  Oeste de Inglaterra. El combustible de dichas celdas fue donado de manera voluntaria por estudiantes y trabajadores de la universidad –del género masculino, de manera exclusiva–, los cuales gozaban de buena salud sin ninguna condición previa de enfermedad, según el artículo de referencia. Una segunda prueba de campo se llevó a cabo durante la celebración del festival musical de Glastonbury en 2015. En este último caso se implementó una instalación con tres puntos de recolección y 432 celdas que podían manejar un total de 300 litros de líquido. Al lado de esta instalación se montó un módulo de información sobre las bondades del proyecto, instando al público a darle un buen uso a sus líquidos de desecho (“Put your pee to good use here” ). La prueba resultó un éxito en cuanto al flujo de donantes –unos mil por día– que, aportaron un aproximado de 330 litros de líquido diariamente.  En ambas pruebas, la electricidad generada fue empleada para iluminación. La idea de utilizar un material de desecho, abundante y con pocos usos potenciales, como combustible para generar electricidad suena de entrada muy atractivo. Debemos hacer notar, sin embargo, que la cantidad de potencia eléctrica generada por los arreglos de celdas de Ieropoulos y colaboradores fue muy reducida. En el caso de la prueba en la universidad las 288 celdas generaron apenas la energía eléctrica necesaria para encender cuatro lámparas LED con una potencia total de 1.2 vatios. La instalación montada en el festival de Glastonbury fue un poco más grande pero allí también los resultados fueron magros en cuanto a producción de energía.No pretenden Ieropoulos y colaboradores, por supuesto, competir en eficiencia con medios más tradicionales de generación de energía eléctrica. Hacen notar, sin embargo, que, adicionalmente a la energía generada, el proceso que han desarrollado descompone el material orgánico contenido en el agua de desecho que pasa por la celda y la limpia a tal grado que podría ser descargada directamente al medio ambiente. Esto le da a su tecnología una ventaja competitiva.  En un artículo publicado esta semana en la revista en línea PLOS ONE por el grupo de Ieropoulos llegan a una conclusión con respecto a su tecnología con una trascendencia aun mayor. En efecto, encuentran que el proceso al que se somete al agua residual en su celda de combustible tiene un efecto antimicrobiano, abriendo la posibilidad de que, además de generar energía y limpiar el agua residual, elimine gérmenes peligrosos para los humanos. Ieropoulos y colaboradores llegan a esta conclusión después de contaminar intencionalmente el agua a la entrada de las celdas de combustible con bacterias de salmonella y observar una significativa reducción de las mismas a la salida. Con este resultado los investigadores arguyen que la celda microbiana de combustible tendría una aplicación de gran trascendencia en países subdesarrollados para limpiar y desinfectar aguas residuales y generar energía en lugares a lo que no llega la red de distribución eléctrica. En la visión de Ieropoulos y colaboradores, cada casa-habitación tendría instalado un arreglo de celdas microbianas de combustible que proporcionarán energía eléctrica, y limpiarán y desinfectarán el agua de desecho antes de que ésta sea vertida a la red pública.Habría que ver esto con algo de escepticismo, sin embargo. En efecto, hace algunas décadas, cuando la generación de energía eléctrica a partir de la radiación solar –conversión fotovoltaica– empezaba a despegar, los costos de los paneles solares eran demasiado altos para competir con las fuentes tradicionales de energía. En esas condiciones, se concebía a los países subdesarrollados –incluyendo a México–, con redes deficientes de distribución de energía eléctrica, como el mercado natural para iniciar la expansión de la energía fotovoltaica. Esto no ocurrió por falta de inversiones públicas y dicha expansión ocurrió en cambio en los países industrializados una vez  que los subsidios y la reducción de costos de fabricación de los paneles solares hicieron competitiva a la energía fotovoltaica. Es posible que algo similar ocurriera con las celdas microbianas de combustible en los países subdesarrollados, y a menos que se hicieran inversiones masivas de dinero público –lo que es improbable–, no habría manera que dichas celdas tuvieran un impacto real en la salud de la población.Todo esto, por otro lado, en nada disminuye el mérito de los cerca de mil británicos que, en la feria musical de Glastonbury, diariamente contribuyeron con sus donaciones al avance de la ciencia.",
    "Con la creación del Consejo Nacional de Ciencia y Tecnología (CONACyT) en diciembre de 1970  la investigación científica y tecnológica en nuestro país experimentó un cambio cualitativo. Antes de esto, la investigación científica como profesión se practicaba en muy pocos lugares en México. Fundamentalmente en instituciones del área metropolitana de la capital, con algunas excepciones. Al inicio de la década de los años setenta la situación empezó a cambiar y la investigación empezó a penetrar al interior del país. Lentamente, sin embargo, como un reflejo del enorme rezago de México en la materia.Entre los factores que frenaron el avance científico en las décadas de los años setenta y ochenta se cuentan, por un lado, las crisis económicas que sufrió el país, y por el otro, los bajos salarios que recibían los investigadores en nuestras universidades y centros de investigación –en parte debido a dichas crisis– que no permitían que se dedicaran exclusivamente a su profesión. Un elemento clave para atacar el problema salarial fue la creación en 1984 del Sistema Nacional de Investigadores (SNI) que, basado en los méritos y resultados académicos obtenidos por cada investigador, les otorga una beca como complemento salarial.  A partir de su creación, el SNI ha crecido de manera continua y hoy en día cuenta con alrededor de 27,000 miembros.Si bien el SNI ha tenido un notable crecimiento en sus poco más de tres décadas de existencia, el número de investigadores en México es todavía muy pequeño en comparación con el tamaño de país –en los Estados Unidos, por ejemplo, alrededor de seis millones de personas están empleadas en puestos clasificados como de ciencia e ingeniería–. Los investigadores en México, además, y ante la ausencia de oportunidades de trabajo en otros sectores, se han incorporado fundamentalmente al medio académico, que ha incrementado sustancialmente su nivel de investigación.En contraste, poco impacto ha tenido la investigación científica que se lleva a cabo en México en cuanto a transferencia de tecnología desde el medio académico hacia el sector productivo.  ¿Cuál es la razón por la que existe una relación tan débil entre los dos sectores? En un artículo publicado en el mes de junio de 2016 en la revista “Technology in Society” por un grupo de investigadores del Instituto Politécnico Nacional encabezado por  Alma Cristal Hernández Mondragón discuten el punto. Consideran Hernández Mondragón y colaboradores una faceta particular de la vinculación universidad-empresa: la creación de nuevas empresas basadas en el conocimiento científico y tecnológico desarrollado en el medio académico, el cual es un proceso de vinculación academia-empresa que ocurre regularmente en los países avanzados. Llegan a la conclusión de que el desarrollo de este tipo de empresas ha sido inhibido en México en buena medida por reglamentaciones contenidas en la Ley Federal de Responsabilidades Administrativas de los Servidores Públicos que se aplica a los investigadores adscritos a instituciones públicas que son así considerados servidores públicos.De acuerdo con la ley, como servidor público el investigador se colocaría en una situación de conflicto de interés según la ley al crear una empresa con tecnología basada en sus resultados de investigación, enfrentando posibles sanciones administrativas e incluso penas de cárcel. Lo anterior, sin embargo, fue cierto sólo hasta el mes de diciembre de 2015. A partir de esta fecha, ya no es el caso, como lo  explica un artículo publicado el pasado martes en el periódico La Crónica de Hoy por el Consejo Consultivo de Ciencias de la Presidencia de la República.  En efecto, en diciembre de 2015 se modificó la ley de servidores públicos para hacer una excepción a la situación de conflicto de interés de tal modo que ahora un investigador en una institución pública puede hacer uso de sus resultados de investigación para obtener un beneficio monetario, sujeto a las restricciones que le imponga su institución de adscripción. En particular, podría crear un empresa de base tecnológica y ser accionista de la misma.  La perspectiva de perder su trabajo o parar en la  cárcel era, sin duda alguna, un disuasivo efectivo para que un investigador moderara –en caso de tenerlos– sus impulsos empresariales. Los cambios en la ley, que hacen una excepción al conflicto de interés, eliminan de este modo un obstáculo mayor para la transferencia de tecnología entre la academia y la empresa.  No es el único obstáculo a remover, sin embargo, y hay otros que merecen atención. Un problema en el horizonte es el del apoyo público a la investigación. La creación de empresas de base tecnológica parte de laboratorios y grupos de investigación consolidados y con un apoyo financiero adecuado. Por el momento, ante la falta de interés del sector privado, dicho apoyo sólo puede provenir del sector público. En las condiciones de crisis en las que se encuentra el país, sin embargo, el financiamiento a la investigación científica más que incrementarse se ha venido reduciendo y esto, sin duda, no ayudará a incentivar la creación de empresas desde la academia. La interacción academia-empresa en nuestro país es sin duda muy débil. Esto, en último término, es producto de la relativamente reciente introducción de la ciencia en México. Después de todo, no ha transcurrido todavía medio siglo desde la fundación del CONACyT. Y poco más de tres décadas desde que se profesionalizó la investigación con la creación del SNI.",
    "Una de las noticias de la semana fue, sin duda, la decisión del presidente de los Estados Unidos de retirarse del acuerdo de París para la reducción de la emisión de contaminantes atmosféricos con el objeto de mitigar el cambio climático. Algunos han opinado que dicha decisión resulta en buena medida teatral, pues el acuerdo no incluye medidas punitivas en caso de que alguno de los firmantes no cumpliera con lo pactado. Al mismo tiempo, sin embargo, ha provocado controversias y reacciones negativas. Como quiera que sea, con su decisión, el presidente muestra desacuerdo en un punto que los expertos consideran es crucial para la salud climática del planeta. Entre otras cosas, la decisión presidencial implicaría que los Estados Unidos no harán más contribuciones al Fondo Verde de la Naciones Unidas para que los países en desarrollo enfrenten los efectos del calentamiento global. Hay que notar que los Estados Unidos  comprometieron tres mil millones de dólares para dicho fondo, de los cuales sólo han aportado mil millones.  Juntamente con la decisión de los Estados Unidos de abandonar el pacto de París, esta semana nos enteramos que una placa de hielo en la costa de la península antártica está a punto de fracturarse, lo que daría origen a un gigantesco iceberg de 5,000 kilómetros cuadrados de área, que sería uno de los más grandes jamás observados. Esto, de acuerdo con investigadores de la Universidad de Swansea en el Reino Unido, encabezados por Adrian Luckman, quienes observaron mediante fotografías satelitales que entre el 25 y el 31 de mayo pasados, una grieta previamente existente en la capa de hielo avanzó unos 17 kilómetros, restando solamente 13 kilómetros para que ocurra una fractura total.Si bien,  según los investigadores, no se tienen pruebas de que la fractura de la placa de hielo sea debida al cambio climático, se sabe que previas desintegraciones de placas de hielo en la península antártica han sido ocasionadas por el calentamiento del océano y de la atmósfera. Al respecto, señala Luckman que la península antártica es uno de los lugares en el planeta en donde más rápidamente se está elevando la temperatura del océano, lo cual, ciertamente, no habría ayudado a detener la formación de la fractura en la placa de hielo.Así mismo, Luckman teme que la factura en curso, que desprenderá un 10% del total de la placa de hielo, dejara a ésta expuesta a una futura desintegración, como ya ha sucedido previamente con otras placas de hielo de la misma zona.La formación de un gigantesco iceberg en la Antártida es una noticia que no causa ya sorpresa en estos tiempos de cambio climático. Así, que existe un calentamiento global y que éste es debido a la emisión de gases de invernadero a la atmósfera, no solamente tiene un fuerte sustento científico, sino que es aceptado por una gran parte del público. El programa sobre “Climate Change Communications” de la  Universidad Yale, por ejemplo, encuentra que el 70% de los adultos norteamericanos cree en el cambio climático y el 53% está convencido que éste es debido a actividades humanas. Así mismo, el 49% cree que una mayoría de científicos piensa que el cambio climático está ocurriendo y un 70% declara su confianza en los científicos.  Por otro lado, un 51% de adultos norteamericanos piensa que el cambio climático está dañando o dañará a sus compatriotas en un lapso de 10 años, mientras que solamente un 40% cree que sufrirá un daño personal. De este modo, mientras que una mayoría de norteamericanos está convencida de que el cambio climático es real y que es debido a  actividades humanas, sólo un porcentaje minoritario está preocupado por sufrir daños personales en el futuro.La distribución de opiniones acerca de la ocurrencia del cambio climático depende, por supuesto, de la geografía. Sin embargo, con pocas excepciones –en el Medio Oeste o el antiguo cinturón industrial alrededor de los Grandes Lagos, entre otras–, hay una opinión mayoritaria acerca de que constituye un hecho real. En contraste, en muy pocos lugares –alrededor de las ciudades de Los Ángeles y San Francisco, por ejemplo– se tiene una opinión mayoritaria en el sentido de que el cambio climático ocasionará daños a su persona. Los efectos del cambio climático se piensan así lejanos y relativamente poco preocupantes, lo que debe ser particularmente cierto para los seguidores a ultranza del presidente. No es el caso, por otro lado, de los científicos del clima que piensan que el incremento de temperatura global debe limitarse a menos de dos grados centígrados por encima de su valores preindustriales, como se acordó en el pacto de París. De otra forma se tendrán efectos desastrosos e irreversibles para el clima del planeta. Entre otros efectos, se espera que se inunden áreas costeras en la medida en que se fundan los hielos de Groenlandia y la Antártida y se expanda el agua de los océanos por el incremento de temperatura. Si esto llega a suceder, uno de los lugares más afectados será la península de la Florida, en donde se encuentra la mansión Mar-a-Lago. Se habrá cumplido así aquello de que dios castiga sin palo ni cuarta.",
    "Como sabemos –excepciones aparte– empleamos las horas de la noche primordialmente para dormir. Para dormir, además, de seis a ocho horas de un solo tirón. Estamos tan acostumbrados a esta rutina que resulta sorprendente enterarnos que ésta no es una práctica generalizada en todas las culturas, y que, de hecho, no lo fue en la cultura occidental –de la que somos herederos en buena medida– sino hasta hace relativamente poco tiempo.  En efecto, como lo ha documentado Roger Ekirch del Instituto Politécnico de Virginia, la costumbre de dormir en una sola etapa surgió en el siglo XIX y  coincidió con la revolución industrial y con la expansión de la iluminación nocturna. Con anterioridad durante la noche se acostumbraba dormir en dos etapas separadas por un intervalo de algunas horas; intervalo que se empleaba de las más diversas maneras, incluyendo la de levantarse de la cama a platicar con los vecinos.En realidad, pensándolo mejor, quizá el dormir en dos o más etapas no sea algo tan sorprendente a fin de cuentas, sino en cierto modo una consecuencia natural del surgimiento de la iluminación nocturna. Antes de ésta, la oscuridad de la noche restringía muchas actividades e ir a la cama temprano podría haber sido una opción a falta de otras mejores. Las horas de oscuridad, sin embargo, son más que las horas necesarias de sueño y de esto resulta inevitable que la noche tenga horas de vigilia.Ekirch especula que parte de los problemas de insomnio que hoy en día padece buena parte de la población, que depende del uso de pastillas para dormir, pueda ser debida al cambio de régimen de sueño de dos etapas –que sería el natural y que prevaleció hasta hace un par de siglos– al régimen de una etapa que hoy acostumbramos.  Si bien no es claro si con respecto a esto último Ekirch está en lo cierto, existe la posibilidad de que en los años por venir la calidad del sueño empeore por efecto del calentamiento global que generará noches más cálidas –todavía más de las que ha generado hasta ahora–. Esto al menos de acuerdo con un artículo publicado en la revista “Science Advances” por un grupo de investigadores de instituciones norteamericanas encabezado por Nick Obradovich de la Universidad de Harvard. Obradovich y colaboradores hacen notar que cuando el cuerpo se prepara para iniciar el periodo de sueño disminuye su temperatura, misma que se recupera nuevamente al final del mismo. De este modo, en caso de que una alta temperatura ambiente interfiera con la regulación de la temperatura corporal, se alterará el ciclo sueño-vigila y se deteriorará la calidad del sueño. En su investigación, Obradovich y colaboradores buscaron primeramente establecer que, efectivamente, existe una correlación entre la temperatura ambiente y la calidad del sueño. Para esto emplearon  los datos de una encuesta llevada a cabo por el Centro de Control y Prevención de Enfermedades de los Estados Unidos entre 2002 y 2011 a cerca de tres cuartos de millón de personas. A los participantes se les hizo la pregunta: “¿En los pasados 30 días, cuántos días sintió que no tuvo suficiente sueño y descanso nocturno?” Seguido de esto se consultaron datos climatológicos de temperatura ambiente en la fecha y lugar que correspondió a cada respuesta. Los investigadores encontraron que el número de noches con sueño inquieto se relacionó en forma directa con la desviación de temperatura ambiente con respecto a su valor promedio normal. De manera específica, un incremento de un grado centígrado en dicha desviación produjo 3 noches adicionales de mal sueño cada mes, por cada 100 personas.Obradovich y colaboradores encuentran, además, que este efecto fue más marcado en los meses de verano y entre las personas de más de 65 años. Encuentran, igualmente, un mayor efecto entre aquellos cuyo salario no rebasa los 50,000 dólares anuales en comparación con aquellos de mayores ingresos. Esto último posiblemente refleje el costo que implica mantener un sistema de aire acondicionado adecuado.En el futuro, cuando se anticipan noches más cálidas por el cambio climático, podríamos anticipar más noches sin dormir. Así, tomando en cuenta los incrementos de temperatura que se predicen para los años 2050 y 2099 el número de noches con sueño intranquilo por mes se incrementarían en 6 y 14, de manera respectiva. Sin embargo, no pretenden Obradovich y colaboradores ser precisos al respecto, pues reconocen que bien pudiera ser que en el futuro, entre otros avances tecnológicos, se desarrollen medios de control de temperatura ambiental más ampliamente asequibles.    No tenemos, ciertamente, una bola de cristal para predecir lo que sucederá en el futuro. El pasado en cambio nos ofrece más certidumbres. En particular, nos enseña que la revolución industrial, al lado de todos los beneficios que nos ha acarreado,  nos ha dado de la misma manera noches más cálidas e intranquilas.",
    "Un artículo publicado en días pasados por el diario británico “The Guardian” habla de la contaminación del aire en el Reino Unido y de cómo ésta provoca más muertes en ese país que las que ocasiona en Suecia, los Estados Unidos y México –si bien por escaso margen en este último caso–. El texto está ilustrado por una fotografía de una estatua de la reina Victoria a la que le fue colocada una máscara anti-gas. El artículo de “The Guardian” hace referencia a estadísticas de la Organización Mundial de la Salud (OMS) de 2017 sobre el número de muertes provocadas por la contaminación atmosférica. Suecia es el país que tiene los mejores números en este respecto con 0,4 muertes por cada 100,000 habitantes. Las cifras correspondientes para los Estados Unidos y México son 12.1 y 23.5, en forma respectiva, en comparación con 25.7 para el Reino Unido.Como sabemos, la contaminación del aire en los centros urbanos es en buena medida producto de la revolución industrial iniciada en Inglaterra a finales del siglo XVIII. De hecho, como lo relata un artículo alojado en el sitio del Servicio Meteorológico Nacional  británico (Met Office), recién despuntó el siglo XIX cuando ya empezaron a sentirse los efectos de la revolución industrial en el aire de la ciudad de Londres. Así, en diciembre de 1813 esa ciudad se cubrió por varios días con una  capa de un smog con olor a alquitrán, tan densa que según testigos no podía verse de un lado a otro de la calle. Emergencias atmosféricas similares –con un aumento significativo en la tasa de mortalidad– se repitieron en Londres en diciembre de 1873, enero de 1880, febrero de 1882,  diciembre de 1891, diciembre de 1892 y noviembre de 1948.El peor episodio de contaminación del aire londinense, sin embargo, se dio entre el 5 y el 9 de diciembre de 1952. En ese periodo se produjo un fenómeno meteorológico de inversión de temperatura del aire que no permitió que se disiparan los contaminantes emitidos a lo largo del día, acumulándose de manera progresiva. Durante este evento, conocido como “El gran smog”, se emitieron diariamente a la atmósfera 1,000 toneladas de partículas de humo, 2,000 toneladas de dióxido de carbono, 140 toneladas de ácido clorhídrico, y 370 toneladas de dióxido de azufre que se convirtieron en 800 toneladas de ácido sulfúrico. El smog que esto generó era tan denso que hubo áreas de Londres en las que no habría sido posible verse los pies. El episodio causó entre 4,000 y 12,000 muertos, fundamentalmente niños y personas mayores.  En la actualidad sabemos de la severa contaminación del aire de Beijín, producto de la rápida industrialización que se ha dado en China, al igual que de las contingencias ambientales en la Ciudad de México, generadas tanto por la emisión de contaminantes por automóviles e industrias, como por las condiciones geográficas particulares del Valle de México. La pregunta es si el crecimiento económico lleva aparejado de manera inevitable un incremento proporcional en el número de muertes asociadas a la contaminación del aire. Un vistazo a las estadísticas de la OMS citadas anteriormente nos muestra que esto no es necesariamente el caso, pues, sin ser una regla general, el índice de muertes por polución atmosférica tiende a ser menor en aquellos países llamados avanzados. Tenemos de este modo que si bien por sí sola una mayor actividad económica tiende a producir una mayor contaminación del aire, al mismo tiempo los países más avanzados tienen mejores herramientas tecnológicas y organización social y política para logran moderar dicho aumento. En efecto, las estadísticas de la OMS nos muestran que mientras que los centros urbanos de Alemania, Gran Bretaña y Francia –las economías más grandes de Europa Occidental–, tienen densidades promedio de partículas suspendidas en el aire inferiores a 15 microgramos por metro cúbico, las densidades equivalentes para otros países europeos como Rumanía, la República Checa y Polonia sobrepasan los 20 puntos. De hecho, la densidad de partículas suspendidas en la atmósfera de las ciudades polacas dobla a la de las ciudades británicas. En cuanto al índice de mortalidad por la contaminación del aire, las estadísticas de la OMS nos muestran igualmente que tiende a ser menor en países avanzados que en países con economías emergentes, lo cual no es sorprendente. Así, un buen número de países de Europa Occidental tienen índices de mortalidad por contaminación atmosférica inferiores a 20 decesos por cada 100,000 habitantes, mientras que en Indonesia y Turquía –cuyas economías se encuentran entre las 20 más grandes del mundo– dicho índice sobrepasa los 50 puntos. Una excepción al respecto es Alemania, cuyo índice de mortalidad se eleva por arriba de los 30 puntos. Otra excepción es la Gran Bretaña que, como se mencionó anteriormente, tiene un índice de 25 puntos, a pesar de tener niveles de contaminación atmosférica relativamente bajos.      Las estadísticas de la OMS son ilustrativas y nos muestran que el progreso económico no necesariamente tiene que resultar en un desastre medioambiental. Son, por otro lado, sin duda imperfectas y existe la posibilidad de que subestimen la contaminación del aire en las economías emergentes que no cuenten con estadísticas suficientemente detalladas al respecto. Cabe preguntarse, por ejemplo, en qué grado los números reportados por la OMS para México reflejan lo que sucede en nuestra ciudad, para la cual existe muy poca información sobre contaminación atmosférica.",
    "A partir de que tomara su forma actual en Cremona en el siglo XVI, el violín escaló posiciones hasta convertirse hoy en día en uno de los instrumentos musicales de más prestigio. Y de acuerdo con este prestigio –o más bien como parte integral del mismo–, el violín se asocia estrechamente con Niccolò Paganini y su extravagante historia, muy propia del siglo XIX. Como sabemos, era tal el virtuosismo de Paganini como intérprete del violín que se decía que había hecho un pacto con el diablo. Y, ciertamente, su aspecto cadavérico y extravagante estilo de vida no ayudaban a disipar la duda. Incluso, al morir en 1840 en Niza, la iglesia no permitió que se le diera cristiana sepultura. Sólo hasta cuatro años después fue posible que sus restos fueran trasladados a Génova con la intervención del papa. No fue enterrado, sin embargo, sino hasta 1876 en Parma. Pertenece también al violín la leyenda de Cremona y sus famosos lutieres –constructores  de instrumentos de cuerda–, entre los que destacan Antonio Stradivari, Giuseppe Guarneri y la familia Amati. Los violines fabricados en la edad de oro de Cremona –siglo XVIII– son altamente apreciados y alcanzan valores que se miden en millones de dólares. De hecho, el precio de un violín Stradivarius o Guarneri puede superar los 10 millones de dólares.  Si bien un violín de 10 millones de dólares es un objeto más propio de coleccionistas o inversionistas, hay también violines antiguos que pertenecen a músicos profesionales. Con respecto a esto último, en el año 2010 se hizo público el robo en Londres de un violín Stradivarius a la violinista coreana Min Jin Kim. El robo del instrumento ocurrió en una cafetería en un descuido de su dueña, quien lo había adquirido a un costo de 450,000 libras esterlinas. Un año después del robo la policía londinense dio con el ladrón, más no con el violín, que no fue localizado sino hasta 2013, afortunadamente sin daños mayores.Un artículo publicado en línea por el diario británico “Daily Mail” el pasado martes refiere el drama sufrido por la violinista que consideraba a su instrumento –que para entonces costaba ya 1.2 millones de libras– parte de sí misma e indispensable para su carrera musical. La pena que sufrió fue tal que dejó de tocar en conciertos y recayó en la anorexia que había sufrido años atrás. Si bien posteriormente superó su depresión, ya no pudo recuperar su violín al haber aceptado el pago de la aseguradora por el robo. Con esta aceptación el violín pasó a manos de la aseguradora, y el costo que había alcanzado el instrumento –2.5  millones de dólares– le impidió negociar su readquisición. Los violines de Cremona son comúnmente considerados superiores en calidad sonora a cualquier violín moderno –de ahí los altos precios que alcanzan–  y han sido objeto de muchos estudios para tratar de desentrañar sus secretos de fabricación. No hay un acuerdo unánime sobre dicha superioridad, sin embargo. Entre aquellos que discrepan se encuentra Claudia Fritz, una experta en acústica, quien con un grupo de colaboradores publicó un artículo en 2012 en el que reporta que violinistas profesionales son incapaces de distinguir entre un violín moderno y uno antiguo.    Como era de esperarse, estas conclusiones fueron criticadas por músicos profesionales. Así, el “New York Times” recogió comentarios del violinista Earl Carlyss quien consideró que el procedimiento empleado fue enteramente inapropiado para evaluar la calidad de un violín.  Hace notar que las pruebas se hicieron en el cuarto de un hotel y que para decidir sobre la calidad de un instrumento un violinista necesita evaluar como se proyecta en una sala de concierto. Equiparó las pruebas llevadas a cabo por Fritz y colaboradores con tratar de comparar a un Ford y a un Ferrari en el estacionamiento de un supermercado.La contestación de Fritz y colaboradores se dio en un artículo publicado esta semana en la revista “Proceedings of the National Academy of Sciences”. En dicho artículo, los investigadores reportan el resultado de estudios hechos en dos salas de concierto, una en París y la otra en Nueva York, en los que se comparan a ciegas tres violines Stradivarius y tres violines modernos. En contraste con el primer estudio, en la nueva investigación se recabaron opiniones tanto de los violinistas profesionales ejecutantes como de la audiencia. Las comparaciones, además, se hicieron tanto con acompañamiento orquestal como sin dicho acompañamiento.  Los investigadores encuentran resultados que refuerzan y extienden sus conclusiones anteriores. Así, en una comparación a ciegas, tanto violinistas como oyentes educados, dados a escoger entre un violín moderno y un Stradivarius, prefieren un violín de manufactura moderna. La superioridad de los violines de Cremona con respecto a violines modernos sería de este modo un mito y  Min Jin Kim habría sufrido gratis por la pérdida de un violín que podría haber sustituido con ventaja y a una fracción del costo del instrumento original.Por lo demás, nada de su bien merecido prestigio perderán los violines y su sonido fantástico por el hecho de que se desvanezca el mito de Cremona. Al igual que nada han perdido porque, igualmente, el pacto de Paganini con el diablo no pase hoy de ser ficción.",
    "Un frío día de invierno de 1650 en Estocolmo, dejó de existir –y concurrentemente de pensar– el célebre filósofo francés René Descartes. Había llegado Descartes a Estocolmo cinco meses antes invitado por la reina de Suecia, quien estaba interesada en que la instruyera en la nueva filosofía por él desarrollada. Tal parece que no disfrutó Descartes de su estancia en Estocolmo por la rígida disciplina a la que lo sometió la reina, misma que le demandaba levantarse todavía de noche en las frígidas mañanas de invierno. Y, efectivamente, tan no disfrutó la estancia que terminó muriéndose; aparentemente de neumonía, aunque hay también una versión de que habría sido envenenado con arsénico.Como quiera que haya sido, los infortunios de Descartes no terminaron con su muerte. En efecto, ocurrió que al morir Descartes fue enterrado en Estocolmo y con esto aparentemente habría descansado en paz por toda la eternidad. Dieciséis años después de su entierro, sin embargo, un grupo seguidor de Descartes de París promovió y obtuvo que sus restos fueran trasladados a Francia. Así, dichos restos fueron desenterrados, bajo la supervisión del embajador francés en Suecia, y mantenidos en la embajada con la custodia de soldados suecos en espera de su traslado a Francia. La custodia de los restos por soldados habría sido aparentemente motivada por el peligro de que fueran robados –en particular por los ingleses– dada la creciente reputación de Descartes que empezaba a ser considerado como el santo patrón de la ciencia. Esta precaución, sin embargo, resultó contraproducente, como veremos más adelante.   El traslado de los restos a Francia fue coordinado por el embajador francés. Éste era un coleccionista de reliquias y habría aprovechado la ocasión para quedarse, con la anuencia de la iglesia católica de Suecia, con un hueso del dedo índice de la mano derecha del casi santo. Al llegar a Francia en 1666, Descartes fue enterrado por segunda vez, ahora en la iglesia de Santa Genoveva del Monte en  París. No paró ahí el asunto, sin embargo, pues durante la revolución francesa los restos de Descartes fueron nuevamente desenterrados y puestos en un museo, hasta que en 1818  la Academia de Ciencias los enterró en la iglesia de “Saint Germain des Prés” en París, en donde reposan actualmente. No completos, sin embargo, pues al abrir el ataúd antes entierro los académicos descubrieron que faltaba el cráneo. La circunstancia en que éste desapareció no fue clara sino hasta tres años después, cuando el químico sueco Berzelius encontró que en una subasta de objetos que pertenecieron a un científico sueco de nombre Anders Sparrman, se listaba el “cráneo del famoso Descartes” como uno de los objetos que habían sido ofertados.Berzelius averiguó que el cráneo había sido adquirido en la subasta por el dueño de un casino. Se entrevistó con éste y logró que se lo vendiera. Una vez con el cráneo en su poder, Berzelius lo envió a la Academia de Ciencias que se dio a la búsqueda de datos que autentificaran los restos. Hoy se sabe, después de la investigación de la Academia,  que dichos restos pasaron por muchas manos. El primer propietario fue el capitán de los soldados que custodiaron los restos antes de su partida a Francia, quien habría robado el cráneo para que “algo del gran filósofo se quedara en Suecia”. Al morir el capitán, el cráneo pasó a las manos de uno de sus deudores, un fabricante de cerveza, como pago por la deuda, y éste a su vez lo heredó a su hijo. La cadena de propietarios del cráneo de Descartes se continuó con un militar, un oficial de gobierno, su yerno, un superintendente, un asesor de impuestos y finalmente Anders Sparrman. Algunos propietarios decoraron el cráneo con leyendas, mismas que ayudaron a los académicos franceses a concluir que sin duda había sido René Descartes su propietario original. El cráneo de Descartes se encuentra actualmente alojado en el Museo del Hombre en París. Al margen de las controversias que generaron en su tiempo sus ideas filosóficas, Descartes fue un genio y como tal se encuentra entre aquellos cuyo cerebro ha sido estudiado después de muertos para tratar de entender el origen de la inteligencia humana. Uno de estos estudios fue hecho público el pasado mes de abril en la revista “Journal of the Neurological Sciences” por un grupo de investigadores en Francia y Holanda, encabezados por Charlier Philippe de la Universidad París Descartes.Al contrario de otros estudios –como el conducido con el cerebro de Albert Einstein– Philippe y colaboradores solamente tuvieron a su disposición el cráneo pues el cerebro de Descartes ya no existe. A pesar de esto, las imágenes de tomografía que obtuvieron muestran con detalle la morfología externa del cerebro de Descartes, la cual quedó impresa en el interior del cráneo. Desafortunadamente, como ha sucedido en otros casos, los investigadores encuentran poco en el cerebro de Descartes que pudiera explicar su excepcional inteligencia. Tendremos así que esperar por más investigaciones antes de vislumbrar los secretos del cerebro de los genios.Dado que lo que más nos diferencia a los humanos de otras especies en el reino animal es la inteligencia, no es sorprendente que un cerebro excepcional nos despierte una gran fascinación. Y nos la despierta en un grado tal que produce historias tan extravagantes y vistosas –lo mismo que macabras– como la de René Descartes y sus aventuras después de dejar el mundo.",
    "Un artículo publicado en Suecia en 1869, dirigido a inhibir la emigración de la población sueca hacia los Estados Unidos, advertía sobre los supuestos peligros que enfrentaría quien se aventurara a viajar al país americano. Dicho artículo incluía dos ilustraciones contrastantes. En una se presentaba a un país idílico, con abundantes recursos de caza y pesca y la posibilidad de descansar recostado en una hamaca bajo la sombra de frondosos árboles. En la otra, reflejando al país supuestamente real, se presentaba al inmigrante enfrentando grandes amenazas, incluido el ataque por fieras, serpientes y nativos salvajes. Estos últimos se muestran vestidos como indios norteamericanos, con plumas en la cabeza y extrayendo el corazón al estilo azteca a una víctima arrojada al suelo.No es claro el efecto que este artículo u otros similares haya tenido en el ánimo de los potenciales inmigrantes suecos a los Estados Unidos, pero ciertamente no inhibió a los más de un millón de suecos que cambiaron su residencia a ese país durante el siglo XIX e inicios del XX en la búsqueda del sueño americano. Como tampoco fueron disuadidos por las múltiples dificultades que enfrentaron los 30 millones de alemanes, irlandeses, polacos, griegos, rusos e italianos, entre otras nacionalidades, que llegaron a los Estados Unidos en este mismo periodo en busca de una mejor vida. Por no hablar de los 16 millones de mexicanos que emigraron a los Estados Unidos entre los años 1965 y 2015 –según el “Pew Research Center– atraídos igualmente por el sueño americano. Por otro lado, sabemos que en las últimas décadas el sueño americano se ha desdibujado en cierta medida. Así, un artículo aparecido esta semana en la revista “Science”, publicado por un grupo investigadores de universidades estadounidenses encabezado por Raj Chetty de “Stanford University” en California, encuentra que uno de los aspectos que definen al sueño americano: la aspiración de que los hijos gocen de un mayor nivel de vida del que tuvieron los padres, ha sufrido un descalabro mayor. De manera concreta, dicho artículo concluye que la proporción de hijos que a los treinta años tienen un salario mayor que el que tuvieron sus padres a la misma edad ha caído estrepitosamente en las últimas décadas. Chetty y colaboradores llegaron a esta conclusión después de analizar datos estadísticos laborales de los Estados Unidos para hijos nacidos entre 1970 y 1984 y sus padres respectivos. Dado que la falta de datos suficientes que liguen a padres e hijos en todo el periodo de interés no hace posible comparar de manera directa el salario percibido por un hijo con el de su padre décadas atrás, los investigadores recurrieron a métodos indirectos y a varias bases de datos. Así, hicieron un uso combinado de datos proporcionados por la oficina recaudadora de impuestos de los Estados Unidos que ligan a padres e hijos, nacidos éstos después de 1980, y de datos de la oficina de censos de ese país para los nacidos anteriormente a ese año. En este último caso, a falta de datos completos, los investigadores tuvieron que recurrir a una hipótesis relativa a la estabilidad de la suma de los salarios de padres e hijos a lo largo del tiempo. De acuerdo con los autores del artículo, sin embargo, su hipótesis es sólida y es respaldada por los datos de que se dispone. Como se mencionó anteriormente, Chetty y colaboradores llegan a la conclusión que la movilidad social intergeneracional en los Estados Unidos ha caído drásticamente entre aquellos nacidos en el periodo 1940-1984. Así, mientras que el 90% de aquellos nacidos en 1940 tenía un salario mayor que el de sus padres, esta proporción cayó al 50% para aquellos nacidos en la década de los años ochenta. La caída en movilidad, además, no fue la misma en todo el territorio norteamericano, afectando particularmente al medio oeste. Así, en el estado de Michigan hubo una caída en movilidad por 48 puntos porcentuales, mientras que en Illinois, Indiana y Ohio, dicha caída fue por 45 puntos. En contraste, en los estados de Nueva York y Massachusetts, la caída en movilidad fue de aproximadamente 35 puntos.Chetty y colaboradores se preguntan por las causas de esta caída tan drástica en movilidad intergeneracional y por las políticas que tendrían que implementarse para restaurarla a sus niveles anteriores. Consideran dos posibles causas: la caída en el ritmo de crecimiento de la economía norteamericana, y un crecimiento de la desigualdad en la distribución del producto interno bruto entre la población.Desechan la primera causa pues encuentran ni aún restaurando el ritmo de crecimiento de la economía norteamericana a los niveles de mediados siglo XX pudiera restablecerse la movilidad intergeneracional. Concluyen, en cambio, que la caída en dicha movilidad es debida a la creciente desigualdad en el reparto de beneficios económicos entre la población. Así, un incremento en el ritmo de crecimiento económico beneficiaría sobre todo a las capas superiores que más ganan.De este modo, de acuerdo con Chetty y colaboradores, la creciente desigualdad social en los Estados Unidos está destruyendo el mito del sueño americano. Y lo hace seguramente con una mayor efectividad con que artículos anti Estados Unidos frenaron la emigración sueca hacia ese país en el siglo XIX.",
    "En su novela de 1912 “El mundo perdido”, Arthur Conan Doyle describe una expedición a una meseta perdida en la selva amazónica por un grupo de exploradores que incluye a científicos, reporteros y simples aventureros. La meseta en cuestión está bordeada por altas paredes, escarpadas e inaccesibles a tal grado que la han aislado del mundo por cientos de millones de años, posibilitando la supervivencia de animales hace mucho tiempo extinguidos. Entre estos animales se incluyen a pterodáctilos e iguanodontes. Los exploradores se encontraron incluso con un grupo primitivo de hombres-mono que les hacen la guerra. “El mundo perdido” es, por supuesto, una obra de ficción. Una obra muy a tono con la época en que fue escrita –poco más de cincuenta años después de que Charles Darwin publicara “El origen de la especies”–, pero que posiblemente nadie tomaría como algo más que una mera ficción. De manera sorprendente, sin embargo, un descubrimiento paleontológico llevado a cabo en 2003 en una isla de Indonesia hace realidad en cierta medida lo imaginado por Conan Doyle. En efecto, en septiembre de 2003 un grupo de investigadores de Australia e Indonesia descubrió a unos seis metros de profundidad en la cueva de Liang Bua en la isla indonesia de Flores, un esqueleto casi completo de un individuo que en vida habría tenido alrededor de un metro de altura. La antigüedad de los restos fue originalmente fechada en 18,000 años y posteriormente extendida hasta 65,000-90,000 años. Por su corta estatura, los paleontólogos originalmente pensaron que estaban ante el esqueleto de un niño. Pronto, sin embargo, les fue claro que se trataba de un individuo adulto de pequeñas dimensiones, con un volumen cerebral que apenas alcanzaba los 400 centímetros cúbicos, que es casi un cuarto del volumen de un cerebro humano moderno. Los investigadores habían así sacado a la luz a un humano diminuto, que vivió en una época relativamente reciente y cuyo origen por el momento constituyó un absoluto misterio. Habida cuenta, además, que después del descubrimiento original se dieron otros similares en la misma isla de Flores, demostrando que dicha isla estuvo hasta hace unos 50,000 años habitada por una raza humana diminuta.Por otro lado, en paralelo con la búsqueda de respuestas satisfactorias al misterio de la isla de Flores, y para beneficio de las relaciones públicas, los investigadores bautizaron –de manera acertada– al grupo humano recién descubierto con el mote de “Hobbit”.¿Cómo llegó el “Hobbit” a la isla de Flores? Esta pregunta ha generado mucha controversia entre los especialistas y se han ofrecido explicaciones contradictorias sobre su origen. Estas explicaciones toman en cuenta el hecho que la isla de Flores ha estado aislada en forma total o relativa a lo largo de los últimos dos millones de años.En estas condiciones de aislamiento, algunos especialistas consideran que el “Hobbit” es en realidad anatómicamente un humano moderno afectado por algún desorden genético. En la misma línea de argumentación, al estar una población humana moderna aislada con recursos limitados, habría evolucionado hacía un tamaño diminuto.   Otros investigadores consideran que los “Hobbit” son descendientes del “Hombre de Java” que llegó desde África al este de Asia hace unos 1.8 millones de años. Como el Hombre de Java era más alto y tenía una cerebro considerablemente más grande que los “Hobbit”, éstos últimos tendrían que haber evolucionado hacia tamaños más pequeños por un efecto de aislamiento insular. No se han encontrado, sin embargo, fósiles del Hombre de Java en la isla de Flores, lo que no  apoya esta hipótesis.Una tercera hipótesis sobre el origen del los “Hobbit” asume que éstos son descendientes de una especie todavía más antigua que la del Hombre de Java. Esta hipótesis es apoyada por un estudio publicado esta semana en la revista “Journal of Human Evolution” por un grupo internacional de investigadores encabezado por Debbie Argue de la Universidad Nacional de Australia.Argue y colaboradores llegan a esta conclusión después de estudiar y comparar 133 fósiles de cráneos, mandíbulas y dientes de un gran número de especies que vivieron a lo largo de los últimos tres millones de años. De este estudio se desprende que los “Hobbit”, con un 100% de seguridad, no son hombres modernos con un patología genética. Igualmente, se desprende, con un 99% de seguridad, que los “Hobbit” no están directamente relacionados con el Hombre de Java. Por el contrario, los investigadores llegan a la conclusión que los “Hobbit” pudieron haberse originado en África hace más de 1.75 millones de años y posteriormente emigrado hasta la isla de Flores. Así, de acuerdo con Argue y colaboradores, los “Hobbit” hasta antes de su desaparición hace 50,000 años pudieron haber sido una reliquia con casi dos millones de años de antigüedad. Una reliquia con la que nuestros ancestros convivieron. De hecho, la extinción de los “Hobbit” coincide en tiempo con la expansión de nuestra especie en el este de Asia.  El descubrimiento de la raza de los “Hobbits”, que sobrevivieron en aislamiento hasta tiempos recientes es sin duda fascinante. Y, ciertamente, nos muestra que algunas veces la realidad supera a la ficción.",
    "El miércoles pasado, José Antonio Sánchez, presidente de Radio Televisión Española, dictó una conferencia en la Casa de América en Madrid, en donde comparó al Imperio azteca con el régimen nazi de Adolfo Hitler. Durante su plática, Sánchez citó a la historiadora australiana Inga Clendinnen quien habría dicho que “lamentar la desaparición del Imperio azteca es más o menos como sentir pesar por la derrota de los nazis en la Segunda Guerra Mundial”. Si bien el conferencista dejó claro que él es un ignorante en el tema y que la suya distó mucho de tener la categoría de una conferencia, sus aseveraciones y citas provocaron rechazo en medios mexicanos. El diario Excelsior, por ejemplo, las tachó de indignantes. Según José Antonio Sánchez, España en América “nunca fue colonizadora, fue evangelizadora y civilizadora” de un pueblo con costumbres bárbaras. Por supuesto, aun asumiendo que esto sea cierto, habría que conceder que en su labor evangelizadora y civilizadora España algunas veces se excedió con prácticas que parecían contravenirla. Un ejemplo de esto es la matanza del Templo Mayor perpetrada a traición durante la fiesta de Tóxcatl por los conquistadores bajo el mando de Pedro de Alvarado. Lo es también la práctica de la encomienda, por medio de la cual se entregaba a un peninsular un cierto número de indios para su explotación y, por supuesto, su evangelización.Por otro lado, a pesar de las atrocidades cometidas por los invasores durante la conquista y por más que los europeos del siglo XV estuvieran acostumbrados a los más refinados métodos de tortura, los conquistadores se horrorizaron por los sacrificios humanos que los aztecas habrían llevado a cabo de manera pródiga. Y, sobre todo, por las prácticas de antropofagia con los cuerpos de los sacrificados que proporcionaron a los españoles un excelente argumento para demostrar la necesidad de someter, evangelizar y civilizar a los mexicas.    Al margen del escándalo de los conquistadores por la práctica del canibalismo, estudios sobre la trasmisión de enfermedades neurodegenerativas asociadas a la ingesta de carne humana contaminada, lo mismo que evidencias de restos fósiles descubiertos en sitios prehistóricos, muestran que el canibalismo ha estado presente entre nosotros desde tiempos remotos. Se ha extendido incluso hasta la época actual en lugares aislados del planeta. No es claro, sin embargo, si dicha práctica tuvo un propósito ritual o propósitos nutricionales.  Un artículo publicado esta semana en la revista “Scientific Reports” se inclina por la primera posibilidad. Dicho artículo, publicado por James Cole de la Universidad de Brighton en el Reino Unido, concluye que el contenido de calorías del cuerpo humano no es especialmente grande y que por lo mismo no constituye una fuente de alimento particularmente atractiva. En efecto, Cole encuentra que la masa muscular de un cuerpo humano de 66 kilogramos de peso contiene alrededor de 32,000 calorías, en comparación con los 3,600,000 calorías de un mamut, las 600,000 calorías de un oso y las 200,000 calorías de un caballo. Además, el contenido de calorías del cuerpo humano por kilogramo de masa muscular es significativamente más pequeño en comparación con animales de gran tamaño como mamuts, rinocerontes lanudos y osos.   En estas circunstancias Cole se pregunta: ¿cuáles podrían haber sido las motivaciones de nuestros ancestros para cazar a los de sus propia especie en lugar de buscar animales más grandes que les proporcionaría, por unidad, una mucho mayor cantidad de alimento? Teniendo en cuenta, además, que en el primer caso se enfrentarían a animales con un nivel similar de inteligencia lo que, sin duda, haría más difícil la caza. Así, Cole concluye que los episodios de canibalismo hasta ahora descubiertos tuvieron probablemente un origen ritual.Con relación a los aztecas ¿el canibalismo que practicaban era con propósitos nutricionales o rituales? En esto no hay un acuerdo unánime, pero Bernad Ortiz de Montellano de la Universidad Estatal Wayne en Detroit sostiene que los aztecas no tenían necesidad de recurrir al canibalismo para satisfacer sus necesidades alimenticias dado que Tenochtitlan producía suficientes alimentos, además de que, como capital del Imperio azteca que era, recibía gran cantidad de tributos. Además, el número de sacrificados no sería suficiente para nutrir al 25% privilegiado de la población que consumía la carne de los mismos.De este modo, al igual que habría ocurrido con nuestros lejanos ancestros según James Cole, la práctica de la antropofagia entre los aztecas habría tenido un propósito ritual y los sacrificados no habrían sido simples animales de matadero. En este contexto, si bien el canibalismo y las guerras floridas de los aztecas fueron una práctica brutal, no lo fueron más que las matanzas y atrocidades llevadas a cabo por los españoles durante la conquista; ni más que el sometimiento posterior de la población indígena. Así, resulta que la comparación que hace José Antonio Sánchez entre el Imperio azteca y el régimen nazi resulta exagerada. Por decir lo menos.",
    "La semana que hoy termina nos enteramos por los medios de comunicación que la empresa norteamericana Space X logró poner en órbita un satélite empleando un cohete reciclado; es decir, un cohete que había ya previamente viajado al espacio.  Para reciclar un cohete es necesario recuperarlo sin daño al final de su misión y para esto Space X ha desarrollado una tecnología para posarlo suavemente sobre una plataforma en el mar. Esta tecnología permitió recuperar sin daño al cohete de referencia –en realidad, sólo su primera etapa– al término de su primera misión. Permitió, igualmente, recuperarlo al final de la segunda misión, de modo que el cohete podrá ser reusado para un tercer viaje y potencialmente para múltiples misiones.Con el reciclado de la primera etapa de los cohetes, Space X espera obtener ahorros significativos en sus lanzamientos de satélites al espacio, habida cuenta que cada lanzamiento tiene un costo total aproximado de 60 millones de dólares, en buena medida por el costo del cohete. Como sabemos, la era de los satélites se inició el 4 de octubre de 1957 cuando la entonces Unión Soviética puso en órbita el Sputnik 1, el primer satélite artificial de la historia. El Sputnik 1 se mantuvo en órbita por tres meses, después de lo cual regresó a la Tierra, incinerándose por la fricción con la atmósfera. Al Sputnik 1 le siguieron otros de su clase, lo mismo que los equivalentes norteamericanos. El envío de satélites se intensificó en las siguientes décadas, incrementando rápidamente el número de objetos en órbita. En la actualidad, de acuerdo la “Union of Concerned Scientists”, hay alrededor de 1,400 satélites operativos circulando a la Tierra.El número de países que poseen satélites también ha crecido sustancialmente. Así, mientras que en 1966 solamente los Estados Unidos, la Unión soviética, Francia, Italia y Gran Bretaña poseían satélites, en 2016 la lista se había ampliado a medio centenar de países, incluyendo a México, si bien no todos con capacidad propia para acceder al espacio.  Los 1,400 satélites en operación están distribuidos en varias órbitas a diferentes alturas. En las órbitas bajas, a unos cientos de kilómetros de altura se localizan casi 800 satélites, mientras en la órbita más alejada, a 36,000 kilómetros de la superficie terrestre se encuentran alrededor de 500 satélites. El resto se localiza en órbitas intermedias. Hay que notar, además,  que aparte de los satélites en operación hay un gran número de objetos orbitando a la Tierra que son producto de nuestras actividades en el espacio y que son clasificados como basura espacial. De acuerdo con la NASA se tienen rastreados en órbita alrededor de 20,000 objetos con dimensiones mayores a 10 centímetros. De estos objetos un cinco por ciento corresponden a satélites activos y un ocho por ciento a restos de cohetes. El restante está constituido por satélites o fragmentos de satélites inactivos. Adicionalmente, según la NASA, hay en órbita alrededor de medio millón de fragmentos del tamaño de una canica o más grandes, y millones de fragmentos menores que no pueden ser observados pero que a las velocidades a que viajan, que podrían alcanzar los 28,000 kilómetros por hora, pueden provocar daños considerables en caso de colisión. Las colisiones en órbita pueden, además, incrementar sustancialmente el número de fragmentos de basura espacial. Por ejemplo, la colisión entre un satélite ruso y un satélite norteamericano en febrero de 2009 produjo más de 2,000 fragmentos que se añadieron al inventario de basura espacial. De la misma manera, una prueba llevada a cabo por China en 2007, en el curso de la cual un misil destruyó a un satélite inactivo, generó más de 3,000 nuevos fragmentos en órbita.Los objetos olvidados o extraviados en el espacio se añaden, igualmente, al inventario de basura espacial. En días pasados, a manera de ejemplo, la prensa nos informó de la pérdida en el espacio de un escudo contra meteoritos, de 1.5 metros de diámetro, que tendría que haber sido instalado en la Estación Espacial Internacional por dos astronautas durante una caminata espacial. En el video correspondiente –que puede ser consultado en Internet– es posible oír que uno de los astronautas le dice a su compañera que no encuentra el escudo, mismo que es visto alejarse lentamente y perderse en las profundidades del espacio.  Según los especialistas, la basura espacial, particularmente aquella lo suficientemente pequeña para que no pueda ser rastreada –producto de más de 5,000 lanzamientos al espacio desde el Sputink 1– representa un peligro creciente para las misiones espaciales. Por otro lado, posiblemente el problema no se solucione al corto plazo. Es decir, en la medida en que se abaraten los viajes espaciales por programas de reciclado de cohetes como el que está llevando a la compañía Space X, la tendencia posiblemente será la de incrementar la frecuencia de lanzamientos al espacio más que disminuirla.Habrá así, por intereses privados, más incentivos para agravar el problema de la basura espacial que para aminorarlo. Al igual que  ocurre, por ejemplo, con el cambio climático, producto de otra basura también generada por nosotros.",
    "Si bien las cifras y estimados del porcentaje de la población que teme volar en avión varían grandemente según la fuente, ciertamente el fenómeno existe y hay quienes se resisten, algunos más que otros, a abordar un avión. Y en cierto modo les asiste la razón –dejando de lado los casos patológicos–, pues la evolución, que nos ha moldeado a lo largo de decenas de millones de años, no tomó en cuenta la aparición de los aviones. Es decir, la posibilidad de que en algún momento futuro tuviésemos que viajar sentados en una silla a diez mil metros de altura encerrados en un tubo metálico con alas. El viajar en avión es así ajeno a nuestra naturaleza de animales de tierra firme.  Dada esta circunstancia, la percepción de peligro al viajar en avión sobre el océano sería doble, pues tampoco estamos particularmente bien adaptados para sobrevivir en el agua; tomando en cuenta, además, que el océano es un lugar desconocido y atemorizador, con profundidades que pueden llegar a varios kilómetros. Así, mientras que ante una emergencia un avión volando sobre tierra firme puede intentar un aterrizaje forzado, esto no lo puede hacer en medio del océano.Como no lo hubiera podido hacer –en caso de haberlo intentado– el vuelo de MH370 de Malasya Airlines que se asume se estrelló en el Océano Índico sur al quedarse sin combustible. Como sabemos, el vuelo MH370 despegó del aeropuerto de Kuala Lumpur con rumbo a Pekin el 8 de marzo de 2014. Poco después del despegue, sin embargo, de manera inexplicable cambió bruscamente de rumbo y enfiló hacia el sur, presumiblemente estrellándose en el mar abierto en un punto al sur-oeste de la ciudad australiana de Perth. Con el fin de localizar los restos del vuelo MH370, el gobierno australiano llevó a cabo una operación de búsqueda en el fondo del océano a lo largo de una franja con un ancho de 75 a 160 kilómetros y una longitud de 2,500 kilómetros, en la que se sospechaba se había desplomado la aeronave. La profundidad del océano en el área explorada oscila ente los 635 y los 6300 metros y, dado que a esas profundidades no llega la luz solar, la exploración se llevó a cabo por medio de ondas de sonido. Así, se enviaron desde barcos especializados haces de ondas sonoras dirigidas hacia el lecho marino, captando su eco después de reflejarse en el fondo del mar. Se busco de este modo delinear la topografía del fondo del océano y, de tener suerte, dar con el fuselaje del avión siniestrado. No se tuvo éxito y al cabo de casi tres años se abandonó la búsqueda.  Como un subproducto de la misma, sin embargo, se obtuvieron imágenes de una superficie de lecho marino de casi 300,000 kilómetros cuadrados de extensión, con una resolución sin precedente para esa región del Océano Índico. Esto lo comenta un grupo de científicos de instituciones en Australia encabezados por Kim Picard de la agencia gubernamental australiana “Geoscience Australia”, en un artículo publicado en la revista “EOS Earth and Space Science News” al inicio de este mes. De acuerdo con Picard y colaboradores, mientras que las imágenes previas de la región explorada –obtenidas de manera indirecta– tenían una resolución de 5 kilómetros cuadrados –un cuadrado de poco más de dos kilómetros de lado–, en las nuevas imágenes es posible distinguir áreas apenas un poco más grandes que un campo de futbol. Esto permitió descubrir la existencia en el fondo del Océano Índico de volcanes con una altura de 1,500 metros y abismos de más de 5,000 metros de profundidad. De este modo, la tragedia del vuelo MH370, todavía no aclarada, ha generado un mayor conocimiento del fondo del océano, de cuya topografía a nivel global tenemos menos conocimiento que de las topografías de Marte, Venus y la Luna que conocemos con una resolución de 100 metros. De hecho, de acuerdo a un artículo publicado en la revista “EOS Earth and Space Science News”  este mes de marzo por un grupo encabezado por Walter Smith de la “National Oceanic and Atmospheric Administration”, existen mapas topográficos del fondo de los océanos con la suficiente resolución para apenas un 8% del área total.  Todo lo anterior para mayor consternación de los aéreo fóbicos a los que les aterroriza volar sobre el mar. Habida cuenta, además, que, de acuerdo con Smith y colaboradores, un 60% de la distancia total que cubren las rutas aéreas comerciales corresponde a regiones del océano para los cuales no hay mapas topográficos. Así, en forma frecuente, lo viajeros transcontinentales vuelan sobre “mare incognitum”. Aun hoy, después de cinco siglos de exploraciones.Llevar a cabo un programa de exploración del fondo de los océanos para conocerlos al detalle, como lo proponen Smith y colaboradores en su artículo, sería sin duda de una gran trascendencia científica. Habría que conceder, no obstante, que para los pasajeros cuyo avión se estrella, poco importa que lo haga contra la tierra o contra el mar. Y que, en este último caso, tampoco importa si el impacto es justo arriba de un volcán o de un abismo submarino.",
    "Las imágenes nocturnas de la Tierra vistas desde el espacio muestran regiones profusamente iluminadas que indican en donde se encuentran las mayores concentraciones de población del planeta. Así, podemos ver áreas de gran brillantez alrededor de las grandes ciudades californianas y de la costa este de los Estados Unidos, lo mismo que alrededor de las grandes ciudades europeas, japonesas y chinas. Esto contrasta con la  iluminación de la región del desierto de Sahara en el norte de África, indicativa de su bajísima densidad de población.La baja población del desierto del Sahara, por supuesto, es resultado de su clima excesivamente seco que ha resultado en un territorio árido y hostil, poco atractivo para echar raíces. De hecho, la imagen popular del desierto del Sahara es la de un páramo con poco más que dunas y tormentas de arena. Esto, sin embargo y aunque resulte difícil de creer, no fue siempre así y hubo una época en la que el desierto de Sahara era un lugar con abundante vegetación y fauna que hoy solo se encuentra en regiones más al sur del continente africano.En efecto, si bien durante el último periodo glacial el Sahara era un desierto tal como lo es hoy, al final del mismo, hace unos 11,000 años, un monzón africano fortalecido llevó humedad y lluvia desde el Océano Atlántico hacia el desierto y lo transformó en un exuberante vergel.  El transporte de humedad desde el océano hacia el continente ocurrió cuando la tierra se calentó más que el agua del mar por efecto de la radiación solar. En estas condiciones, el aire en contacto con la tierra elevó su temperatura y ascendió generando una región de baja presión, la que, a su vez provocó una corriente de aire húmedo desde el mar hacia el continente. Adicionalmente, al ascender el aire húmedo y disminuir su temperatura tuvo menos capacidad para retener el vapor de agua que se precipitó en forma de lluvia. De esta manera, el Sahara se convirtió de un territorio seco y yermo en uno con abundante vida vegetal y animal que incluía cocodrilos, elefantes y jirafas, lo mismo que con una red de ríos y lagos; estos últimos de una dimensión tal que fueron capaces de albergar peces de hasta 150 kilogramos de peso. El periodo verde del Sahara se prolongó hasta hace unos 5,000 años cuando regresó a la condición de desierto que hoy ostenta al declinar la fuerza de los monzones.El nacimiento del Sahara Verde ha sido tradicionalmente explicado en función de un incremento de la intensidad de la radiación solar que incidió sobre el desierto. Esto, a su vez, habría sido producto de cambios en la órbita de la Tierra alrededor al Sol y de la inclinación de su eje de rotación, los cuales se habrían alineado hace 11,000 años de tal manera que incrementaron el calentamiento del Sahara durante el verano. Con el tiempo, hace unos 5,000 años, la alineación de nuestro planeta en relación al Sol cambió nuevamente y con esto disminuyó la cantidad de radiación solar incidiendo sobre el Sahara. En estas condiciones disminuyó la fuerza de los monzones y con esto el Sahara regresó a su condición original. Esta explicación, sin embargo, fue puesta en duda por un artículo aparecido el pasado mes de enero en la revista “Frontiers in Earth Science”, publicado por David Wright de la Universidad Nacional de Seul en Corea del Sur. De acuerdo con Wright, es posible que la desaparición del Sahara verde no haya sido debido a causas por completo naturales sino que las actividades humanas hayan contribuido de manera decisivaWright encuentra que hace unos 8,000 años, cuando habría iniciado el declive del Sahara Verde, aparecieron poblaciones dedicadas al pastoreo en la región del Nilo que empezaron a emigrar hacia el oeste. Esto coincidió con una sustitución de la vegetación del Sahara con matorrales, lo que es indicativo del inicio de la desertificación. La pérdida de la vegetación del Sahara Verde –por causas naturales, o  artificiales en beneficio del pastoreo– incrementó la reflexión de la luz solar y con esto se calentó menos la tierra y en consecuencia se debilitaron los monzones. Así, disminuyó aun más el área cubierta por la vegetación y la fuerza de los monzones, hasta que, siguiendo una cadena de auto reforzamiento, terminamos con el desierto del que somos testigos. Tenemos de este modo que el inicio de ambas, las actividades de pastoreo y el proceso de desertificación del Sahara, habrían coincidido en el tiempo. No podemos decir mucho más, sin embargo, pues como lo señala Wright, nos encontramos en este punto con el problema del huevo y la gallina. Es decir, no es posible concluir si la desertificación es producto del pastoreo, o bien si el pastoreo fue la práctica de una población de cazadores-recolectores que fueron obligados a cambiar de modo de subsistencia por la desertificación de su hábitat. Queda, no obstante, abierta la posibilidad de que prácticas nuestras hayan contribuido de manera decisiva a modificar un vergel con una rica vegetación, con cocodrilos, elefantes y jirafas y una red de ríos y lagos, en un desierto en extremo seco y desolado de 9 millones de kilómetros cuadrados.",
    "La mañana del 1 de septiembre de 1923 un terremoto de magnitud 7.9 destruyó la ciudad de Tokio con un saldo de más de 100,000 muertos. La devastación fue producto tanto del terremoto mismo como de los incendios que provocó. En las horas y días que siguieron corrieron rumores falsos, originados en cuerpos militares y policiacos, según los cuales los coreanos residentes en Tokio estaban llevando a cabo robos y provocando desmanes armados con bombas. Habrían incluso envenenado los pozos de agua potable de la ciudad; esto, de acuerdo con notas aparecidas en periódicos locales de la época. Como resultado de la campaña de información falsa, 6,000 coreanos fueron linchados, algunos de manera atroz, tanto por grupos organizados de la población civil como por militares y policías. Entre las víctimas se encontraron incluso japoneses que fueron confundidos con coreanos en medio del desorden.Los comportamientos irracionales producto del esparcimiento de rumores y noticias falsas no son algo fuera de lo común. Así, aunque en un contexto diferente, se han lanzado campañas de desinformación e información falsa que buscan desprestigiar hechos científicos y provocar respuestas irracionales en el público. Estas campañas, si bien no llevan a acontecimientos tan dramáticos como las de Tokio en 1923, pueden tener consecuencias de gran envergadura. En esta categoría se incluye la campaña de desinformación que se ha montado en torno al problema del calentamiento global y el cambio climático. Como sabemos, existe consenso entre los científicos del clima  que el planeta se está calentando de manera paulatina, en buena medida debido a la emisión a la atmósfera de gases de invernadero por actividades humanas. No ha faltado, sin embargo, quien ponga en duda este hecho.   Con relación a esto último y para ser precisos, habría que notar que está fuera de toda duda que la concentración de dióxido de carbono –el principal gas de invernadero– en la atmósfera está creciendo año con año. En efecto, las mediciones de dicha concentración que el  Observatorio de Mauna Loa, Hawaii, ha llevado de manera continua desde el año 1958 a la fecha –disponibles en Internet– lo demuestran de manera contundente. Es claro, también, que dicha concentración está ligada a actividades humanas desarrolladas a partir de la revolución industrial.Igualmente, es claro –por más que hubo quien en los pasados años estuviera en desacuerdo– que la temperatura de la superficie de nuestro planeta está aumentando de manera progresiva pues las mediciones de la misma –también disponibles en Internet–  así lo demuestran. Por otro lado, ligar el aumento de temperatura de la Tierra con el aumento en la concentración de gases de invernadero en la atmósfera y con las actividades humanas es un asunto técnico sólo abordable por especialistas. Con respecto a este punto, sin embargo, un 97% de científicos del clima está de acuerdo con que dicha liga existe. En contraste, para los escépticos del cambio climático, el aumento en la temperatura de la Tierra –si es que existe– es debido a causas naturales y no a causas humanas. Y para apoyar a esta tesis han lanzado una campaña de desinformación e información falsa con el fin de neutralizar la opinión de los especialistas.   Para investigar la efectividad de dicha campaña y la forma de neutralizarla, un grupo de especialistas de universidades en Gran Bretaña y los Estados Unidos, encabezado por Sander van der Linden de la Universidad de Cambridge en el Reino Unido, llevó a cabo un estudio con más de 2,000 participantes a través de Internet. Los resultados de la investigación se publicaron el pasado mes de enero en la revista “Global Challenges”. En una parte del estudio los investigadores se propusieron determinar que mensajes falsos tenían una mayor efectividad entre el público y para esto tomaron ejemplos de una campaña real de desinformación. En otra fase de la investigación, se propusieron determinar la efectividad de posibles “vacunas” contra la desinformación y las afirmaciones falsas.Como resultado de su trabajo, van der Linden y colaboradores encontraron que si bien los mensajes de desinformación tienen un efecto adverso sobre la percepción del cambio climático, es posible “inocular” al público en contra de las afirmaciones falsas mediante mensajes que alerten sobre la existencia de grupos interesados en desprestigiar las opiniones científicas. Es decir, en lugar de reaccionar en contra de un mensaje falso una vez difundido, habría que alertar a priori al público sobre la futura difusión del mismo, dando detalles de su posible contenido.    Las campañas de desinformación y noticias falsas tienen propósitos definidos que en el caso de la masacre de coreanos en Tokio quizá no sean completamente claros. Si lo son, en cambio, los propósitos detrás de las campañas en contra del cambio climático. Aquí los científicos del clima exponen un problema de urgente solución que demanda grandes cambios en la manera como usamos las fuentes de energía del planeta. Esto afecta intereses y de manera natural genera grandes resistencias y campañas desinformativas. En estas circunstancias y en contra de las noticias falsas y los hechos alternativos –tan de moda hoy en día– sólo nos queda vacunarnos y defendernos en la medida de lo posible.",
    "Con la introducción y comercialización de la luz eléctrica a finales del siglo XIX las noches aceleradamente perdieron su negrura y con esto cambiaron buena medida nuestras costumbres nocturnas. Para ser precisos, hay que mencionar que la negrura de la noches se empezó a perder a lo largo del siglo XIX con la lámparas de gas. Estas, sin embargo, no tenían el potencial de los focos incandescentes y en su momento fueron rápidamente desplazadas por la luz eléctrica.El espíritu de las noches sin electricidad fue capturado fielmente por pintores como el francés Georges de La Tour –siglo XVII– en cuadros que muestran escenas nocturnas en interiores iluminados por la luz de una vela, con una intensidad tan pobre que apenas alcanzaría para no tropezar. Todo esto cambió con la luz eléctrica y a los cuadros de La Tour podríamos anteponer la imagen de un estadio norteamericano de beisbol durante la celebración de un juego nocturno, con una iluminación tan intensa y bien distribuida que permite a los jugadores practicar su profesión con la increíble precisión que ésta les demanda.  Al igual que las lámparas incandescentes desplazaron a las lámparas de gas, las primeras serán irremediablemente desplazadas por las nuevas lámparas LED que cuentan con ventajas considerables, incluyendo una mayor eficiencia y tiempo de vida. Al contrario de las lámparas incandescentes, que generan luz calentando un filamento metálico, los LEDs producen la luz haciendo pasar una corriente eléctrica por un material semiconductor. Y dependiendo de la composición química de dicho material, es el color de la luz que emite el LED. Los primeros LEDs fueron fabricados en la década de los sesenta del siglo pasado y emitían luz de color rojo –los había también que emitían luz infrarroja invisible–. Posteriormente se fabricaron LEDs que emitían en colores amarillo o verde. En estos desarrollos, los laboratorios de investigación de los Estados Unidos jugaron un papel prominente.  Un LED de un solo color, sin embargo, no puede se usado como una lámpara para iluminación pues para esto es necesario que emita luz blanca, es decir, luz con una mezcla de varios colores. En estas condiciones la revolución en la iluminación tuvo que esperar hasta la década de los años noventa, cuando los japoneses Isamu Akasaki, Hiroshi Amano y Shuji Nakamura lograran fabricar LEDs azules. Aquí hay que señalar que si bien los LEDs azules no pueden ser tampoco utilizados como fuentes de iluminación, parte de la luz que emiten puede ser convertida en colores verde y rojo.  De esta manera, mezclando el verde y rojo con el azul original, se obtiene luz blanca. Por otro lado, si bien los LEDs de un sólo color no son adecuados como fuentes de iluminación, si tienen una gran número de otras aplicaciones, incluyendo su uso en pantallas y semáforos controladores del tráfico, por mencionar unas de las más comunes. Existen incluso aplicaciones para LEDs que emiten en colores más allá de los que podemos ver y esto motiva que en diferentes laboratorios en el mundo se lleven a cabo proyectos de investigación para el desarrollo de LEDs con características de emisión de luz invisible.Con relación a este último punto, esta semana un grupo de investigadores de las Universidades Cornell y Notre Dame en los Estados Unidos,  publicaron un artículo en la revista “Applied Physics Letters” en el que reportan el desarrollo de un LED que emite luz ultravioleta. Dado que la luz emitida por dicho LED es muy energética tiene poder bactericida y, entre otras aplicaciones, puede emplearse para purificar agua o higienizar alimentos, lo mismo que en dispositivos para detectar la falsificación de papel moneda.La tecnología empleada para fabricar dicho LED ultravioleta es en extremo sofisticada e implica la manipulación de capas de materiales de diferente composición química con espesores atómicos. Dicha tecnología, por supuesto, es posible sólo dentro de laboratorios que cuenten con conocimientos técnicos y científicos de frontera, lo que ha sido tradicional en muchos laboratorios norteamericanos.Es, sin embargo, interesante hacer notar la composición étnica del grupo que desarrolló el LED ultravioleta de referencia. Un listado de los nombres de sus integrantes nos da una indicación de la misma: S.M. Islam, Kevin Lee, Jai Verma, Vladimir Protasenko, Sergei Rouvimov, Shyam Bharadwaj, Huili (Grace) Xing, y Debdeep Jena. De este listado y haciendo una búsqueda rápida en Internet nos revela que el grupo está formado por tres investigadores procedentes de la India, dos de China, dos de Rusia, uno de Bangladesh y cero de los Estados Unidos. Esto último, que resulta sorprendente, no es, sin embargo, un caso aislado. De hecho, el que una mayoría de integrantes de un grupo de investigación en laboratorios norteamericanos esté compuesto mayoritariamente por especialistas que emigraron a los Estados Unidos no es algo infrecuente.Así, restringir la entrada de investigadores extranjeros a los Estados Unidos, como se teme en ciertos círculos, pareciera ser algo que va en contra de los intereses de ese país, pues si bien es cierto que los Estados Unidos es un líder tecnológico, dicho liderazgo está basado en el flujo de inteligencia del exterior. De este modo, el país que más contribuyó para dar la luz eléctrica al mundo, y el que ha jugado un papel central en el desarrollo de las fuentes LED, podría entrar en un periodo de oscuridad.",
    "En días pasados vimos publicadas en los medios de comunicación notas con encabezados vistosos que hablaban acerca de la posibilidad de resurrección del mamut en un par de años. Esto, después de que George Church, que encabeza un equipo de investigación en la Universidad de Harvard dedicado a producir células de elefante modificadas con genes de mamut, dictara una conferencia en el congreso de la Asociación Americana para el Avance de la Ciencia celebrado en Boston la pasada semana. En dicha conferencia, Church expuso los últimos resultados de sus investigaciones que resultaron ser más modestos que lo publicado por algunos medios. No fue el caso del periódico británico “The Guardian”, según el cual Church y su equipo de científicos está a dos años de crear un embrión en el que rasgos de mamut serán programados en un elefante asiático con el objeto de “producir un embrión híbrido de elefante-mamut”, lo que “podría ocurrir en un par de años”. Esto no concuerda con la impresión creada por otros medios de comunicación, pues de acuerdo con lo realmente reportado por Church, en realidad estaríamos todavía lejos de volver a ver mamuts pastando en  Siberia.Resulta así que algunos medios de comunicación publicaron notas claramente exageradas. De hecho, Church y colaboradores no están en estos momentos preocupados por resucitar al mamut –extinguido hace unos cuatro mil años– sino en generar el embrión híbrido, que más se acercaría al de un elefante que al de un mamut. Y una vez que hubiesen logrado esto –si es que algún día lo hacen–, el camino a recorrer para producir un híbrido mamut-elefante sería todavía muy largo.  Según John Hawks de la Universidad de Wisconsin, quien se muestra escéptico del proyecto de Church, un problema formidable a resolver para la creación de un híbrido mamut-elefante es el del medio o matriz subrogada en la que se desarrollaría el embrión hasta el final de la gestación. Una posibilidad sería la utilización de una elefante asiática pero esto se considera poco probable dado que la especie está en peligro de extinción. Según “The Guardian”,  Church considera la posibilidad de emplear una matriz artificial lo que Hawks considera es un problema de difícil solución. Por lo demás, la exageración en los medios de comunicación en cuanto a noticias científicas no es algo fuera de lo común. En efecto, dado que la ciencia ha originado innumerables avances que han cambiado radicalmente nuestras vidas, existe una extendida fe pública en la misma que lleva algunas veces a sobrestimar sus capacidades reales. Sobre todo si se trata de temas de gran atractivo mediático como es el relativo a la resucitación de los mamuts. Una situación similar se presenta, por ejemplo, en cuanto a los viajes a Marte, que es el planeta del sistema solar más parecido al nuestro y por lo mismo llama mucho la atención. Marte ha sido motivo de especulaciones sobre la posibilidad de que esté habitado. En el siglo XIX, por ejemplo, el astrónomo italiano Giovanni Schaparelli observó líneas sobre la superficie de Marte que otros después interpretaron como canales de irrigación construidos por una civilización marciana. Si bien posteriormente quedó claro que dichas líneas no eran sino una ilusión óptica, Marte siguió siendo motivo de nuestro interés y ha sido tema de novelas y películas. En este contexto, no sorprende que exista una sociedad, “The Mars Society”, cuyos objetivos son los de “Impulsar la exploración y la colonización del planeta rojo”, por más que estos objetivos representan dificultades considerables. En efecto, Marte está lejos de ser hospitalario. Para empezar, tiene una atmósfera en extremo tenue y sin ningún gas que podamos respirar, lo que nos obligaría a vivir permanentemente dentro de edificios herméticamente cerrados con atmósfera artificial; o bien, si hemos de salir, a enfundarnos en un engorroso traje de astronauta.Pero quizá lo que más problema nos daría serían las radiaciones de alta energía provenientes de Sol y del espacio interestelar que podrían matarnos y de las cuales no tendríamos protección. No esperaríamos así que se dieran viajes tripulados a Marte en un futuro previsible.  Y a pesar de esto muchas veces en los medios de comunicación dichos viajes aparecen como algo que estuviera a nuestro alcance.    Con relación a esto último, el pasado 13 de febrero el diario Excelsior encabezó una nota de la siguiente manera con referencia a un estudiante mexicano: “Con apenas 20 años, Yair Piña fue elegido por la NASA para formar parte de la historia al formar parte de la tripulación 180 que irá a Marte; alista su viaje desde el desierto de Utah.” Una búsqueda rápida en Internet nos indica que el entrenamiento para viajar a Marte se llevará a cabo en la “Mars Desert Research Station” localizada en el sur del estado de Utah en los Estados Unidos y tendrá una duración de dos semanas. El campo de entrenamiento guarda una cierta similitud con la que encontrarían los futuros colonizadores de Marte. Con la salvedad, sin embargo, de que en Utah se puede respirar a diferencia de Marte. Y, lo más importante, sin mortíferas radiaciones de alta energía. Algo así como parque temático sobre Marte sin las inconveniencias del planeta real.Y en cuanto a los mamuts pastando en Siberia, tal parece que tendríamos por lo pronto que conformarnos también con un parque temático. Y todo esto debería ser quizá reflejado por los medios de comunicación.",
    "Según sostenía Aristóteles, algunos insectos se generan de manera espontánea a partir del rocío, al igual que algunos peces y gusanos lo hacen del fango y de la carne putrefacta, en forma respectiva. El que Aristóteles hubiera sostenido este tipo de opiniones no resulta sorprendente si recordamos que, según Bertrand Russell, el filósofo griego tenía también la creencia que las mujeres tienen menos dientes que los hombres, sin que se le hubiera ocurrido comprobarlo contándoselos a cualquiera de sus dos esposas. Aristóteles tuvo una considerable influencia en el pensamiento europeo a lo largo de casi dos milenios. No todo mundo, sin embargo, estaba convencido de la generación espontanea de seres vivos. Así, podemos leer en la Wikipedia que Francisco Redi, un médico y naturalista italiano del siglo XVII, realizó un par de sencillos experimentos para refutarla. En un primer experimento, Redi colocó sendos pedazos de carne en dos frascos, uno lo selló herméticamente para impedir el paso de las moscas y el otro lo dejó abierto. Encontró que en ambos casos con el tiempo la carne se pudrió, pero sólo en el caso del frasco abierto se generaron gusanos. Pensando que quizá el resultado era producto de que en el frasco cerrado no entró el aire del ambiente, en un segundo experimento colocó un pedazo de carne en un frasco que cubrió con una gasa. El resultado fue que la carne se pudrió sin generar gusanos, y que las moscas, al no haber podido llegar hasta la carne, habían depositado sus huevos sobre la gasa.Los experimentos de Redi demostraron que los gusanos nacían, no por generación espontánea, sino a partir de lo huevos de las moscas. No todo mundo se convenció, sin embargo, y tocó a Luis Pasteur, hasta el siglo XIX, enterrar la idea de la generación espontánea. Aunque no del todo, sin embargo, pues si bien los gusanos de la carne putrefacta resultaron ser larvas de mosca, cabe preguntarse como fue que se generó la  mosca primigenia y su larva correspondiente. Por supuesto, después de Darwin quedó claro que las especies no se mantienen inmutables en el tiempo sino que están en continua evolución. Aun así, no obstante, cabe preguntarse cómo fue que apareció el primer antecesor de todas las especies sobre la tierra. O de manera más propia, cuál fue el origen de la vida en nuestro planeta. Y en este respecto, sin invocar a una creación divina, la hipótesis mas razonable es que la vida se generó de manera espontánea a partir de materia inanimada, siguiendo las leyes de la física y la química y una vez que se dieron las condiciones ambientales adecuadas.Los resultados de un experimento realizado en 1953 por  Stanley Miller y Harold Urey en la Universidad de Chicago apuntan en esta dirección. Como parte del experimento, Miller y Urey  trataron de reproducir las condiciones ambientales de la tierra hace miles de millones de años cuando se habría originado la vida. Para esto, mezclaron agua, metano, amoniaco, dióxido de carbono, nitrógeno y agua, y sometieron la mezcla a descargas eléctricas de 60,000 voltios. Como resultado, si bien no obtuvieron materia viva, si lograron sintetizar aminoácidos y otros compuestos orgánicos asociados a la misma. Los especialistas, por otro lado, contemplan también la posibilidad de que la vida se haya originado en algún otro sitio del sistema solar y no necesariamente en la tierra, y haya sido traída hasta aquí por medio de un meteorito que haya penetrado hasta la superficie de nuestro planeta. Esta posibilidad, por supuesto, no resuelve las incógnitas sobre el origen de la vida, pero si aumenta las posibilidades de que en algún lugar del sistema solar se hayan dado las condiciones necesarias para su aparición.  Con relación a este último punto, en el número de esta semana de la revista “Science” se publicó un artículo en el que se reportan resultados que sugieren que en el planeta enano Ceres, que se encuentra situado en el cinturón de asteroides entre las órbitas de Marte y Júpiter, pudiera contar con las condiciones propicias para la existencia de vida. El artículo fue publicado por un grupo internacional de investigadores encabezado por Maria Cristina De Sanctis del Instituto Nacional de Astrofísica, en Roma, Italia.  De Sanctis y colaboradores estudiaron datos obtenidos por la sonda Dawn, que está actualmente en órbita  alrededor de Ceres, y encontraron la presencia de materia orgánica en una región de unos 1,000 kilómetros cuadrados alrededor del cráter Ernutet de 50 kilómetros de diámetro. En base a los datos analizados y a un estudio de la geología de la zona, los investigadores concluyen que la materia orgánica descubierta no procede del impacto de un meteorito sino que se originó dentro del mismo planeta. Si bien el hecho de encontrar materia orgánica fuera de nuestro planeta no es en si un hecho extraordinario, pues la misma se ha ya encontrado en otros cuerpos del sistema solar, el descubrimiento de De Sanctis y colaboradores es relevante, pues Ceres cuenta con otros elementos que son indispensables para la vida. De acuerdo con Simone Marchi, uno de los autores del artículo de referencia, “Ceres tiene evidencia de minerales hidratados de amonia, agua congelada, carbonatos, sales y ahora materiales orgánicos. Con este nuevo descubrimiento, Dawn ha demostrado que Ceres contiene los ingredientes clave para la vida”.Y así, después de todo, resulta que sí existe la generación espontánea. Aunque no como Aristóteles se la imaginaba.",
    "Posiblemente los pterodáctilos no sean tan famosos o llamen tanto la atención como los tiranosaurios o los brontosaurios, que se cuentan entre los favoritos de los niños, pero sin duda no dejan de tener su encanto. Así, tienen, por ejemplo, una aparición destacada en la novela “El Mundo Perdido” de Arthur Conan Doyle –creador de Sherlock Holmes– publicada en 1912. Aparecen también, aunque quizá con un papel menos destacado, en la película homónima del cine mudo, basada en dicha novela y estrenada en el año de 1925. En su novela, Conan Doyle describe una expedición encabezada por un profesor Challenger a una meseta perdida en la selva amazónica, en la que habían supuestamente sobrevivido dinosaurios y otros animales extinguidos hace ya mucho tiempo. Esto incluía a pterodáctilos con alas de seis metros de extensión. En un pasaje de dicha novela, cuando los exploradores se acercaron a un nido con crías de pterodáctilo, fueron atacados por una bandada de ejemplares adultos  salvándose por poco. Según la descripción del novelista, del nido emanaba “un horrible, mefítico y rancio hedor que nos daba náuseas”, al mismo tiempo que los feroces machos, “parados cada uno en su propia roca, absolutamente inmóviles salvo por el rodar de sus ojos rojos”, más parecían “ejemplares muertos y disecados que seres llenos de vida”. En seguida, los pterodáctilos empezaron a volar en círculos cada vez más cerrados sobre los exploradores terminando por atacarlos. Pero fue hacia el final de la novela que Conan Doyle concede a los pterodáctilos una actuación estelar. Esta se desarrolló en el Instituto de Zoología de Londres, cuando, ante una audiencia incrédula de sus hallazgos en la selva amazónica, Challenger libera de manera teatral a una cría de pterodáctilo que habían logrado llevar hasta ahí. Con tan mala suerte, sin embargo, que la cría logra escapar por una ventana del recinto perdiéndose para siempre.Los pterodáctilos reales, sin embargo, no tenían el enorme tamaño que les atribuía Conan Doyle. Por el contrario, según se lee en la Wikipedia, no pasaban de medir un metro y medio con las alas abiertas. No es el caso, por otro lado, de los azdárquidos, parientes de los pterodáctilos y también reptiles voladores. Esto es particularmente cierto del llamado Hatzegopteryx, cuyos restos fósiles fueron originalmente descubiertos en 1991. Según el paleontólogo Mark Witton de la Universidad de Portsmouth en el Reino Unido, estos animales llegaban a tener una envergadura de diez a doce metros –lo que es equivalente a la envergadura  de una avioneta Cessna– y un peso de 220 kilogramos.De acuerdo con Witton, el Hatzegopteryx vivió hacia el final del periodo cretácico –que se empezó hace 145 millones de años y terminó hace 66 millones de años– en la isla de Hateg, en lo que hoy es territorio de Rumania. Durante el cretácico la isla de Hateg estaba separada del resto de Europa por aguas profundas y en estas condiciones la fauna en la misma evolucionó de manera aislada. Esto llevó al desarrollo de especies enanas tal como ha sucedido en otros lugares con condiciones similares. Al mismo tiempo, la falta de depredadores habría permitido al Hatzegopteryx evolucionar hasta proporciones gigantescas.  En un articulo aparecido la semana que hoy termina en la revista “PeerJ”, Mark Witton y Darrel Naish de la Universidad de Southampton en el Reino Unido, conjeturan que, a diferencia de otros azdárquidos gigantes, de 4-5 metros de altura y cuello cercano a los tres metros de largo pero relativamente delgado, los Hatzegopteryx habrían tenido un cuello de sólo un metro y medio de largo pero mucho más musculoso. Esto les habría permitido ejercer una fuerza con las mandíbulas considerablemente mayor que la que habrían podido ejercer sus parientes más esbeltos. Así, los investigadores concluyen que los Hatzegopteryx estaban notablemente bien dotados para la caza mayor y, de hecho, habrían sido los mayores depredadores de la isla de Hateg. Sin duda tendría que haber sido aterrador el que nos atacara un reptil volador de varios metros de alto, más de 200 kilogramos de peso, una enorme mandíbula en forma de pico y alas extendidas por más de diez metros. Lo habría sido tanto como encontrarse de frente con un T. Rex.  Visto de esta manera, posiblemente estemos siendo injustos con los Hatzegopteryx, a los que quizá debamos colocar junto a los tiranosaurios como los animales más terroríficos que han existido sobre la faz de nuestro planeta y darles así la atención que merecen. Y para ser consecuentes con lo anterior, también debemos dar a Conan Doyle el crédito correspondiente. Aunque, pensándolo bien, quizá debamos esperar un poco a que haya más descubrimientos fósiles del Hatzegopteryx, pues para concluir que éste era un depredador así de formidable, Naish y Witton se apoyaron solamente en una sola vertebra del cuello del animal, la única que ha sido descubierta hasta ahora.",
    "Posiblemente los pterodáctilos no sean tan famosos o llamen tanto la atención como los tiranosaurios o los brontosaurios, que se cuentan entre los favoritos de los niños, pero sin duda no dejan de tener su encanto. Así, tienen, por ejemplo, una aparición destacada en la novela “El Mundo Perdido” de Arthur Conan Doyle –creador de Sherlock Holmes– publicada en 1912. Aparecen también, aunque quizá con un papel menos destacado, en la película homónima del cine mudo, basada en dicha novela y estrenada en el año de 1925. En su novela, Conan Doyle describe una expedición encabezada por un profesor Challenger a una meseta perdida en la selva amazónica, en la que habían supuestamente sobrevivido dinosaurios y otros animales extinguidos hace ya mucho tiempo. Esto incluía a pterodáctilos con alas de seis metros de extensión. En un pasaje de dicha novela, cuando los exploradores se acercaron a un nido con crías de pterodáctilo, fueron atacados por una bandada de ejemplares adultos  salvándose por poco. Según la descripción del novelista, del nido emanaba “un horrible, mefítico y rancio hedor que nos daba náuseas”, al mismo tiempo que los feroces machos, “parados cada uno en su propia roca, absolutamente inmóviles salvo por el rodar de sus ojos rojos”, más parecían “ejemplares muertos y disecados que seres llenos de vida”. En seguida, los pterodáctilos empezaron a volar en círculos cada vez más cerrados sobre los exploradores terminando por atacarlos. Pero fue hacia el final de la novela que Conan Doyle concede a los pterodáctilos una actuación estelar. Esta se desarrolló en el Instituto de Zoología de Londres, cuando, ante una audiencia incrédula de sus hallazgos en la selva amazónica, Challenger libera de manera teatral a una cría de pterodáctilo que habían logrado llevar hasta ahí. Con tan mala suerte, sin embargo, que la cría logra escapar por una ventana del recinto perdiéndose para siempre.Los pterodáctilos reales, sin embargo, no tenían el enorme tamaño que les atribuía Conan Doyle. Por el contrario, según se lee en la Wikipedia, no pasaban de medir un metro y medio con las alas abiertas. No es el caso, por otro lado, de los azdárquidos, parientes de los pterodáctilos y también reptiles voladores. Esto es particularmente cierto del llamado Hatzegopteryx, cuyos restos fósiles fueron originalmente descubiertos en 1991. Según el paleontólogo Mark Witton de la Universidad de Portsmouth en el Reino Unido, estos animales llegaban a tener una envergadura de diez a doce metros –lo que es equivalente a la envergadura  de una avioneta Cessna– y un peso de 220 kilogramos.De acuerdo con Witton, el Hatzegopteryx vivió hacia el final del periodo cretácico –que se empezó hace 145 millones de años y terminó hace 66 millones de años– en la isla de Hateg, en lo que hoy es territorio de Rumania. Durante el cretácico la isla de Hateg estaba separada del resto de Europa por aguas profundas y en estas condiciones la fauna en la misma evolucionó de manera aislada. Esto llevó al desarrollo de especies enanas tal como ha sucedido en otros lugares con condiciones similares. Al mismo tiempo, la falta de depredadores habría permitido al Hatzegopteryx evolucionar hasta proporciones gigantescas.  En un articulo aparecido la semana que hoy termina en la revista “PeerJ”, Mark Witton y Darrel Naish de la Universidad de Southampton en el Reino Unido, conjeturan que, a diferencia de otros azdárquidos gigantes, de 4-5 metros de altura y cuello cercano a los tres metros de largo pero relativamente delgado, los Hatzegopteryx habrían tenido un cuello de sólo un metro y medio de largo pero mucho más musculoso. Esto les habría permitido ejercer una fuerza con las mandíbulas considerablemente mayor que la que habrían podido ejercer sus parientes más esbeltos. Así, los investigadores concluyen que los Hatzegopteryx estaban notablemente bien dotados para la caza mayor y, de hecho, habrían sido los mayores depredadores de la isla de Hateg. Sin duda tendría que haber sido aterrador el que nos atacara un reptil volador de varios metros de alto, más de 200 kilogramos de peso, una enorme mandíbula en forma de pico y alas extendidas por más de diez metros. Lo habría sido tanto como encontrarse de frente con un T. Rex.  Visto de esta manera, posiblemente estemos siendo injustos con los Hatzegopteryx, a los que quizá debamos colocar junto a los tiranosaurios como los animales más terroríficos que han existido sobre la faz de nuestro planeta y darles así la atención que merecen. Y para ser consecuentes con lo anterior, también debemos dar a Conan Doyle el crédito correspondiente. Aunque, pensándolo bien, quizá debamos esperar un poco a que haya más descubrimientos fósiles del Hatzegopteryx, pues para concluir que éste era un depredador así de formidable, Naish y Witton se apoyaron solamente en una sola vertebra del cuello del animal, la única que ha sido descubierta hasta ahora.",
    "Cuando Moctezuma Xocoyotzin envió a un grupo de mensajeros cargados con numerosos presentes para dar la bienvenida a los desconocidos que arribaron por el mar a las costas de Veracruz, lo hizo respondiendo a un mal diagnóstico de la situación. Este mal diagnóstico fue resultado de la concepción mágica del mundo que tenían los mexicas. En efecto, basado en una serie de presagios funestos, derivados de acontecimientos ocurridos antes de la llegada de los españoles, Moctezuma llegó a la conclusión de que los recién llegados eran dioses. De manera específica, creía que estaba ante el esperado regreso de Quetzalcoatl; creencia que sabemos era completamente equivocada. El diagnóstico erróneo del problema no ayudó a que Moctezuma respondiera al reto que enfrentaba de la manera más efectiva posible y todas las medidas que llevó a cabo para detener el avance de los españoles hacia Tenochtitlán resultaron infructuosas. En particular, los presentes que les hizo llegar para darles la bienvenida, lejos de complacerlos aumentaron su codicia y su deseo de avanzar hacia el interior del territorio. Sirva lo anterior como una introducción y ejemplo de la importancia de tener un buen diagnóstico en el momento de intentar resolver un problema; o, poniéndolo en otros términos, de la importancia de entender con precisión una situación para intentar predecir su evolución futura e influir en la misma. Y para esto es necesario despojarnos de concepciones mágicas del mundo –como las que contribuyeron a la caída de Moctezuma y del Imperio Azteca–, sustituyéndolas por una concepción científica del mismo.Por otro lado, aun renunciando a adivinos, nigromantes y chamanes,  la predicción del futuro no está garantizada y el grado en el que la podamos lograr depende de la complejidad que enfrentemos. Así, es más fácil predecir la trayectoria de una bala disparada con una cierta dirección y velocidad, que predecir el tipo de cambio que tendrá nuestra moneda al final de 2017. Poniéndolo en términos de disciplinas científicas: es más fácil predecir acontecimientos en el área de la física que en el de la economía y en general de las ciencias sociales. Tenemos así que, mientras que la física es una ciencia que tiene un notable poder de predicción, y que por lo mismo ha permitido desde aterrizar suavemente sondas en la superficie de Marte hasta construir computadoras, las ciencias sociales son más modestas en este sentido en virtud de que sus sujetos de estudio son considerablemente más complejos que los de la física. Entre los expertos, sin embargo, hay el convencimiento de que en un futuro no muy lejano, empleando nuevas técnicas de análisis y la información acumulada en Internet, será posible revertir esta situación.   Con relación a lo anterior, en una sección especial del número de esta semana de la revista “Science” se publican una serie de artículos en los que se discute la ciencia de la predicción y la posibilidad actual y futura de predecir la ocurrencia de acontecimientos sociales de la más diversa índole. Entre éstos se incluyen el resultado de elecciones, el desarrollo de conflictos políticos violentos, y la aparición de descubrimientos científicos trascendentes. En particular, un artículo escrito por investigadores de la Universidades de Pensilvania y Harvard en los Estados Unidos encabezados por Phillip Tetlock, discute el uso de la sabiduría colectiva para predecir eventos futuros. Esto se hace por medio de torneos en los que los participantes hacen predicciones sobre la futura ocurrencia de eventos en una amplia variedad de tópicos económicos y geopolíticos. Los participantes deben dar respuestas numéricas a las preguntas formuladas, indicando su estimación de la probabilidad de que un evento específico ocurra en un cierto lapso de tiempo. Entre otros resultados, Tetlock y colaboradores encuentran que hay participantes que obtienen consistentemente las mayores calificaciones. Dichos participantes, además, tienen perfiles definidos, incluyendo una mente más abierta para considerar a sus opiniones personales como “una hipótesis a poner a prueba y no una posesión sagrada”. Tienden también a pensar en el ejercicio de predicción como una habilidad que puede ser cultivada. Los mejores predictores, además, tienden a contar con grados profesionales avanzados y un conocimiento general de política. Si bien la ciencia de la predicción tiene por delante todavía mucho campo por avanzar, ciertamente tenemos hoy en día mejores herramientas para analizar situaciones de índole social que las que en su momento tuvo Moctezuma para evaluar el predicamento en el que lo puso la llegada de los conquistadores españoles. Aunque, por lo demás, un mayor entendimiento de su situación probablemente a la larga de nada le hubiera servido por la superioridad militar de los invasores.",
    "Si de muros divisorios se trata, el ejemplo más notable es quizá el de la Gran Muralla China. Fue durante la dinastía Ming, que gobernó a China entre los años 1368 y 1644, cuando fue construida la Gran Muralla tal como la conocemos ahora. Se sabe, sin embargo, que ésta fue antecedida por otras murallas de diferentes extensiones, que en algunos casos se remontan hasta el siglo III antes de nuestra era. Las versiones que antecedieron a la Gran Muralla fueron construidas con tierra apisonada u otros materiales relativamente poco duraderos; esto, en contraste con la Gran Muralla de la dinastía Ming, en la que se emplearon piedra y ladrillo.   Aunque el propósito de la Gran Muralla fue la defensa del imperio ante el embate de las tribus nómadas del norte, China fue invadida y conquistada por los manchúes en el siglo XVII –anteriormente, en el siglo XIII, lo había sido por los mongoles– los cuales establecieron una dinastía que prevaleció hasta el siglo XX. Así, la Gran Muralla, si bien a la vista es una construcción sólida, impresionante y vistosa –mucho más vistosa que lo que podría llegar a ser el muro “impenetrable, físico, alto, poderoso y bello”, que se planea construir en la frontera norte de nuestro país–, como defensa para el imperio chino fue poco efectiva.Según los historiadores, además, la Gran Muralla no solo no resistió a los embates militares de los bárbaros del norte, sino que, aun en tiempos menos violentos, lejos de ser impenetrable fue considerablemente porosa. Así, tal parece que los soldados de las guarniciones que custodiaban la frontera norte de China –los cuales estaban en un aislamiento considerable y tenían incluso que buscar por ellos mismos lo indispensable para sobrevivir– mantenían contacto y hacían tratos amistosos con los bárbaros, sus supuestos enemigos, en el otro lado de la muralla.   Por otro lado, lo que es poroso en algunos casos no lo es tanto en otros. Así, la Gran Muralla, al establecer una barrera física entre dos áreas geográficas, dificultó el flujo genético entre poblaciones de plantas con hábitats aislados por dicha barrera. Esto, al menos de acuerdo con un artículo publicado en el año 2003 en la revista “Heredity” por un grupo de investigadores de la Universidad de Pekin en China, encabezado por H. Gu. Gu y colaboradores estudiaron especies de plantas encontradas en las inmediaciones de la Gran Muralla en su tramo de Juyongguan, a unos 60-70 kilómetros al norte de Beijing, y encontraron que dicha muralla afectó la evolución de plantas viviendo en su entorno. Para su estudio, Gu y colaboradores recolectaron, en dos sitios cerca de la muralla y a ambos lados de la misma, seis especies de plantas. En los puntos de recolección la muralla tiene una altura de 6 metros y un ancho de 5.8 metros en promedio. Como muestras de control, recolectaron también cinco especies de plantas en ambos lados de un camino en un sitio alejado de la muralla. Al final de su investigación, Gu y colaboradores encontraron una diferencia significativa entre  plantas de la misma especie viviendo en uno y otro lados de la muralla, diferencia que fue mayor que aquella encontrada entre las muestras de control.Adicionalmente, el estudio arrojó que las diferencias genéticas son mayores para plantas cuya polinización ocurre a través de insectos, que para aquellas que son polinizadas por el viento. Así, la altura de la muralla sería un obstáculo más severo para los insectos, que tendrían que volar por arriba de la misma, que para los fuertes vientos que soplan en Juyongguan. Y para animales que no vuelan, por supuesto, una barrera física de seis metros o más  de altura es todavía más severo y en esto se asientan muchas de las críticas que se han hecho al muro que se construiría a todo lo largo de la frontera entre México y los Estados Unidos. En particular, se alerta sobre la pérdida de diversidad genética en especies animales por la interrupción del flujo de genes a través de la frontera, tal como ocurrió a las plantas divididas por la Gran Muralla en Juyongguan. Un obstáculo físico, además, restringe el flujo de animales y puede impedir el acceso a fuentes de agua y alimento, lo mismo que a sus rutas de migración. De acuerdo con el Servicio de Pesca y Fauna Silvestre de los Estados Unidos, la construcción del muro fronterizo –que tendría una altura entre 10 y 20 metros– impactaría negativamente a 111 especies en peligro.Ciertamente, las especies animales tienen pocos recursos para defenderse de una agresión a su hábitat como lo sería la construcción del muro de marras –o de manera más precisa, su terminación, pues ya está parcialmente construido–. No es necesariamente el caso de nosotros, los humanos, que de una manera u otra posiblemente nos las arreglaríamos para convertir en poroso un muro supuestamente impenetrable. Y en caso de duda, favor de consultarlo con los chinos.",
    "En la página web de la nueva administración federal estadounidense, bajo el encabezado “Un Plan Energético para América Primero”, se asienta que el entrante presidente norteamericano está comprometido a “eliminar políticas dañinas e innecesarias, tales como el Plan de Acción Climática y del Agua de los EUA”. Este plan establece políticas para atajar el calentamiento global con fuentes de energía menos contaminantes del medio ambiente y llevar al país hacia un futuro energético sustentable. En contraste, en el documento de la nueva administración estadounidense se privilegia el uso de los combustibles fósiles, aprovechando las vastas reservas de gas y petróleo de esquisto existentes en el territorio norteamericano, y no se hace mención alguna a energías limpias tales como la solar o la eólica.No ha sido sorprendente el cambio de la política energética norteamericana dado que se sabía de antemano que el nuevo presidente no es un creyente en la existencia de una relación de causa efecto entre el calentamiento global y el uso de combustibles fósiles. Esto, sin embargo, es lo que acepta una mayoría abrumadora de científicos del clima, quienes sostienen que la evidencias que apoyan a esta relación son muy sólidas. En efecto, si bien el incremento de la concentración en la atmósfera de dióxido de carbono –el principal causante del calentamiento global– muestra oscilaciones estacionales, el promedio anual de dicho incremento es paulatino y cada año establece una nueva marca. Así, según el observatorio de Mauna Loa en Hawaii, operado por la “National Oceanic and Atmospheric Adminstration” de los Estados Unidos, la concentración de dióxido de carbono en la atmosfera alcanzó el pasado mes de diciembre un valor promedio de aproximadamente 405 partes por millón, lo que representa un incremento de casi 1%  con respecto al nivel medido un año antes. De manera concurrente, según la misma fuente, 2016 ha sido el año más cálido desde que se llevan estadísticas. Y aun con esta evidencia de trasfondo, la nueva administración estadounidense no incluye de entrada un plan para el desarrollo de fuentes limpias de energía, las cuales tendrán necesariamente que prevalecer en el largo plazo si hemos de mitigar y superar el cambio climático. Hay que considerar, además, que las técnicas empleadas para la explotación del gas y el petróleo de esquisto –como el que existe en el subsuelo norteamericano– son motivo de muchas controversias.Con relación a lo anterior, recordemos que en la medida en la que en algunos lugares del mundo se han ido agotando los yacimientos de petróleo y gas, se ha tenido que recurrir a yacimientos con un cada vez mayor grado de dificultad para su explotación, y en este contexto se tuvieron que desarrollar técnicas sofisticadas de extracción. En particular, para la explotación de los yacimientos de esquisto se desarrolló la técnica de fracturación hidráulica, o “fracking”, como se le conoce en inglés. La fracturación hidráulica se aplica a yacimientos en los que el petróleo o el gas natural están alojados en rocas de baja porosidad, lo que dificulta o impide su fluir hacia la superficie. En estas condiciones, para la explotación de los mismos se hace una perforación vertical hasta alcanzar el yacimiento y a partir se prolonga con una perforación horizontal. Seguidamente, se inyecta agua a presión, en combinación con algunos agentes químicos y arena, con el objeto de fracturar la roca del yacimiento y de esa manera liberar el gas o el petróleo atrapado en el mismo. Si bien la técnica de fracturación hidráulica tiene virtudes pues permite explotar yacimientos que son inaccesibles con técnicas convencionales, es también objeto de mucha controversia por la posible contaminación de los mantos acuíferos –que están a menos profundidad que los yacimientos de esquisto– por los fluidos inyectados al pozo que eventualmente regresan a la superficie. De manera adicional, durante la explotación de un yacimiento de esquisto se producen fugas de gas metano –principal componente del gas natural– hacia la atmósfera y se sabe que el metano es un gas de invernadero muchas veces más potente que el dióxido de carbono. Así, el uso del gas y del petróleo de esquisto tiene que ser evaluado a la luz de las posibles fugas de metano hacia la atmósfera.El consumo descontrolado de energía que hemos llevado a cabo a lo largo de los últimos dos siglos nos ha llevado a un conflicto profundo con el planeta. Para superarlo, debemos llevar a cabo un proceso de sustitución de combustibles fósiles por fuentes limpias de energía, proceso que será lento por necesidad.  Entre más nos tardemos en desarrollar estas fuentes, sin embargo, nuestro conflicto con el planeta más tardará en superarse. Vale quizá la pena terminar este artículo con una cita del ex presidente Obama, con la que abre el documento “Climate Action Plan”: “Es nuestra convicción que nuestras obligaciones como americanos no son solamente con nosotros sino con la posteridad. Responderemos a la amenaza del cambio climático sabiendo que si fallamos traicionaremos a nuestros hijos y a las generaciones futuras. Algunos todavía pueden negar el avasallador juicio de la ciencia, pero nadie puede evitar el impacto devastador de un fuego embravecido, de una sequía devastadora o de una poderosa tormenta”.",
    "Un artículo publicado por la BBC hace un par de años afirma que “Por miles de años los humanos han tomado el placer masoquista de añadir chile a la comida”. En México, en donde ponemos chile hasta en los dulces, no consideraríamos que es masoquista condimentar con chile los alimentos. Habría que reconocer, no obstante, que el artículo de referencia en alguna medida tiene razón, pues el gusto por el picante varía grandemente de persona a persona, y una comida muy picosa que resulte agradable para algunos, pudiera ser intolerable para otros.Como quiera que sea, hay de picores a picores y, ciertamente, hay niveles de capsaicina –el compuesto químico que da el característico sabor al chile– que nadie puede tolerar. Tanto que este compuesto es el ingrediente activo de los aerosoles empleados para dispersar multitudes, el cual irrita los ojos y puede causar ceguera temporal.El chile, que se asume es originario de México, ha estado entre nosotros por muchos miles de años. Una evidencia de su presencia entre los aztecas nos lo da el Códice Mendoza, una de cuyas láminas nos muestra a un padre castigando a su hijo de 11 años, el cual se habría portado mal, sosteniéndolo boca abajo encima de una fogata con chiles ardiendo. El efecto de los vapores de chile en los ojos del niño es evidente en el dibujo. Fueron los conquistadores españoles los que llevaron el chile a Europa y de ahí habría llegado a Asia, en donde se incorporó a las diferentes cocinas regionales. Con mucho entusiasmo, además, tanto que en Asia el uso del chile como condimento puede ser incluso mas generoso que en México. Así, no es sorprendente que sean China y la India, los dos países más poblados del planeta, los principales productores de chile en el mundo. Es tan grande y competitiva en precio la producción china, que una gran proporción del chile que se vende en México, es importado precisamente de China. Tiene así el chile, tanto un lado positivo, si se usa con moderación, como negativo, por sus propiedades tóxicas. Y las buenas nuevas es que tal parece que por el lado positivo las ventajas van más allá de su uso como condimento para dar sabor a la comida. Al menos esto es lo que se afirma en un artículo publicado esta semana en la  revista en línea Plos One por Mustafa Chopan y Benjamin Littenberg de la Universidad de Vermont en los Estados Unidos. Estos investigadores llevaron a cabo un estudio a lo largo de 23 años con más de 16,000 voluntarios en ese país y encontraron que el consumo cotidiano de chile disminuyó en un 13% la probabilidad de morir por cualquier causa.El hallazgo de Chopan y Littenberg está de acuerdo con un artículo publicado en 2015 en la revista British Medical Journal por un grupo de investigadores de China y de los Estados Unidos, encabezado por Jun Lv de la Universidad de Pekín. Dicho artículo reportó los resultados de un estudio llevado a cabo con más de medio millón de personas de 10 regiones de China, reclutadas entre los años 2004-2008. El objetivo de dicho estudio fue determinar el efecto que el consumo de comida condimentada con chile tiene en la esperanza de vida. Al igual que en la investigación de Chopan y Littenberg, Jun Lv y colaboradores encontraron que aquellos que consumieron chile casi todos los días de la semana tuvieron al término del estudio un 14% menos de probabilidades de morir por cualquier causa, en comparación con aquellos que declararon consumir chile menos de un día a la semana en promedio. La misma relación inversa entre consumo de chile y mortalidad se observa si se consideran causas particulares de muerte, tales como cáncer, enfermedades del corazón o enfermedades respiratorias, y es más fuerte entre aquellos que no consumen alcohol que entre aquellos que los hacen. Los resultados son, además, válidos tanto para hombres como para mujeresA partir de su origen en México, el chile se expandió, primero al continente americano y después a todo el planeta a raíz de la conquista española del Nuevo Mundo. Además de su uso como conservador de alimentos y como condimento para mejorar el sabor de los mismos –o para enmascararlo en el caso de que se hubieran descompuesto–, al chile se le atribuían efectos medicinales. Esto último está de acuerdo con los hallazgos de Chopan y Littenberg, y Jun Lv y colaboradores.Así, tal parece que al chile podemos aplicar aquello de que poco veneno no mata. Y por el contrario, puede incluso ser saludable.",
    "Una ojeada al mapamundi nos muestra que, si bien no de manera exclusiva, los países con altos ingresos per cápita tienden a estar situados en regiones frías del planeta. Es el caso, por ejemplo, de países del norte de Europa como Noruega, Suecia y Alemania. En el otro extremo, los países más pobres resultan estar localizados, de manera sistemática, en el continente africano, en donde las temperaturas promedio son sustancialmente más altas que aquellas del norte de Europa. ¿Hay una relación de causa-efecto entre el clima y el desarrollo económico y social? Esta relación hipotética, que corresponde a lo que se conoce como determinismo climático o determinismo geográfico, ha tenido defensores a lo largo de la historia, algunos de buena fe y otros no necesariamente desinteresados. Así, a manera de ejemplo, el determinismo geográfico fue muy útil a finales del siglo XIX para justificar la colonización por parte de los europeos de aquellas partes del planeta habitadas por razas y civilizaciones que consideraban inferiores por su situación geográfica desfavorable.Las tesis sobre la influencia del clima en el desarrollo de las civilizaciones han sobrevivido hasta nuestros días y son del interés de algunos especialistas en la materia que emplean métodos de investigación rigurosos. Esto, en contraste con las opiniones prejuiciadas de los europeos colonialistas de hace un siglo.Con relación a lo anterior, un artículo publicado en la revista “Nature” en octubre de 2015 por Marshall Burke, de la Universidad de Stanford, y Solomon Hsiang, y Edward Miguel de la Universidad de California en Berkeley, concluye que la temperatura ambiental es un factor que está directamente relacionado con la productividad económica de un país. El clima puede así considerarse como un “capital natural”, análogo al capital físico y al capital humano. Los investigadores llegaron a esta conclusión al estudiar el desempeño económico de 166 países entre los años 1960-2010 y correlacionarlo con las variaciones de temperatura ambiental que cada país experimentó de manera azarosa a lo largo del tiempo de estudio. No se hicieron comparaciones entre países, sino que cada país se comparó consigo mismo a intervalos de dos años. De este modo, se determinó la productividad económica de los 166 países estudiados a lo largo de cada periodo de dos años, en función de la temperatura ambiental promedio a la que estuvieron expuestos en esos periodos.Burke y colaboradores encontraron que dicha productividad económica es máxima para una temperatura ambiente de 13 grados centígrados. Por debajo de esta temperatura la productividad económica disminuye y lo mismo sucede, de manera acelerada, por arriba de la misma. ¿Por qué la productividad económica depende de la temperatura ambiente? Según Burke y colaboradores, algunos factores que afectan la productividad económica son dependientes de esta temperatura. Entre estos encuentra la productividad agrícola y la productividad de los trabajadores que puede disminuir a más altas temperaturas. De manera adicional, las altas temperaturas promueven enfermedades y conductas violentas que pueden afectar la productividad, según los investigadores.Todo lo anterior adquiere una mayor relevancia en las condiciones de cambio climático y calentamiento global por la que atraviesa el planeta. De acuerdo con Burke y colaboradores, este cambio afectará de manera diferenciada a los países del mundo. Aquellos que tienen en la actualidad temperaturas ambientales promedio por debajo de los 13 grados centígrados verán aumentada su productividad económica en la medida en que avance el calentamiento global. En contraste, los países cuya temperatura promedio está ya por arriba de los 13 grados centígrados verán en el futuro su productividad disminuida. Los investigadores estiman que, dadas las tendencias actuales del calentamiento global, la productividad económica del mundo disminuirá en un 23% en el año 2100, con respecto a la productividad que se tendría de no haber un cambio climático.De estar Burke y colaboradores en lo cierto, y habida cuenta que los países ricos tienden a tener en la actualidad menores temperaturas ambientales promedio que los países pobres, el cambio climático llevará a una mayor desigualdad entre el mundo desarrollado y el subdesarrollado. Así, mientras que Noruega tendría un incremento en su producto interno bruto (PIB) per cápita de 250% en el año 2100, Nigeria vería una disminución del mismo en un 90% en ese mismo año. México estará del lado de los países perdedores y su PIB per cápita se reduciría  en un 73%.   Así, el calentamiento global,  que por global pensaríamos fuera democrático y afectara a todos los países por igual, no lo sería tanto y se ensañaría con los países pobres que son los que menos culpa tienen del cambio climático. Es decir, y como es frecuente, al perro más flaco se le cargarían las pulgas.",
    "Así como el siglo XX fue el siglo de la lámpara incandescente, el siglo XXI lo será seguramente de la lámpara LED. En efecto, a pesar de que el costo de las lámparas LED es todavía relativamente alto, las mismas son cada vez más populares y están gradualmente reemplazando a los focos incandescentes y a las lámparas fluorescentes como fuentes de luz. Dadas las múltiples ventajas de los LEDs, se espera que este reemplazo sea prácticamente total en las próximas décadas. En efecto, un reporte dado a conocer el pasado mes de septiembre por el Departamento de Energía de los Estados Unidos pronostica que para el año 2035 el 99% de las luminarias en las carreteras y lugares públicos de ese país consistirán de fuentes LED. En cuanto a las aplicaciones residenciales, si bien en esta área la penetración de los LEDs será relativamente lenta, el reporte de referencia pronostica que en el año 2035 el 90% de las fuentes de luz de los hogares estadounidenses serán lámparas LED. Las lámparas incandescentes son dispositivos particularmente ineficientes, que convierten en energía luminosa apenas el 5% de toda la energía que consumen, transformando el 95% restante en calor. Si consideramos que en la generación de energía en una central termoeléctrica y en su posterior distribución se pierden dos tercios del total de la energía contenida en el combustible –gas, carbón o petróleo– empleado por la central, concluimos que generar luz por medio de una lámpara incandescente es un proceso en extremo ineficiente, que aprovecha solo el 1-2% de la energía consumida en primera instancia.En la primera mitad del siglo XX, cuando no se habían todavía declarado ni la crisis energética ni el cambio climático, las ineficientes lámparas incandescentes no eran motivo de mayor preocupación. Esto cambió a partir de la crisis del petróleo en la década de los años setenta y, sobre todo, en las últimas décadas a raíz de que se hizo evidente que nuestro planeta atraviesa por un proceso de calentamiento global por el uso y abuso de los combustibles fósiles. En particular, las nuevas condiciones de crisis generaron iniciativas para mejorar la eficiencia de los dispositivos de iluminación, que emplean el 15% de toda la energía eléctrica generada en los Estados Unidos. Un desarrollo en esta dirección son las lámparas fluorescentes compactas –de uso extendido en la actualidad– que son varias veces más eficientes que las lámparas incandescentes. Al contener mercurio en su interior, sin embargo, las lámparas fluorescentes son potencialmente contaminantes del medio ambiente y su desecho al final de su vida útil se debe hacer con las debidas precauciones.De manera afortunada, los LEDs tienen ventajas tanto sobre los focos incandescentes como sobre las lámparas fluorescentes. En efecto, por un lado, son de 6 a 10 veces más eficientes que las fuentes incandescentes. Por otro lado, en comparación con las lámparas fluorescentes, son más eficientes y no presentan el problema de contaminación con mercurio. Aunado a lo anterior, los LEDs son compactos, resistentes y tienen una vida útil más larga. Con todo y sus virtudes, sin embargo, los LEDs no están exentos de críticas. A manera de ejemplo, según informaciones aparecidas en los medios de comunicación el pasado 29 de diciembre, un programa para sustituir 270,000 lámparas de sodio por lámparas LEDs en las calles de la ciudad de Chicago ha sido motivo de discusiones por parte de aquellos que consideran que la brillantez de las lámparas LED resulta visualmente molesta y demandan que éstas sean equipadas con pantallas para evitar deslumbramientos. Hay, igualmente, preocupación por la tonalidad de la luz emitida por los LEDs. Con  relación a esto último hay que notar que los LEDs que emiten luz blanca lo hacen mezclando colores y que la luz resultante tiene una apariencia que depende de la proporción de luz azul que entra en dicha mezcla. Así, si esta proporción es alta se obtendrá una luz “fría” y en caso contrario una luz “cálida”. Por otro lado, hay evidencias científicas que asocian un alto contenido de luz azul en la iluminación artificial con algunos problemas de salud, incluyendo perturbaciones del sueño y es en este contexto que lo críticos de las lámparas LED a instalar en Chicago demandan que éstas tengan una baja proporción de luz azul. Tal parece, sin embargo, que las virtudes de las lámparas LED compensan con creces a sus defectos y no es sorprendente que los expertos consideren que los LEDs serán las fuentes de luz que nos iluminarán en las décadas por venir. Sin descartar, por supuesto, su sustitución por fuentes de luz tecnológicamente más sofisticadas, con eficiencias luminosas todavía más altas. Eficiencias, ciertamente, que estarían cada vez más alejadas que aquellas propias de las lámparas incandescentes, que florecieron en una época de abundantes combustibles fósiles y poca preocupación por el medio ambiente.",
    "El pasado 14 de diciembre la casa Christie´sde Nueva York subastó en 3.7 millones de dólares una copia de la primera edición del libro “Principia Mathematica” de Isaac Newton publicado en el año 1687. Con esto,“Principia Mathematica” se convirtió en la obra científica impresa que ha alcanzado un mayor precio de venta. Fue Edmund Halley –el descubridor del cometa homónimo– quien animó a Newton a recopilar en un libro sus ideas acerca de las causas del movimiento. Halley incluso pagó los costos de publicación del libro de Newton, dado que la “Royal Society” –que se había comprometido a llevarla a cabo– se quedó sin fondos después de pagar por la publicación del libro “Historia de los peces” de John Ray y Francis Willughby. Ellibro de Ray y Willughby, que contiene numerosas y detalladas imágenes de peces y por lo mismo tuvo altos costos de publicación, no se vendió bien y resultó en un fracaso económico para la Royal Society. Así, esta sociedad científica –la primera de la historia– se privó de publicar el que se considera es eltexto científico más influyente de todos los tiempos.En su obra “Principia Mathematica”, Newton primeramente enuncia sus famosas tres leyes del movimiento y después las usa para explicar el movimiento de los planetas. Su teoría ha resultado tan exitosa que aun hoy, más de trescientos años después de que fue enunciada, se enseña en cursos de física, tanto elementales en la escuela preparatoria, como avanzados en la universidad. Otra medida del éxito de las ideas de Newton sobre las causas del movimiento nos lo da el hecho de que las mismas se usan para calcular los impulsos y las trayectorias necesarias para enviar naves al espacio, incluyendo las naves con destino a otros planetas en el sistema Solar. Lograr insertar, por ejemplo, un satélite en una órbita alrededor de Saturno no es una tarea sencillay, de hecho, sería imposible sin las ideas de Isaac Newton. Aparte de formularuna teoría acerca del movimiento de los cuerpos materiales empleando matemáticas desarrolladas por él mismo y usarla para explicar las trayectorias de los planetas –con lo que dio origen a la disciplina de la física tal como la conocemos en la actualidad–, Isaac Newton modificó radicalmente nuestra concepción del Universo al postular la universalidad de su teoría, que se aplicaría tanto a objetos terrenales como a cuerpos celestiales. Así, según Newton, los objetos que vemos en el firmamento siguen las mismas leyes que los objetos en la Tierra y el comportamiento de los primeros puede en consecuencia ser entendido estudiando a estos últimos.La física,impulsada por Isaac Newtoncon sus “Principia Mathematica” sirvió de modelo para el desarrollo de otras ciencias que han producido tecnologías que han modificado radicalmente nuestro estilo de vida, particularmente en los últimos cien años. En este contexto,podríamos quizá entender que una copia de la primera edición de “Principia Mathematica” haya alcanzado en subasta un precio de 3.7 millones de dólares.En realidad, pensándolo dos veces, esta cantidad resulta demasiado baja si la comparamos con los precios que han alcanzado entre los coleccionistas otros librosque han estado lejos de tener la trascendencia de los “Principia Mathematica”.Así, consultando la Wikipedia encontramos que el libro impreso que más alto costo de venta ha alcanzado es “BayPsalm Book”, publicado en 1640 en los Estados Unidos. Este libro, que fue el primero publicado en ese país, alcanzó en noviembre de 2013 un precio de subasta de  14.2 millones de dólares; esto, a pesar de contener numerosas fallas de impresión. En Wikipedia aprendemos también que la obra escrita que más alto precio de venta ha alcanzado es el “Códice Leicester”, que es una compilación de dibujos y textos manuscritos de Leonardo da Vinci. El Códice Leicester fue adquirido por Bill Gates en 1994 en 30.8 millones de dólares –49.3 millones en dólares ajustados por la inflación.La obra pictórica, por otro lado, puede alcanzar precios de venta todavía más altos. Así, cuadros pintados por Gaugin, Cézanne, Modigliani, Picasso y van Gogh, entre otros, se han vendido a precios que oscilan entre los 150 y los 300 millones de dólares actuales; es decir, se han vendido a precios 40-80 veces más altos que el que alcanzó la copia de “Principia Mathematica” subastada el pasado 14 de diciembre.El precio que un coleccionista está dispuesto a pagar por un obra producto del intelecto humano depende de factores subjetivos y de la rareza de la obra. Así, el cuadro “¿Cuándo te casas?” de Paul Gauguin se vendió en 2015 en 300 millones de dólares en comparación con los 3.7 millones de dólares alcanzado por la primera edición de “Principia Mathematica”, de la que se estima han sobrevivido unas 150 copias. Con seguridad, la relativa abundancia de copias de la edición original de la obra cumbre de Newton influyen en su “bajo” precio de subasta. Aun así, posiblemente dicho bajo precio es un indicadorde que no estamos apreciando en su justa dimensión el formidable impacto que la obra de Isaac Newton, y la ciencia en general, han tenido en nuestra vida.",
    "En su novela de 1898 “La guerra de los mundos”, el escritor británico H.G. Wells describe como los marcianos invasores de la Tierra fueron vencidos por un enemigo inesperado: los microbios terrestres para los que no estaban inmunizados. En su novela, por otro lado,Wells no consideró la posibilidad de que los marcianos hubieran llegado con sus propios microbios, para los que seguramente no hubiéramos tampoco tenido defensas. Esto hubiera resultado en un desastre y diezmado a la población de nuestro planeta.Si bien el escenario descrito por Wells es sólo ficción, no está lejos de lo que puede ocurrir en una situación real al mezclarse dos poblaciones que hayan evolucionado de manera separada. Así, tenemos que los pobladores indígenasdel Nuevo Mundo fuerondiezmados por las enfermedades infecciosas que llegaron con los conquistadores españoles y para las cuales notenían defensas.Aunque los expertos no se ponen de acuerdo en lamedida en que fue afectada la población indígena por lo agentes infecciosos del Viejo Mundo, sabemos que uno de éstos, presumiblemente la viruela, fue un factor para la caída de Tenochtitlan en manos de los españoles.En su libro “La visión de los vencidos”, Miguel León Portilla reproduce la descripción contenida en el Códice Florentino sobre la epidemia infecciosa que se desató enTenochtitlan previo al asedio por los españoles. Según el testimonio indígena: “….primero se difundió entre nosotros una gran peste, una enfermedad general… Sobre nosotros se extendió: gran destruidora de gente. Algunos bien los cubrió, por todas partes de su cuerpo se extendió. En la cara, en la cabeza, en el cuerpo”. Y continua el testimonio indígena: “Era muy destructora enfermedad. Muchas gentes murieron en ella. Ya nadie podía andar, no más estaban acostados tendidos en la cama. No podía nadie moverse, no podía volver el cuello, no podía hacer movimientos de cuerpo; no podía acostarse cara abajo, ni acostarse sobre la espalda, ni moverse de un lado a otro. Y cuando se movían algo, daban gritos. A muchos dio la muerte la pegajosa, dura enfermedad de granos”.La descripción de la enfermedad contenida en el Códice Florentino, que incluye undibujo que muestra a enfermos acostados con el cuerpo cubierto de granos, sugiere –y es comúnmente aceptado– que fue la viruela la causante de la epidemia que debilitó a los mexicas y contribuyó a su derrota ante los españoles. Y se asume, igualmente, que la viruela fue causante de epidemias que diezmaron a  la población de la Nueva España en el siglo XVI. Un artículo aparecido esta semana en la revista “CurrentBiology”, sin embargo, arroja dudas en este respecto. Dicho artículo fue publicado por un equipo internacional de investigadores encabezado por Ana Duggande la Universidad McMaster en Ontario, Canadá, y en el mismo se concluye que la viruela se originó en una época relativamente reciente, hace unos 400 años, lo que sugiere queeste virus no fueelcausante de la epidemia en Tenochtitlan. Duggan y colaboradores basan sus conclusiones en un estudio del ADN de la momia de un niño, con una edad entre los dos y los cuatro años, encontrado en la iglesia del Espíritu Santo en Vilna, Lituania. La momia fue fechadacon carbón radiactivo entre los años 1643-1665. Si bien el propósito original de la investigación no se centraba en el virus de la viruela –la momia no muestra indicios de las cicatrices características de esta enfermedad– para sorpresa de los especialistas, el análisis de ADN de la momia reveló la presencia de dicho virus. En estas circunstancias, los investigadores compararon el ADN del virus de la viruela encontrado en la momia con aquel de las cepas modernas y encontraron que están cercanamente relacionados. Una comparación de las diferencias observadaspermite encontrar su evolución en el tiempo y determinar en que momento tuvieron un antecesor común.Encontraronque esto ocurrió entre los años 1588 y 1645, lo que es consistente con las epidemias de esta enfermedad ocurridas en Europa a lo largo del siglo XVII.De estar Duggan y colaboradores en lo cierto, la epidemia que asoló a Tenochtitlan tendría que haber sido causada por un agente infeccioso diferente a la viruela y lo mismo se podría decir de otras epidemias aun más antiguas que se especula podrían haber sido debidas a este virus. La evidencia sólida más antigua de la presencia de la viruela, sin embargo, es precisamente la que se obtuvo con la momia de Vilna. Más allá de esto no hay certidumbre.De acuerdo con los especialistas, más estudios son necesarios para tener una mayor certeza de la historia evolutiva de la viruela, y como un primer paso hay que confirmar los resultados de Duggan y colaboradores. Esto es importante, pues si bien la viruela está ahora erradicada, entre más conozcamos acerca de ella más podremos defendernos en caso de que aparezca nuevamente en escena. En contraste, para los aztecas, que vivieron varios cientos de años antes de que pudiéramos tener un manejo medianamente efectivo de la enfermedades, poco importante habría sidosaber si lo que estaban sufriendo era un ataque de viruela o de algún otro virusde identidad desconocida.",
    "El presidente electo de los Estados Unidos dio a conocer en días pasados que había logrado convencer ala compañía United Technologies de no seguir adelante con sus planes de mover de Indiana a México una planta de fabricación de equipo de aire acondicionado. Si bien la cifra real de empleos involucrados es confusa,con esto se habrían salvado alrededor de 1,000 puestos de trabajo en los Estados Unidos –mismos que se habrían perdido en México–. Como bien sabemos, esto va de acuerdo con una de las banderas de campaña del presidente electo que prometió revertir el proceso de pérdida de empleos en los Estados Unidos por la migración de fabricas norteamericanas hacia nuestro país.  El tratado de libre comercio de América de Norteha provocado un éxodo de compañíasestadounidenses que buscan aprovechar los bajos sueldos que se pagan en México, circunstancia quenos ha convertido en la doceava economía exportadora en el mundo. Al mismo tiempo,sin embargo, nos ha colocado en una situación vulnerable con respecto a los Estados Unidos, como la actual situación nos lo hace patente; habida cuenta, además, de que más del setenta por ciento de nuestras exportaciones son hacia ese país. La inversión extranjera en México ha traído plantas manufactureras de alta tecnología, que ha llevado a que los productos mexicanosalcancen un alto grado de sofisticación. Una medida de la misma nos lo da el índice de complejidad económica (ICE)desarrollado por un grupo de investigadores del Instituto de Tecnología de Massachusetts y de la Universidad de Harvard, encabezados por Ricardo Hausmann y César Hidalgo. El ICE mide la sofisticación y diversidad de los productos manufacturados por un país en función de la cantidad de conocimientos necesarios para fabricarlos. De acuerdo con el ICE, México ocupó en el año 2014 el vigésimo segundo lugar de complejidad económica en el mundo, superando, entre otros, a Canadá, Brasil y Argentina en el continente americano.El ICE de nuestro país, además, ha crecido alrededor de un 30 por ciento las últimas dos décadas, según el Atlas de Complejidad Económica publicado por la Universidad de Harvard. Hausmann e Hidalgo encuentran quela posición en el ICE está directamente relacionado con el grado de desarrollo de un país y con el ingreso per cápita de sus habitantes. Así, encabezando la lista del ICE se encuentran, Japón, Alemania y Suiza, que cuentan con ingresos per cápita iguales o superiores a los 40,000 dólares por año. México, con un ingreso per cápita cercano a los 8,000 dólares anuales, resulta ser una excepción.Esto podría quizá ser un reflejo de que, por lo común, las tecnologías contenidas en los productos fabricados en México por compañías extranjeras nofueron desarrolladas aquísino en sus lugares de origen.  Los especialistas asumen, sin embargo, que el alto grado de complejidad económica de nuestro país constituye un caldo de cultivo para a un desarrollo económico acelerado. Para esto, en todo caso, tendremos que desarrollar capacidades para absorber las tecnologías que las inversiones extranjeras están trayendo a nuestro país, y al mismo tiempo desarrollar las capacidades necesarias para generar tecnologías propias.Como lo planteanHausmann e Hidalgo, el desarrollo de una mayor complejidad económica enfrenta el dilema del huevo y la gallina. Es decir, se avanza hacia un economía más compleja desarrollando productos en los cuales se han integrado conocimientos y capacidades provenientes de fuentes múltiples.En un determinado caso pudiera no contarse todas las capacidades necesarias para la fabricación de un cierto producto, y al mismo tiempo, al no existir una industria que lo fabrique, no se tendrían los incentivos para desarrollar las capacidades faltantes, sobre todo si estas son múltiples ydiversas. En estas condiciones, la estrategia sería la de crear productos nuevos a partir de las capacidades existentes,mediante el desarrollo de una sola y nueva capacidad complementaria. Una componente fundamental de la estrategia seríael impulso de nuestro sistema de educación universitaria –particularmente en el nivel de posgrado y en contraposición con una educación puramente técnica– en áreas afines a la planta productiva ya establecida. Nuestros centros de investigación y educación universitarios deben desarrollar yacumular conocimientos científicos y tecnológicos, lo mismo queeducar especialistas en áreas estratégicas. Y lo deben hacer sin descuidar el entrenamiento de los profesionales que demanda la planta productiva.Una mayor complejidad económicabasada en recursos tecnológicos propios –acorde con nuestra situación como doceavo exportador del mundo– sería sin duda de utilidad para mitigar situaciones de apuro como las que estamos pasando ahora, provocadas por nuestra excesiva dependencia del exterior.",
    "En la semana que hoy termina se tuvo la confirmación de que el fin de la sonda europea Beagle 2, enviada a explorar la superficie de Marte hace más de diez años, no fue tan violento como se llegó a pensar.De acuerdo a lo planeado, la sonda Beagle 2 se desprendió de la nave nodriza Mars Express –en órbita alrededor de Marte– el 19 de diciembre de 2003 con rumbo a la superficie marciana, sobre la que debía posarse de manera suaveseis días después. Una vezllegado a su destino, el Beagle 2  –que  tenía la forma de un reloj de bolsillo de 1 metro de diámetro y 33 kilogramos de peso–debía haberse abierto cual reloj de bolsillo y desplegado sus instrumentos científicos y su antena de comunicación, al igual que los paneles solares que le darían energía. Después de que se desprendió del Mars Express, sin embargo, nada más se supo del Beagle 2 que no se comunicó con el orbitadortal como se esperaba al arribar a la superficie marciana. En estas condiciones se temió que hubiera tenido un fin catastrófico al estrellarse a gran velocidad por una falla en sus sistemas de frenado. El misterio sobre la suerte del Beagle 2 empezó a aclararse cuando la NASA dio a conocer en enero de 2015 imágenes de alta resolución tomadas por el satéliteMarsReconnaissanceOrbiter –en órbita alrededor de Marte desde 2006– en  las que aparentemente se muestra al Beagle 2 en su lugar de aterrizaje. Si bien en las imágenesse ve a la sonda solamente como una mancha brillante en la que apenas se distinguen algunos detalles –incluyendo el perfil de lo que presumiblemente serían uno o dos de sus paneles solares–dichas imágenes dan una indicación de que el Beagle 2 habría sobrevivido al descenso.Todo esto fue confirmado y ampliado por un estudio dado a conocer esta semana por el departamento de prensa de la Universidad deLeicesteren Gran Bretaña. El estudio fue llevado a cabo por un grupode especialistas de las universidades deLeicestery De Montfort, que estudió las imágenes del Beagle 2 proporcionadas por la NASA empleando una técnica novedosa de análisis de imágenes. Sus conclusiones son que la sonda Beagle 2 arribó sin daño a la superficie de Marte e incluso pudo desplegar la mayor parte de sus paneles solares.El por qué la sonda no estableció comunicación una vez en la superficie de Marte no es claro, pero una posibilidad al respecto es que uno de los paneles solares haya impedido el correcto despliegue de la antena de comunicación.Por otro lado, la sonda europeaSchiaparelli, que intentó posarse en la superficie de Marte el pasado 19 de octubre, no tuvo la misma suerte que el Beagle 2 –si es que se le puede llamar suerte a lo experimentado por esteúltimo– y sufrió al aterrizar una colisiónseguida de una explosión que la destruyó por completo.Esto es evidente, según los especialistas, de imágenes proporcionadas por el MarsReconnaissanceOrbiter. La sonda Schiaparelli debía haber aterrizado en Marte frenando su caída,  primeramente por medio de un paracaídas y posteriormente, justo antes del aterrizaje, empleando cohetes retro-propulsores. Aparentemente, una falla en la programación de la computadora de control de la sonda habría hecho que los cohetes se encendieran solamente por 5 segundos en lugar de 30 segundos como se pretendía. Esto habría ocasionado que alcanzara la superficie de Marte a una velocidad devarios cientos de kilómetros por hora ocasionando su destrucción. Además, puesto que colisionó con la superficie con los tanques llenos de combustible, al choque habría sobrevenido una explosión. El fracaso de las sondas Beagle  y Schaparelli deja a los Estados Unidos como el único país que ha sido capaz de colocar de manera suave una sonda sobre la superficie marciana, y muestra que llegar hasta Marte no es una empresa sencilla. Y siesto es difícil para un ingenio mecánico, lo es en un grado mayor para un organismo vivo que, entre otros obstáculos, tiene que sortear el peligro de las radiaciones de alta energía provenientes del Sol y del espacio interestelar. En estas condiciones no parece factible que en las próximas décadas pudiéramos colonizar Marte, tal como se plantea en algunas instancias. Y en el mejor de los casos, sin aire para respirar, con variaciones extremas de temperatura entre el día y la noche, y con un continuo bombardeo de radiaciones de alta energía, nuestra presencia en Marteen el mediano plazo probablemente se limite a unas pocas estaciones científicas o de exploración.Así, para calmar nuestras ansias de viajeros espaciales debemos recurrirala ficción literaria,la cual nos permite imaginar viajes y colonizaciones espaciales al margen de los problemas técnicos. Un ejemplo notable es el libro Crónicas Marcianas del escritor norteamericano RayBradbury. En su libro, Bradbury relata las primeras expediciones humanas al planeta Marte y su subsecuente colonización. Si bien esta colonización no podría ocurrir tal cual Bradbury lo describe, Marte le dio al escritor el pretexto para escribir un libro fantástico que crea para el lector una atmósfera que no es de este mundo –y  que, como bono extra, al mismo tiempo refleja problemas que son propios de la civilización terrestre.     Por lo demás, con todo y los fracasos que se han dado en la exploración de Marte, no podemos sino admirar nuestra capacidad para colocar a control remoto una sonda en la superficie de un planeta que es apenas un punto luminoso en el cielo.",
    "El descubrimiento del noveno planeta del Sistema Solar, más allá de la órbita de Neptuno, fue anunciado por el director del Observatorio Lowell en Flagstaff, Arizona, el 13 de marzo de 1930. En una nota publicada el 21 de marzo de 1930, la revista “Science” daba cuenta del hecho.La existencia de un noveno planetahabía sido predicha por varios investigadores desde mediados del siglo XIX y el descubrimiento hecho público por el observatorio Lowell parecía confirmarla. Las predicciones de los astrónomos se basaban en las perturbaciones observadas en las órbitas de Urano y Neptuno –séptimo y octavo planetas del sistema solar, en forma respectiva– que indicaban la existencia de un cuerpo celeste más allá de la órbita de Neptuno. El hipotético noveno planetallegó a ser conocido a principios del siglo XX como el planeta X.Hay que recordar que el descubrimiento de Neptuno ocurrió después de que el matemático francés Urbain Le Verrier predijera su existencia a partir de las perturbaciones observadas en la órbita de Urano. Siguiendo las indicaciones de Le Verrier, el astrónomo alemán Johann Galledescubrió Neptuno sin gran esfuerzo y en el lugar preciso del firmamento que Le Verrier le había señalado.Dado el éxito de Le Verrier, no resultaba descabellado asumirque la existencia de un noveno planeta pudiera ser predichaa partir de la observación de las perturbaciones delas órbitas de Urano y Neptuno. Y con respecto a esto, el descubrimiento de Plutón lo habría confirmado.Con el tiempo, sin embargo, fue claro que el tamaño de Plutón –500 veces más pequeño que la Tierra– no es lo suficientemente grande para perturbar las órbitas de Urano y Neptuno.  Además, los astrónomos encontraron que dicha perturbación en realidad no existe y que estaba basada en una sobrestimación de la masa de Neptuno. De este modo, si bien el descubrimiento de Plutón fue motivado por la predicción matemática de la existencia de un noveno planeta, Plutón en realidad existe al margen de dicha predicción. Como quiera que haya sido, con Plutón el Sistema solar se hizo de un noveno planeta, el cual se sumó a los cinco conocidos desde la antigüedad: Mercurio, Venus, Marte, Júpiter y Saturno –todos visibles a simple vista–y la Tierra. Se sumó igualmente a Urano –descubierto en 1781 por William Herschel empleando un  telescopio– ya Neptuno. En el año 2006, sin embargo, la situación se descompuso pues Plutón fue degradado a la categoría de planeta enano y con esto el Sistema Solar volvió a quedarcon solo ocho planetas.En la actualidad hay visos de que la situación quepudiera dar un nuevo giro pues la búsqueda del noveno planeta ha vuelto a tomar fuerza, ahora en los confines del Sistema Solar, más allá de las órbitas de Neptuno y Plutón.Así, los astrónomos KonstantinBaygin y Michalel Brown del Instituto Tecnológico de California en un artículo publicado el pasado mes de enero en la revista “TheAstronomicalJournal”  proponen la existencia  de un noveno planeta en una órbita excéntrica quelo llevaría a una distancia mínima del Sol igual a unas cinco veces el radio de la órbita de Plutón –unas150 veces la distancia de la Tierra al Sol–. En su punto más alejado del Sol, el hipotético noveno planeta alcanzaría unas treinta veces la distancia de Plutón al Sol. En cuanto a su tamaño, el noveno planeta tendría una masa unas 10 veces la masa de la Tierra y tardaría de 10,000 a20,000 años en completar su órbita. Las evidencias de las que Byagin y Brown disponen para apoyar su hipótesis sobre la existencia del noveno planeta son similares a las que se esgrimieron en el pasado y se basan en la influencia que el hipotético planeta tendría en el movimiento de objetos astronómicos de la banda de Kuiper que se encuentra más allá de la órbita de Neptuno. Entre estos objetos se encuentra Sedna, que tiene un diámetro de unos 1,000 kilómetros  y viaja en una órbita muy excéntrica con un máximo acercamiento al Sol de dos veces el radio promedio de la órbita de Plutón. Se encuentran igualmente otros objetos de la misma clase, incluyendo el llamado 2012VP113,de unos 450 kilómetros de diámetro, descubierto en el año 2014. Byagin y Brown argumentan que las peculiaridades y coincidencias en las órbitas de estos objetos transneptunianos puede ser explicada por la influencia de un noveno planeta. Como es normal, no todos los astrónomos están de acuerdo con las conclusiones de Byagin y Brown. Así, hay quien argumenta que las observaciones se han hecho con un grupo demasiado pequeño de objetos transneptunianos, y que de llevarse a cabo con un grupo más amplio seperdería el sustento para la hipótesis del noveno planeta.No obstante, al margen de la controversia, hay varios grupos de astrónomos compitiendo por ser los primeros en avistar al noveno planeta. En los años por venir veremos si tienen éxito, al igual que Le Verrier y Galle. O bien,si la nueva historia del noveno planeta resulta un fiasco, tal como lo fue el planeta X.",
    "RayEmmet, el ficticio guitarristaque apareceen la película de Woody Allen  “Sweet and Lowdown” –“El gran amante” en México–, habría sido el segundo mejor del mundo, sólo superado por Django Reinhardt, el legendario guitarrista gitano de jazz de los años treinta y cuarenta del siglo pasado.En la ficción, Emmet reconocía la abrumadora superioridad de Reinhardt como guitarrista y lo admiraba enormemente. Tanto, que incluso se habría desmayado el par de veces que pudo verlo de lejos. Reinhardt sufrió severos daños en su mano izquierda durante un incendio, perdiendo en gran medida el uso de los dedos meñique y anular,lo que lo obligó a desarrollar una técnica para tocar la guitarra empleando solamente  los dedos índice y medio.  Y lo notable es que,a pesar de esta limitación, Reinhardtfue un guitarrista muy influyente que, según los conocedores,  creó un particular estilo de música de jazz.Así, no estabaEmmet falto de razones para admirar a Reinhardt.Por otro lado, además de su afición por la guitarra y su admiración por Reinhardt, Emmet tenía adiciones por demás extrañas. Una de éstas era simplementela de contemplar por largo rato el paso de los trenes. Otra era la de matar ratas pardas a balazos.Y con respecto a esta última afición, como bien sabemos víctimas no le habrían faltado. En efecto, las ratas pardas son huéspedes inevitables enlos centros urbanos. En la ciudad de Nueva York, por ejemplo, se ha llegado a considerar que habitan hasta unos 8 millones de rataspardas –diferentes de las ratas negras–, aunque los expertos consideran que este número es demasiado alto y estiman que en Nueva York viven sólo unos dos millones deratas pardas; esto, no obstante, sin contar aquellas que pudieran vivir bajo tierra en las cañerías. En cuanto al tamaño de las rataspardas neoyorkinas, MattCombs, estudiante doctoral de la FordhamUniversity en Nueva York, considera que las historias de ratas del tamaño de un gato son exageradas y que el peso de las más grandes posiblemente no sea mayor a un kilogramo. En cuanto a su propia experiencia, Combs–quien se especializa en estudiar las poblaciones de ratas pardas en la ciudad de Nueva York y que para tal fin las caza con perros y trampas– afirma que el peso de la más grande que ha logrado atrapar es de apenas unos 700 gramos.Dos millones de ratas son sin duda muchas ratas pero el númerose antoja pequeño para otros centros urbanos con peores condiciones sanitarias, por ejemplo la Ciudad de México. Nadie sabe en realidad cuantas ratas pardas tienen su residencia en la capital del país, pero el periódico El Universal en un artículo de hace algunos año citó al delegado de la Delegación Cuauhtémoc afirmando que en la Ciudad de México había en esos momentos alrededor de 45 millones de ratas pardas, sin que haya en realidad manera de comprobar la cifra. Ciertamente, si bien en mayor o menor número según el lugar, las ratas pardas están en todas partes y estamos tan acostumbrados a su compañía que resulta sorprendente enterarnos que hemos empezado a convivir con ellas  solo hasta fechas relativamente recientes: en Europa desde hace unos 500 años y por 200 años menos tiempo en el continente americano. Lo anterior se desprende de un estudio genético llevado a cabo por un equipo de investigadores de cuatro continentes con 314 ratas atrapadas en 30 países. El equipo de investigadores fue encabezado por Emily Puckett de FordhamUniversity y en el mismo estuvo incluido MattCombs. De acuerdo con Puckett y colaboradores, las ratas pardas se originaron en Mongolia y el norte de China y desde ahí se esparcieron por todo el mundo. Su movimientofue a través de asentamientos humanos, incluyendo aquellos a lo largo de la Ruta de la Seda. Los investigadores encuentran varias rutas de expansión. Una hacia el sudeste asiático, otra hacia el este en dirección del continente americano –posiblemente vía los traficantes de pieles– y  otra hacia Europa, a donde habrían arribado alrededor del año 1,500 de nuestra era. Una vez en Europa, la rata parda se habría expandido por el mundo viajando a bordo de losbarcos de las expediciones europeas de exploración y conquista. Una conclusión interesante de Puckett y colaboradores con relación al estudio  genético de la población de ratas pardas de la ciudad de Nueva York, es que dicha población no se mezcla con las ratas que siguen arribando a bordo de los barcos que visitan el puerto de esta ciudad.  La interpretación que los investigadores dan a este hecho es que las ratas pardas son muy territoriales y una vez que una población de estas ratas se asienta en un centro urbano no permite que otra población diferente de ratas pardas haga lo mismo.  De lo anterior podemos concluir que, aunque resulte sorprendente, no hemos sido desde siempre vecinos de las ratas pardas. Tal parece, sin embargo, que en la medida en que se desarrollaron los centros urbanos y se estableció el intercambio comercial entre los mismos, fue inevitable que las ratas pardas se convirtieran de manera forzada en nuestras vecinas incómodas. Tan incómodas que nos parece divertido queRayEmmettuviera por afición matarlas a balazos.",
    "El 13 de septiembre de 2007, a raíz de la  muerte de Alex, el loro famoso por su inteligencia, el periódico británico “TheGuardian” publicó: “América está de luto. Alex, el loro gris africano que era más inteligente que el promedio de los presidentes norteamericanos, ha muerto a la relativamente tierna edad de 31 años. Alex podía contar hasta seis, identificar colores, entender conceptos tales como más grande y más pequeño, y tenía un vocabulario de 150 palabras. Para sus seguidores, Alex era una prueba de que la frase “cerebro de pájaro” debe ser suprimida del diccionario”. Alex fue adquirido en 1977 en una tienda de mascotas por Irene Pepperberg, quien en esos momentos era estudiante doctoral en “Harvard University” y posteriormente profesora de sicología de “BrandeisUniversity” en Waltham, Massachusetts. Durante treinta años Pepperberg entrenó al lorocon los resultados notables mencionados en la nota de “TheGuardian”. Ciertamente, lo que podía hacer Alex era extraordinario y sin duda contribuyó a erosionar la imagenque se tenía de los animales como meros autómatas cuyo comportamiento es el resultado de instintos heredados y estímulos externos, lejos de la capacidad de raciocinio que sería exclusiva de nuestra especie. El caso de Alex no es, por otro lado, el único que ha hecho patente la inteligencia de ciertas especies animales. Hace dos semanas, por ejemplo, la revista “Science” publicóun artículo en el que se reportan experimentos llevados a cabo por un grupo internacional de investigadores con diferentes especies degrandes simios –bonobos, chimpancés y orangutanes– que indican que estosprimates tienen consciencia de sí mismos y son capaces de comprender lo que piensan otros individuos. Pueden incluso tener claro que otros están equivocados, de ser éste el caso.  Durante los experimentos, los primates participantes –uno a la vez– observaron a través de un monitor de televisión cómo un individuo con un disfraz de King Kong “atacaba” a uno de los investigadores para después esconderse bajo uno de dos montones de heno. El individuo atacado, que pudo ver el lugar en el que se ocultó su atacante, salió momentáneamente de escena y regresó con una vara para tomar desquite, dirigiéndose hacia el montón de heno en el que pudo ver se escondió su atacante. En una segunda versión de la representación, el atacante sale de su escondite y desaparece de escena mientras el investigador atacado está ausente y no se percata de ello. Al regresar a escena se dirige hacia la pila de heno en el que supone se encuentra su atacante.  Para evaluar la reacción de los primates, los investigadores grabaron el movimiento de sus ojos durante las representaciones y determinaron el lugar hacia donde dirigían la vista. Este lugar resultó ser en ambos casos la pila de heno en la que el atacado creía que se ocultaba su agresor. Los primates sabían así lo que pensaba el atacado. Y sabían también que en ocasiones sus creencias eran erróneas. Entre los humanos y las demás especies animales superiores no existiría entonces una diferencia  cualitativa sino sólo cuantitativa en cuanto a capacidades cognitivas. Dada la relación en términos de la evolución de la especies que pueblan la Tierra, por lo demás, esto no debería ser sorprendente.Un artículo publicado esta semana en la revista “Nature” por un grupo de arqueólogos de universidades en Gran Bretaña y Brasil encabezados por Tomos Proffitt de “Oxford University” nos da otra indicación –en una dirección diferente– de nuestra relación cercana  como especie con otras en la faz de la Tierra. Proffitt y colaboradores estudiaron el comportamiento delos monos capuchinos en el parque nacional de Sierra de Capivara en el nordeste brasileño. De manera específica, estudiaron la práctica que tienen los monos capuchinos de golpear dos piedras, una contra la otra, hasta fracturarlas.Si bien dicha práctica por sí misma podría no ser algo extraordinario, en el caso de los monos capuchinos ésta resulta en la generación de fragmentos de piedra muy afilados que son sorprendentemente similares a los encontrados en sitios arqueológicos pertenecientes a los primeros humanos con una antigüedad de dos millones de años. La fabricación de herramientas de piedra ha sido considerada como distintiva de la evolución de la inteligencia humana. Los hallazgos de Proffitt y colaboradores, sin embargo, demuestran que dichas herramientas pueden ser fabricadas empleando cerebros considerablemente más pequeños que el humano lo que, según los expertos, llevaría a reinterpretar el papel que la fabricación de herramientas tuvo en el desarrollo de nuestra especie.Hay que señalar, no obstante, que mientras que hay evidencias de que los primeros humanos dieron uso a sus herramientas de piedra, los monos capuchinos no parecen comprender su utilidad y una vez fabricadas las dejan de lado. En la medida en que se acumulan las evidencias científicas se fortalece la idea según la cual –al margen de que somos la especie dominante en la Tierra– nuestras diferencias con otras especies estarían más en lo cuantitativo que en lo cualitativo. Y no solamente con especies de las que nos separan millones de años de evolución, sino también con los loros grises africanos de los que hemos estado aislados por 300 millones de años.",
    "Como sabemos, por el Tratado de Tordesillas alcanzado en junio de 1494 por los representantes de los Reyes Católicos y el rey de Portugal, el mundo por descubrir quedaba dividido por una línea recta de polo a polo que pasaba a 370 leguas al oeste de las islas Cabo Verde. Con este arreglo, Portugal tendría derecho a las tierras que descubrieran por la ruta del cabo de Buena Esperanza, mientras que España no tendría la interferencia de Portugal en sus exploraciones de las recién descubiertas Antillas. De esta manera, España se hizo de un territorio inmenso para su explotación. En la actualidad, si bien ya no hay tierras por descubrir, existe un territorio inhabitado, específicamente el continente antártico, que tiene una extensión territorial siete veces la de nuestro país, el cual es reclamado por varias naciones. Entre éstas se encuentran Argentina y Chile que, por su relativa cercanía a la Antártida, podrían de algún modo justificar su reclamo. En otros casos, incluyendo a la Gran Bretaña, Francia y Noruega, situados a más de 15,000 kilómetros de distancia del continente antártico, dicho reclamo es más difícil de justificar. Como quiera que sea, la Antártida está dividida políticamente por líneas rectas que corren de norte a sur, de manera similar a la líneadel Tratado de Tordesillas,y quemarcan los territorios–en algunos casos traslapados– reclamados por los diferentes países. Notamos,por otro lado, que las rectas divisorias se juntan en el polo sur, ya que el mismo está situado en el centro del continente. Así, el mapa político de la Antártida simula un enorme pastel dividido en rebanadas triangulares; si bien no todas del mismo tamaño.En la actualidad la explotación de los recursos de la Antártidaestá prohibida por un tratado internacional yposiblemente así se mantenga. Esto, sin embargo,en la medida en quela explotación de este continente sea económicamente inviable por lo riguroso del clima –el 98% de su territorio está cubierto de hielo– y las dificultades para el transporte de los productos de dicha explotación. Por lo demás, la explotación de los recursos naturales se está moviendo hacia lugares aún más inhóspitos que la Antártida. En efecto, un tema que está tomado cada vez más fuerza es el de la explotación de los recursos minerales de los asteroides. Esto, quizá, no tanto para traerlos a la Tierra sinopara emplearlos en órbita en la construcción de, por ejemplo, estaciones solares para dotar de energía a nuestro planeta. En este respecto, sin embargo, han surgido preguntas. ¿A quién pertenecen los asteroides? ¿Es legal su explotación? Según el Tratado sobre el espacio ultraterrestrefirmado por 103 países:  “El espacio ultraterrestre, incluso la Luna y otros cuerpos celestes, no podrá ser objeto de apropiación nacional por reivindicación de soberanía uso u ocupación, ni de ninguna otra manera”. No habría de esta manera líneas de Tordesillas que pudieran establecer derechos soberanos sobre un asteroide o grupo de asteroides.Esto último, no obstante, ha sido puesto en jaque por la ley conocida como “SpaceAct” firmada por el presidente Obama el pasado mes de diciembre, la cual prevé que todo material encontrado en un asteroide por un ciudadano de los Estados Unidos o un empresa de ese país le pertenece. Todo esto dentro del impulso que el gobierno estadounidense está dando a la exploración privada del espacio.La “SpaceAct” ha sido considerada por algunos expertos como una ley que viola  el Tratado sobre el espacio ultraterrestre que considera que los recursos en el espacio son patrimonio de la humanidad. Si bien dicho tratado habla sobre “apropiación nacional” de los recursos ultraterrestres y no menciona una posible apropiación privada de los mismos, esto es así porque en la época en que fue redactado –los años sesenta– no se consideraba como posibilidad que una empresa privada pudiera enviar naves al espacio. Una interpretación de la ley en el sentido de que los particulares no están impedidos de poseer un pedazo del espacio –como sí lo están las naciones– no deja de ser forzada.Una evidencia ilustrativa al respecto nos la daen su página de Internet la compañía “DeepSpace Industries”, una de los dos empresas interesadas en la explotación de asteroides. Según esta compañía: “Bajo la nueva ley los ciudadanos estadounidenses tienen el derecho de retener los materiales extraídos de los asteroides y cuerpos celestiales pero no el derecho de poseer al asteroide o cuerpo celestial. De la misma manera que los pescadores tienen derecho a retener los peces que agarran pero no tienen derechos de propiedad sobre el mar”.  Como justificación para sus actividades espaciales “DeepSpace Industries” afirma que éstas impulsarán el crecimiento económico mundial y darán origen a tecnologías avanzadas, aun desconocidas, que beneficiarán a toda la humanidad.Las tecnologías que estarán involucradas en la explotación de los cuerpos celestiales serán, sin duda de una gran sofisticación. Mucho más grande que la de aquellas que usaron los españoles y portugueses para la conquista y explotación de los recursos del continente americano; o de las que se emplearían en la Antártida si se dieran las condiciones adecuadas. En contraste, las justificaciones para usar tecnologías de manera ventajosa no han cambiado tanto.",
    "Durante el Congreso Internacional de Astronáutica celebrado en Guadalajara hace un par de semanas, ElonMusk, fundador de la compañía SpaceX dedicada a la industria del transporte espacial, reveló detalles de su proyecto para realizar un viaje tripulado a Marte. La intención de Musk es que con este viajese inicie la colonización terrestre de dicho planeta, que plantea se lleve a cabo a lo largo del presente siglo.Durante su conferencia –que de acuerdo a las crónicas produjo una gran expectación y entusiasmo por parte de los asistentes–, Musk dio detalles técnicos de la misión que, sin embargo, no tuvieron la precisión necesaria.En particular, no habló Muskcon el suficiente detalle del modo en que los astronautas serían protegidos de las radiaciones de alta energía provenientes tanto del Sol,como de fuentes más allá del sistema solar, los llamados rayos cósmicos galácticos. Dichas radiaciones pueden ocasionar el desarrollo de cáncerese incluso la muerte por envenenamiento radiactivo. Aunque en principio la nave interplanetariapodría ser blindada dotándola de paredes lo suficientemente gruesas para bloquear los rayos cósmicos, el costo que esto implicaríasería prohibitivo con la tecnología actual. No parecería, entonces, que los planes de Musk sean practicables en lo inmediato. Al respecto, hay que recordarqueel espacio interplanetario y sus radiaciones son incompatibles con la vida.Hay, no obstante, que matizar esta última afirmación, pues se sabe de un caso en el que las radiaciones de alta energía, lejos de ser incompatibles con la vida, sonfuente de la misma. Este es el caso de una bacteria descubierta en 2008 en una mina de oro de Sudáfrica a 2800 metros de profundidad, la cual vive en total aislamientoy ausencia de luz solar. En estas condiciones la bacteria genera por si sola los alimentos que necesita para subsistir a partir del material inorgánico a su alrededor y empleando la energía proveniente de la desintegración radiactiva del uranio. Todo esto, en contraste con el común del reino animalque se alimenta, en último término, de los productos orgánicos que generan las plantas por medio de la fotosíntesis, empleando a la luz solar como fuente de energía. El caso de la bacteria de la mina sudafricana,que es, ciertamente fuera de lo común, inspiró a DimitraAtri, del “Blue MarbleSapaceInsitute of Science” de Seattle, Washington, para aventurar una hipótesis en el sentido de que existen formas de vida en lugares como el planeta Marte o el satélite Encelado de Saturno, cuya fuente de energía son los rayos cósmicos galácticos.Atri expone sus argumentos en un artículo publicado esta semana en la revista británica Journal of The Royal Society Interface”.Como lo discute Atri en su artículo, al penetrar en la atmósfera de un planetalos rayos cósmicos son frenados por las moléculas de la misma produciéndose una lluvia de partículas secundarias de naturaleza diversa. Si, por el contrario, el planeta no tiene una atmósfera los rayos cósmicos impactan directamente sobre su superficie. En este caso,algunas delas partículas generadas por la interacción de los rayos con los materiales bajo la superficiepueden penetrar varios kilómetros.Atri estudió tres casos: 1) la Tierra para la que casi toda la radiación es absorbida por la atmósfera antes de llegar a la superficie,2) Marte, que tiene una atmósfera considerablemente más tenue que la de la Tierra,y 3)un cuerpo sin atmósfera como el satélite Encelado. En el caso de Marte, la radiación que llega a su superficie es unas mil veces superior a la que se alcanza en la superficie de la Tierra y algo similar sucede con el cuerpo sin atmósfera. En comparación con el medio ambiente en el que habitan las bacterias de la mina en Sudáfrica, la energía que liberan los rayos cósmicos justo por debajo de la superficie de Marte y de Enceladoes unas diez veces más grande. Se tendría así un escenario en el que la radiación cósmica galáctica podría mantener vida en estratos por debajo de las superficie de Marte y Encelado, entre otros cuerpos rocosos. La profundidad para que esto ocurriera tendría que ser tal que la intensidad de los rayos fuera la adecuada para el desarrollo de la vida. Los organismos vivos estarían así sin el beneficio de la luz solar, pero al mismo tiempo protegidos de las radiaciones cósmicas letales. ¿Si esto es verdad por qué no se ha descubierto hasta ahora vida subterránea sostenida por rayos cósmicos en nuestro planeta? Atri razona que esto es así debido a que a la superficie de la Tierra arriba una intensidad de rayos cósmicos relativamente pequeña debido a la absorción de los mismos por la atmósfera. De hecho, esta intensidad es unas 100 veces más pequeña que la de las radiaciones nuclearesde las que disfrutan las bacterias sudafricanas.Asi,si Arti está en lo correcto, Marte sería un lugar hospitalario para la vida. Aunque, definitivamente, no para la clase de vida que Musk y otros empresarios quieren establecer allá.",
    "De acuerdo con el reporte de un grupo internacional de investigadores hecho público esta semana, el calentamiento global por la contaminación atmosférica ha llegado a un punto tal que amenaza con consecuencias de gran gravedad para el planetade no tomarse medidas para frenarlo. Medidas incluso más drásticas que aquellas comprometidas por 189 países en el acuerdo sobre cambio climático suscrito por los países participantes en la cumbre climática de Paris el pasado mes de diciembre. El grupo que elaboró el reporte referidoes encabezado por Robert Watson, quien presidió el Panel Intergubernamental sobre Cambio Climático (PICC) de la ONU de 1997 a 2002. Dicho reporte discute de manera concisa los hechos relativos al calentamiento global y las medidas que se tendrían que adoptar para mitigarlo. Así, tenemos que entre los años 2012 y 2015 la temperatura de la superficie de la Tierra se elevó 0.15 grados centígrados, alcanzando un valor que es 1 grado centígrado superior al nivel preindustrial. La meta del PICC es limitar el incremento en temperaturaa2 grados centígrados y preferentemente a 1.5 grados centígrados, por arriba dedicho nivel. Según el PICC, para lograr esto se requiere que para el año 2050 se alcance un balance entre el volumende gases de invernadero emitidosa la atmósfera y el volumen de dichos gases que es removido de la misma.El principal gas de invernadero es el dióxido de carbono,el cual es producto de la quema de combustibles fósiles y constituye el 65% del total emitido a la atmósfera. Actualmente, el 50% de este gas en la atmósfera es removido por los océanos, al igual que por los árboles y plantas mediante el proceso de fotosíntesis. Para lograr una emisión neta cerode dióxido de carbonose tendría entonces quereducir el volumen generado,y al mismo tiempoimplementar medios artificiales para removerlo de la atmósfera y almacenarloaislado del medio ambiente. Sin embargo, como lo discuten Watson y colaboradores, el límite fijado por el PICC de 1.5 grados centígrados por arriba del nivel preindustrial se alcanzaráprobablementeen la década de 2030. Esto, como resultado de los gases que se han ya emitidoa la atmósfera y dada la lenta respuesta del sistema océano-atmósfera al incremento en el volumen de gases de invernadero. Por lo demás, aun si se llevarán a cabo todas las acciones comprometidas por los países participantes en la cumbre climática de París, según Watson y colaboradores éstas resultarían insuficientes para alcanzar un equilibrio entre la emisión y la remoción de los gases de invernaderode la atmósfera. Como resultado, en la década de 2050 se alcanzaríaigualmente el límite de los 2 grados centígrados por  encima del nivel preindustrial fijado por el PICC.En estas condiciones, Watson y colaboradores consideran que para evitar una catástrofe ambiental se necesitarían compromisos y acciones más ambiciosas que las planteadas por los países asistentes en la cumbre de París. Puesto que estas acciones se realizarán a partir del año 2020, y ya que está programado que en 2018 se revisen los compromisos, habría tiempo para adecuarlos a las proyecciones climáticas.La principal fuente de la energía que mueve al mundo son los combustibles fósiles –petróleo, carbón y gas natural– que contribuyen con más del 80% del total de la energía generada a nivel global. Una reducción en la emisión de gases de invernadero pasa entonces por la sustitución gradual de los combustibles fósiles por otras fuentes de energía menos contaminantes. Entre éstas destacan las energías eólica y solar para la generación de energía eléctrica.Con respecto a esta última forma de energía, según la Organización para la Cooperación y el Desarrollo Económico, en 2014 el 67% del total generado a nivel mundialse obtuvo quemando combustibles fósiles, mientras que un 11% tuvo un origen nuclear. El 22% restante se originó en fuentes renovables y de estas la más importante por volumen fue la hidroeléctrica, que contribuyó con un 16% del total. El faltante 6% provino de diversas fuentes, notablemente de la energía del viento. Una fuente de energía renovable particularmente atractiva es el Sol, que potencialmente puedesatisfacer por completo nuestras necesidades energéticas. La energía solar puede ser aprovechada por medio de paneles solares que convierten la luz solar directamente en electricidad, lo que la hace en extremo atractiva. Si bien en la actualidad esta forma de energía juega todavía un papel menor, su uso está rápidamente creciendo por la rápida caída en los costos de los paneles solares. Después de todo, ¿qué hay más natural que hacer uso de una energía abundante y no contaminante que, además,  nos cae gratis del cielo?¿Cuál será el futuro climático del mundo?¿Se actualizarán lo compromisos de Paris para buscar mitigar los efectos del calentamiento global? Como lo hacen notar Watson y colaboradores, uno de los problemas que el mundo tiene para enfrentar con éxito el cambio climático es la concepción del mismo como algo abstracto y lejano que no veremos en nuestra generación. Por el contrario, hacen notar, el cambio climáticoes algo concreto que ya vivimos y sufrimos. Un signo de esto es el incremento en la frecuencia de eventos climáticos adversosque incluyen cambios en los patrones de lluvias que producen inundaciones o sequías, así como tormentas y ondas de calor más intensas.",
    "Sólo hasta después de la invención de la fotografía en la primera mitad del siglo XIX pudimos contar con imágenes las cuales,podemos estar razonablemente seguros, nos reflejan el pasado de manera fidedigna. Así, en la que se considera es la primera fotografía de una persona, podemos saber con certezacomo lucía en 1838 el “Boulevard du Temple” de París. La fotografía muestra a dicho boulevard  con árboles a ambos lados, un edificio de tres pisos en primer plano, y la silueta de una persona parada en una esquina, al parecer lustrándose los zapatos. Antes de la invención de la fotografía el retrato artístico de una persona podía ser alterado por deseos y factores subjetivos, ya sea por parte del artista o de la persona retratada. En contraste, una máquina fotográfica es por necesidad objetiva y refleja la naturaleza tal cual es –a menos que la fotografíasufra retoques que, como sabemos, son ahora muy fáciles de realizar. Sirva lo anterior como introducción al tema que hoy queremos tratar:los dinosaurios y el aspecto que éstos tenían en vida. Dado que las bestias que típicamenteidentificamos como dinosaurios se extinguieron hace unos 65 millones de años, no tenemos fotografías de los mismos ni tampoco retratos artísticosde modelos en vivo–así fueran con alteraciones subjetivas–. Sí, en cambio, tenemos numerosas recreaciones artísticas de estos animalesbasadas en los datos científicos disponibles al momento, si bien hechas con más o menos libertades artísticas. Dado de que sólo disponemos de restos fósiles de dinosaurios, determinar a partir de los mismos cuál era su aspecto en vida no es una tarea fácil. No obstante, los científicos poco a poco se las han arreglado para desentrañarsus secretos, en particular su aspecto y estilo de vida.Si bien,como algo inherente almétodo científico, la imagenque tenemos de los dinosaurios ha ido cambiando en la medida en la que se han descubierto nuevos fósiles y desarrollado maneras más sofisticadas para estudiarlos.Uno de los dinosaurios más famososes el  tiranosaurio rex, que de acuerdo con sus restos fósiles medía más de diez metros de largo, tenía un peso entre 5 y 10 toneladas, enormes mandíbulas con dientes de hasta 30 centímetros de largo hasta la raíz, y hábitos carnívoros. Más allá de las dimensiones de los tiranosaurios, sin embargo, hay incertidumbre sobre el aspecto que habrían tenido en vida. ¿Tenían, por ejemplo,la piel oscura, escamosa y dura tal como ocurre en la película “Parque jurásico”? Hoy en día los científicos no están seguros y algunos consideran que podrían incluso haber estado cubiertos con plumas, al menos en ciertas partes del cuerpo. Por lo demás, al margen de la incertidumbre sobre si los T. Rex tenían plumas o no, hay evidencias científicas sólidas de que otros dinosaurios sí las tenían. Se sabe, por ejemplo, que el velociraptor –una de las estrellas de Parque jurásico– estaba cubierto de plumas, lo que le dabaun aspecto muy diferente al de la bestia imaginada por Hollywood –además de que tenía un tamaño considerablemente más pequeño.Un artículo publicado la semana pasada en la revista “CurrentBiology” por un grupo internacional de investigadores encabezado por Jakob Vinther de la Universidad de Bristol en el Reino Unido, nos muestra a que grado de sofisticación ha llegado el estudio de los restos fósiles de dinosaurios. En dicho artículo, Vinther y colaboradores llevaron a cabo una investigación con restos de un dinosauriodel tamaño de una gacela y con una cabeza que recuerda a la de unacotorra, el cual vivió en el noreste de China hace más de 100 millones de años. Los investigadores estudiaron lo que suponen son los restos fosilizados de los pigmentos que daban color a la piel del dinosaurio. De manera específica, estaban interesados en determinar la distribución de dichos pigmentos sobre el cuerpo del dinosaurio con el objeto de determinar cuál era su hábitat. Con respecto a esto último, se sabe que los animales que viven en espacios abiertos tienen el lomo más oscuro que el vientre y esto los ayuda a ocultarse de sus depredadores. En efecto, la luz solar hace que los objetos se vean mas brillantes en la parte superior que en la inferior y esto da una impresión de volumen. El tener un animal una mayor coloración en el lomo que en el vientre le ayuda a que se pierda el contraste de brillantez y con esto la impresión de volumen,haciendo más difícil distinguir su presencia. Como resultado de sus investigaciones, Vinther y colaboradores encontraron que el paso de la parte oscura a la parte clara del dinosaurio estudiado no era tan abrupto como sucede en los animales que habitan en espacios abiertos. De aquí concluyeron que dicho dinosauriohabitaba enespacios cerrados rodeados de vegetación, en donde no recibía luz solar en forma directa sino de manera dispersa.Dado que las máquinas fotográficas no se inventaron sino hasta unos 65 millones de años después de que se extinguieran los dinosaurios, no es posible ciertamente contar con imágenes fidedignas de los mismos. En estas condiciones, para averiguar como habrían lucido en vida tenemos que recurrir a métodos indirectos. Específicamente, al estudio científico de sus restos fósiles. Y esto, por necesidad, es proceso lento que sin duda producirá errores que, sin embargo, serán corregidos con el tiempo.",
    "La conexión resulta quizá sorprendente, pero en el pasado hubo una relación estrecha entre algunas universidades norteamericanas que hoy gozan de gran prestigio,y la práctica de esclavitud en contra de los nativos africanoscomoexistía en los Estados Unidos en los siglos XVIII y XIX. En efecto, universidades como Princeton en Nueva Jersey y Columbia en Nueva York, fueron apoyadas financieramente en sus inicios por ricos mercaderes que habían hecho su fortuna mediante el tráfico de esclavos entre África y los Estados Unidos.Y, todavía más sorprendente,la relación de las universidades con las instituciones esclavistas no fue solamente pasiva –como hubiera sido siaquellashubieran actuado simplemente como receptoras de las donaciones de los traficantes de esclavos–sino que al menos en un caso dicha relación fue activa, recibiendola universidaddirectamente beneficios de la propiedade inclusodel tráfico de esclavos.Este casoes elde la Universidad de Georgetown, el cual ha estado presente en los medios de comunicación en los últimos días. La Universidad de Georgetown es una escuela jesuita fundada en 1789con sede en Washington, D.C. Es una de las universidades más prestigiosas de los Estados Unidos y entre sus egresados se encuentran el expresidente Bill Clinton, los actuales reyes de España y Jordania, así como ex presidentes de Panamá, Costa Rica y El Salvador.En 1838 la universidad pasaba por problemas financieros y para solucionarlos se decidió la venta de 272 esclavos africanosquepertenecían a una plantación que los jesuitas poseían en el estado de Maryland.Las ganancias de dicha plantación, se destinaban a la Universidad de Georgetown. En esos momentos, sin embargo, la plantación estaba mal administrada y las ganancias no eran suficientes. Los esclavos fueron vendidos a plantaciones sureñas en Luisiana.La operación de la venta de esclavos reportó un ingreso de unos tres millones de dólares y fue llevada a cabo por dos religiosos jesuitas, ex presidentes de la Universidad de Georgetown.Una lista de los esclavos vendidos se puede encontrar en la página de Internet “Georgetown Slavery archive” de la Universidad de Georgetown. En dicha lista de incluyen más de sesenta niños, uno de ellos de dos meses de nacido, y varios ancianos.   Podemos imaginar la conmoción que la venta produjo entre los afectados –y entre algunos de los jesuitas en la plantación que,entre otras cosas, estaban preocupados porque había la posibilidad de que en Luisiana los esclavos no pudieran oír misa–, dado que se sabía que las condiciones para los esclavos en las plantaciones sureñas eran brutales. Encontramos los detalles de dicha venta en un artículo publicado por el periódico “New York Times” el pasado mes de abril. Ahí podemos leer que hubo niños que fueron separados de sus padres y que los esclavos fueron subidos a la fuerza al barco que los transportaría hasta Luisiana. Hubo esclavos que huyeron, siendo, no obstante, algunos de ellosposteriormente capturados y obligados a abordar. Una anciana, de rodillas, suplicaba que le dijeran qué había hecho para merecer aquella suerte.La semana pasada, el presidente de la Universidad de Georgetown reconoció que ésta se encuentra en deuda con los descendientes de los 272 esclavos vendidos a las plantaciones sureñas y que para recompensarlos les daría las mismas facilidades para ingresar a dicha universidad que las que se dan a aquellos cuyos padres o abuelos fueron alumnos de la misma.La universidad, igualmente, cambiará el nombre de dos de sus edificios –que actualmentellevan los de los dos religiosos que orquestaron la venta de esclavos–; unopor el nombre de una de las víctimas de la transacción –el primero que apareció en la lista de embarque– y el otro por el de la fundadora en 1827 de una escuela para muchachas negras.Esta semana, sin embargo, los descendientes de los esclavos hicieron público que consideran que el ofrecimiento de la Universidad de Georgetown no es suficiente para reparar los hechos ocurridos hace 180 años, y piden el establecimiento de un fondo de reparación por la enorme cantidad de 1,000 millones de dólares, el cual, por otro lado, no es claro cómo se empleará. La Universidad de Georgetown está pues recibiendo un castigo merecido por sus pecados del pasado. Cabe la posibilidad, por otro lado, de que los 1,000 millones de dólares demandados sean excesivos y que algo más cercano a los 3 millones de dólares obtenidos con la venta de esclavos sea más realista. Por lo demás, y al margen del arreglo al que lleguen los deudos de las víctimas con la Universidad de Georgetown –y si bien es cierto que  la esclavitud se veía hace 180 años con ojos diferentes– es difícil entender que los religiosos jesuitas de la plantación de Maryland hayan logrado compaginar sus convicciones con la práctica de la esclavitud.",
    "Tal como fue informado por los medios de comunicación, el pasado jueves una explosión ocurrida en Cabo Cañaveral, Florida, destruyó un cohete de la empresa norteamericana “SpaceX” que se encontraba en su plataforma de lanzamiento. Con la  explosión, que sacudió los edificios cercanos y dejó nubes de humo en el cielo,se perdió a bordo del cohete un satélite de comunicaciones con un valor de 195 millones de dólares.“SpaceX” es una compañía privada fundada en 2002 que ofrece servicios comerciales de envío de carga al espacio. Entre otros clientes, “SpaceX” trabaja para la NASA con la quetiene firmado un contrato para el reaprovisionamiento de la Estación Espacial Internacional (EEI).Con relación a esto, hay que recordar que después de que la NASA descontinuarasu programa de transbordadores espaciales en el año 2011, la agencia se quedó sin capacidad propia para enviar carga al espacio. Depende así de otros para hacerlo.La compañía“SpaceX”proyecta realizar en el futuro inmediato también misiones tripuladas a la EEI bajo contrato con la NASA. En estas condiciones, el accidente del pasado jueves debe haber sido motivo de preocupación para dicha compañía, pues compromete fechas para el inicio de los vuelos. Así, con el fin de restar importancia al accidente –que ocurrió durante las maniobras de llenado de los tanques de combustible del cohete–“SpaceX” se refirió al mismo de manera eufemística como “anomalía en la plataforma de despegue”.  La del pasado jueves, por otro lado, no es el primer accidente que sufre “SpaceX”. En junio de 2015 un cohete de esta compañía explotó en el aire a los dos minutos del despegue. Dicho accidente, sin embargo, fue antecedido por seis lanzamientos exitosos.El fundador del “SpaceX” es ElonMusk, que también lo ha sido de otras compañías como PayPal, Solar City y Tesla Motors, esta última fabricante de automóviles eléctricos que pueden ser equipados para viajar de manera semiautónoma. Al respecto, recordemos que el pasado mes de mayo uno de los automóviles de Tesla Motors tuvo un accidente en una carretera de Florida en el que el conductor falleció al estrellarse el automóvil, que viajaba con el piloto automático activado, contra un camión de carga que le cerró el paso. El accidente se produjo cuando el sistema automático del vehículo no pudo distinguir la silueta del camión en contra del cielo claro yabrió interrogantes sobre la viabilidad de los automóviles sin conductor en el plazo inmediato.  Musk es impulsor de proyectos extravagantes como el hipotético medio de transporte “Hyperloop”, que consiste en un tubo de poco más de dos metros de diámetro en cuyo interior viaja, levitada por un colchón de aire,una cápsula con28 pasajeros. La cápsula se impulsa eléctricamente y para disminuir la fricción con el aire al tubo se le extrae aire hasta alcanzar un vacío moderado. En estas condiciones,según Musk,se podría viajar entre las ciudadescalifornianas de Los Ángeles y San Francisco en menos de 35 minutos,a una velocidad superior a los 1,100 kilómetros por hora. A través de “SpaceX”,Musk también impulsa un proyecto para realizar una misión tripulada a Marte en el año 2024, la cual se antoja imposible por los enormes obstáculostecnológicos que presenta y los riesgos que correrían los viajeros, incluyendo las radiaciones de alta energía provenientes del Sol y del espacio interestelar que no son aptas para la vida.Algunos proyectos de Musk resultan así claramente fantasiosos y posiblemente tengan más un propósito de relaciones públicas que intenciones serias de realizarlos. En contraste, en lo que respecta a los proyectos para llevar carga al espacio cercano, “SpaceX” ha tenido éxito y con su ofertaha logrado presionar a la bajalos costos para colocar cargas en órbita. Adicionalmente, con el objeto de todavía reducir más estos costos, “SpaceX” está llevando a cabo pruebas para reusar los cohetes, los cualestradicionalmente se han destruido en cada lanzamiento. Con este propósito,la compañía está realizando pruebas para lograr que dichos cohetes regresen a tierra de manera controladay se posen en el suelo suavemente.Esto resulta en extremo complicado, pues lejos de la manera como los transbordadores de la NASA aterrizaban planeando, los cohetes –al no contar con alas para planear– tienen que descender verticalmente haciendo equilibrio y ayudados por los motores.En videos que es posible encontrar en Internet podemos constatar lo complicado y espectacular que esto resulta cuando tiene éxito–e incluso cuando fracasa, aunque de manera diferente–.El primer descenso suave en tierra firme de un cohete de “SpaceX” ocurrió el pasado mes de diciembre en Florida. Posteriormente, en el mes del abril del presente año, “SpaceX” logró aterrizar un cohete sobre una plataforma marina. En ambos casos, la maniobra resultó ciertamente espectacular.Casi siete décadas han pasado desde el lanzamiento del primer satélite artificial por la entonces Unión Soviética. Desde entonces hemos sido testigos de acontecimientos espaciales memorables que incluyen el primer viaje espacial tripulado y el primer viaje tripulado a la Luna. Fuimos, igualmente, testigos de la aparición y posterior desaparición de los abrumadoramente costosos transbordadores espaciales. Y con todo esto, aun nos queda espacio para admirarnos de la increíble precisión con que se puede hacer que vuelva a tierra un cohete de cientos de toneladas de peso.",
    "Cada una de las versiones de los juegos olímpicos de verano ha tenido características extradeportivas por las que es recordada. Durante los juegos de Munich en 1972, por ejemplo, ocurrió el ataque terrorista palestino a la villa olímpica que resultó en la muerte de 11 miembros de la delegación israelí, incluyendoa cinco atletas. Podemos mencionar también que alos juegos olímpicos de Moscú en 1980 sólo acudieron 80 países por el boicot organizado por los Estados Unidos en contra de la Unión Soviética; y que cuatro años más tarde, en represalia, a los juegos olímpicos de Los Ángeles no acudió la Unión Soviética secundada por países del bloque comunista. Recientemente, la revista “SportsIllustrated” le hizo un favor a la olimpiada de México 68 al colocarla en la lista de las peores de la historia. La razón: la altura de la Ciudad de México de más de dos mil metros, con una atmósfera más tenue que a nivel del mar y por tanto con menos oxígeno. Esto,que afectó negativamente a las pruebas de resistencia,tuvo un efecto positivo en pruebas en las que la resistencia del aire es un factor importante. Quizá el ejemplo más notable es el del atleta norteamericano Bob Beamon, que en el estadio de Ciudad Universitaria estableció una nueva marca mundial de salto de longitud de 6.90 metros,la cual superó a la anterior por 55 centímetros y permaneció vigente por 22 años. El velocista JimHines, igualmente, estableció en la olimpiada de México una nueva marca mundial corriendo los cien metros planos en 9.95 segundos, marca que permaneció por 15 años.Los juegos olímpicos de Río de Janeiro, que este domingo llegan a su fin, no han sido la excepción en cuanto a sucesosextradeportivos que quedarán para la historia.En efecto, los juegos de Río de Janeiro han estado marcados por una serie de circunstancias y problemas de la más diversa índole; desde las dificultades económicas y políticas por los que está atravesando Brasil, hasta el controvertido escándalo de dopaje que, con una sola excepción, marginó de los juegos a los atletas rusos de pista y campo. Incluyendo a la garrochistaYelenaIsimbayeva, que ha sido dos veces campeona olímpica, tres veces campeona mundial, que ostenta desde 2004 el récord mundial de su prueba, y que nunca ha dado positivo en pruebas de dopaje.En los últimos días nos hemos enterado de otro escándaloextradeportivo que se dio en el marco de la olimpiada de Río y que involucró a cuatro miembros del equipo de natación de los Estados Unidos, todos medallistasde oro. El grupo de nadadores incluyó a RyanLochte que ha ganado un total de 12 medallas olímpicas en pruebas de natación para los Estados Unidos, sólo por detrás de Michael Phelps.El incidente de referencia, ampliamente difundido por los medios de comunicación, se dio la madrugada del pasado domingo cuando los atletas regresaban a la villa olímpica a bordo de un taxi a las seis de la mañana, después de una noche de juerga. Necesitados de un sanitario, le pidieron al chofer del taxi que se detuviera en una gasolinera. Ahí, los nadadores se orinaron en la calle y causaron destrozos en el establecimiento, al parecer instigados por Lochte. Esto los enfrentó con los encargados, uno de los cuales sacó una pistola para calmar a los vándalos. Lochte, no contento con su actuación, se inventó la historia que habían sido asaltados por falsos policías. Según el atleta, fueron obligados por los asaltantes a bajar del taxi y acostarse en el suelo. Al negarse, Lochtehabría sido encañonado en la cabeza.El nadador posiblemente consideró que en los Estados Unidos su historia sería creíble dada la imagen que los norteamericanos tienen de Brasil –quizá no del todo errada, pero sí muy probablemente exagerada–  como un país desordenado e inseguro.La historia de Lochte, sin embargo, pronto se vino abajo cuando la policía brasileña, después de una investigación, hizo público lo que realmente había sucedido. El enojo que provocó en Brasil es más que entendible y el jefe de la policía civil de Río de Janeiro declaró que los cariocas merecían una disculpa por parte de los nadadores por propalar hechos fantasiosos que mancharon la imagen de la ciudad. En los Estados Unidos hubo igualmente reacciones en la prensa quecondenaron la actuación de los nadadores. El diario “New York Times”, por ejemplo, cita a Brian Winter,editor de la revista “AmericasQuarterly”,  quien opina que la historia de Lochte alimenta entre los brasileños “una imagen de los norteamericanos como gringos que tratan a su país como un destino de tercera clase para “spring break”, en donde se puede mentir a la policía sin sufrir consecuencia alguna”.  Los juegos de Río pasarán así a la historia no solamente por el notable desempeño deportivo de algunos atletas, sino también por el desempeño de algunos otros en actividadesajenas a su especialidad deportiva que, ciertamente, demandan un menor esfuerzo de preparación.Por lo demás, la juerga que corrieron los nadadores norteamericanos está lejos de ser inusitada. A manera de ejemplo, según podemos leer en la Wikipedia, en su ingreso a la villa olímpica de Munich 1972 –que se llevó a cabo en horas de la madrugada– los terroristas palestinos habrían sido ayudados por deportistas estadounidenses que andaban de fiesta y que en ese momento regresaban a la villa. Los norteamericanos habrían pensado que, al igual que ellos, los terroristas  intentaban ingresar a  la villa de manera furtiva.",
    "En un comunicado emitido esta semana por el ministerio de cultura griego, se da a conocer que un grupo de arqueólogos griegos y norteamericanos descubrió un esqueleto de 3,000 años de antigüedad en Monte Lykaion en el suroeste de Grecia. Esto desató especulaciones en el sentido de queel esqueleto descubierto pudiera corresponder al de una víctima sacrificada al dios Zeus. Autores griegos de la antigüedad, incluyendo a Platón,señalaban a Monte Lykaion como un sitio en el que se practicaban sacrificios humanos. Hasta el hallazgo anunciado en días pasados, sin embargo, no se había encontrado una evidencia directa que asílo indicara.En este sentido, según el diario británico “TheGuardian”, David Gilman Romano de la Universidad de Arizona, uno de los autores del descubrimiento de Monte Lykaion, hace notar quesi bien no hay seguridad de que efectivamente se trate de un sacrificio humano,el lugar donde fueron encontrados los restos no es un cementerio sino un altar de sacrificios. Además, menciona que al esqueleto le falta la parte superior del cráneo y que el cuerpo fue colocado entre dos hileras de piedra alineadas en la dirección este-oeste,con la pelvis cubierta con placas de piedra. La posibilidad de que se trate de un sacrificio humano no podría entonces desecharse.JanBremmer de la Universidad de Groningen en Holanda, por su lado, no está convencido de que el esqueleto descubierto en Monte Lykaion corresponda a un sacrificio y menciona que la imagen de Grecia como la cuna de la filosofía, la democracia y el pensamiento racional en el mundo occidental no está de acuerdo con la práctica bárbara de los sacrificios humanos.Sin embargo, como sabemos el raciocinio no es suficiente para evitar atrocidades si hay ganancias de por medio. Y con respecto a esto, un grupo internacional de investigadores encabezados por Joseph Watts de la Universidad de Auckland en Nueva Zelanda, afirma que los sacrificios humanos tenían una función adicional a la de aplacar la ira de los dioses: promover y preservar la estratificación social. Los investigadores ofrecen sus argumentos y conclusiones en un artículo publicado el pasado mesde abril en la revista “Nature”.Watts y colaboradores llevaron a cabo un estudio de93 culturas tradicionalesque a partir de un origen común en Taiwan se distribuyeronen una amplia zona geográfica, desde la Polinesia hasta Madagascar y desde Hawái a Nueva Zelanda. Los pueblos investigadosdesarrollarona lo largo del tiempo una gran diversidad cultural y formaron desde sociedades igualitarias hasta sociedades altamente estratificadas. Así mismo, ocuparon muy diversos hábitats,desde pequeños atolones hasta territorios continentales. En las culturas estudiadas, además, la práctica de los sacrificios humanos era algo extendido.En este contexto,el propósito de la investigación fue el de determinar la relación que existió entre dicha práctica y la estratificación social.En su estudio, los investigadores determinaron la existencia o ausencia de sacrificios humanos en cada cultura y las clasificaron como igualitarias, si la riqueza y el status social no se heredaban entre generaciones, medianamente estratificadas si riqueza y posición social se heredaban pero tenían la posibilidad de cambiar dentro de una generación, y altamente estratificadas si riqueza y estatus social se heredaban y tenían nula o poca posibilidad de cambio dentro de una generación. Las 93 sociedades estudiadas quedaron divididas en 20 igualitarias, 46 moderadamente estratificadas y 27 altamente estratificadas.Con respecto a la práctica de los sacrificios humanos, Watts y colaboradores encontraron evidencia de que existía en 40 de las culturas estudiadas, lo que representa un 43% del total. Con respecto a la frecuencia de dicha práctica en relación a la estratificación social, encontraron que existía en 5 culturas de las 20 igualitarias (25%), en 17 de las 46 moderadamente estratificadas (37%), y en 18 de las 27 altamente estratificadas (67%).Además de lo anterior, Watts y colaboradores buscaron encontrar quien fue primero, si los sacrificios humanos o la estratificación social. A partir de la evolución de las sociedades estudiadas encontraron que los sacrificios humanos promueven la estratificación social y ayudan a mantenerla una vez que se ha desarrollado.En este contexto, la función de los sacrificios humanos fue la de mantener la desigualdad social por medio del terror que inspiraba la posibilidad de convertirse en una víctima más;habida cuenta que las víctimas de dichos sacrificios eran comúnmente personas con un rango social bajo. El sacrificio humano era un asesinato con justificaciones religiosas que pretendía influir en la voluntad de los dioses que se pensaba eran capaces de desencadenar las mayores desventuras sobre el mundo.  En la medida en que las causas de los fenómenos terrenales se concibieron más en el ámbito de lo natural que en lo sobrenatural, los sacrificios humanos perdieron su justificación y tendieron a desaparecer.O quizá sólo a transformarse,sustituyendo su justificación religiosa por otra más adecuada, según lo requieran las circunstancias.",
    "El documento “El estado del clima en 2015” publicado esta semana en el “Bulletin of the American MeteorologicalSociety”, y que contiene contribuciones de científicos de un gran número de países compiladas por la “NationalOceanic and AtmosphericAdminstration” de los Estados Unidos, confirma lo que diversas instancias adelantaron al iniciar este año: 2015 fue el año más cálido en la historia.En 2015 la temperatura promedio a nivel global superó en 1 grado centígradolos niveles preindustriales. Concurrentemente, la concentración promedio de dióxido de carbono en la atmósfera –el principal causante del calentamiento global–, medida en la estación de Mauna Loa, Hawái, rebasó por primera vez la barrera de las 400 partes por millón.Este valor hay que compararlo con las 280 partes por millón que imperaban antes de la revolución industrial.  Entre los efectos que, según los especialistas, produce el calentamiento global se incluye la fusión de las capas de hielo enGroenlandia, misma que está contribuyendo a incrementar el nivel de los océanos y que a mediano plazopodría inundar áreas bajas en todo el mundo. Un artículo aparecido esta semana en la revista “GeophysicalResearchLetters”apunta a otra potencial consecuencia–inesperada– de la fusión de la capa de hielo de Groenlandia. Dicho artículo fue publicado por un grupo internacional de investigadores encabezado por William Colgan de la “York University” en Canadá, y en el mismo se discute la potencial desaparición de la capa de hielo en Groenlandia que podría dejar al descubierto instalaciones militares construidas por los Estados Unidos en la década de los años cincuenta en el contexto de la Guerra Fría.En 1959 el “ArmyCorp of Engineers” del Ejército de los Estados Unidos construyó “Camp Century”,un complejo subterráneo en el noroeste de Groenlandia,a unos 200 kilómetros de la base aérea norteamericana de Thuley a 1,300 kilómetros del Polo Norte. La presencia norteamericana en Groenlandia siguió a la firma en 1951 de un acuerdo entre los Estados Unidos y Dinamarca para defender a Groenlandia –que era y sigue siendo territorio danés– de un posible ataque de la Unión Soviética. “Camp Century”, al que el ejército norteamericano describía como una “ciudad bajo el hielo”, comprendía 3,000 metros de túnelesperforados en el hielo a una profundidad de unos 8 metros, el mayor de los cuales tenía una longitud de 350 metros y un ancho y una altura de casi diez metros. La instalación tenía capacidad para alojar de manera permanente a 200 soldados. Además de habitaciones y laboratorios de investigación, contaba con letrinas, regaderas hospital con quirófano, dispensario médico, peluquería y unacapilla;así como con unteatro, biblioteca, tiendas y espacio de recreación, entre otras amenidades.Recibían energía de un reactor nuclear portátil y suministros desde la base aérea de Thule.De manera oficial, y así fue manejado ante el gobierno danés, el propósito de “Camp Century” era el de estudiar técnicas de construcción en el ártico, explorar los problemas prácticos asociados al uso de un reactor nuclear portátil, y realizarestudios científicos de la capa de hielo ártica. “Camp Century”, sin embargo, fue además utilizadopara explorar la posibilidad de construir un sitio en el árticopara el lanzamiento de misiles. Así, con absoluto secreto, en 1962se propuso el proyecto “Iceworm” paraconstruir una instalación bajo el hielo, con 4,000 kilómetros de túneles en un área de 130,000 kilómetros cuadrados, que desplegaría 600 misiles nucleares con capacidad de alcanzar a la Unión Soviética. El proyecto “Iceworm”, no obstante, nunca fue aprobado debido a que se encontró que  las construcciones de “Camp Century” eran inestables por el movimiento de la capa de hielo. Enesas condiciones, “Camp Century” fue abandonado sin más preocupación que la de remover del sitio el reactor nuclear. Todas las demás instalaciones del complejo, incluyendo 10,000 toneladas de materiales de construcción, 200,000 litros de combustible diesel, desechos radiactivos, ingentes cantidades de aguas residuales y contaminantes químicos en cantidades desconocidas, fueron dejados en el lugar asumiendo que serían sepultados para siempre por la acumulación de nieve.Lo que se daba por descontado hace cincuenta años, sin embargo, no puede garantizarse hoy en día y es posible que “Camp Century” termine por emerger a la superficie en la medida que el hielo desaparezca por el cambio climático. Esto cuando menos de acuerdo con Colgan y colaboradores quienes concluyen que hacia el final del presente siglo la acumulación de hielo dejará de compensar a las pérdidas y a partir  de ahí el espesor de la capa de hielo disminuirá de manera progresivay terminará por desaparecer, dejando expuestos todos los contaminantes acumulados en el sitio.¿Quién será responsable de remediar “Camp Century” una vez que emerja a la superficie, si es que alguna vez lo hace?SegúnColgan y colaboradores no es claro quien tendría que hacerlo en base al acuerdo firmado por los Estados Unidos y Dinamarca que es ambiguo al respecto. Lo que sí es claro, sin embargo, es que en su caso el perjudicado será Groenlandia –que nada hizo para merecerlo– y que busca cada vez una mayor independencia de Dinamarca.Ciertamente, no viviremos lo suficiente para atestiguar el desenlace del asunto.  Sí, en cambio, hemos vivido lo necesario para comprobar quecon “Camp Century”  la realidad superó  a la ficción.",
    "Las imágenes que nos han enviado desde Marte las diferentes sondas que la NASA ha colocado en la superficie de ese planeta lo muestran como un lugar desolado, seco y pedregoso, poco adecuado para cualquier forma de vida superior. En realidad, las condiciones para la vida en Marte son peores que las que podríamos intuir a partir de las imágenes de su superficie. Marte, por ejemplo, tiene una atmósfera muy tenue con una densidad que es apenas un centésimo de la de nuestro planeta y que está, además, compuestaen un 95% de dióxido de carbono. No habría de este modo oxígeno que respirar y tendríamos que vivir en espacios cerrados o enfundados en un traje especial. Por estar más alejado del sol que nuestro planeta, la temperatura promedio en la superficie de Marte es de menos 55 grados centígrados. Además, por lo tenue de la atmósfera marciana, las variaciones de temperatura entre el día y la nochepueden alcanzar los 100 grados centígrados en el ecuador del planeta. No es pues Marte un lugar atractivo para visitar. Excepto, por supuesto, para aquellos pocos con el suficiente espíritu de aventura.  Aun contando con el más fuerte de los espíritus aventureros, sin embargo, llegar hasta Marte y regresar vivo a la Tierra no resultaría una empresa sencilla. Hay muchas y serias razones para esto; una de ellastiene que ver con las radiaciones a las que estaría expuesto quien intentase viajar hasta allá –en un viaje que le tomaría algo así como nueve meses–. En efecto, el espacio interplanetario es un lugar muy peligroso, lleno de radiaciones de alta energía provenientes del Sol y de fuentesexternas al Sistema Solar. Dichas radiaciones producen alteraciones en el tejido orgánicoque provocan enfermedades entre las que se incluye elcáncer.Y no obstante todos los obstáculos previsibles, desde el inicio de la era espacial, en la década de los años cincuenta del siglo pasado, las propuestas y proyectos para viajar a Marte han sido recurrentes. La Wikipedia, por ejemplo, lista más de medio centenar de iniciativas en este sentido en los últimos sesenta años. Hoy en día, la NASA planea realizar una misión tripulada a Marte en la década de los años treinta del presente siglo y existen iniciativas en ese sentido por parte de organizaciones privadas para fechas aun más tempranas.Hasta ahora, los únicos astronautas que han viajado en condiciones de radiación como las que enfrentaría un astronauta en misión a Marte son aquellos que participaron en el proyecto Apollo de los años sesenta y setenta. Como recordamos, mediante este proyecto la NASA logró colocar al primer humano sobre la superficie de la Luna en julio de 1969. En total, 24 astronautas participaron en dicho proyecto, que por vez primera llevó viajeros fuera de la magnetosfera que protege a nuestro planeta de las radiaciones de alta energía provenientes de espacio. Durante su viaje a la Luna, los astronautas del Apollo perdieron esta protección y quedaron expuestos a dichas radiaciones.Con respecto a lo anterior, un artículo aparecido esta semana en la revista “ScientificReports”, un grupo de especialistas de centros de investigación en los Estados Unidos, encabezados por Michael Delp de la Universidad Estatal de Florida, sugiere que los astronautas del proyecto Apollo de los años sesenta y setenta fueron afectados a tal grado por las radiaciones de alta energía en el curso de sus misiones que a la larga les provocó la muerte. Delp y colaboradoresllevaron a cabo un estudio estadístico sobre las causas de muerte de los astronautas de la NASA que han fallecido hasta la fecha. Incluyeron a 7 astronautas del proyecto Apollo y a 35 astronautas que habían viajado sólo hasta órbitas bajas, relativamente cerca de la Tierra y dentro de la protección de su magnetosfera. Los investigadores incluyeron también a 35 astronautas que, si bien recibieron entrenamiento como tales, no viajaron nunca al espacio. El estudio encontró que de los astronautas del Apollo, el 43% murió por una enfermedad cardiovascular, mientras que el 29% falleció por cáncer. En lo que se refiere a los astronautas que viajaron sólo a órbitas bajas, los porcentajes respectivos son 11% y 31%, cifras que son muy similares para aquellos que nunca viajaron al espacio.Es decir, mientras que la probabilidad de fallecer por cáncer fue la misma entre todos los astronautas, sin importar que tanto se alejaran de la Tierra, los astronautas del Apollomurieron por enfermedades cardiovasculares con una frecuencia 4-5 veces mayor que el resto de los astronautas que nunca dejaron la magnetosfera.Un problema con los resultados de Delp y colaboradores es que fueron obtenidos con muestras muy pequeñas, particularmente en lo que se refiere a los astronautas del proyecto Apollo. Para apoyar sus estadísticas, sin embargo, los investigadores llevaron a cabo un estudio experimental con ratones a los que sometieron a condiciones simuladas de ausencia de gravedad y de radiación de alta energía, las cuales provocaron daños en las paredes de los vasos sanguíneos de los animales. Muchos planes hay para enviar una misión tripulada a Marte, algunos más serios y otros con plazos sorprendentemente cortos. Faltaría saber silas radiaciones de alta energía –lo mismo que otros obstáculos que se antojan igualmente formidables–no  disponen de otra cosa.",
    "De acuerdo con un reportaje de la televisión alemana de diciembre de 2014, el 99% de los atletas rusos de alto nivelempleansustancias prohibidas para aumentar su rendimiento.Además, según el testimonio de la atleta rusaYuliaStepanova, lo hacen como partede una política oficial para la preparación de atletas olímpicos.Un año después del reportaje de la televisión alemana, la Agencia Internacional Contra el Dopaje(WADA) recomendó que el equipo ruso de pista y campo fuera excluido de la olimpiada de Río de Janeiro a celebrarse el próximo mes de agosto.  De acuerdo con la WADA, entrenadores, funcionarios de la federación rusa de pista y campo, funcionarios de la agencia rusa anti-dopaje, y otros, organizaron esfuerzos para “promover el dopaje y hacer posible que tales esfuerzos alcanzaran éxito, incluyendo el ocultamiento de ciertos casos positivos de dopaje”. Quedando claro, además, que “el gobierno federal no sólo fue cómplice en la colusión sino que todo fue un régimen impulsado por el estado”.Para ganar una medalla olímpica con la ayuda de una sustancia prohibida que pueda ser detectada se necesita, por supuesto, la cooperación de la Asociación Internacional de Federaciones de Atletismo  (IAAF) la que, según la WADA, no hizo nada ante casos de dopaje ypermitió la participación en la olimpiada 2012 de atletas que debíanhaber sido descalificados en primera instancia. Además de que algunos altos funcionarios de la IAAF habrían recibidos sobornos para ocultar los casos de dopaje.  El pasado mes de junio, la IAAF tomó la decisión de excluir la participación de todo el equipo ruso de pista y campo de la olimpiada de Río alegando que “los atletas rusos no pueden regresar de manera convincente a las competencias internacionales sin socavar la confianza de sus competidores y del público”. Como respuesta, el presidente ruso afirmó que es injusto que sufran aquellos atletas que compiten sin el uso de drogas y que no acepta un castigo colectivo, que sería equivalente a meter a la cárcel a toda una familia por un crimen cometido por uno de sus miembros.Los atletas rusos, por otro lado, no son los únicos que recurren al dopaje para aumentar su rendimiento. Con relación a este punto, un reportaje de la televisión alemana y del periódico británico “Sunday Times”, hizo púbico en agosto de 2015 un estudio sobre una base de datos de sangre de atletas de pista y campo mantenida por la IAAF. El estudio encuentra que los datos de 146 atletas, queentre 2001 y 2012 ganaron campeonatos mundiales o medallas olímpicas –55  de oro– dan pie para sospechar que usaron sustancias para aumentar su rendimiento. Si bien 80 de estos 146 atletas son rusos, los datos indican que éstos no son los únicos que se dopan.  Por lo demás, el uso de sustancias prohibidas en las competencias deportivas no es un fenómeno nuevo. En la olimpiada de San Luis Misuri de 1904 –apenas la tercera de la era moderna– se registró un caso de dopaje  con estricnina, un estimulante inesperado que hoy en día más asociaríamos a las novelas de misterio que a los juegos olímpicos. El episodio ocurrió durante la carrera de maratón, que es además recordada por la cantidad de episodios extraños que ocurrieron durante la misma.La crónica de la carrera se puede encontrar en el sitio de Internet de la “SmithsonianInstitution”. Ahí nos enteramos que, si bien en el maratón de San Luis 1904 compitieron corredores con experiencia en la prueba, entre ellos el norteamericano Thomas Hicks, la mayor parte de los participantesno habían corridoun maratón con anterioridad. Uno de ellos, Fred Lorz, por su profesión de albañil tenía que entrenar de noche. Participaron, igualmente, dos miembros de la tribu Tsuana de Sudáfrica, quienes estaban ahí por razones diferentes a las del maratón y corrieron sin zapatos. Lo mismo que un cubano, Félix Carbajal, quien trabajaba como cartero en su país y que al llegar a los Estados Unidosperdió todo su dinero jugando a los dados y tuvo que trasladarse a pie y pidiendo aventones desde Nueva Orleans hasta San Luis. Una vez iniciado el maratón, Lorz tomó la delantera pero pronto fue superado porHicks.Faltando unos once kilómetros para finalizar la carrera Hicks se agotó y sus auxiliares le proporcionaron una dosis de estricnina y clara de huevo, misma que repitieron, añadiendo brandy, a pocos kilómetros de la meta.Lorz, por su lado, al llegar a los 16 kilómetros se cansó y decidió pedir un aventón a uno de los automóviles que acompañaban a los corredores. Más adelante, sin embargo, decidió bajarse del automóvil y siguió corriendo, entrando primero a la metacon la aclamación de los espectadores. Fue descalificado, por supuesto, pero no antes de estar a punto de recibir la medalla de oro de manos de la hija del presidente Roosevelt.Al final el ganador fue Hicks, quien entró a la meta medio muerto. Así, ganóHicksla medalla de oro del maratón de San Luis, ayudado por un coctel de estricnina, clara de huevo y brandy.Mucho tiempo ha pasado desde 1904 y las olimpiadas ciertamente han sufrido una enorme transformación, tanto cualitativa como cuantitativamente. De este modo, si  hace más de un siglose dio el caso de un atleta que ganó una medalla de oro con ayuda de estimulantes, es inevitable que hoy en día –con la enorme cantidad de dinero e intereses políticos involucrados en las olimpiadas– el dopaje no sea de ninguna manera algo infrecuente. Ni entre atletas rusos ni entreaquellos de otras nacionalidades.",
    "El propósito nunca se ha aclarado, ni los culpables han sido plenamente identificados,pero hace poco más de un siglo hubo quien o quienes en Inglaterra tuvieron la ocurrencia de falsificar fósiles y hacerlos pasar por restos de un ancestro nuestro. Nos referimos al llamado Hombre de Piltdown,cuyos restosfueron supuestamente encontrados entre los años 1908-1912en una mina de grava cerca dePiltdown, un pueblo en el sur de Inglaterra. La historia se inicia en 1908, cuando un trabajador de la mina habría localizado algunos fragmentos de un cráneo fosilizado, mismo que hizo llegar a Charles Dawson, un arqueólogo aficionado,quien se dio a la tarea de explorar el sitio en busca de más fósiles. Posteriormente se le unieron en la búsqueda el padreTeilhard de Chardin y Arthur Smith Woodward, un paleontólogo profesional del Museo Británico, logrando encontrar más fragmentos de cráneo, una mandíbula con dos molares y un diente canino. Lo peculiar de los restos fósiles es que, mientras que los fragmentos de cráneo recordaban a los de un hombre moderno, la mandíbula tenía las características de la de un simio.Smith Woodward y Dawsonpresentaron sus descubrimientos ante la Sociedad Geológica de Londres en una sesión llevada a cabo en el mes de diciembre de 1912. En dicha sesión expresaron su convicción de quelos restos fósiles de Piltdown, correspondíanal eslabón perdido entre nuestra especie y la nuestros antecesores simios.Aunque controvertidas, las opiniones de Smith Woodward y Dawson tuvieron una buena aceptación y fueron apoyadas por prominentes paleontólogos británicos. El apoyo fue tal que en 1938 se erigió en el sitio del supuesto descubrimiento del Hombre de Piltdown un monumento conmemorativo del acontecimiento.En 1953, sin embargo, quedó claro que todo había sido un engaño y que el cráneo pertenecíaa un humano anatómicamente modernomientras que la mandíbula era de un orangután. Los restos habían sido tramposamente alterados para disimular el hecho de que nopertenecieran al mismo individuo.Una de las maneras en que se ha entendido la razón por la que el Hombre dePiltdown pudo sobrevivir por cuatro décadas, invoca al hecho de que mientras que en el continente europeo había abundantes fósiles que daban fe de la evolución de la especie humana, en Inglaterra éstos eran casi inexistentes. En palabras del biólogo evolucionista y divulgador científico Stephen JayGould, “Inglaterra no sabía nada de sus antepasados más antiguos. Francia, por otro lado, disfrutaba de una superabundancia de neandertales, cromañones y sus herramientas y arte asociados. Los antropólogos franceses gozaban pasándoles por las narices a los ingleses esta exagerada disparidad de evidencias. Piltdown no podría haber sido mejor diseñado para darle la vuelta a la tortilla. Parecía muy anterior al neandertal.” Aunque al final del díaPiltdown no contribuyó a quelos ingleses pudieran darle la vuelta a la tortilla ante los franceses, esta semana se dio a conocer un descubrimiento con el que podrían hacer algo al respecto;si bien en un contexto diferentey con alcances considerablemente más limitados.Este descubrimiento,se llevó a cabo en Peterborough, localizado a unos 120kilómetros al norte de Londres, sitio de un asentamiento humano de la Edad de Bronce. En el sitio de Peterboroughlos arqueólogos encontraron que hace unos 3,000 años un conjunto de chozas se incendiaron –por accidente o vandalismo– con una velocidad tal que sus moradores tuvieron que salir huyendo a toda prisa sin llevar consigo ninguna de sus pertenencias.Las chozas, que estaban construidas sobre pilotes por encima de la superficie de un río, colapsaron y se hundieron con todo en su interior. Para los arqueólogos –pero, obviamente, no para sus moradorespor más que hayan salvado la vida– el incendio, carbonización y posterior hundimiento de las chozas constituyó una gran suerte, pues posibilitó la conservación por tres mil años de su restos con todo en el interior.Así, los investigadoresencontraron enterrados en el lecho del río numerosos objetos de cerámica, desdepequeñas tazas hasta platos y recipientes para almacenar comida. Encontraron también objetos de madera, lo mismo que objetos de metal, incluyendo cinceles y pinzas. Algo que los arqueólogos destacan es el descubrimiento de restos de telas que fueron conservados por la carbonización que sufrieron.Adicionalmente, la distribución de los objetos encontrados en los diferentes espacios de las casas colapsadas da una indicación de cómo éstos eran usados.  Los arqueólogos descubrieron igualmente restos de comida y recipientes con restos de comida en su interior. En un caso incluso se encontraron con un platocon su cuchara, sumergida éstaen el contenido carbonizado. Todo estonos proporciona información valiosa sobre los hábitos alimenticios de la Edad de Bronce.De este modo, el sitio de Peterboroughconstituyeuna cápsula del tiempo que nos da una visión del mundo 3,000 años atrás. Tienenasí los británicos su propia versión de Pompeya. Una versión que es incluso 1,000 años más antigua que la ciudad sepultada por el Vesubio.Tal pareciera que con Peterboroughlos británicos serán más afortunados de lo que lo fueron con Piltdown",
    "Un evento al que se le dio mucha difusión en los medios de comunicación esta semana fue el arribo aJúpiter de la sonda Junode la NASA. Juno fue lanzada rumbo a este planeta en agosto de 2011 y después de recorrer 2,800 millones de kilómetros alcanzó su destino el pasado 5 de julio.La sonda tiene la misión de estudiar de cerca al que es el planeta más grande del sistema solary entre otras cosas averiguar cuál es su estructura interna. Una vez en las inmediaciones de Júpiter, Juno maniobró para entrar en órbita. La NASA escogió una órbita polar excéntrica que minimizara daños a los instrumentos de medición y a los paneles solares de la sonda por los cinturones de radiación que rodean a Júpiter. Se espera que Juno lleve a cabo un total de 37 órbitas, cada una con una duración de catorce días, y que se acerque hasta unos 4,300 kilómetros sobre la superficie de Júpiter. Al final de la misión, la NASA hará descender a la sonda de manera controlada hacia la atmósfera del planeta, en donde se desintegrará por las enormes presiones y el calor a los que se encontrará sujeta.Ciertamente, la semana que hoy termina Júpiter atrajo mucha atención pública y en cierto modo se convirtió en una celebridad. Lo que, por lo demás, no debe haber quitado el sueño al planeta, pues no es la primera vez que ocurre. De hecho, de los planetas gigantes–Júpiter, Saturno, Urano y Neptuno–, Júpiter ha sido el más estudiado por la agencia espacial estadounidense.Previamente a la misión Juno, siete sondas fueron enviadas por la NASA a las inmediaciones de Júpiter. De éstas, la que mayor tiempo le dedicó fue Galileo, quese mantuvo en órbita alrededor del planeta entre 1995 y 2003.  Las demás lo han explorado sólo de pasada, si bien han proporcionado una gran cantidad de información.La primerafue la Pioneer 10  que en diciembre de 1973 se acercó a unos 130,000 kilómetros de Júpiter en su camino hacia el espacio interestelar. El Pioneer 10 envió a la Tierra fotografías de Júpiter que superaron en resolución a lasque en esos momentos habían sido obtenidas desde nuestro planeta.Por otro lado, la NASA no ha sido la única en interesarse en Júpiter. Lejos de esto, el planetaha dado siempre de que hablar. Incluso desde hace miles de años y antes de ser identificado como tal. Y hay razones para ello: en el firmamento aparece como la segunda estrella más brillante, sólo después de Venus. Sabemos, por supuesto, que Júpiter no es una estrella y que si lo vemos como un punto luminoso essólo por la distancia a la que se encuentra. En la antigüedad, si bien no se conocían las distancias interestelares, ni Júpiter ni ninguno de los otros cuatro planetas visibles a simple vista –Mercurio, Venus, Marte y Saturno– eran consideradosestrellas ordinarias, pues se movían en el firmamento siguiendo trayectorias más complejas que las de las estrellas comunes.Tanto así que los griegos las llamaban estrellas errantesy los romanos les dieron nombres de dioses. Y entre los dioses romanos, Júpiter era el de mayor jerarquía.  Júpiter ha jugado papeles centrales a lo largo de la historia. En los inicios del siglo XVIIcontribuyó de manera determinante para echar abajo la suposicióno prejuicio geocéntrico prevaleciente entonces, según la cual la Tierra es el centro del universo y como tal todos los objetos que vemos en el firmamento giran a nuestro alrededor. Si bien el movimiento aparente de la Luna, el Sol y las estrellas ordinarias no contradice esta suposición, no puede decirse lo mismo con respecto a los planetas,que se mueven la mayor parte del tiempo en una dirección,pero que también en ocasiones se mueven en la dirección contraria.Para explicar el movimiento complejo de los planetas –dentro del prejuicio geocéntrico– Claudio Ptolomeo hace casi dos mil años empleó un complicado modelo según el cual la Tierra estaba inmóvil en el centro del universo y los planetas giraban alrededor de un punto que a su vez giraba alrededor de la Tierra. A la distancia el modelo de Ptolomeoluce bastante artificial. Gozó, no obstante, de gran influencia por más de un milenio hasta que se encontró en el siglo XVIcon Nicolás Copérnico, que propugnaba por un modelo con el Sol ocupando el lugar de la Tierra.La defunción del modelo geocéntrico ocurrió en 1610 cuando Galileo Galilei apuntó un rudimentario telescopio de su construcción hacia Júpiter ydescubrió un disco luminoso con tres puntos brillantes en su cercanía. En una segunda observación días después,los puntos originales habían cambiado de posición y había aparecido un cuarto punto. Galileo dedujo de manera correcta que los puntos luminosos eran satélites que orbitaban alrededor de Júpiter, los hoy conocidos como satélites galileanos en su honor.Así, Galileo demostró con datos duros que no todos los objetos del firmamento tienen necesariamente que orbitar alrededor de la Terra y con esto quitó a nuestro planeta el lugar especial que erróneamente se le había adjudicado. Como consecuencia, resultó natural asumir que los fenómenos terrestres están gobernados por las mismas leyes físicas que gobiernan a los fenómenos celestialesy esto abrió camino para el desarrollo de la ciencia de la mecánica que serviría de modelo para el desarrollo de otras disciplinas científicas.Así, Júpiter, el planeta que ahora la NASA intenta conocer más a fondo, jugóun papel estelar, que no de estrella–al menos no de estrella ordinaria– en el desarrollo de la civilización occidental.",
    "Como fue ampliamente dado a conocer por los medios de comunicación, el pasado martes un ataque terrorista en el aeropuerto de Estambul, Turquía, dejó un saldo de al menos 44 muertos, incluyendo dos atacantes suicidas. Este hecho se suma al creciente número de incidentes terroristas en el mundo, el cual se multiplicó por un factor de nueve entre los años 2000 y 2014, según la base de datos sobre terrorismo global alojada en la Universidad de Maryland.Según esta base de datos, casi 33,000 personas a nivel global murieron por actos terroristas en 2014, incluyendo 6,200 atacantes. Los muertos por terrorismo, por otro lado, no se distribuyeron de manera igual en todo el mundo y el 78% del total se dieron en sólo cinco países: Irak, Nigeria, Afganistán, Pakistán y Siria,con Irak el peor librado al registrar aproximadamente 10,000 muertos. En contraste, en los Estados Unidos murieron18 personas en 2014 víctimas del terrorismo.    La gran mayoría de los ataques terroristas en el mundo se dan de este modo en unos pocos paísesy esto contrasta con la desproporcionada atención que los medios de comunicación dan a los incidentes terroristas que ocurren en los países desarrollados. Así, recordamos sin dificultad los ataques de noviembre de 2015 en París con el resultado de 137 personas muertas, pero quizá no recordemos con la misma facilidadaquellos que se llevaron a cabo el 20 de septiembre del mismo año en dos localidades de Nigeria,con el resultado de al menos 145 víctimas fatales.Como quiera que sea, si bien en los pasados años el número de víctimas del terrorismo ha sido relativamente pequeño en los países desarrollados, particularmente en los Estados Unidos, en donde las fatalidades por terrorismo en 2014 fueron apenas un 0,06% de número total de muertos por accidentes detráfico, existe interés en estudiar el fenómeno del terrorismo con el fin de anticipar sus ataques.Con respecto a lo anterior, el pasado 17 de junio un grupo de investigadores en los Estados Unidos, encabezado por N.F. Johnson de la Universidad de Miami, publicó en la revista “Science” un artículo en el que reportan los resultados de un estudio llevado a cabo con la esperanza de entender el proceso mediante el cual se forman los grupos terroristas, al igual que las condiciones que propician su actividad como tales. El estudio se llevó a cabo con información obtenida de Internet, que es la vía de comunicación que promuevela integración y coordinación dedichos grupos con una eficiencia sin precedentes. Johnson y colaboradores asumieron que, mas que seguir los mensajes por Internet de todos aquellos que de manera ocasional hubieran mencionado, por ejemplo, al Estado Islámico (ISIS), hay que seguir la autoformación y evolución de grupos de apoyo –a los que se refieren como “agregados”– a dicha organización terrorista. Cada uno de estos agregados está formado por un grupo de seguidores de una determinada página de Internet, los cuales “de manera frecuente discuten en línea detalles operativos, tales como rutas de financiamiento, “knowhow” tecnológico y la manera de evitar ataques por aviones no tripulados”. Para llevar a cabo su estudio, Johnson y colaboradores se enfocaron en páginas alojadas en Vkontakte, que es una red social análoga a Facebook, con base en Rusiay 350 millones de usuarios a nivel global. Vkontakte permite varios idiomas yen la misma las páginas pro-ISIS tienen mayor tiempo de supervivencia que en Facebook. La red es, además, utilizada por ISIS para diseminar propaganda entre la población de lengua rusa.Para llevar a cabo su investigación, Johnson y colaboradores primeramente identificaron agregados que expresaran explícitamente apoyo a ISIS, que diseminaran noticias o propaganda de esta organización, o bien que llamaran a la guerra santa en su nombre. Encontraron entre el 1 de enero y 31 de agosto de 2015, 196 agregados pro-ISIS con más de 100,000 seguidores. Johnson y colaboradores hacen notar quedichos agregados viven en un medio virtual en el que encuentran depredadores –la policía cibernética, “hackers” individuales, o los administradores de la red– y están sujetos a una evolución que puede incluir la fusión con otros agregados o su desaparición. En este último caso, el agregado puede re-encarnar con otro nombre.  Estudiando la actividad decreación de nuevos agregados, los investigadores encontraron que, en un caso particular, la ocurrencia de un acto terrorista por parte de ISISfue precedida por un incremento de dicha actividad. Estudiando el movimiento en la redes sociales de los grupos de apoyoa una determinada organización terrorista se podría anticipar entonces la inminencia de un atentado.No todo mundo está de acuerdo, sin embargo,y hay quien señala que Johnson y colaboradores en realidad no están prediciendo naday que el incremento en la actividad de los agregados de apoyo a ISIS puede tener varias causas. Para establecer una conexión entre un acto terrorista y el incremento de actividad de sus grupos de apoyo en la redes sociales sería entonces necesario demostrarlo en más casos.Por lo demás, la peculiar distribución geográfica de la actividad terrorista, que se concentra en unas pocas regiones del planeta, sugiere que la misma obedece a causas objetivas, en parte asociadas a las desigualdades sociales que imperan en el mundo, y que para prevenir atentados terroristas quizá fuera más efectivo tratar de mitigarlas.",
    "Según la Organización Mundial de la Salud,  un millón y cuarto de personas mueren anualmente por accidentes de tráfico en el mundo. Los vehículos motorizados contribuyen así, de manera indirecta, con más del 2% de las muertes que anualmente ocurren a nivel global. El automóvil, un invento relativamente recienteque ha tenido una gran influencia en nuestro estilo de vida –y de muerte también, en cierta medida–, está en vías de sufrir una transformación sustancial.Esto, con relación al desarrollo de vehículos autónomos capaces de viajar sin la intervención de un conductor. Este tipo de vehículos, que pueden en principio responder de manera más rápida y precisa a situaciones de emergencia, llevaría a una reducción sustancial en el número de muertes por accidentes de tráfico.Como sabemos, hay varias compañías que están experimentando con prototipos de automóviles sin conductor. El ejemplo más conocido es el de Google, que ha estado probando varios vehículos equipados con sofisticados sistemas para la percepción de sus alrededores y el control de su movimiento de manera autónoma.De acuerdo con Google, sus automóviles sin conductor han recorrido más de dos millones y medio de kilómetros sin conductor en los estados de California y Texas, y si bien han tenido algunos choques, éstos no han sido responsabilidad de sus vehículos viajando en modo autónomo.En realidad éste había sido el caso hasta el pasado mes de febrero, cuando un automóvil de Google le cerró el paso a un autobús urbano en California. Google reconoció que el culpable del accidente fue su automóvil. El choque se produjo cuando el automóvil autónomo pretendía dar vuelta hacia la derecha para lo cual se desplazó hasta el extremo derecho de la calle. Antes de llegar a la esquina, sin embargo, se encontró con un obstáculo lo que lo obligóa abrirse hacia su izquierda sin tomar en cuenta al autobús urbano que se le acercaba a baja velocidad. Google explicó que el “software” de control del vehículo asumió que el autobús se detendría y le dejaría el paso, cosa que no ocurrió sobreviniendo la colisión. Con relación a esto, Google afirma haber hecho cambios en el “software” de control para tomar en cuenta este tipo de eventualidades. Según algunas opiniones, los automóviles autónomos son ya una realidad y no pasarán muchos años antes de que sea común verlos en la vía pública. Algunos problemas técnicos y de otro tipo tendrán, sin embargo, que resolverse antes de que esto suceda. Uno de estos problemas es tema de un artículo publicado esta semana en la revista “Science” por investigadores deFrancia y los Estados Unidos encabezados por J.F. Bonnefon de la Universidad de Toulouse.Bonnefon y colaboradores abordan un problema relacionado con la programación de automóvil para evitar accidentes en los que se ponga en riesgo la vida de personas. Los investigadores consideran la siguiente situación. Suponga que un automóvil autónomo encuentra que un grupo de peatones se atraviesa en su camino y para evitar atropellarlos tiene la opción de virar de manera violenta lo que lo llevaría a chocar con un muro y posiblemente matar a sus ocupantes. ¿Qué es lo que el automóvil debe hacer? ¿Priorizar la vida de los peatones o la de sus ocupantes?Bonnefon y colaboradores llevaron a cabo una encuesta por Internet con aproximadamente 2,000 personas en busca de una respuesta. Los encuestados se inclinan por una solución utilitaria que minimice elnúmero de víctimas. Por ejemplo, si los peatones son más de uno y en el automóvil sólo viaja el chofer, el “software” lo debe hacer virar y estrellarse en el muro, posiblemente matando al conductor. Si, por el contrario, los ocupantes son varios y el peatón sólo uno, el automóvil debe mantener su ruta.Las opiniones mayoritarias, sin embargo, sólo corresponden a una versión de “hágase la voluntad de Dios en la mulas de mi compadre”, pues cuando a los entrevistados se les preguntó si comprarían un automóvil con un “software utilitario”, programado para matarlo a él o a algún familiar en un caso dado, la mayoría contestó que no lo haría.Así, un automóvil programado de este modo con seguridad sería un fracaso comercial.¿Cómo debe programarse un automóvil autónomo? En primera instancia debería hacerse obligatoria una programación con un enfoque utilitario. Paradójicamente, sin embargo, tal como apuntan los investigadores, esto no contribuiría a disminuir el número de fatalidades por accidentes de tráfico al retrasar la adopción de los automóviles sin conductor.Muchos problemas deben resolverse antes de que tengamos a los automóviles sin conductor en la vía pública, incluyendo la disyuntiva moral relativa a su programación para manejar situaciones riesgosas para la vida, y para la cual Bonnefon y colaboradores no ofrecen una solución. Los automóviles sin conductor están de este modo todavía a años por delante.Y más lejos todavía están en nuestro medio por el  desorden urbano que impera en el país. En efecto, si en las ordenadas calles de California se dio una colisión entre un autobús urbano y automóvil sin conductor¿qué no podrá pasar  en el poco ordenado tráfico de nuestra ciudad?Ciertamente, los problemas técnicos a resolver para adaptar un automóvil sin conductor a nuestro medio se antojan formidables.",
    "¿Cuánto tiempo hace que vio por última vez a la Vía Láctea? A menos que haya viajado recientemente al campo, es posible que hayan transcurrido varios años desde que tuviera la oportunidad. En posible incluso que en las nuevas generaciones encontremospersonas que nunca hanvistola Vía Láctea y las miles de estrellas que pueden observarse a simple vista en una noche oscura.  Y todo esto por efecto de la creciente contaminación por luz eléctrica artificial en los centros urbanos,que hace cada vez más difícil observar los detalles del firmamento.Estamos de este modo rápidamente perdiendo consciencia del mundo que nos rodea, lo que contrastacon las épocas anteriores a la expansión de la luz eléctrica, cuando las noches eran oscuras y el firmamento y sus estrellas eran parte de la vida diaria.  Al respecto, un grupo internacional de investigadores, encabezado por Fabio Falchi del Instituto de Ciencias y Tecnología sobre Contaminación Lumínica enThiene, Italia, publicó el pasado 10 de junio en la revista “ScienceAdvances” un atlas sobre la contaminación lumínica a nivel global. De acuerdo con este artículo, aproximadamente el 83% de la población del mundo, y más del 99% de las población de los Estados Unidos y Europa, vive en áreas contaminadas por luz.Se encuentra también que la Vía Láctea está oculta para un tercio de la población del mundo, lo mismo que para el 60% por ciento de los europeos y el 80% de los norteamericanos.Falchi y colaboradores ofrecen también información detallada por países. Así, el país más contaminado lumínicamente es Singapur,seguido, en ese orden,por Kuwait, Catar, los Emiratos Árabes Unidos y Arabia Saudita.En el otro extremo, dieciocho de los veinte países menos contaminados se localizan en África; el primer lugar lo ocupa Chad, seguido de la República Central Africana y Madagascar.Con respecto a nuestro país, si bien no se encuentra en el grupo de los veinte países más contaminados lumínicamente, un vistazo a los mapas de distribución de luz incluidos en el artículo de referencia muestra áreas de gran luminosidad. La más intensa, por supuesto, corresponde al área metropolitana de la Ciudad de México, con Puebla y Cuernavaca como satélites. Destacan, igualmente, Guadalajara y Monterrey-Saltillo, lo mismo que La Laguna. En el centro del país destacan el Bajío-Querétaro, Aguascalientes y San Luis Potosí.Sabemos que la contaminación lumínica es sólo una de las tantas emergencias globales que sufre nuestro planeta. La contaminación atmosférica por la emisión de gases de invernadero es otra, al igual que la contaminación de los océanos por materiales no biodegradables, la congestión de la vecindad espacial de la Tierra por satélites artificiales, o la contaminación de la estratósfera por compuestos químicos que destruyen la capa de ozono que protege a la Tierra de la radiación ultravioleta del sol, por mencionar algunas. Dichas contaminaciones, que tienen un rango global, son en último término producto de la revolución industrial ocurrida dos siglos atrás y han ocurrido en un tiempo extremadamente corto; corto en comparación con otros tiempos característicos de nuestro paso por el planeta.En efecto, el periodo de tiempo –medido en miles de años– que ha transcurrido desde la invención de la agricultura es una muy pequeña fracción del tiempo –medido en cientos de miles de años– que le tomó a nuestra especie evolucionar hasta su estado actual a partir de nuestro ancestro inmediato –común con la especie neandertal–. Y si comparamos el tiempo que nos tomó evolucionar como especie con el que nos bastó para contaminar de diferentes maneras al planeta –medido en cientos de años–,  el contraste resulta ser todavía más grande.Como lo apunta el paleontólogo Stephen JayGould en el ensayo “Sombras de Lamarck”, incluido en su libro “El pulgar del panda”, los rápidos tiempos característicos de nuestra evolución cultural –con todas su consecuencias positivas y negativas, incluida la rápida contaminación del planeta– son un reflejo de que dicha evolución tiene una naturaleza lamarckiana. Jean-BaptisteLamarck explicaba la evolución postulando que las especies desarrollan nuevas características físicas como una manera de adaptarse al medio en el que se viven, mismas que heredan a su descendencia. La evolución lamarckiana resulta de este modo un proceso muy eficiente, capaz de producir cambios sustanciales en un tiempo corto. La evolución culturalsigue la regla de Lamarck. Un desarrollo científico o tecnológico, por ejemplo, puede ser heredado a una siguiente generación por algún medio oral o escrito. El desarrollo heredado puede a su vez ser modificado y trasmitido a la siguiente generación, evolucionando de este modo de manera eficiente.  El mecanismo de selección natural postulado por Darwin –que es el mayormente aceptado para las especies animales– esen contraste poco eficiente en el corto plazo y necesita de periodos de tiempo largos para producir cambios sustanciales.  El corto tiempo en el que hemos generado cambios mayores en el planeta, en comparación con el tiempo que nos ha tomado evolucionar como especie, puede ser entonces entendido en términos de las diferencias  entre los modelos de Lamarck y Darwin. Lo que, por supuesto, de poco consuelo nos sirve. En particular, más nos valdría poner un freno a la contaminación lumínica antes de que se nos olvide que el cielo tiene estrellas.",
    "El 17 de febrero de 1600, Giordano Bruno fue ejecutado en la hoguera acusado de herejía por la inquisición romana. Un monumento en la plaza romana “Campo deiFiori” –lugar de la ejecución–conmemora el acontecimiento. Entre otras opiniones heréticas, Brunoargumentaba en favor de la existencia en el Universode una multiplicidad de mundospoblados por seres inteligentes. Si bien para los estudiosos no es claro hasta qué punto esta particular creencia de Bruno influyó en la sentencia dictada en su contra, la misma iba en contra de la ortodoxia religiosa vigente y nosdespojaba de nuestro lugar privilegiado como única especieinteligentedel universo.Hoy en día,a cuatro siglos de la ejecución de Bruno, la posibilidad de que puedan existir mundos extraterrestres poblados por seres inteligentes, incluso más avanzados que nosotros, es ampliamente aceptada. Tan aceptada que ha dado lugar a iniciativas serias –si bien controvertidas– para descubriralguno de estos mundos hipotéticos. En una de estas iniciativas, desde los años sesenta del siglo pasado se ha estado escudriñandoel firmamento en busca de señales provenientes de una civilización extraterrestre; esto, con el apoyo de diferentes fuentes de financiamiento, incluyendo a la NASA.Sin embargo, a pesar de todos los esfuerzos no se han encontrado pruebas de la existencia de inteligencia extraterrestre –en realidad, ni aun de vida microscópica fuera de nuestro planeta–. Así, la expectativa relativa a la existencia de vida extraterrestre avanzada por Brunosigue aunen vías de confirmación. Por otro lado, hoy sabemos que no ocupamos un lugar especial en el Universo como única especie inteligente. Y esto lo averiguamos, no buscando en el espacio interestelar, sino escudriñando el pasado de nuestro planeta. De hecho, sabemos que nuestra especie es el resultado de una evolución que tomó millones de años y que no siempre fuimos lo que hoy somos. Incluso hubo momentos en los que el proceso de evolución dio pie a que convivieran dos especies humanas, diferentes.Así, por ejemplo, los paleoantropólogos saben de la existencia de los neandertales, una especie humana diferente a la nuestra, perocon un grado de inteligencia similar, con la que convivimos por decenas de miles de años hasta su extinción hace unos 30,000 años. En este periodo de convivencia las dos especies se habrían incluso cruzado y dejado descendencia.Otro ejemplo que ha tomado a los especialistas por sorpresa es el hombre de Flores, una especie humanahasta hace poco desconocida que habría vivido en Flores,una isla del archipiélago indonesio hasta hace unos 50,000 años.La especie del hombre de Flores era diminuta, de apenas un metro de estatura, y por esta circunstanciaha llegado a ser conocida como “hobbit”.Nadie sabía de la existencia de los “hobbit” hasta septiembre de 2003, cuando un grupo de investigadores de Australia e Indonesia descubrió un esqueleto de dicha especie enLiangBua, una cuevaen la isla de Flores. El esqueleto perteneció a un individuo –posiblemente hembra– de 1.06 metros de estatura, 25 kilogramos de peso y un cerebro con un volumen de 380 centímetros cúbicos –aproximadamente una tercera parte tamaño del cerebro de nuestra especie–.Aunque originalmente fue fechado en 18,000 años,hoy se sabe que la antigüedad de los restososcila entre los 100,000 y los 60,000 años.Algunos especialistas han sugerido que el esqueleto de LiangBuacorresponde al de un hombre modernoque sufría de una patología congénita, de manera específica de microcefalia. Un artículo que apareció esta semana en la revista “Nature”, sin embargo, descarta esta hipótesis.Dicho artículo fue publicado por un grupo internacional de investigadores encabezado por Gerrit van der Bergh de la Universidad de Wollongong en Australia. En su artículo, van der Bergh y colaboradoresreportan el descubrimiento de la parte inferior de la mandíbula y un molarde un individuo,un 20 por ciento más pequeño que el hombre de Flores y que pareciera ser su ancestro. El descubrimiento se llevó a cabo en un sitio llamado Mata Menge en la isla de Flores, a unos 75 kilómetros de LianBua,Lo sorprendente del hallazgo es que los restos tienen una antigüedad de 700,000 años. Según el nuevo descubrimiento, el esqueleto LiangBua no corresponde al de un humano moderno con una patología, pues sus ancestros habitaban la isla de Flores desde hace más de 700,000 años,  cuando nuestra especie aun no existía. De acuerdo con van der Bergh y colaboradores, el hombre de Flores procede de un especie humana primitiva que arribó a Flores hace un millón de años y una vez ahí evolucionó hacia un cuerpo más pequeño por la escasez de alimentos en la isla. Consideran, sin embargo, es necesario contar con nuevos fósiles para llegar a una conclusión más sólida. Si bien obtener información del pasado remoto no es una tarea sencilla, hoy en día es claro que en el camino que ha seguido nuestra especie en su evolución hasta el momento actual, en más de una ocasión hemos convivido con especies cercanas a la nuestra que finalmente se extinguieron, quizá por influencia nuestra. ¿De no haberse extinguido el neandertal o el hombre de Flores, habrían ayudado a Giordano Bruno a librar la hoguera? No podemos saberlo, pero dada nuestra tendencia a la discriminación racial, es posible que no hubiera hecho diferencia.",
    "Las cifras varían de acuerdo a la fuente, pero entre los siglos XV y XVIII alrededor de 50,000 personas en Europa, en su mayor parte mujeres, fueron condenadas y ejecutadas –algunas en la hoguera–acusadas de brujería. A partir del siglo XVIIlas ejecuciones poreste motivoempezaron a disminuir hasta desaparecer en el siguiente siglo. Esto, debido a que se impuso la concepción racional del mundoa la visión mágica del mismo, quefue el sustento de los juicios en contra de la brujería. No existe un acuerdo entre los expertos sobre las causas que llevaron a la caza de brujas en los siglos XV-XVIII, pero algunos especulan que los supuestos brujos y brujas enjuiciados fueron en realidad chivos expiatorios de las autoridades civiles y religiosas, que fueron presentados como responsables indirectos de desastres naturales y económicos. Así, se suponía que brujos y brujas obedecían a la fuerzas del demonio, verdadero culpable de las desventuras de la gente, y por tanto merecían ser castigados. De este modo, además de desembarazarse de cualquier posible responsabilidad en las catástrofes, las autoridades jugaban un papel positivo a los ojos de la población castigando a los culpables de sus desgracias.Con relación a esta interpretación, Emily Oster de “Harvard University”, en un artículo publicado en 2004,hace notar que la caza de brujas coincidió con el periodo de enfriamiento de la tierra conocido como “Pequeña edad del hielo”, ocurrido entre los siglos XVI y XIX. Este periodo produjo inviernos fríos en Europa y una baja en la producción agrícola que trajo hambrunas e inestabilidad social. Es quizá interesante hacer una digresión aquí para mencionarque Antonio Stradivarius fabricó sus famosos violines con árboles crecidos durante la Pequeña edad del hielo; violines que, según algunos especialistas, deben su calidad sonora precisamente a esta circunstancia.Regresando a los desastres agrícolas producidos por el enfriamiento del planeta, podemos apuntar que, dada la concepción mágica que se tenía en esa épocaacerca de los fenómenos naturales,no es difícil entender que el común de la gente pudiera fácilmente haber llegado a convencerse de que estaban siendo atacados porel diablo,  al igual quepor las brujas,sus intermediarias en este mundo.Si bien las brujas no juegan hoy en día el mismo papel que jugaban hace 500 años,ciertamente no han desaparecido del planeta. Así, muchos asociamos a una bruja con una vieja desdentada, vestida de negro y volando durante la noche montada en una escoba.Estereotipos aparte, sabemos queexisten brujas practicantes en el mundo, si bien no con las habilidades sobrenaturales que reclaman y queles serían propias a su supuesta naturaleza.Es posible que las brujas actuales no estén expuestas a los mismos peligros que hace algunos siglos, los cuales  incluían el de ser enjuiciadas, y condenadas a morir en la hoguera. Esto, sin embargo, no significa que no corran peligro, fundamentalmente en aquellos lugares del mundo en donde la creencia en la brujería está todavía enraizada en la población. Podemos mencionar, por ejemplo, queen una plantación de té en Jalpaiguri, en el noreste de la India, cinco mujeres fueron atadas, torturadas y asesinadas en el año 2003, falsamente acusadas de haber practicado la brujería en contra de un habitante del pueblo que murió aquejado de un mal estomacal.  Por otro lado, aun despojadas de sus poderes sobrenaturales las brujas siguen teniendo efectos perniciosos. Esto,al menos,de acuerdo con Boris Gershmande la “American University”en Washington, DC, quien estudió la influencia que tiene la creencia en la brujería en 19 países africanos al sur del Sahara, en los que la creencia en la brujería está extendida. Según los resultados de su estudio, publicados en el número de mayo pasado de la revista “Journal of DevelopmentEconomics”, Gershmandocumenta con datos sólidos y confirma lo que otros investigadores habían planteado: que la creencia en la brujería tiene un efecto social negativo que constituye un freno para el progreso económicoal aumentar la desconfianza entre la población e impactar negativamente las actitudes de cooperación y camaradería. En contraste con la brujería, Gershman no encuentra un impacto social equivalente en otras creencias sobrenaturales entre las que se encuentran el infierno, la reencarnación, los milagros y los espíritus malignos. La brujería pareciera de este modo estar sola como creenciaen lo sobrenatural que promueve actitudes antisociales.La concepción mágica de las fuerzas que mueven al mundo fue un elemento esencial para que se diera la caza de brujas en la Europa de los siglos XV-XVIII. Esta concepción admitía la posibilidad de que un determinado fenómeno natural –la enfermedad de una persona, por ejemplo– fuera causado por una fuerza sobrenatural –el diablo a través de un habitante de este mundo–. Hoy sabemos –o al menos no tenemos prueba de lo contrario– que esto no es así y de que todos los fenómenos que podemos observar tienen una causa natural. Y que aun la creencia en las brujas –que sobrevive entre nosotros de manera persistente– podría tener una explicación racional.",
    "En un congreso internacional llevado a cabo esta semana en Salónica, Grecia, para conmemorar los 2,400 años del nacimiento del Aristóteles, el arqueólogo griego KonstantinosSismanidisafirmó creer haber descubierto la tumba del filósofo griego en Estagira, su lugar de nacimiento en el norte de Grecia. Esto, después de una búsqueda que se extendió a lo largo de dos décadas. El que alguien dedique veinte años de su vida a buscar la tumba de Aristóteles,a mas de dos mil años de su muerte, es una muestra de la enorme y duradera influencia intelectual queeste filósofo ha tenido en la civilización occidental a lo largo de más de dos milenios. Como lo es igualmente el que esta semana se haya celebrado un congreso internacional en su honor. Ciertamente, Aristóteles es una de las personas más influyentes de la historia y como tal no es sorprendente que su nombre nos sea familiar –y que incluso esté aun activo como nombre propio–. Y todo esto a pesar del tiempo que ha transcurrido desde su muerte y de no haber jugado un papel político o militar, papeles que más fácilmente hacen trascender a las personas. La influencia de las ideas aristotélicas, por otro lado, ha sido variable a lo largo de la historia. Con la caída del imperio romano de occidente,Aristóteles tuvo poca influencia en la Europa del inicio de la Edad Media. La situación cambióen la baja Edad Media cuando Aristóteles fue redescubierto al ser traducidas algunas de sus obras del griego antiguo al latín o del árabe al latín. En el siglo XIII, algunas ideas aristotélicas fueron conciliadas con la doctrina de la iglesia e incorporadas a la misma por Tomás de Aquino.A pesar de la persistencia de Aristóteles  lo largo de la historia, sin embargo, no todo mundo tiene una opinión positiva sobre su legado intelectual.  El matemático británico Bertrand Russell, por ejemplo, era muy crítico del papel negativo que en su opinión han jugado –a través de la iglesia–las ideas aristotélicasen el desarrollo de la ciencia ocurrido en Europa en los siglos XVI y XVII. Entre otros aspectos, Bertrand Russell reprobaba la poca importancia que Aristóteles daba a la observación, que sabemos es un elemento fundamental del método científico. En su libro “El impacto de la ciencia en la sociedad”, Russell escribe: “Observación versus autoridad: Para la personas modernas y educadas parece obvio que las cuestiones de hecho deban ver verificadaspor la observación y no mediante la consulta de autoridades del pasado lejano. Pero ésta es una concepción enteramente moderna que difícilmente existía antes del siglo XVII. Aristóteles afirmaba que los hombres tenían más dientes que las mujeres; aunque estuvo casado dos veces, nunca se le ocurrió verificar esta afirmación examinándole la boca sus esposas.”Como lo apunta Russell, en su obra “Historia de los animales” Aristóteles también afirma que la mordedura de la musaraña es peligrosa para los caballos, especialmente si la musaraña está preñada y que el insomnio de un elefante puede curarse frotando los hombros del animal con sal,  aceite de oliva y agua caliente.En el campo de la física y a astronomía son igualmente conocidas algunas ideas que sostenía Aristóteles y que la observación y la experimentación propias de la ciencia  demostró que eran erróneas. Aristóteles afirmaba, por ejemplo, que en ausencia de un impulso un objeto en movimiento perdería su velocidad. Hoy sabemos que esto es incorrecto, gracias a Galileo Galilei.Aristóteles identificaba cuatro causas para explicar los cambios que observamos en el mundo. Dos de estas causas son la causa eficiente yla causa final.  La ciencia reconoce sólo la primera de éstas. La segunda, que debería haber desaparecido de nuestravista hace ya un buen tiempo, sigue, no obstante,aun vigente. La causa final es empleada por aquellos que defienden que el mundo y su evolución son producto de un diseño inteligente que busca un fin determinado y al margen de las leyes físicas.Tenemos así ideas aristotélicas aun viviendo entre nosotros. Más de dos mil años después de que fueron enunciadas y a pesar de las múltiples evidencias incontrovertibles de que lo que verdaderamente nos funciona es el método científico.Para convencernos de esto último basta pensar en las múltiples aplicaciones tecnológicas de la ciencia, entre la que se incluyen a la medicina moderna, la red Internet y las exploraciones interplanetarias, por poner sólo tres ejemplos. Así, empleando el método científico ahora sabemos, por ejemplo, que la locura no es producto de una posesión demoniaca y que para curarla no hay que someter al enfermo a un maltrato físico tal que incomode al demonio adentro y lo haga salir a toda prisa.  Sabemos también que los cuerpos celestes se mueven siguiendo las leyes de la física –las mismas que siguen los objetos en nuestro planeta– y no impulsados por dioses como sostenía Aristóteles. Ciertamente, empleando el método científico ahora entendemos que muchas cosas que antes creíamos ciertas son en realdad falsas. Lo que es más difícil de entender es que haya todavía quienes no estén convencidos.Al margen del lo anterior, lo que sí resulta notable es que, a 2,400 años de distancia, de un modo u otro sigamos hablando de Aristóteles.",
    "El 1 de agosto de 1859, Vicente Chico Sein, a la sazón gobernador del estado, dio vida al Instituto Científico y Literario de San Luis Potosí –antecedente inmediato de la UASLP–y le confirió la misión de impartir la educación preparatoria y profesional en el estado. El Instituto cumplió su función, sobreviviendoincluso a las desventuras por las que cruzó nuestro país en el siguiente medio siglo. Al inicio de la década de los años veinte del siglo pasado, sin embargo, estaba bajo el ojo crítico de aquellos que consideraban que sus egresados tenían una baja calidad profesional y que mejor harían en cerrarlo. En un discurso pronunciado durante la ceremonia de graduación de estudiantes del Instituto Científico y Literario en febrero de 1921, el gobernador Rafael Nieto, refriéndose a los muchos enemigos de dicho Instituto, expresó: “Muchos de esos enemigos argumentan que la enseñanza superior es un lujo en la mezquindad de nuestro medio económico; que si no estamos en condiciones de atender siquiera medianamente la enseñanza elemental y primaria del Estado, es un absurdo gastar buena parte de nuestro presupuesto en la formación de una aristocracia intelectual; que el raquitismo de nuestros recursos económicos sólo permite al Instituto una vida precaria y mezquina, y por lo tanto, sólo pueden salir de allí profesionales mediocres que van a engrosar las filas del proletariado intelectual”.Hoy en día, cuando es evidente el papel crítico que la educación superior tiene en el desarrollo de un país, es sorprendente que hubiera quien se atreviera a blandir este tipo de argumentos, por más que, obviamente, tenían un trasfondo político. En respuesta, Rafael Nieto arguyó: “Es cierto que es una anomalía reprobable que tengamos un establecimiento profesional más o menos bien atendido y que descuidemos en cambio la educación elemental y primaria. El remedio no está, sin embargo, es suprimir lo bueno sino en corregir lo malo”. Como solución, Nieto resolvió hacer del Instituto una institución autónoma con respecto del gobierno estatal y alejarlo así “de los vaivenes de la política”. Nació de este modo –el 10 de enero de 1923– la Universidad Autónoma de San Luis Potosí, la primera con este carácter en México.Al igual que el instituto que le dio origen, la nueva universidad tenía la misión de formar profesionales en diversas disciplinas y era así una institución dedicada a la trasmisión del conocimiento, más que a la generación del mismo. Esta situación prevaleció hasta la década de los años cincuenta cuando, entre otras iniciativas de investigación, fueron fundados el Instituto de Investigación de Zonas Desérticas (1954) y el Instituto de Física (1955).  En la década de los cincuenta, sin embargo, los recursos con los que contaba la Universidad no eran suficientes para que en la misma floreciera la investigación. A nivel federal, si bien existía la posibilidad de conseguir fondos para investigación, estos eran exiguos e intermitentes. Así las cosas, en su infancia, la investigación científica en la UASLP pasó por épocas difíciles.    La situación comenzó a mejorar al despuntar los años setentas cuandoel gobierno federal reconoció que la ciencia y la tecnología son dos elementos indispensables para el desarrollo del país y cambió sustancialmente su política al respecto. Así, en 1970 fue creado el CONACyT, con la misión de impulsarel desarrollo científico y tecnológico del país, y en 1984 el Sistema Nacional de Investigadores (SNI), que complementó el salario de los investigadores y posibilitó el surgimiento de la ciencia profesional en México. A estas acciones se aunaron programas implantados por la Secretaría de Educación Pública que establecieron incentivos para profesionalizar la educación y la investigación en nuestras universidades.El cambio en la políticas de apoyo a la ciencia y la educación en México fueron aprovechadas de manera espléndidapor la UASLP, que se ha convertido en una de las mejores instituciones de educación superior del país, tanto por sus programas educativos como por su capacidad de investigación. Y lo que es más importante, el avance de la Universidad no se ha detenido ysigue viento en popa; de manera acelerada, además. Así, a lo largo de los últimos cuatro años, bajo el rectorado del maestro en arquitectura Manuel Fermín Villar Rubio, la Universidad incrementó en más de un cincuenta por ciento elnúmero de sus profesores que son miembros del SNI, creó dos nuevos institutos de investigación y expandió su cobertura estatal con dos nuevos campus: Tamazunchale y Salinas de Hidalgo.El cambio de la Universidad,  por otro lado, no se ha limitado a lo cuantitativo. Así, el rector Villar Rubio implantó igualmente cambios cualitativos al crear el Centro Universitario de la Artes, que acercó las actividades artísticas a los estudiantes de las diversas carreras ofrecidas por la UASLP. Y en este orden creó también la Licenciatura en arte contemporáneo, que empezará a operar el próximo ciclo lectivo, y que representa una oferta educativa cualitativamente diferente a lo ofrecido hasta ahora por la Universidad. De este modo, a casi un siglo de que el Instituto Científico y Literario estuviera sujeto a críticas sin sustento, el futuro inmediato de la UASLP luce brillante. Muy lejos del panorama sombrío que sus detractores se empeñaban en mostrar.",
    "En un anuncio comercial a color aparecido en 1965 en la cubierta de una revista, Raquel Welch, la entonces famosa estrella de Hollywood, nos espeta: “No puedo permitirme estar flaca“, para luego aclarar: “Si bien filmar películas para el cine o la televisión es excitante y con muchas recompensas, también implicauna implacable rutina diaria de  largas horas de trabajo, desde temprano en la mañana hasta tarde en la noche. Esto a menudo nos obliga a suspender la comida o la cena.”  Así, la actriz habría tenido problemas para mantener la figura. Quienquiera que haya visto una de sus películas de los años sesenta, sin embargo, estará de acuerdo en queRaquel Welchlogró, de alguna manera, solucionarsu problema. Cómo fue que lo hizo ella mismanos lo explica en el anuncio de marras:el secreto fue un complemento alimenticio, tónico y pastillas, para aumentar el apetito y con los nutrientes necesarios para mantener el peso.En estos tiempos, en los que la moda es bajar peso –no subirlo– un anuncio como el de Raquel Welchnos resulta sorprendente. Nos ilustra, no obstante, sobre los vaivenes que ha tenido la figura femenina ideala lo largo de la historia. En la Grecia clásica, y a juzgar por las esculturas que nos han llegado de esa época –la Venus de Milo, por ejemplo–, el cuerpo ideal tendía a ser robusto. Esto, en contraste con la moda de Egipto antiguo en donde el cuerpo ideal era más bien delgado. Al despuntar el siglo XX la estética demandaba cuerpos robustos –por más que las mujeres se enfundaran en corsés para lucir cinturas de avispa–. Los años veinte trajo una moda andrógina en la que las mujeres perdieron la cintura. Para volverla a recuperar en los años cincuenta, con Marilyn Monroe como una de sus figuras representativas, y perderla nuevamente en los años noventa. En contraste con las oscilaciones de la moda, el peso de los habitantes de muchos países del mundo, incluyendo el nuestro, se está incrementando en forma continua, y de manera alarmante. Una medida del sobrepeso de una personanos lo da su índice de masa corporal, el cual se obtiene dividiendo su peso en kilogramos por el cuadrado de su altura en metros. Un peso normal se define por un índice de masa corporal entre 18.5 y 25. Un índice entre 25 y 30 define al sobrepeso, y uno mayor a 30 a la obesidad. Así, una persona de 1.75 metros de altura y un peso de 75 kilogramos tiene un índice de masa corporal de 24.5, cerca del límite superior del intervalo que se considera normal. Si esa misma persona pesara 95 kilogramos su índice sería de 31, lo que lo coloca dentro del rango de obesidad.Por otro lado, considerando casos específicos, Raquel Welch habría tendido en las años sesenta un índice de masa corporal de aproximadamente 19, un poco menor que el de Marilyn Monroe. De este modo, ambas estaban en el rango normal bajo, a pesar de la percepción que se tiene de ellas.No es el caso, desafortunadamente, de la mayoría de la población de los Estados Unidos, pues según fuentes oficiales, alrededor del setenta por ciento de los estadounidenses tiene sobrepeso y de éstos aproximadamente la mitad están clasificados como obesos. En México, según datos de la Organización para la Cooperación y el Desarrollo Económicos, las cifras no son muy diferentes.Un número de enfermedades, incluyendo las cardiovasculares y la diabetes, están asociadas al sobrepeso y las recomendaciones médicas es que nos tratemos de mantener dentro de un rango de peso normal. Un artículo de investigación aparecido esta semana, sin embargo, encuentra que el índice de masa corporal para el cual se tiene un menor índice de mortalidad, para cualquier enfermedad, cae en el rango que corresponde al sobrepeso. El artículo de referencia fue publicado en la revista“Journal of the American Medical Association” por un grupo de investigadores del “CopenhagenUniversity Hospital” en Dinamarca. En dicho artículo se reportan los resultados de un estudio llevado a cabo para correlacionar la mortalidad, por cualquier enfermedad, con el índice de masa corporal. Se investigaron datos de tres diferentes épocas: 1976-1978, 1991-1994 y 2003-2013, de más de 100,000 voluntarios extraídos de la población general de Copenhague.Los investigadores encontraron que para el periodo 1976-1978 el mínimo de mortalidad se dio para un índice de masa corporal de 23.7, el cual está dentro del rango considerado normal. Para el periodo 1991-1994, sin embargo, dicho índice se incrementó a 24.6 y para el periodo 2003-2013 a 27. Este último valor corresponde al rango considerado como de sobrepeso. De acuerdo con esto, unos pocos kilos de más no solamente no serían dañinos, sino que nos permitirían incrementar nuestra esperanza de vida. Como apuntan los autores, sin embargo, el suyo es un estudio llevado cabo con un tipo específico de población, daneses de raza blanca, que podría no ser válido para otros grupos étnicos. Además de que tendría que ser confirmado por otros estudios independientes. Por otro lado, un índice de masa corporal de 27 significa apenas unos pocos kilos de más con respecto al índice de 25. No habría pues que echar las campanas al vuelo y, por si las dudas, seguir cuidando nuestro peso.Habría que hacerlo para cuidar nuestra salud siguiendo estudios científicos racionales. No para satisfacer las exigencias de las moda, las cuales son bastante caprichosas y cambiantes, más allá de toda racionalidad.",
    "Candelario Pérez Rosales, quien falleció el pasado domingo 1 de mayo, fue, sin duda alguna, un personaje singular, de los que no nacen todos los días. A élla UASLP le debe haber cofundado la física en la institución, sacrificando en más de una ocasión su interés personal. Candelario Pérez nació el 16 de diciembre de 1930 en Peotillos, municipio de Villa Hidalgo, San Luis Potosí. Al terminar la escuela primaria, su padre, de origen campesino y a la sazón comerciante,  lo inscribió en una academia comercial para que aprendiera los principios de la contabilidad. Después de un año de estudios, sin embargo, decidió que aquello no era lo suyo y se enroló en la entonces escuela secundaria de la Universidad Autónoma de San Luis Potosí. Dado que su familia no tenía medios para sostener su educación, Candelario Pérez se inscribió en el turno nocturno de dicha escuela, lo que le permitió trabajar en la tesorería general del estado durante las mañanas. Al terminar la escuela secundaria Candelario Pérez continuó con susestudiosen la escuela preparatoria de la UASLP en donde desarrolló un gran interés por la física y las matemáticas. Esto, a pesar de que “En esos tiempos no había un buen control para la asistencia de maestros, algunos faltaban mucho y uno tenía que suplir por su cuenta los estudios”, según comentarios del propio Candelario Pérez recogidos por José Refugio Martínez Mendoza en su libro “Una vida dedicada a la ciencia: el papel de Candelario Pérez Rosales”, editado por el Museo de la Historia de la Ciencia de San Luis Potosí.Dado su interés por la física, al final de su educación preparatoria en 1951 Candelario Pérez decidió realizar estudios profesionales en esta materia.En esos momentos,sin embargo, no había manera deque los llevara a cabo en la UASLPy se encontró con una disyuntiva: o se iba a la UNAM o a los Estados Unidos, según sus palabras.Como lo relata Martínez Mendoza en su libro, citado anteriormente, Candelario Pérez se decantó por la segunda opción, de manera específica, por la UniversidadPurdue en el estado de Indiana. Esto último,después de escuchar una conferencia impartida en la UASLP por Gustavo del Castillo –también potosino–, quien en esos momentos realizaba un doctorado en físicaen Purdue.Así, Candelario Pérez hizo solicitud en 1952a esta universidad para realizar estudios de licenciatura en física,siendo admitido sin dificultad. Para ingresar aesta universidad, sin embargo, Candelario Pérez tuvo que sortear dificultades económicas puesno contaba con recursos propios para costearse los estudios. Así, se vio forzado a tocar puertas en busca de patrocinio.Intentó conseguirlo, infructuosamente, con el entonces gobernador del estado Ismael Salas.Tampoco lo obtuvo del Consejo Directivo Universitario.Tuvo más suerte con el rector Manuel Nava, quién vio con simpatía sus intenciones de estudiar física –algo que para el común de la gente en San Luis Potosí debe haber resultado poco menos que exótico– y organizó una colecta entre su allegados con el fin de apoyarlo. Así, gracias a la generosidad de sus patrocinadores, Candelario Pérez pudo iniciar sus estudios en Purdue. Tenía Candelario Pérez planes para continuar con una maestría después de concluir sus estudios de licenciatura. No llegó a realizarlos sin embargo, pues poco después de obtener su grado de licenciatura en enero de 1956, fue urgido a regresar a San Luis Potosí por Gustavo del Castillo –quien estaba de regreso en México después de doctorarse– con el fin de que lo ayudara a poner en marcha la escuela de física de la UASLP que recientemente había sido aprobada por el consejo universitario. Candelario Pérez aceptó los planteamientos de del Castillo y decidió postergar sus planes de seguir estudiando y regresó a la UASLP para inaugurar los cursos el 5 de marzo de 1956.Sacrificó así Candelario Pérez sus planes inmediatos para continuar su formación académica en favor del proyecto para introducir a la física profesional en San Luis Potosí. Es decir, consideró que en esos momentos dicho proyecto era más importante que su interés personal de superación académica. La segunda oportunidad de continuar con su formación académica le llegó a Candelario Pérez en 1958, cuando inicióestudios doctorales en la Universidad de Estrasburgo, Francia. Una vez más, sin embargo, hubo de modificar sus planes  cuando se enteró,a los pocos meses de su llegada a Estrasburgo,que Gustavo del Castillo había dejado la UASLP, desanimado por el poco apoyo que encontraba. Ante esta circunstancia, Candelario Pérez resolvió regresar a San Luis Potosí y así evitar la desaparición de la Escuela de Física.  Permaneció Candelario Pérez en la UASLP hasta 1966, cuando a su vez tuvo que emigrar por la precaria situación económica personal que enfrentaba, para incorporarse al entonces naciente Instituto Mexicano del Petróleo. Ahí hizo una brillante carrera como investigador nato que era, hasta su retiro en 2007.Regresó Candelario Pérez a San Luis Potosí en 2015 y tuvimos el privilegio de que aceptara, de manera honorífica y a sus 85 años, impartir un curso sobre historia de la física a estudiantes de la licenciatura en Ingeniería Física de la UASLP. Había aceptado, igualmente, participar en un proyecto para la reconstrucción de los instrumentos científicos que él construyó en la UASLPen la década de los años sesenta. La muerte, desafortunadamente, lo sorprendió el pasado domingo, y con esto vio una vez más interrumpidos sus planes.",
    "En un coloquio sobre la historia de la física profesional en México en el que participé hace algunas semanas en la capital del país, uno de los ponentes caracterizó al desarrollo de esta disciplina en Méxicocomoun proceso al azar en el que las cosas se han dado, no como resultado de una política científica orquestada a nivel federal, que la hubiera hecho avanzar paso a paso, sino por esporádicas iniciativas personales que lo mismo han tenido éxito que encontrado el fracaso; todoesto en función de las circunstancias que han enfrentado. Dado el poco interés que ha despertado la ciencia entre nuestras máximas autoridades, no es sorprendente que ésta haya llegado al país con un retraso considerable. En efecto, tal como la conocemos hoy en día, la ciencia se originó en Europa en los siglos XVI y XVII y empezó a mostrar todo su potencial tecnológico en el siglo XIX. En México, en contraste,  fue hasta 1970 –cuando se creó el CONACyT– que el gobierno federal reconoció formalmente –si bien con altibajos en posteriores administraciones–que una ciencia y una tecnología propiasson dos elementos insustituibles para el desarrollo del país. Con anterioridad, si bien se crearon algunas instituciones para promover la ciencia en México,los recursos con los que se les dotó fueron modestos y no guardaron proporción alguna con las necesidades del país. De este modo,México permaneció ajeno al progreso científico del mundo hasta fechas muy recientes. Ciertamente, hay razones objetivas que lo explican. Por ejemplo, el clima de violencia e inestabilidad política que caracterizó a nuestro país durante buena parte del siglo XIX no fue, sin duda, el mejor caldo de cultivo para el desarrollo de una ciencia mexicana. La explicación resulta, sin embargo, solo parcial si consideramos que el Japón logró, a partir de la segunda mitad del siglo XIX, un desarrollo científico notable en circunstancias muy desfavorables, no radicalmente diferentes de las que prevalecían en nuestro país. Es así inevitable hacer comparaciones.Como es conocido, después de la expulsión de los españoles y portugueses en la primera mitad del siglo XVII, Japónse aisló deliberadamente del mundo por mas de doscientos años y solo mantuvo un contacto,muy restringido, con holandeses y chinos.Este aislamiento tuvo un fin abrupto en la década de los años cincuenta del siglo XIX, cuando el país fue obligado a abrirse al comercio con los Estados Unidos bajo la amenaza de los cañones del comodoro Matthew Perry de la armada de los Estados Unidos. Perry –que participó también en la invasión estadounidense de 1847 a nuestro país– arribóa la bahía de Tokio en 1853 con una flota de cuatroamenazantes “barcos negros”y un carta del presidente norteamericano solicitando la reapertura de Japón. Una vez entregada la carta, Perry prometió regresar al año siguiente por la respuesta. En 1858, tras de las gestiones diplomáticas de Perry y otros  enviados del gobierno norteamericano,elJapón firmó con los Estados Unidos el “Tratado de amistad y comercio” con el que reanudó sus relaciones comerciales con occidente.La claudicación de Japón ante las amenazas de los Estados Unidos fue forzada por las gran disparidad tecnológica que existía entre los dos países. Y esta claudicación no fue un asunto menor, pues el hecho de verse el país obligado, ante las amenazas de una potencia extranjera, a cambiar una política que habían sostenido por más de dos siglos, provocó a los pocos años la caída del régimen feudal que hasta entonces imperaba en Japón y la implantación de un gobierno central encabezado por el emperador. Con fin de prevenir en el futuro nuevas amenazas del exterior, el nuevo gobierno japonés se dio a la tarea de tratar de alcanzar a los países avanzados en su desarrollo científico y tecnológico mediante una reforma del sistema educativo y una política para la introducción de la ciencia y tecnología de occidente en el Japón.Una medida del grado de éxito que alcanzaron en su propósito es la victoria que en 1905 obtuvieron en la guerra contra Rusia, una de las potencias europeas de la época.Podemos igualmente evaluar la pertinencia de las reformas educativas implantadas en Japón por el premio Nobel en física otorgado a HidekiYukawa en 1949. Yukawa fue un físico formado en universidades japonesas en las décadas de los años veinte y treinta del siglo XIX y en el momento de recibir el premio Nobel era profesor de Universidad de Kioto. Esto es, la reforma del sistema educativo japonés alcanzó para que, en el curso de unas cuantas décadas, se pudiera formar a un científico de talla mundial y se pudieran realizar investigaciones de frontera en instituciones japonesas.Si bien no es posible establecer un paralelismo cercano entre México y Japón, dos países con diferencias sustanciales en geografía y cultura, sí podemos apuntar que en el siglo XIXlos dos países sufrieron por igual de convulsiones políticas y agresiones por parte de países más avanzados tecnológicamente. Y, no obstante, el curso que siguió la ciencia en Méxicofue muy diferente delcurso que siguió en Japón: hoy en día este último está a la vanguardia tecnológica, mientras que México sufre de un atraso considerable.Por lo demás, si hemos de aprender de la experiencia japonesa –y de la de otros países asiáticos– no todo estaría perdido y podríamos superar en buena medida nuestro rezago científico en un periodo de tiempo medido en décadas. Esto, si nos ponemos a trabajar fuertemente y de manera coordinada. Todo está en que nos lo propongamos.",
    "No es difícil convencernosde que nuestros ancestros, hace cientos de miles  o millones de años, estuvieron sujetos a todo tipo de amenazas que ponían en riesgo su supervivencia, incluyendo la posibilidad de ser devorados por un animal depredador, lo que no fue,seguramente,la menor de sus preocupaciones. Estabande este modo obligados a tener un ojo en el gato y otro en el garabato a riesgo terminar en las fauces de un tigre dientes de sable o algún otro depredador.Aparentemente, el dicho “echar un ojo al gato y otro al garabato” tuvo su origen en la necesidad, en tiempos pasados, de proteger a la carne colgada en el garabato –instrumento de hierro con punta en forma de semicírculo, que sirve para tener colgado algo, según el diccionario de la Real Academia Española– en contra de los  embates del gato de la casa. Como quiera que haya sido, y ya sea que lo apliquemos a la protección de alimentos o a la preservación de la vida, el dicho apunta a una misma necesidad: que nuestro cerebro se ocupe de dos cosas a la vez, so pena de encontrarnos con sorpresas desagradables.Por supuesto, las consecuencias de descuidar el garabato y perder la reserva de comida no son comparables con aquellas que nos pudieran sobrevenirde un encuentro inesperado con un leopardo o un oso hambriento. Después de todo, si bien sin comida inevitablementepasaríamos hambre,si nos devorara un león no podríamos disfrutar de la comida aunque la tuviéramos. Un estado de vigilia permanente en previsión del  ataque deun depredadorfue así útil para la supervivencia de nuestros ancestros. Y esto, sabemos, no fue exclusivo de nuestra especie y esigualmente importante para otros animales. Así, tenemos que la evolución tuvo que ingeniárselas para desarrollar mecanismos para mantener una vigilia aun durante el sueño cuando la percepción de los alrededores está muy disminuida. ¿Cómo fue resuelto este problema?Según los especialistas, desarrollando dos formas de dormir. Una primera forma con los dos hemisferios cerebrales en sueño profundo, y una segunda con uno de dichos hemisferios en sueño profundo y otro en un estado intermedio de vigilia, lo que se conoce como sueño unihemisférico. Así, se sabe que los cocodrilos pueden dormir, si perciben un peligro, con un ojo abierto y el hemisferio cerebral opuestoa dicho ojo en estado de vigilia. El ojo abierto, además, apunta en la dirección en la que el animal percibe la amenaza. En un artículo publicado en 1999 en la revista “Nature”, se reportan los resultados de un experimento llevado a cabo con un grupo de patos de collar que muestran que estos animales duermen en un estado de semivigilia, con un ojo abierto, en respuesta a la percepción de una situación peligrosa. En el experimento se formaron cuatro grupos de cuatro patos cada uno y se colocaron en línea. Los patos colocados en los extremos de cada línea estaban en una posición más expuesta en comparación con aquellos colocados en el interior de la misma. Como resultado, los investigadores encontraron que los patos de los extremos durmieron con un ojo abierto casi tres veces más frecuentemente que los patos del centro. Una medición de la actividad cerebral mostró, además, que el dormir con un ojo abierto está acompañado con una reducción de la actividad de sueño profundo en el hemisferio cerebral conectado al ojo abierto.Los cocodrilos y los patos, entre otros animales, pueden dormir en un estados de semivigilia. ¿Pasa los mismo con nuestra especie?Si bien sabemos que nosotros no practicamos el sueño unihemisférico,segúnMasakoTamaki –Brown University– y colaboradores, el llamado “efecto de la primera noche”, por el cual muchas personas duermen intranquilas la primera noche que duermen en un lugar desconocido es una manifestación de que uno de los hemisferios cerebrales está más vigilante que el otro durante el sueño, como resultado de la falta de familiaridad con la habitación. Tamaki y colaboradores llegan a esta conclusión en un artículo publicado esta semana en la revista “CellBiology”, en el que reportan los resultados de un estudio de sueño en un ambiente desconocido llevado a cabo con 35 voluntarios. Los investigadores encontraron que había una asimetría en la actividad de cierta región de los hemisferios cerebrales durante la primera noche del experimento y que esta asimetría se correlaciona con la dificultad para conciliar el sueño. Encontraron también que el hemisferio cerebral que mostraba menos actividad de sueño profundo respondía más a ciertos estímulos externos y provocaba una mayor respuesta física por parte del voluntario. Por otro lado, las asimetrías observadas la primera noche desaparecieron en las noches subsecuentes.Basados en sus resultados, Tamaki y colaboradores concluyen que el sueño inquieto que algunos experimentan la primera vez que duermen en un lugar desconocido constituye “un acto de sobrevivencia en un medio ambiente desconocido y potencialmente peligroso, que mantiene un hemisferio parcialmente más vigilante que el otro y que despierta al durmiente cuando se detectan señales externas desconocidas”.O dicho en otras palabras, un acto en el que mantenemos un ojo en el gato, puesto que necesitamos dormir, y otro en el garabato, en previsión de alguna eventualidad. Aunque, por supuesto, esto sólo de manera figurada, pues al contrario de los cocodrilos y los patos, nosotros dormimos con ambos ojos cerrados.",
    "En septiembre de 2014, un equipo compuesto por organizaciones públicas y privadas de Canadá, descubrió en el fondo del estrecho Victoria, en el ártico canadiense, los restos del navío HMS Erebus hundido en 1847. Este navío, juntamente con el HMS Terror, formaba parte de una expedición británica de exploración del ártico encabezada por Sir John Franklin. Dicha expedición, que partió de Inglaterra el 19 de mayo de 1845 con una tripulación de 24 oficiales y 110 marineros, tenía como propósito descubrir una ruta navegable entre los océanos Atlántico y Pacífico a través del ártico canadiense.Lejos de cumplir su objetivo, sin embargo, la expedición de Franklin tuvo un final trágico. El Erebus y el Terror fueron avistados por última vez por balleneros hacia finales de 1845 en la bahía de Baffin. Ambos navíos habrían quedado atrapados en el hielo en septiembre de 1846 obligando a la tripulación a invernar por dos años consecutivos en el ártico, muriendo algunos, incluyendo a Franklin. Al final, los sobrevivientes intentaron llegar caminando hasta un lugar habitado,falleciendo todos en la empresa.La búsqueda de una ruta entre el Atlántico norte y el Pacífico, conocida como el Paso del Noroeste, había sido una obsesión para los británicos por varios siglos. De haber existido dicha ruta, habría acortado considerablemente el trayecto marítimo entre Europa y el lejano oriente. No existía, sin embargo,por los hielos árticos que cerraban el paso a los navíos de la época.En la actualidad, la extensión de los hielos árticos está disminuyendo por efecto del cambio climático y esto ha abierto a la navegación el Paso del Noroeste durante el verano. De continuar con el ritmo actual de calentamiento del planeta, los expertos pronostican que en un futuro no lejano el Paso del Noroeste podría ser navegable todo el año.Y no parece que dicho ritmo vaya a disminuir;de hecho, en los últimos meses hemos visto incrementos récord de temperatura. Así, segúnla agencia Meteorológica de Japón, la temperatura promedio del pasado mes de marzo  superó en 1.07 grados centígrados a la temperatura promedio del Siglo XX. Y para el pasado mes de febrero, la cifra correspondiente fue de 1.04 grados centígrados. En ambos casos, las temperaturas medias son las mayores desde 1891, cuando se iniciaron los registros climáticos. Los datos que proporciona la NASA confirman que los meses de febrero y marzo pasados fueron particularmente cálidos. Según esta agencia, las temperaturas de febrero y marzo pasados fueron 1.71 y 1.65 grados centígrados superiores a la media del periodo 1951-1980. En términos anuales, el año 2014tuvo una temperatura media 0.57 grados centígrados superior al promedio correspondiente al periodo 1961-1990, lo que lo convirtió en el año más cálido desde 1850 cuando se iniciaron los registros climáticos. El año 2015 a su vez superó a 2014 en este respecto, registrando una anomalía de temperatura de 0.75 grados centígrados por arriba del promedio del periodo referido. Además de lo anterior, los expertos esperan que 2016 supere a su vez a 2016 como el año más cálido desde 1850. De cumplirse la predicción de los climatólogos, por tercer año consecutivo se incrementará la temperatura del planeta.Si bien los expertos hacen notar que los incrementos de temperatura observados en los últimos años son parcialmente debidos al fenómeno de El Niño –que tiene una influencia cíclica sobre el clima– su principal causa es el cambio climático debido a la emisión de gases de invernadero a la atmósfera y que aun sin el fenómeno de El Niño se hubieran producido aumentos récord de temperatura.En apoyo a esta opinión, según datos de la NASA 15 de los últimos 16 años más cálidos desde que se tienen registros ocurrieron desde 2001.En cuanto al recubrimiento de hielo en el ártico, éste ha sido el más bajo observado desde que se tienen registros en 1969.Por todo lo anterior es previsible que la extensión de los hielos árticos disminuya de manera paulatina. De hecho, los especialistas contemplan que a mediano plazo el Pasaje del Noroeste sea navegable todo el año. Cuando esto suceda,curiosamente se habrán cumplido las expectativas –a costa de la salud del planeta– de aquellos que hace cientos de años y sin mayor base objetiva creían firmemente en la existencia de un paso entre Europa y Asia por el  Atlántico norte. Y si bien para algunos la búsqueda de este paso no resultó una empresa afortunada, cuando menos tendrán el consuelo depensarse visionarios adelantados a su tiempo.",
    "Da acuerdo con el reporte global sobre la diabetes dado a conocer esta semana por la Organización Mundial de la Salud, se estima que en el año 2014,422 millones de adultos,el 8.5% de la población adulta, vivía con este padecimiento. En comparación, el porcentaje correspondiente al año 1980 era de 4.7%, lo que representa108 millones de personas. Esto es,en el curso de 35 añosel porcentaje de adultos que padecen diabetes en el mundo casi se duplicó, mientras que en términos absolutos los números casi se cuadruplicaron. El crecimiento de la diabetes es, además, más acelerado en los países de ingresos medios.El incremento en la prevalencia de la diabetes ha estado acompañado por un incremento simultáneo en el porcentaje de personas con sobrepeso, que es uno de los mayores factores de riesgo para el desarrollo de la enfermedad. Por ejemplo, de acuerdo con el sitio de Internet de los Institutos Nacionales de Salud de los Estados Unidos, en 1962 alrededor del 46% de la población adulta estadounidense tenía sobrepeso, mientras que en 2010 este porcentaje se había elevado hasta el 75%, con 31% y 5% clasificados como obesos y extremadamente obesos, de manera respectiva.Y como sabemos, México no se queda atrás en este respecto, ocupando el segundo lugar entre los países con mayor porcentaje de obesos, solamente por debajo de los Estados Unidos.En esta situación, cabe preguntarse por las razones de dicho incremento, que alcanza las dimensiones de una catástrofe de salud pública, y que guardarelación, obviamente, con la comida que ingerimos. Según los cánones establecidos por los nutriólogos desde hace varías décadas, una alimentación sana debe limitar la ingesta de grasas saturadas. Así, en nuestra alimentación hemos sustituido las grasaspor azúcares –sustituimos, por ejemplo, el yogurt normal por uno bajo en grasas en el que la pérdida de sabor se compensaañadiendo azúcar–las cuales, según algunos especialistas,son  los verdaderos culpables de la epidemia de obesidad que nos aqueja. Un interesante artículo periodístico aparecido esta semana en el periódico británico “TheGuardian” nos relata las circunstancias en las que se generó la idea según la cual las grasas son dañinas para nuestra salud, particularmente por su asociación con las enfermedades cardiovasculares. En 1955, el presidente Dwight Eisenhower tuvo un ataque cardiaco, mismo que fue hecho público al día siguiente por su médico. Éste, además, aprovechó para recomendar a los estadounidenses que se cuidaran de sufrir un problema similar parando de fumar y reduciendo la ingesta de grasas y alimentos ricos en colesterol. Para hacer estas recomendaciones, el médico del presidente citó a AnselKeys, un nutriólogo de la Universidad de Minnesota, autor de la hipótesis según la cual existe una asociación entre el consumo de grasa y las enfermedades del corazón.Por la forma en que se dio a conocer, la teoría de Keysno podía haber tenido un mejor lanzamiento público, por más que Eisenhower tuviera una historia médica plagada de problemas cardiacosque lo llevaronfinalmente a la muerte en 1969, a pesar de haber seguido una dieta baja en grasa.Y a pesar también del nutriólogo británicoJohn Yudkin, quien era profesor del “Queen Elizabeth College” en Londres. Yudkin tenía la seguridad de que detrás de los problemas cardiacos estaban los azúcares y no las grasas.Desafortunadamente para él, sin embargo, Yudkinno tenía la misma capacidad de relaciones públicas que tenía Keys y sus ideas fueron desechadas a favor delas de este último. Así, por varias décadas  hemos vivido con la creencia de que el consumo de alimentos ricos en grasas saturadas y colesterol es dañino para nuestra salud.En la actualidad, si bien ésta es aun la opinión prevaleciente, hay investigadores queconsideran que no hay una evidencia sólida que conecte al consumo de grasas saturadas con la obesidad y el desarrollo de enfermedades cardiovasculares.  En su lugar, postulan que, efectivamente, tal como lo defendía Yudkin, el verdadero villano son los azúcares y no las grasas saturadas. No es difícil entender, sin embargo, que encontrar la verdad sobre esta materia, con evidencia científica y no empleando otro tipo de técnicas, no es una empresa sencilla por lo complejo de los experimentos involucrados que, además y por necesidad, se extienden a lo largo de muchos años.Por lo demás, si después de que se despeje la polvareda actual resulta que debemos evitar a los azúcares más que a las grasas la sorpresa posiblemente no sería grande.Después de todo, hemos sido consumidores de grasas saturadas por millones de años y de azúcares por apenas unos pocos miles de años, en el mejor de los casos. Sin dejar de considerar que la teoría sobre la toxicidad de las grasas saturadas, lejos de ayudarnos a mantener una línea esbelta nos ha deteriorado la figura de manera sustancial. Y algo debe de andar mal en consecuencia.",
    "Como es sabido, la conquista española de México hace 500 años tuvo un efecto devastador sobre la población nativa que se vio reducida en un 90% en el curso de un siglo. Esto, debido al maltrato y a las condiciones insalubres que los conquistadoresimpusieron a los indígenas,pero sobre todo por las enfermedades que los primeros importaron a México y para las cuales los nativos no tenían inmunidad biológica. Las enfermedades infecciosas importadas por los españoles, en forma particular la viruela, han sido parcialmente responsabilizadas por la caída de Tenochtitlán en manos de Cortés.Y lo que ocurrió en México, consignado en relatos de la época, tuvo contrapartes a lo largo de todo el continente americano. Un artículo aparecido esta semana en la revista “ScienceAdvances” echa un vistazo a la catástrofe poblacional que sufrió nuestro continente con la conquista europea desde un punto de vista novedoso: el que nos ofrece la genética poblacional. Dicho artículo fue publicado por un grupo internacional de investigadores de Australia, Estados Unidos, Perú, México, Bolivia, Chile y Argentina, encabezados por Bastien Llamas de “TheUniversity of Adelaide”, Australia. En el artículo de referencia, Llamas y colaboradores reportan un estudio genético mitocondrialllevado a cabo con92 momias yesqueletos, con antigüedades entre los 500 y los 8,600 años, provenientes de sitios arqueológicos en Perú,los Estados Unidos, México, Bolivia, Chile y Argentina. Se sabe que el ADN mitocondrial se hereda solamente a través de la madre –es decir que la información genética heredada no se mezcla con la del padre– y esto abre una ventana para reconstruir la historia genética de restos antiguos por la vía materna. En el caso reportado por Llamas y colaboradores,el ADN mitocondrial permitió vislumbrar la historia genéticade los esqueletos estudiados,remontándose en el tiempo hasta el momento en el que el primer grupo de pobladores de América inició su viaje hacia el Nuevo Mundo y se aisló de otros grupos en Siberia . De manera adicional, la técnica de ADN mitocondrial permite determinar cuando fue que grupos humanos que han permanecido aislados por largo tiempo tuvieron un ancestro común, y en este sentido, Llamas y colaboradores concluyen que los grupos a los que pertenecían los individuos cuyos esqueletos fueron estudiados compartieron un ancestro en Siberia hace unos 18,000-25,000 años.La opinión prevaleciente entre los expertos es que los primeros pobladores de América cruzaron desde el extremo este de Siberia hacia Alaska, a través de Beringia, el puente de tierra que se formó entre Asia y América durante el último periodo glacial. Hasta hace unos 16,000 años, sin embargo, las capas de hielo que cubrían el norte del continente americano impidieron el tránsito hacia el sur, de modo que Bastien y colaboradoresespeculan, en base a sus resultados,que los primeros pobladores de América podrían haber quedado aislados en el este de Beringia hasta por 9,000 años, antes de que el retroceso del hielo les permitiera viajar hacia el sur, posiblemente por una ruta marítima a lo largo de la costa del pacífico.Por otro lado, según escriben Bastien y colaboradores“todos loslinajes mitocondriales antiguos detectados en este estudio están ausentes en los grupos de la actualidad, lo que sugiere una gran velocidad de extinción”. Los investigadores concluyen que un análisis de sus datos apoya un escenario en el cual la colonización europea causó una perdida sustancial de linajes precolombinos. El estudio genético de Llamas y colaboradores coincide de este modo con las historias de catástrofe poblacional en el continente americano en el siglo XVI. La situación que se dio en el mundo hace 500 años, en el que una porción del planeta que estuvo aislada por algo así como 20,000 años fue invadida y apabullada por otra porción más avanzada tecnológicamente, fue inédita y no se  vislumbra que pueda repetirse en un futuro previsible. Para esto tendríamos que esperar por un tiempo indeterminado hasta que se descubran otros mundos fuera del sistema solar que podamos conquistar –si no es que nos conquistan antes a nosotros.Al margen de lo anterior,podríamos preguntarnos qué hubiera sucedido con la conquista del Nuevo Mundo si los microbios en lugar de ayudar a los españoles hubieran estado del lado de  los nativos americanos. Así como estuvieron del lado de los terrícolas en la novela de H.G. Wells “La guerra de los mundos”. Esto habría balanceado la superioridad tecnológica de los conquistadores. No fue así, sin embargo, con el resultado conocido.  O sea, que se cumple el dicho según el cual al perro más flaco se le cargan las pulgas.",
    "¿Debemos confiar en los resultados científicos? Dadas las aplicaciones que se han derivado de las diferentes ciencias, tal parecería que la respuesta obvia es que sí debemos hacerlo.Se da el caso, sin embargo, de personas o grupos de personas quebajociertas circunstancias, no solamente nieganevidencia científica, sino que la usan paraafirmar prejuicios en contra de la misma, como lo discutimos en lo que sigue.El auge científico que vivimos tiene su origen en la Europa de los siglos XVI y XVII. Uno de los científicosmas relevantesde esos siglos –y de los que les siguieron– es Isaac Newton.Entre muchas otras cosas, Newton desarrolló una teoría matemática con la que pudo explicar por qué los planetas se mueven siguiendo elipses alrededor del sol.Esta teoría ha resultado altamente exitosa, y entre muchas otras aplicaciones ha sido empleada por la NASA, con éxito evidente, para lanzar y dirigir las naves espaciales que han visitado otros planetas.Los grandes logrosde la física inspiraron el desarrollo de otras ciencias. El éxito que éstas han alcanzado, no obstante, ha dependido de la complejidad de sus sujetos de estudio. Los más simples –con todo y lo complejos que pueden llegar a ser– son los que estudia la física –planetas, átomos, materiales sólidos, etc.–. Más complicados son los de la biología –seres vivos–, y todavía más los de la sicología; por no mencionar a la sociología, que estudia a grupos de humanos.    Así, mientras que la física y la biología han tenido éxitos fenomenalesque han derivado en la ingeniería y la medicina modernas, la sociología es más modesta en este sentido. Con todo, dadas sus implicaciones, resulta de enorme interés estudiar aplicaciones de las ciencias del comportamiento humano al desarrollo social. En la semana que hoy termina, una de esta aplicaciones –en ciencia política– fue discutida en un artículo de divulgación publicado en el sitio de Internet “TheConversation” por John Cook and Margaret Crane de la “University of Queensland”, Australia, y se refiere a una posible técnica a emplear para descarrilar a Donald Trump en su carrera hacia la presidencia de los Estados Unidos.Cook y Crane hacen notar que, según el sitio “Politifact”,el 78% de las afirmaciones que hace Trump caen en las categorías de 1) falsas en gran medida, 2) falsas o 3) rotundamente falsas, y apenas el 3% son catalogadas como verdaderas. El nivel de apoyo de los seguidores de Trump, sin embargo, no ha decaído por la falsedad de sus declaraciones y, por lo contrario, se ha fortalecido. Según los sicólogos, las causas que motivan este comportamiento, aparentemente irracional, son equivalentes a aquellas que mueven a los que niegan una evidencia científica si ésta contradice alguna creencia propia; en cuyo caso, en lugar de debilitar dicha creencia la fortalecen. Un ejemplo típico es el cambio climático que es apoyado por una mayoría de científicos del clima y que, sin embargo, tiene su legión de no especialistas que lo rechazan.Así, Cook y Crane consideran que sería poco efectivo tratar de cambiar las opiniones de aquellos que apoyan a Trumpponiendo en evidencia la falsedad de sus afirmaciones, y que sería más inteligente enfocar las baterías hacia los indecisos, confiando que tengan una mente suficientemente abierta para reaccionar de manera objetiva ante la evidencia. Para esto se emplearía una teoría de inoculaciónque los sicólogos han desarrollado para combatir la negación científica.Al igual que en el caso de una vacuna biológica, la negación científica se combatiría exponiendo a la persona a una forma debilitada del “virus de desinformación”, con el fin de que desarrolle “anticuerpos” para combatir la enfermedad en pleno.  Para esto, primeramente se presentan los hechos tal cual son, enseguida el mito  relacionado y finalmente se explican las técnicas que el mito emplea para distorsionar los hechos. Dichas técnicas incluyen el empleo de expertos falsos ylasobre simplificación de problemas complejos –como el relativo al muro que Trump quiere construir en la frontera con nuestro país, que es imposible de realizar por costosoy que resultaría inútil según algunos expertos–. Incluyen, igualmente, la presentación de opiniones individuales como representativas de la opinión contraria de todo un colectivo, y el empleo de teorías de conspiración como aquellas que emplean los que niegan el cambio climático,afirmando que el apoyo científico almismo es en realidad resultado de un complot por parte de una mayoría de científicos que se han puesto de acuerdo para engañar alagente.¿Funcionaría la teoría de inoculación en contra de Trump? Cook y Crane no están seguros por lo complejo de la arena política en la que se aplicaría. La predicción de comportamiento de un grupo humano es de suyo problemático y en este caso Cook y Crane hacen notar que hay elementos que lo hacen aun más complicado, incluyendo la insatisfacción de la gente con el establishment político en los Estados Unidos.Por lo pronto, lo que si resulta inocultable es que1) a pesar del demostrado valor del conocimiento científico –medido en función de sus aplicaciones– hay quien cierra los ojos cuando confronta evidencia objetiva contraria a sus creencias y 2) que de todo se vale si de grilla se trata.",
    "¿Qué tantos son cincuenta años en la historia del mundo?Si bien medio siglo es un periodo de tiempo relativamente corto –incluso menor que la expectativa de vida de una persona en la actualidad– es lo suficientemente largo para que durante el mismo ocurran cambios sustanciales en el mundo. Durante la primera mitad del siglo XX, por ejemplo, acontecieron dos guerras mundiales que alteraron el equilibrio global, y marcaron el declive de la Gran Bretaña como potencia imperial y la ascensión de los Estados Unidos y la Unión Soviética como países dominantes. Igualmente, a lo largo de la segunda mitad del siglo XX fuimos testigos del enfrentamiento entre estos dos países en la forma de la Guerra Fría,al igual que del fin de la mismacon la disolución de la Unión Soviética. Esto últimodejó a los Estados Unidos en solitario como potencia dominante a nivel global.En el ámbito de la historia local, en días pasados me dediqué arevisar algunos aspectos de la historia de la Universidad Autónoma de San Luis Potosí a lo largo del último medio siglo; en particular, la historia del surgimiento y consolidación de la física en nuestra universidad.Todo esto con motivo de mi participaciónel pasado miércoles en el coloquio “Historia del surgimiento de la física profesional en México”, organizada por la UNAM en honor del Dr. Jorge Flores Valdez, quien fue uno de los creadores en 1984 del Sistema Nacional de Investigadores. La física llegó a San Luis Potosí en 1955 de la mano de Gustavo del Castillo y Gama, quien un año antes había obtenido un grado doctoral en física por parte de“PurdueUniversity”, Indiana, Estados Unidos.Una vez de vuelta en México y hacia finales de ese año, del Castillo  consiguióque la UASLP creara tanto un Instituto de Física como una Escuela de Física e invitó a Candelario Pérez Rosales –también graduado de “PurdueUniversity”– a conjuntar esfuerzos para iniciar la operación de esta última.Así, la escuela inició cursos el 5 de marzo de 1956 con 9 estudiantes, ycon Gustavo del Castillo y Candelario Pérez Rosalescomo profesores de física y matemáticas, en forma respectiva. Como lo consigna Candelario Pérez en su libro “Física al amanecer” –editado por la UASLP–la prensa local informó sobre el suceso con el siguiente encabezado a ocho columnas: ”Desde hoy San Luis formará sus propios físicos. La nueva facultad nace con los mejores auspicios. El progreso de México requiere de un gran número de físicos.” Los augurios de la prensa, sin embargo, tardaron en cristalizar, pues si bien la física en nuestra universidad contó en sus inicios con dos promotores fuera de serie, Gustavo del Castillo y Candelario Pérez,éstos posiblemente se adelantaron a su tiempo ya quela UASLP en las décadas de los años cincuenta y sesenta no estaba todavía  en condiciones de albergar y hacer crecer un campo en el que la enseñanza y la investigación están fuertemente ligados.Ya que no tenemos aquí el espacio suficiente para relatar con detalle el desarrollo de la física en nuestra universidad, baste mencionar que en1978 se crean cuatro plazas de investigador a tiempo completo para el Instituto de Física y que esto constituyó un  acontecimiento clave, pues uno de los problemas que tuvo la física en sus inicios en la UASLP fue precisamente la falta de plazas de tiempo completo.Años después, en 1983, se crea el programa de doctorado en física que tuvo su primer egresado en 1986 –Pedro Villaseñor González, con una tesis dirigida por Jesús Urías Hermosillo–, que significó el primer grado doctoral en física otorgado en México por una universidadfuera del área metropolitana de la Ciudad de México.A partir de 1984, y tomando impulso con la creación ese año del Sistema Nacional de Investigadores, la física en la UASLP ha crecido de manera paulatina y la Universidadhoy en día se ha consolidado como uno de los centros de investigación más importantes en esta disciplina en México.    Por lo demás, el desarrollo de la investigación en la UASLP no se ha limitado a la física y muchos otros campos han tenido igualmente un notable crecimiento. Una medida de la fortaleza de la UASLP en el campo de la investigación es el número de sus profesores que son miembros del Sistema Nacional de Investigadores y que hoy en día alcanza los 464. Una medida adicional de esta fortaleza nos la da la velocidad con que este número está creciendo. Así, tenemos que en los cuatro años que correspondenal periodo del Rector, maestro en arquitectura Manuel Fermín Villar Rubio, este número creció en más de cincuenta por ciento. MI primer contacto con la UASLP ocurrió en el año de 1967 cuando ingresé como estudiante a la entonces Escuela de Física hoy Facultad de Ciencias. En esa época la Universidad contaba con una infraestructura muy pobre y con recursos para operar en extremo limitados. Hoy en díapodemosver un cambio drástico. Así, y por poner sólo un ejemplo,en cincuenta años la UASLP ha pasadode ser una institución en la que aun una máquina de escribir eléctrica era un lujo impensable, a ser una universidadque cuenta con sofisticados laboratorios de investigación. Cantaba Gardel que veinte años son nada, lo cual claramente no es una verdad universal. En el caso de la UASLP los últimos cincuenta años han sido tanto que hoy en día está irreconocible.",
    "¿Fue el mundo creado por Dios o bien éste se creó solo? Al margen de que, puesta así,la pregunta es poco precisa, la opinión mayoritariamente abrumadora entre los científicos es que no hay manera de averiguarlo; no la hay al menos empleando los métodoscaracterísticos de su profesión.Así, no sorprende que un artículo publicado en la revista científica PlosOne el pasado mes de enero, en el que se hacenrepetidas referencias al “Creador” –así, con mayúscula inicial–,haya provocado una serie de comentarios negativos; tan negativos que incluso el editor de la revista decidió en días pasados retirarlo de la misma.El artículo de referencia fue publicado por tres investigadores chinos encabezados por  Ming-JinLiude la Universidad de Ciencia y Tecnología deHuazhong, Wuhan, China, y en el mismo se describen los resultados de una investigación sobre la biomecánica de la mano humana. De manera específica, describe una investigación dirigida aponer en claro cómo los dedos y falanges de la mano humana se coordinan entre sí para realizar todas las diferentes y complejas funciones que les son características –y que nos diferencian de otras especies en el planeta.Para esto, los investigadores conjuntaron un grupo de voluntarios, 15 mujeres y 15 hombres, a los cuales se les sentó en una mesa con el brazo derecho descansandocómodamente cerca del borde de la misma y se les pidió realizar 33 tareas manuales. Éstas incluían desde sujetar con fuerza varas de diferentes diámetros, hasta accionar unas tijeras o tomar objetos de manera delicada con los dedos índice y pulgar. Con el objeto de seguir en detallelos movimientos de los dedos y sus falanges, los voluntariosusaron guantes especiales equipados con resistencias eléctricas que permitieron registrar los ángulos cambiantes de las coyunturas de los dedos. De este modo, los investigadores registraron los complejos movimientos que puede realizar la mano humana y la coordinación que existe entre los diferentes dedos para realizar cada tarea.En el artículo en el que describen los resultados de su estudio, sin embargo, los investigadores incluyeron en varias ocasionesreferencias al Creador, que habría diseñado manos capaces de realizar hábilmente movimientos de una gran complejidad. En el resumen del artículo, por ejemplo, incluyen la siguiente frase: “La conexión funcional explícita entre la arquitectura y la coordinación de la mano indica que las características biomecánicas de la arquitectura de los tendones que conectan músculos y articulaciones es un diseño del Creador para llevar a cabo una multitud de tareas diarias de una manera confortable”. Hacen, además, comentarios en la misma dirección en dos ocasiones más en el texto principal del artículo.La reacción negativa a los comentarios desafortunados de Min-JinLiu y colaboradores fue dirigida más a la revistaPlosOne que a los autores del artículo y fue iniciada por James McInerney de la “Universityof Manchester” quien calificó a dicha revista de ser una broma por no cuidar los textos que publica. Relativo a esto último, hay que hacer notar que para ser aceptado por una revista científica, un artículo debe pasar por una revisión crítica por otros expertos no involucrados con la investigación. Por alguna razón no determinada, sin embargo, los árbitros del artículo de referencia, al igual que el editor de la revista, no detectaron el uso del lenguaje inapropiado empleado por los autores y lo habrían aceptado para publicación sin comentarios.En un mensaje posterior, McInernyescribió que su comentario original fue fuerte debido a que el creacionismo había sido un fastidio para él por más de veinte años. Como sabemos, han existido conflictos entre la ciencia y la religión desde el siglo XVI cuando la primera empezó a tomar su forma actual y empezó a cuestionar lavisión del mundo que ofrecía la religión. Si bien con el tiempo las asperezas entre ciencia y religión se han ido limando en la medida en que se han deslindado sus respectivos campos de acción, los conflictos no han desaparecido del todo y, por ejemplo, han sobrevivido en la forma del movimiento creacionista.No resulta claro si Min-Jin Liu y colaboradores son adeptos a este movimiento y esto es lo que habría motivado sus referencias al Creador–las cuales resultan innecesarias y nada aportan al artículo,de no ser publicidad, así sea ésta negativa–.Los autores, por su lado, se defienden señalando que el inglés –lenguaje en el que escribieron el artículo– no es su idioma nativo y que para ellos la palabra Creador no tiene la connotación que se le ha dado y que consideran que la palabra “Naturaleza” se acerca más a lo que quisieron decir.De un modo u otro, los autores de un artículo de investigación tienen el derecho de expresarse como lo crean más conveniente, al mismo tiempoque el editor de una revista científica tiene la obligación y la prerrogativa de aceptar o rechazar un texto sobre la base de su contenido y de las políticas editoriales de la revista en cuestión. En estas circunstancias, PlosOne, que se supone es un foro científico de alto perfil con una rigurosa evaluación editorial, aparece como elúnico culpable del incidente. Tan fácil como hubiera sido haberle pedido a Min-JinLiu y colaboradores que eliminaran cualquier referencia inapropiada en su texto –o bien que aclararan lo que habrían querido decir.",
    "Como sabemos, la historia corta “La Metamorfosis” de Franz Kafka inicia cuando  Gregor Samsa, su personaje central, se despierta una mañana convertido en un monstruoso insecto. Si bien el autor no proporciona información detallada al respecto,algunos en el pasado han asumido que Gregor se transformó en una especie de cucaracha gigante. Vladimir Nabokov, el autor de “Lolita”, sin embargo, no estuvo de acuerdo con esta interpretación. Según este autor, la espalda y el vientre abultados del insecto tal como los describe Kafka, indican que se trataba de un escarabajo y que en lo único que recordaba a las cucarachas era en el color café de su cuerpo. Analizando pasajes del cuento, Nabokov incluso determina que el insecto media alrededor de un metro de largo.La morfología del bicho en el que se transformó Gregor, sin embargo, no es relevante a la historia de Kafka. De hecho, como apunta Nabokov, su transformación no fue completa, pues todavía acostado en la cama, cuando aun no era consciente de dicha transformación, Gregor abría y cerraba los ojos pensando que alucinaba, y “un escarabajo normal no tiene párpados y por tanto no puede abrir y cerrar los ojos”; se habría tratado entonces de un “escarabajo con ojos humanos”.Por otro lado, al margen de la discusión entomológica alrededor de Grego rSamsa,  vale comentar que las cucarachas –de tamaño real– con todo y lo repugnante que a algunos nos resultan, tienen también sus aspectos notables. En particular, como bien sabemos, las cucarachas pueden moverse con gran rapidez y transitar a través de rendijas sumamente reducidas. Sabemos igualmente que son sumamente resistentes y pueden sobrevivir a un pisotón sin daño aparente. Y todo esto nos puede inspirar aplicaciones, como veremos a continuación.En un artículo publicado el 8 de febrero pasado en la revista “Proceedings of the National Academy of Sciences” por Kaushik Jarayam y Robert Full, investigadores de la University of California, Berkeley, se describen los resultados de un estudio llevado a cabo con cucarachas nativas de los Estados Unidos. El propósito de dicho estudio fue el de evaluar la habilidad de las cucarachas para transitar a través de rendijas estrechas y espacios limitados verticalmente. Mediante filmaciones de alta velocidad, los investigadores encontraron que las cucarachas estudiadas, con una altura de aproximadamente 12 milímetros, son capaces de atravesar una rendija de 3 milímetros de altura en una fracción de segundo, comprimiendo su cuerpo en un 40-60%. El procedimiento empleado por los cucarachas para transitar por la abertura es complejo e involucra una serie de pasos. Primeramente, la cucaracha explora con sus antenas el espacio a su alrededor en busca de una ruta de escape. Una vez que detecta una rendija introduce sus dos antenas y decide si el espaciomás allá de la misma es una ruta viable. En caso positivo, introduce repetida y rápidamente la cabeza por la rendija y empieza a introducir el resto del cuerpo, primero el tórax y después el abdomen, ayudándose con un rápido movimiento de patas. Durante el pasaje del tórax por las rendijas más estrechas es cuando el animal experimenta las más grandes fuerzas de compresión, al mismo tiempo que el movimiento de las patas queda severamente limitado. Es en esta etapa en la que la cucaracha está más expuesta a fracasar en su intento de cruzar la rendija.Jarayam y Full también estudiaron la habilidad de las cucarachas para moverse a gran velocidad a través de espacios restringidos verticalmente. Encontraron que las cucarachas pudieron moverse a una velocidad aproximada de medio metro cada segundo, aun en espacios con 6 milímetros de altura –la mitad de la altura del animal– y que solo para alturas menores reducen esta velocidad.Esto implica que aun con el cuerpo severamente deformado y bajo grandes fuerzas de compresión,las patas de la cucaracha no pierden su habilidad para impulsarla.Con el objeto de determinar cual es el grado de fuerzas de compresión que puede soportar una cucaracha, los investigadores las sometieron a diferentes pruebas y encontraron que pueden soportar fuerzas de hasta 900 veces su peso sin sufrir daño.Las cucarachas basan sus notables habilidades en una cubierta externa al cuerpo –exoesqueleto–formada por partes rígidas, lo que le permite dar fuerza a las patas, y que es al mismo tiempo flexible y capaz de sufrir grandes deformaciones sin sufrir daños. Inspirados en las cucarachas, Jarayam y Full diseñaron y construyeron un robot capaz de comprimir su cuerpo en un 54% y sufrir fuerzas de compresión de 20 veces su peso, el cual podría deslizarse a través de espacios reducidos.  Las cucarachas no tienen buena fama y posiblemente no encontremos muchos que simpaticen con ellas. Aun la familia de Gregor se horrorizaba con su aspecto, que si bien no necesariamente era el de una cucaracha, no era tampoco demasiado diferente. Quizá, no obstante, les pudiéramos encontrar algunos fans entre aquellos a losque les sirven de fuente de inspiración.",
    "En 1856 fueron descubiertos en una mina del valle del río Neander, Alemania, los restos de un esqueleto humano, incluyendo la parte superior del cráneo, dos fémures, huesos de los dos brazos y parte de la pelvis. El significado del descubrimiento no fue entendido de inmediato y sólo más tarde los especialistas se dieron cuentade que se trataba de una especie diferente a la nuestra. Los individuos de esta especie, con similitudes pero también diferencias con respecto a nosotros, son ahora conocidos como neandertales, por el lugar en que sus restos fueron reconocidos por vez primera.Los neandertales, ciertamente, compartían algunas características con nosotros –empezando porque caminaban erguidos– al mismo tiempo que eran diferentes. Anatómicamente, si bien no eran más altos que nosotros, sí eran más corpulentos, con un tórax más voluminoso y en forma de barril, y huesos de brazos y piernas también más gruesos y cortos.Adicionalmente, carecían de mentón, tenían ancha la nariz, un cráneo menos elevado que el nuestro yhuesos de las cejas muy prominentes.La imagen de los neandertalesque nos han proporcionado los expertosha evolucionado grandemente a lo largo de los años que han transcurrido desde que conocemos de su existencia. Hoy en día esta imagen les favorece más que hace 150 años, cuando los neandertaleseran poco más que simios peludos con un mínimo de inteligencia. Entre otras cosas, ahora sabemos quefabricaban y empleaban herramientas,  enterraban a sus muertos, desarrollaron estrategias de caza y se pintaban el cuerpo de colores. Se sabe inclusoque tenían en promedio un cerebro más grande que el nuestro –aunque no se ha probado, por ejemplo, que tuvieran la capacidad de hablar.Los neandertales, ciertamente, han incrementado considerablemente su estatus entre nosotros en las últimas décadas. Y lo han hecho a un  grado tal que ahora se les reconoce como primos lejanos nuestros, de  los que evolutivamente nos separamos hace un medio millón de años. Con todo y ser primos lejanos, sin embargo, la apariencia física de los neandertales era diferentede la de los hombres modernos con los que convivieron hasta hace unos 40,000 años. Esto, al menos, según las reconstrucciones de la anatomía de los neandertales llevadas a cabo por especialistas que los muestran como individuos con los que posiblemente no nos gustaría encontrarnos en una noche oscura en un callejón sin salida.Esto último, sin embargo, aparentemente no era compartido poralgunos ancestros nuestros,contemporáneos delos neandertales. En efecto, mediante estudios genéticos llevados a cabo con restos fósiles, se ha encontrado que los humanos modernos, con la excepción de los africanos, comparten información genética con los neandertales. Esto ha sido interpretado como evidencia de que ambas poblaciones se cruzaronhace unos 50,000 años, presumiblemente en el Medio Oriente, una vez que los humanos modernos salieron de África hacia Europa y Asia. Así,de ser cierta esta hipótesis, nuestros ancestros, lejos de evitar o hacer la guerra a los neandertales, se habrían cruzado con ellos.Un artículo aparecido esta semana en la revista “Nature” va todavía más lejos y sugiere que los contactos entre hombres modernos y neandertales ocurrieron en varias ocasiones y por un tiempo considerablemente más largo de lo que hasta ahora se pensaba. En particular, ocurrieron hace unos 100,000 años, posiblemente en el Medio Oriente, entre una población de neandertales y un grupo de hombres modernos provenientes de África. Dicho artículo fue publicado por un grupo internacional de investigadores encabezados por MartinKuhlwilm, del Instituto Max Planck de Biología Evolutiva en Leipzig, Alemania.Kuhlwilm y colaboradores basan sus conclusiones en un análisis genético de un fragmento de hueso neandertal descubierto en una cueva en las montañas de Altai en Siberia. Dicho análisis muestra que, si bien este primer grupo de hombres modernos que salió de África terminó por extinguirse, una pequeña fracción de sus genes sobrevivió como parte del genoma neandertal asiático. En contraste, un estudio genético de neandertales europeos llevado a cabo por los mismos autores no mostró una contribución equivalente de genes de hombre moderno. A lo largo de los últimos años se han acumulado evidencias del cruce de la población neandertales con nuestra especie. Esto, se pensaba, habría ocurrido después de que estos últimos emigraron desde África hacia Europa y Asia hace unos 60,000 años. Los resultados de Kuhlwilm y colaboradores sugieren que los contactos entre ambas especies en realidad ocurrieron mucho antes, al menos desde hace 100,000 años. Hay que tomar en cuenta, no obstante, que los cruces entre nuestra especie y las de los neandertales no es algo que sea aceptado enforma unánime por los expertos. Así, una explicación alternativa a lo encontrado por Kuhlwilm y colaboradores es que en realidad los fragmentos de genoma que aportarían evidencias del cruce entre neandertales y humanos modernos, es en realidad parte del genoma original común a ambas especies antes de que se separaran hace medio millón de años.Por lo demás, de ser cierta la explicación de Kuhlwilm y colaboradores, se demostraría que, efectivamente, en gustos se rompen géneros, y que nuestros ancestros, en lo que se refiere a escoger pareja, tenían una actitud bastante liberal.",
    "La red Internet –sin duda, unos de los elementos más distintivos del siglo XXI– ha transformado en no poca medida nuestro estilo de vida. Por medio de la Internet, por ejemplo, podemos mantenernos en contacto permanente –y a un costo accesible– con un grupo de personas de nuestra elección. Podemos igualmente consultar documentos en línea –muchas veces sin costo– de una manera tal que en pocos años hará que las bibliotecas tal como las conocemos pasen a la historia –como, de hecho, lo están ya haciendo las librerías tradicionales en los Estados Unidos– y cambien su rol al de almacenes de documentos impresos. Ciertamente, la red Internet ha impactado muchos aspectos de nuestra vida; y lo que es más asombroso: todo esto ha ocurrido en el espacio de poco más de dos décadas.El teléfono inteligente, que apareció en escena de manera masiva apenas despuntaba el siglo XXI, se ha convertido en uno de los protagonistas de la revolución impulsada por la Internet. Si bien, atendiendo a su propósito original –en la versión de Alejandro Graham Bell–, los teléfonos inteligentes tendrían la función primordial de posibilitarnos la comunicación verbal a distancia, el uso que les damos en la práctica es muy variado, al grado de que su función como trasmisores de sonidos  por lo general no es la más frecuente. Así, usamos los teléfonos inteligentes para leer en línea las últimas noticias o para consultar la predicción del clima; lo mismo que para enviar mensajes de texto, oír música o ver videos. También como agenda telefónica, como cámara fotográfica, como calculadora o como reloj despertador. Incluso los llegamos a usar como lámparas de emergencia si la situación lo amerita. Por la manera como los utilizamos, los teléfonos inteligentes hoy en día se acercan más a las navajas suizas que a los aparatos telefónicos tal como eran concebidos en la era previa a  la Internet.  En estas circunstancias, no es sorprendente que el crecimiento en el número de teléfonos inteligentes en el mundo haya sido extremadamente rápido, al grado tal que en la actualidad existen más de dos mil millones de estos aparatos en el mundo –un tercio del número de habitantes del planeta.   Igualmente, dado que los teléfonos inteligentes son en realidad computadoras en miniatura, tampoco es sorprendente que sea frecuente la aparición de nuevas aplicaciones para los mismos. Una de éstases tema de un artículo publicado esta semana en la revista “Science Advances”. En el mismo se propone la creación de una red de teléfonos inteligentes para detectar la ocurrencia de un terremoto y alertar a la población sobre el evento con el tiempo de antelación suficiente para que pueda ponerse a salvo. El artículo fue publicado por un grupo de investigadores encabezado por Quingai Kong de la “University of California, Berkeley”.Para detectar la ocurrencia de un terremoto, Kong y colaboradores proponen hacer uso del acelerómetro con el que están equipados los teléfonos inteligentes. Al generarse un terremoto, el acelerómetro detectará las sacudidas producidas por el mismo y enviará una alerta a una estación central que recibirá reportes similares de otros teléfonos participantes en la red. Dicha estación hará un análisis de todos los reportes recibidos y sobre la base de los mismos determinará en tiempo real la magnitud del sismo y el lugar del epicentro. Esta información será inmediatamente enviada a la población a través de la red de teléfonos inteligentes con el fin de que tome las providencias del caso.     Lo anterior, por supuesto, presupone que los teléfonos inteligentes serán capaces de distinguir entre las sacudidas debidas a un terremoto y aquellas que son propias de su uso normal. Esto lo hará mediante un programa, desarrollado por Kong y colaboradores, el cual se carga en el teléfono. Dicho programa es supuestamente capaz de detectar un sismo de magnitud 5  a una distancia de 10 kilómetros. Su operación, además, no impacta mayormente en la duración de la carga de la batería del teléfono.La red sísmica propuesta por Kong y colaboradores se basa en el gran número de teléfonos inteligentes actualmente en operación y por tanto hace uso de una infraestructura física ya existente. Para participar en la red, un teléfono requierá solamente de la instalación del programa de detección. Los investigadores señalan que una red de este tipo tendría beneficios evidentes en zonas sísmicas localizadas en países no desarrollados que no cuentan con redes de alerta temprana. Adicionalmente, Kong y colaboradores hacen notar la información que proporcionarán los numerosos telefónos inteligentes participantes en la red tendrá utilidad desde un punto de vista científico para un mejor entendimiento de los fenómenos sísmicos.En el futuro, en la medida en que se desarrollen nuevas aplicaciones para los teléfonos inteligentes, el nombre con que conocemos a estos dispositivos será cada vez más y más inapropiado y es previsible que eventualmente sea sustituido por otro más representativo de sus funciones. Entre tanto esto sucede, habría que reconocer que su identidad original como trasmisores de voz no se ha desvanecido del todo y quehabemos todavía algunos que los usamos, hasta para hablar por teléfono.",
    "De acuerdo con los expertos, los orangutanes se separaron de la línea evolutiva que condujo a nuestra especie hace unos 12 millones de años. Si bien esto parece un tiempo muy largo, en términos evolutivoses relativamente pequeño. Como consecuenciaambas especies comparten algunas características físicas. En particular –juntamente con los otros dos grandes simios, gorilas y chimpancés–humanos y orangutanesposeen un grado de inteligencia superior que los distingue de otras especies menos evolucionadas. Al mismo tiempo –y en esto habremos de coincidir–, existen grandes diferencias entre los orangutanes y los humanos que tienen que ver con nuestro mayor desarrollo cerebral. Así, si bien los orangutanes son capaces de emplear herramientas, éstas son relativamente sencillas, sin ningún punto de comparación con el grado de desarrollo tecnológico que los humanos hemos alcanzado.Otra diferencia que tradicionalmente se ha pensado existe entre orangutanes y humanos es nuestra capacidad de hablar, la cual no tendrían los primeros. Esto último, sin embargo, fue puesto en duda por  un artículo publicado en enero de 2015 en la revista “PlosOne”, por un grupo de investigadores encabezado por Adriano Lameira de la Universidad de Amsterdam. En dicho artículo se discute el caso de Tilda, un orangután hembra actualmente confinado en el zoológico de Colonia, Alemania. Tilda es capaz de articular consonantes y vocales con un ritmo que se asemeja al de la voz humana. Lo hace, además, en ocasiones específicas, como cuando demanda alimentos a sus cuidadores,Tilda fue capturada en Borneo en 1967 a la edad de aproximadamente dos años. No se sabeque fue de ella sino hasta 8 años después de su captura, cuando fue adquirida por un coleccionista privado. Después de esto, fue huésped de varios zoológicos en Suiza y Alemania hasta parar en Colonia.Lameira y colaboradores piensan queen algún momento durante los ocho años en que permaneció oculta, Tilda fue entrenada paraparticipar en algún tipo de espectáculo y que de este entrenamiento resultó su habilidad para articular sonidos.Según Lameira y colaboradores, sus resultados demuestran que los orangutanes no tienen impedimentos respiratorios, neurológicos o articulatorios para emitir sonidos vocales y consonantes a una frecuencia propia del lenguaje humano. Con esto, de alguna manera se disminuye la brecha que existe entre los humanos y los orangutanes.Un artículo publicado esta semana en la revista “BehavioralEcology and Sociobiology” disminuye esta brecha aun más. En dicho artículo se muestra que los orangutanes son capaces de ponerse de acuerdo con el fin de, con ventaja, propinar una tunda mortal a otro orangután. El trabajo fue publicado por un equipo internacional de investigadores, con Anna Marzec de la Universidad de Zurich como líder, y en el mismo se describe un episodio de violencia protagonizado por una pareja de orangutanesen contra de un orangután hembra. El episodio ocurrió en la reserva natural Mawas en la isla de Borneo.Los participantes en el incidente fueron Kondor, una hembra joven que fue la que inició el ataque, Ekko, un macho que la acompañó en la agresión, y Sidony, la víctima, una hembra de edad avanzada quien se encontraba en compañía de su hijo, aun dependiente de ella.Kondor y Sidony habían ya tenido cuando menos un encuentro violento: años atrás,Sidony fue vista golpeando y mordiendo a Kondor, quien se había acercado a un hijo de aquella. Momentos antes del ataque descrito por Marzec y colaboradores,Kondor y Ekko se encontraron con Sidony, y aunque Ekko se acercó a esta última, no mostró interés alguno por ella; por el contrario, se alejó para aparearse con Kondor. Ésta, sin embargo, repentinamente lo interrumpió y se lanzó sobre Sidony seguida de Ekko. Sidony se vio así atacada por la pareja de orangutanes que actuaron en consonancia:  mientras uno atacaba el otro le impedía huir, o bien ambos atacaban en forma simultánea.  Kondor instigaba cadaataque mientras que Ekko infringía el mayor daño. La agresión se prolongó por 33 minutos y solo fue interrumpida por la aparición de Guapo, un orangután macho que se interpuso entre las dos hembras cada vez que Kondor intentaba atacary eventualmente escoltó a Sidony fuera de la escena. El ataque, sin embargo, resultó fatal para Sidony, quien murió días después a causa de las heridas sufridas.El incidente documentado por Marzec y colaboradores muestra que una orangután hembra es capaz de manipular a un macho e instigar un ataque mortal en contra de otra hembra orangután con la que había tenido anteriormente problemas. Así, resulta que los orangutanes son capaces de llevar a cabo acciones que antes hubiéramos pensado son sólo propias de los humanos.No debemos, sin embargo, adelantarnos a satanizar a todos lo orangutanes por una pareja de rufianes que bien pudieran no representar a toda la especie. En todo caso, por si las dudas, en la eventualidad de encontrarnos de frente con un orangután –macho o hembra–, lo más prudente sería emprender la huida a la brevedad posible.",
    "Hoy en día, las gráficas en las que se muestra como una cierta cantidad varía con el tiempo no nos resultan algo fuera de lo común. En las pasadas semanas, por ejemplo, hemos visto publicadas en diferentes medios gráficas que nos muestran como el tipo de cambio peso-dólar se ha ido incrementando, con pequeñas fluctuaciones, hasta alcanzar en algún momento los 19 pesos por dólar. Igualmente,no es difícil encontrar gráficas que indican quea lo largo del último año el precio del barril de la mezcla mexicana de petróleo se ha desplomadodesde un precio cercano a los 100 dólareshasta alcanzaren algún momento un valor por debajo de los 20 dólares.Como sabemos, una imagen dice más que mil palabras y esto es particularmente cierto de las gráficas. Por medio de una gráfica podemos visualizar la tendencia que guarda una cierta cantidad variable y estimar que es lo que podríamos esperar para el futuro.Por ejemplo, en el caso particular de las tendencias creciente y decreciente, en forma respectiva, del tipo de cambio  peso-dólar y del precio del petróleo mexicano, podemos quizá llegar a la conclusión de que las cosas no nos pintan demasiado bien para el presente año.Ciertamente, las gráficas en estos tiempos nos resultan familiares. No siempre ha sido así, sin embargo. Graficar una cierta cantidad conforme transcurre tiempo –el precio del petróleo, la cotización del dólar, la esperanza de vida a lo largo del último siglo, o la posición de un planeta en el firmamentoa lo largo del año, por poner algunos ejemplos.–, ha requerido de una cierta capacidad de abstracción que nos ha tomado milenios desarrollar.Los especialistas hasta ahora habían pensado que no fue sino hasta fechas relativamente recientesquelogramos desarrollar esta capacidad. Específicamente hasta el siglo XIV como resultado del trabajo de un grupo de pensadoresestablecido en el “MertonCollege”, Oxford, conocido como los “Calculadores de Oxford”.Un artículo publicado esta semana por MathieuOssendrijver del la Universidad Humboldt en Berlín, Alemania,en la revista”Science” – y que ganó la portada de la misma–, llega, sin embargo,a una conclusión diferente y sorprendente. Según Ossendrijver, los astrónomos de Babilonia de los años 350-50 antes de nuestra erahabían ya desarrollado una técnica matemática para calcular la posición del planeta Júpiter que precede por casi 1,500 años al trabajo de los Calculadores de Oxford.Si miramos al cielo en una noche clara podemos constatar que las estrellas se mueven siguiendo una trayectoria simple siempre en una misma dirección. Los planetas, en contraste, siguen una trayectoria más caprichosa, avanzando algunas veces en una dirección y otras veces en sentido contrario. Dada esta complicación,para los astrónomos de la antigüedad era más difícil describir el movimiento de un planeta que el de una estrella. Los astrónomos en Babilonia, sin embargo, lograron superar esta dificultad empleando gráficas de la velocidad de Júpiter para diferentes tiempos. Ossendrijver  llega a esta conclusión después de estudiar a lo largo de varios años varias tablillas de arcilla con inscripciones cuneiformes provenientes de Babilonia, las cuales están resguardadas en el Museo Británico.Para concebir una gráfica en la que se describe cómo cambia la velocidad de Júpiter –y  no su posición, como sería más natural– es necesaria una capacidad de abstracción considerable. Según Ossendrijver, los babilonios sabían además como hacer uso de esa gráfica para calcular la posición del planeta en un tiempo dado. Los conceptos involucrados para llevar a cabo esto último corresponde a la disciplina conocida como cálculo integral, desarrollada por los pensadores europeos en el siglo XVII. El conocimiento y abstracción matemática que habían alcanzado los babilonios de alguna manera se perdió y tuvo que ser redescubierta mucho tiempo después en Europa. Incluso no habría llegado hasta los griegos, quienes no habrían concebido gráficas con el grado de abstracción logrado en Babilonia. El que los babilonios hace más de dos mil años hayan sido capaces de concebir gráficas abstractas y hayan entendido algunos conceptos básicos del cálculo integral –que aun hoy en día no es una asignatura simple, como lo puede atestiguar cualquier estudiante de carreras de ciencia o ingeniería–es sorprendente. Por más que no podamos esperar que la capacidad de abstracción haya sido una regla general entre la población de Babilonia, como no lo es tampoco –aunque sí más extendida– aun en la actualidad en el mundo. De hecho, como lo comenta Ossendrijver, no es incluso claro sí las gráficas abstractas eran de uso corriente entre los astrónomos babilonios.Como quiera que sea, lo que sí es claro es que tanto hoy como hace dos mil años una imagen dice más que mil palabras.",
    "El 6 de septiembre de 1977 fue lanzada desde Cabo Cañaveral la sonda espacial Voyager 1 con la misión primaria de estudiar a los planetas gigantes Júpiter y Saturno, lo mismo que a Titán, el mayor satélite de este último. Una vez completadasu misión primaria, la nave Voyager 1 continuó una trayectoria espacial que la ha llevado a internarse en el espacio interestelar. En estos momentosla sonda se encuentra a una distancia talque las señales que emite tardan unas 17 horas en llegar a la Tierra. El Voyager 1 es además conocido porque lleva a bordo un disco fonográfico de cobre recubierto de oro con 115 imágenes de la Tierra, además de mensajes en 55 idiomas y grabaciones de música de diversas partes del mundo, incluyendo una pieza de música mexicana –El Cascabel de Lorenzo Barcelata en versión de mariachi–. Todo esto con el objeto de hacer saber de nuestra existencia y de nuestra cultura a los hipotéticos extraterrestres que por casualidadpudieran toparse con la nave en un futuro remoto.Dado que la guerra y los conflictos entre grupos es una de lascaracterísticas que más distinguen a nuestra civilización, no han faltado las críticos que hacen notar queninguna de las imágenes enviadas al espacioreflejadicha característica. Esto habría sido  debido a la pretensión delos impulsores del proyecto –encabezados por Carl Sagan de CornellUniversity–de no presentarnos ante los extraterrestres como una civilización agresiva y dada a los conflictos.Por otro lado, como parte del proyecto “Thelastpicture”, en noviembre de 2012 fue lanzado al espacio adherido a un satélite de comunicaciones un disco de silicio en el que se encriptaron cien imágenes de la Tierra. El propósito del proyecto era, no tanto hacer saber a los extraterrestres de nuestra existencia, sino el de crear una cápsula del tiempo que preservara nuestra memoria en una escala de tiempo geológica. Con relación a esto último, los impulsores del proyecto “Thelastpicture” hacen notar que a la altura a la que orbitan los satélites de comunicaciones –unos 36,000 kilómetros– no hay rozamiento con la atmósfera de modo que dichos satélites permanecerán en órbita de manera indefinida –a menos que por accidente choquen con un meteorito o sufran algún otro percance–. La información almacenada en el disco de silicio será de este modo y en principio preservada hasta que el Sol crezca yengulla a la Tierra,lo cual ocurrirá algunos miles de millones de años en el futuro. En contraste con el proyecto Voyager, “Thelastpicture” no tiene reticencias en mostrar nuestro comportamiento violento e incluye una fotografía de una explosión nuclear, lo mismo que de soldados durante la Primera Guerra Mundial portando máscaras antigás.  Por lo demás, al margen de cualquier actitud de pretender ocultar o hacer explícita nuestra vocación guerrera a los hipotéticos extraterrestres o terrícolas del futuro remoto, esinteresante preguntarnos por el origen de esta vocación. En este punto los especialistas no se ponen de acuerdo. ¿Eran violentos los grupos de cazadores-recolectores nómadas? O bien ¿Es nuestro comportamiento violento producto del sedentarismo que trajo  la invención de la agricultura, como una manera de defender un territorio o un depósito de alimentos? Con respecto a esto, un artículo publicado esta semana en la revista “Nature” por un grupo internacional de investigadores encabezados por Marta MirazónLahr de“Cambridge University” en el Reino Unido, se reporta un estudio llevado a cabo con los esqueletos de cuando menos 27 individuos, que fueron encontrados semienterrados en Nataruk, cerca del lago Turkana en Kenia. Los esqueletos corresponden a cazadores-recolectores, tanto hombres como mujeres, que murieron hace unos 10,000 años. Diez de los esqueletos están bien preservados y esto permitió determinar que pertenecieron a individuos que tuvieron una muerte violenta.Algunos por golpes en la cabeza y otros por facturas producidas por proyectiles o instrumentos cortantes. Igualmente, la disposición en la que fueron encontrados los restos de dos individuoshace pensar que al morir estaban atados de manos. Fue también encontrado el esqueleto de un feto o de un recién nacido junto a los restos de la que sería su madre. Los descubrimientos de MirazònLahr y colaboradores sugieren que la violencia existía entre los grupos de cazadores-recolectores y que ésta no está necesariamente relacionada al sedentarismo. Los mismos investigadores, sin embargo, ofrecen una explicación alternativa, según la cual hace 10,000años la vida entre los cazadores-recolectorestenía algunos de los elementos propios del sedentarismo. En todo caso, concluyen, “los muertos de Nataruk son un testimonio de la antigüedad de la violencia intergrupal y de la guerra”. Así, de un modo o de otro, resulta que la guerra nos ha acompañado por un largo tiempo, ya sea como resultado de nuestra evolución como especie, o bien como un elemento intrínseco a nuestra organización social.En estas condiciones ¿deberíamos ocultar nuestra verdadera naturaleza a los extraterrestres?Dado que la probabilidad de que alguien encuentre en un futuro distante alguno de los mensajes enviados al espacio es prácticamente nula, esta pregunta es quizá ociosa. Y lo es, sobre todo, porque enviar mensajes al espacio en una botella no pasa de ser una actividad divertida.",
    "La ciudad siberiana de Norilsk ostenta un record mundial: es la ciudad más cercana al Polo Norte entre aquellas con una población mayor a los 100,000 habitantes. De hecho, Norilsk está situada por arriba del círculo polar ártico, a una latitud que ronda a los 68 grados. Dado lo que sabemos acerca de Siberia, podríamos quizá sospechar que Norilsksufre de inviernos muy crudos. Una consulta al sitio de internet “WeatherChannel” confirma nuestras sospechas: la temperatura pronosticada para el día de hoy domingo rondará los 27 grados centígrados bajo cero, con una sensación térmica –debida al viento– de 37 grados centígrados bajo cero.Para los que vivimos en el trópico es posible queNorilsk no resulte una ciudad especialmenteatractiva para vivir. Y no solamente por las bajas temperaturasinvernales que pueden alcanzar los cincuenta grados centígrados bajo cero, sino porque al estar localizada más allá del circulo ártico sufre durante el invierno de poca luz solar –incluyendo mes y medio de noche polar–,y esto presumiblemente nos predispone a la depresión.  No obstante todo lo anterior, no habría que juzgar a las condiciones de vida en Norilsk bajo nuestra particular óptica tropical y con seguridad habrá quienes disfruten del clima siberiano. Así, según el periódico británico “TheDailyTelegraph”, “no parece que los habitantes de Norilsksonrían menos que aquellos de otras ciudades rusas”. De  hecho, a algunos incluso les agrada el inviernopor la oportunidad que les proporciona de ir a cazar y pescar.Por otro lado, habría también que reconocer, que si bien de todo hay en la viña del señor, una gran mayoría de personas, de tener lalibertad de hacerlo, escogerían para vivir un clima más benigno que el que impera en el norte de Siberia. Y esto posiblemente sea tan válido hoy como en el pasado remoto.En este contexto, es interesante comentar el descubrimiento de los restos de un mamut llevado a cabo en la bahía de Yenisey en el Mar de Kara, a unos 300 kilómetros al noroeste deNorilsk y a una latitud de 72 grados. Dicho descubrimiento no tendría nada en particular si no fuera porque los restos del mamut tienen 45,000 años de antigüedad y muestran señales indudables de corresponder a un animal cazado por un grupo de humanos. Estoindica que nuestros ancestros llegaron el ártico unos 10,000 años antes de lo que se pensaba. El descubrimiento se reporta en un artículo publicado esta semana en la revista “Science” por un grupo de científicos rusos encabezados por Vladimir Pitulko de la Academia de Ciencias Rusa en San Petersburgo.   Como sabemos, hay un acuerdo entre los especialistas que nuestra especie humana se originó en África y que de ahí emigró hacia Asia, y posteriormente haciaEuropa, Oceanía y América. Con respecto a nuestro continente, los especialistas igualmente están de acuerdo en que los primeros pobladorescruzaron desde Siberia hasta Alaska a través del puente de tierra que entonces existía en lo que ahora es el estrecho de Bering. No está claro, sin embargo, cuando esto último ocurrió, si fue después del periodo conocido como Último Máximo Glacial –que se extendió desde hace 26,500 años hasta hace 19,000 años–, o bien antes de dicho periodo. De acuerdo conPitulko y colaboradores, su descubrimiento puede ayudar a resolver el punto, pues demuestra que hubo grupos humanos que habitaron el norte de Siberia en una época temprana, los cuales tuvieron la oportunidad de atravesar el puente de Bering  antes del Último Máximo Glacial.  Los resultados de Pitulko y colaboradores también demuestran que nuestros ancestros hace 45,000 años habían ya desarrollado toda la tecnología necesaria para, no solamente cazar animales de gran tamaño, sino para sobrevivir a las condiciones inclementes del norte de Siberia. En el caso particular de los cazadores del mamut de la bahía de Yenisey no podemos saber si se vieron por alguna razón obligados a avanzar hacia el norte o si fueron hasta ahí en busca de comida; o biensi eran un grupo de aventureros que disfrutaban de cazar y pescar en medio del frío siberiano,tal como lo hacen sus contrapartes modernos de Norilsk. De un modo u otro, el avanzar hacia el norte resultó a la postre esencial para poblar el continente americano.Por lo demás, todos aquellos que no consideraríamos a Norsilsk entre nuestras opciones como lugar de residencia –por más que tengamos a nuestra disposición todo un arsenal de tecnologías para sobrevivir en el ártico– no podemos sino mostrarnos agradecidos de vivir 45,000 años después, sin necesidad de cazar mamuts o soportar fríos extremos.",
    "Como sabemos, la esperanza de vida a nivel global ha crecido de manera sostenida en los últimos ciento cincuenta años, hasta superar los 80 años en el caso de los países que tienen la cifras mas elevadas en esta materia. Según la Organización Mundial de la Salud, el país con la población más longeva es el Japón, en donde las esperanzas de vida al nacer para mujeres y hombres son de 87 y 80 años, en forma respectiva –las cifras respectivas para México son 73 y 79 años–. Esto contrasta con el promedio de vida anterior a la Revolución Industrial que rondaba a los 40 años. El incremento en la esperanza de vida es resultado no solamente de avances médicos enla prevención y tratamiento de enfermedades infecciosas,sino también de una mejora en las condiciones de higiene de la población. Esto último incluyeel desarrollo de sistemas de distribución de aguay de disposición de desechos humanos en las ciudades,así como la mejora de los hábitos de higiene personal. Podríamos esperar que la pobre infraestructura de la ciudades de Europa en cuanto a cañerías y sistemas de disposición de desechos humanos en los siglos anteriores a la Revolución Industrial no fueron lo mejor para prevenir las epidemias que asolaron a Europa; particularmente las epidemias de peste bubónica, entre las que destaca la Muerte Negra que diezmó a la población europea en el siglo XIV.Resulta interesante, por otro lado, saber que dichas condiciones de higiene constituyeron un retroceso con respecto a aquellas existentes en el Imperio Romano al inicio de nuestra era. En efecto, es sabido que Roma contaba en esa época con un sistema de cañerías y de distribución de agua corriente, lo mismo que de baños y letrinas públicas, y que esto –lo mismo que la ingeniería involucrada– se perdió en Europa en los siglos que siguieron a la caída del Imperio Romano de Occidente.Se podía esperar que las condiciones sanitarias benignas del Imperio Romano habrían llevado a una disminución de enfermedades infecciosas entre la población. Para averiguar si esto fue realmente lo que ocurrió,PiersMitchell, de la Universidad de Cambridge en el Reino Unido, realizó un recopilación de la evidencia que ha sido descubierta en diversos sitios arqueológicos –fosas sépticas, letrinas públicas y entierros humanos– sobre la presencia parásitos humanos en la Roma Imperial; esto, con el objeto de hacer una comparación con los parásitos que han sido encontrados en los mismos lugares pero en diferentes épocas.Mitchell publicó sus resultados esta semana en la revista “Parasitology”.El análisis llevado a cabo por Mitchell incluyó estudios llevados cabo en sitios arqueológicos que en la actualidad están localizados en Italia, Austria, Alemania, Francia, Egipto e Israel, entre otros países. Para su sorpresa, encontró que no solamente los romanos sufrían de parásitos intestinales, sino que en algunos casos la incidencia de las infecciones era incluso mayor que la de otras épocas, en principio sanitariamente menos avanzadas.Entre los parásitos más comunes que sufrían los romanos se encontraban las lombrices intestinales, los tricocéfalos, las amibas y las tenias,trasmitidasestas últimas por el consumo de pescado sin la suficiente cocción.  Con relación a la infección por tenia, Mitchell hace notar que la evidencia encontrada indica que la incidencia de la misma durante el Imperio Romano era probablemente mayor que la correspondiente incidencia, en los mismos sitios, durante las edades del bronce y de hierro. Para explicar sus descubrimientos,  Mitchell aventura varias hipótesis. En una de estas considera la posibilidad de que la costumbre de los romanos de tomar baños en instalaciones públicas hubiera provocado la trasmisión de parásitos intestinales. Lo anterior debido a que el agua de los baños no se renovaba frecuentemente y mantenía una capa flotante de suciedad, producto de los bañistas anteriores.Mitchell hace notar también que la transmisión de parásitos podía haberse dado por la fertilización de los campos agrícolas empleando aguas negras, que habría sido una costumbre romana.Una hipótesis adicional, ésta, para explicar la infección con tenias, se refiere a la afición romana por una salsa fermentada conocida como “garum”, elaborada a base de pescado crudo, sal y hierbas. Mitchell aventura que el comercio de esta salsahabría provocado la diseminación de la infección por tenia, a lo largo de todo el imperio.Examinando peines y textiles de la época, no encuentra Mitchell tampoco que la higiene de los romanos los hubiera librado de parásitos externos, como son los piojos y las pulgas. Estospudieran haber sido la vía para trasmitir enfermedades,en particular la peste bubónica, que se sabe asoló al imperio en tiempos del emperador Justiniano.Aparte de la higiene, habría sido entonces necesario algo más para que dicha práctica hubiera sido efectiva en la prevención de enfermedades. Tomando en cuenta lo sucedido en los últimos ciento cincuenta años, no es difícil entenderlo que les faltó a los romanos: un entendimiento del origen de las enfermedades. Para los médicos romanos las enfermedades eran el resultando de un desequilibrio entre nuestros cuatro humores: la bilis negra, la bilis amarilla, la flema y la sangre, y para combatirlas habría que restaurar este equilibrio. Además, para ellos los parásitos se generaban de manera espontánea. Y en todo esto, hoy lo sabemos, estaban rotundamente equivocados. La higiene es necesaria pero no suficiente.",
    "Una visita reciente a la Universidad de Ciencia e Ingeniería de Sichuan, en Zigong, China, de alguna manera y en forma inesperadanostransportó en el tiempo; específicamente, hasta mediados de la década de los años sesenta del siglo pasado, cuando dicha universidad fue fundadaen circunstancias muy particulares que resulta interesante comentar.Como sabemos, los años sesenta fueron pródigos en acontecimientos diversosde gran trascendencia parael mundo. En particular, fue un periodo en el que se recrudeció la carrera armamentista entre los Estados Unidos y la Unión Soviética, la llamada Guerra Fría.Y fue precisamente en el marco de la Guerra Fría, poco después de que China se convirtiera en el quinto miembro del club nuclear, que se fundó la universidadreferida anteriormente.Un elemento presente en el año de 1965 era la amenaza permanente de un holocausto nuclear.Los Estados Unidos y la Unión Soviética estaban enfrascados en una carrera de acumulación de armas nucleares que tenía como propósito disuadir al adversario de iniciar un guerra nuclear so pena de resultar aniquilado. En este tenor, ambos países contaban con suficientes bombas nucleares para destruir varias veces al mundo. Si bien dicha carrerano se inició en ladécada de los sesenta, sí fue durante la misma que crecieron sustancialmentelos inventarios de armas nucleares en el mundo.Fue también en la década de los sesenta –específicamente en octubre de 1962– que habría estado a punto de desatarse una tercera guerra mundial durante la llamada crisis de los misiles de Cuba.El clima de amenaza de guerra nuclear imperante en la década de los años sesenta fue recreado por el director de cine Stanley Kubricken su película de humor negro, “Dr. Strangelove o: Cómo aprendí a dejar de preocuparme y amar a la bomba” realizada en 1964. En la película, un general norteamericano está empeñado en provocar una confrontación nuclear con la Unión Soviética y logra hacer que despegue un escuadrón de bombarderos con armamento nuclear con la misión de atacar ciudades soviéticas. A última hora fue posible ordenar el regreso de todos los bombarderos, con la excepción de uno de ellos con el que no se pudo establecer comunicación y que logracompletar su misión. Se da inicio así a la tercera guerra mundial.En la década de los años sesenta fuimostambién testigos de la primera prueba nuclear china ocurrida en octubre de 1964.  Con esto, China se unió a los Estados Unidos, la Unión Soviética, Francia y Gran Bretañacomo quinto miembro del club de naciones nucleares. Hay que recordar, igualmente, que si bien China desarrolló su bomba nuclear con ayuda de la Unión Soviética, en los años sesenta ambos países estaban distanciados. Chinaestaba así aislada y bajo amenaza de las dos potencias nucleares dominantes.En este contexto, el año1965fue testigo de la creación en la provincia de Sichuan, en el suroeste de China, del instituto que con el tiempo se convertiría en la Universidad de Ciencia e Ingeniería de Sichuan; esto, como una rama del Instituto de Tecnología Química del Este de China con sede en Shanghai. La misión del nuevo instituto fue la de contribuir al desarrollo de la tecnología china de fabricación de armas nucleares. Consecuentemente con estos objetivos, la sede del nuevo instituto se construyó en un lugar apartado en el sur profundo de China, a unos 40 kilómetros de la ciudad de Zigong en la provincia de Sichuan.Un busto de Marie Curie en uno de los jardines del campus, por lo demás,  no deja duda de la vocación técnica del mismo.Las últimas décadas del siglo XX vieron aumentar la membresía del club de países nucleares y a China le siguieron la India, Pakistán, Corea del Norte y posiblemente Israel.Sin embargo, fuera de los Estados Unidos y Rusia, que en conjunto, según Robert Norris y Hans Kristensen en un artículo publicado en el “Bulletin of theAtomicScientists” en el número de julio/agosto de 2010, tenían en 2010 un inventario del orden de 20,000 armas nucleares, todos los demás países del club suman apenas unas 1,000 armas de este tipo. Por otro lado, si bien en las últimas décadas se ha reducido sustancialmente el inventario de armas nucleares a nivel global a partir del máximo alcanzado en 1986, el desarrollo de armas más modernasen realidad no se ha detenido. Dada esta circunstancia, se han vertido opiniones en los últimos meses en el sentido de que está en curso una nueva Guerra Fría. William Perry, quien fuera Secretario de Defensa en la administración de Bill Clinton,analiza en un artículo publicado en el “Bulletin of AtomicScientists” el pasado 8 de diciembre las posibilidades que se dé una nuevaversión de Guerra Fría entre los Estados Unidos y Rusia, y no la descarta en función de que hoy en día “las relaciones entre ambos países son tan malas como lo fueron en los años oscuros de la Guerra Fría”.Así, de ser ciertas las opiniones expertas, estaríamos en peligro de que se de una reedición de la locura que vivimos por cinco décadas y que nos tuvo al borde de una catástrofe. También es posible que en realidad el peligro nunca haya desaparecido dado que, también según opiniones expertas, el desarrollo de nuevas armas nucleares, mas sofisticadas, nunca se ha detenido.Así, quizá debamos cruzar los dedos para que no tengamos que ver con nostalgia a las anticuadas armas nucleares de la lejana década de los años sesenta.",
    "En la cumbre climática que se está llevando a cabo en París desde el 30 de noviembre pasado y hasta el próximo 11 de diciembre, representantes de más de 190 países buscan llegar a un acuerdo sobre la emisión de gases de invernadero a la atmósfera que limite el calentamiento de nuestro planeta. Ayer sábado, según informes periodísticos, los delegados en París habían logrado finalizar un  borrador de poco más de 40 páginas con el que, sin embargo, no todos los países están de acuerdo. Aún así, según un observador, esto representa un avance con respecto a la cumbre de Copenhague en 2009 –conocida por sus pobres resultados– en la que a estas alturas el borrador respectivo tenía 300 páginas y reinaba un ambiente de desesperanza en la reunión. Los problemas climáticos por los que atraviesa nuestro planeta son resultado del incremento acelerado del consumo de energía del mundo desde hace dos siglos, particularmente en el periodo desde el final de la Segunda Guerra Mundial hasta la década de los años setenta. A lo largo de los últimos doscientos años el consumo de energía a nivel global se ha multiplicado por casi un factor de 25, lo que, ahora sabemos, ha sido demasiado para el planeta.Si tomamos en cuenta que la población del mundo ha crecido unas siete veces en los dos últimos siglos, resulta claro que el consumo per cápita de energía ha crecido también. Este consumo está ligado a la calidad de vida y de esto nos convencemos si echamos un vistazo a la energía que los habitantes de países desarrollados consumen en comparación con aquellos en desarrollo. Las estadísticas de la ”Energy Information Administration” de los Estados Unidos, por ejemplo, nos indican que en 2011 un estadounidense consumía en promedio más de cuatro veces la energía que usaba un habitante de nuestro país. De la misma manera, un europeo consumía en 2011 menos de la mitad de la energía empleada por un estadounidense.   En el otro extremo, un haitiano y un habitante de Chad hacían uso en ese año, en forma respectiva, apenas un centésimo y un milésimo de la energía consumida per cápita en los Estados Unidos. Los números anteriores nos indican que, si bien el desarrollo tecnológico que ha experimentado el mundo en los últimos dos siglos ha llevado a un incremento promedio en nuestro nivel de vida, dicho incremento no ha sido de modo alguno parejo. Por el contrario, dado que todos viajamos en el mismo barco –aunque con boletos de variadas categorías–, las consecuencias del uso –y en ocasiones abuso– de la energía se reparten globalmente.En este contexto, según el periódico británico “The Guardian”, uno de los motivos de discrepancia en la cumbre de París gira en torno a la definición de quien deberá pagar los costos de la remediación ambiental. Ciertamente, no han de ser los países más pobres, que poco han contribuido al problema climático. La controversia en realidad surge en relación con países, China, de manera particular, que hasta hace poco eran clasificados como subdesarrollados pero que ahora algunos consideran que ya no lo son tanto. En el caso de China, sobre todo porque es ahora el máximo generador de gases de invernadero, por arriba de los Estados Unidos que ocupa el segundo lugar, si bien con un generación per cápita sustancialmente menor.    Por otro lado, el calentamiento global no es el único problema que ha resultado del crecimiento de la población del mundo y de la elevación de su nivel de vida. Otro de estos problemas es el de desertificación del suelo agrícola. Con relación a esto, en la cumbre climática de París, expertos de la “University of Sheffield” en el Reino Unido dieron  conocer el pasado miércoles que el 33% de la tierra cultivable se ha perdido por contaminación o erosión en los últimos 40 años. Los investigadores encuentran que la fertilidad del suelo agrícola se está perdiendo a una velocidad 10-100 veces mayor que la velocidad con la que se repone por métodos naturales, y que este problema tiene el potencial de producir una catástrofe alimentaria. En entrevista a “The Guardian”, Duncan Cameron de la “University of Sheffield” afirma que se necesita una solución radical para revertir la tendencia, la cual apunta hacia una re-ingeniería de nuestro sistema de producción agrícola. De acuerdo con Cameron, “Necesitamos sacar de producción algunas tierras por un largo tiempo para permitir que el carbón del suelo se reconstruya y se estabilice. Tenemos grandes extensiones de tierras que son usadas para pastoreo por las industrias de la carne y los productos lácteos. En lugar de mantenerlas separadas, necesitamos ponerlas en rotación de modo que haya más tierras en el sistema agrícola y que no se usen éstas de manera permanente“.Al problema del cambio climático podría añadirse entonces el de un potencial desastre alimentario por la degradación del suelo fértil. Con seguridad ambos problemas podrían ser abordados empleando recursos tecnológicos existentes, o bien  recursos a ser desarrollados en el futuro. Lo que no resulta claro es la celeridad con la que se hará, dados los intereses económicos y políticos involucrados. Esperemos que la conclusión de la cumbre climática en una semana más nos dé una respuesta.",
    "Ayer sábado por la mañana, y después de algunos retrasos, se dio a conocer el borrador del acuerdo que suscribirían los países participantes en la cumbre climática de París. A bote pronto se dieron reacciones encontradas. Hubo quien consideró que el acuerdo implicaba el fin de la era de los combustibles fósiles. Más comúnmente, las opiniones fueron en el sentido de que se trataba de un documento sin dientes que poco obliga a los países que más gases de invernadero generan. En particular, se comenta que el compromiso según el cual los países desarrollados aportarán 100,000 millones de dólares anuales a los países no desarrollados para combatir al problema ambiental, se menciona solamente en el preámbulo del documento y no resulta de este modo vinculante y sólo voluntario.  Una razón para esto último es la posición adversa del Senado de los Estados Unidos a suscribir un compromiso formal en este sentido. De hecho, hay senadores norteamericanos que niegan que exista el calentamiento global y en estas circunstancias un acuerdo vinculante tendría una muy baja probabilidad de ser aprobado por el Senado norteamericano. Paul Krugman, premio Nobel de Economía, en un artículo publicado en el diario New York Times el pasado 4 de diciembre, arremete en contra de los republicanos en el Senado estadounidense y los acusa de asumir actitudes anticientíficas en torno al cambio climático que podrían llevar a un fracaso de la cumbre de París. Escribe Krugman: “Y la ortodoxia de negar el cambio climático no dice solamente que el consenso científico está equivocado. Hay miembros distinguidos del Congreso que respaldan teorías de conspiración alegando que la evidencia a favor del cambio climático es el producto de un fraude gigantesco perpetrado por miles de científicos alrededor del mundo. Que hacen todo lo que pueden para amenazar e intimidar a científicos individuales”.    El consenso sobre el calentamiento global se ha generado a través de estudios científicos  llevados a cabo a lo largo de varias décadas. En un artículo publicado por Thomas Peterson y colaboradores en el “Bulletin of the American Meterological Society” en septiembre de 2008 se da un recuento de la literatura científica al respecto. Un punto interesante es que en la década de los años sesenta no era claro si el planeta se estaba calentando o bien se estaba enfriando, y este contexto se publicaron artículos de investigación que defendían tanto un punto de vista como el otro. No obstante, según Peterson y colaboradores, el punto de vista dominante era que el planeta se estaba calentando; y esto, posiblemente,  como consecuencia de la emisión de gases de invernadero a la atmósfera.     Al mismo tiempo, mediciones precisas de la concentración de dióxido de carbono –el principal gas de invernadero– en la atmósfera, llevadas a cabo a lo largo de varios años, demostraron que dicha concentración se estaba incrementando de manera paulatina. Se estableció así una conexión entre el calentamiento global y la emisión de dióxido de carbono a la atmósfera, hecho que es hoy en día aceptado de manera amplia por la comunidad científica –si bien no de manera unánime. La superficie de la Tierra, incluyendo su atmósfera es un sistema muy complicado cuyo comportamiento no es fácil de entender, aun con los métodos científicos más sofisticados. El cambio climático ha estado así expuesto a interpretaciones y explicaciones anticientíficas y/o interesadas. En este sentido, el diario “The Guardian”, en un artículo publicado esta semana dio a conocer una operación encubierta llevada a cabo por la organización “Greenpeace” que expuso a dos académicos dispuestos a escribir artículos supuestamente científicos bajo comisión. En un primer caso “Greenpeace”, simulando ser un representante de una compañía de gas y petróleo del Medio Oriente, contactó a William Happer, quien es profesor de física en “Princeton University” y un escéptico del cambio climático, con el objeto de que escribiera un artículo en el que expusiera los beneficios de incrementar la concentración de dióxido de carbono en la atmósfera. Happer se mostró dispuesto a hacerlo mediante el pago de 8,000 dólares. En un segundo caso, “Greenpeace”, aparentando ser un empleado de una compañía consultora de Indonesia, contactó a Frank Clemente, quien es profesor jubilado de “Pennsylvania State University”, con el objeto de que escribiera un artículo en el que tratara sobre los beneficios del uso del carbón como combustible. Al igual que Happer, Clemente aceptó escribir un artículo de 10 páginas mediante el pago de 15,000 dólares. Dados estos dos casos, puede uno preguntarse sobre la frecuencia con las que se han dado otros similares en los que se busca desestimar el problema del cambio climático en respuesta a intereses económicos que saldrían afectados por una reducción en el consumo de combustibles fósiles.   Volviendo a la cumbre climática de París, en la reunión plenaria llevada a cabo el sábado por la tarde  fue aprobado por todos los delegados el texto del acuerdo dado a conocer horas antes. Esto provocó gran optimismo y la esperanza de que el cambio climático haya entrado en una vía de solución. Esto, a pesar de que algunos consideran que al acuerdo le faltan dientes. Como quiera que sea, un punto sin duda positivo es que se haya reconocido, de manera unánime, que el cambio climático es un problema real, en contra de opiniones anticientíficas e interesadas.",
    "A finales del año 2012, los estados de Colorado y Washington en los Estados Unidos legalizaron el uso de la marihuana con fines recreativos, abriendo la posibilidad de que dicha legalización se extendiera a otros estados de ese país, si bien no a nivel federal. En la actualidad, aparte de Colorado y Washington, la marihuana es legal en Alaska, Oregón y el Distrito de Columbia, y se espera que pronto lo sea en 11 estados más, incluyendo California y Nueva York. A nivel país, Uruguay se convirtió en 2013 en el primero en legalizar la producción y venta de marihuana.En este contexto, como sabemos, a principios de este mes la Suprema Corte de Justicia de nuestro país autorizó a cuatro personas, integrantes de la organización SMART, el uso de la marihuana con fines recreativos; esto, en medio de un debate en torno a la conveniencia de que dicha droga sea legalizada en México, asunto que resulta altamente controvertido. Entre otras facetas que le son inherentes, el consumo adictivo y extendido de la marihuana es un asunto de salud pública con un costo social. En este sentido, un artículo aparecido esta semana en la revista “Psychological Medicine”, publicado por investigadores de centros de investigación en Italia e  Inglaterra, encabezados por Silvia Rigucci de la Universidad La Sapienza en Roma, sugiere que el consumo de la variedad, altamente potente, de marihuana conocida en inglés como “skunk”, provoca cambios estructurales en el cerebro de los consumidores. Se sabe que el consumo de esta droga potente está asociado a la ocurrencia episodios de psicosis.De manera específica, los investigadores encuentran evidencia que la marihuana “skunk” provoca cambios micro-estructurales en la materia blanca del cuerpo calloso, que se sabe es particularmente afectada por la sustancia activa de la droga. Es importante señalar que la materia blanca contiene fibras nerviosas que tienen la función de enviar señales entre los dos hemisferios cerebrales y cuyo daño pudiera afectar la comunicación entre los mismos. Rigucci y colaboradores llegaron a sus conclusiones después de estudiar, por técnicas de resonancia magnética, el cerebro de 56 pacientes que habían sufrido episodios de psicosis y de 43 voluntarios sanos. En ambos grupos había tanto consumidores como no consumidores de marihuana. De manera precisa, de los 56 pacientes y 43 voluntarios, 37 y 22 eran consumidores de marihuana, en forma respectiva. Los investigadores afirman en su artículo que: “el uso frecuente de marihuana de alta potencia está asociado de manera significativa a una alteración de la integridad micro-estructural de cuerpo calloso”; esto, en comparación con aquellos que, o bien no consumen marihuana, o bien consumen la versión menos potente. Encuentran, además, que estas alteraciones fueron similares en todos los usuarios de “skulk”, hayan o no hayan sufridos episodios psicóticos. No encuentran, por otro lado, diferencia entre aquellos que empezaron a usar la marihuana de alta potencia antes de los 15 años con aquellos que iniciaron su consumo posteriormente.  En declaraciones al periódico británico “The Guardian”, Paola Dazzan del Instituto de Psiquiatría del King´s College de Londres y uno de los autores del artículo de referencia, menciona que a partir de sus resultados no se puede afirmar con certeza que el consumir marihuana de alta potencia lleva a un daño cerebral. Por el contrario, bien pudiera ser que aquellos adictos al “skunk” en realidad lo sean porque sufren de este daño por alguna otra razón. Habría de este modo una incertidumbre entre causa y efecto.Sin embargo, según Dazzan y al margen de la incertidumbre, lo que sí se puede afirmar es que aquellos que consumen regularmente mariguana de alta potencia –cuyo uso se estaría incrementando–  “tienen un cerebro diferente de aquellos que no la emplean o que consumen una variedad menos potente”.    Ciertamente, el trabajo de Rigucii y colaboradores sobre el efecto de- “skulk” en el cerebro tendrá que ser confirmado, y en su caso validado, por investigaciones independientes. De una u otra manera, sin embargo, pone en relieve una de los aspectos del complejo problema que enfrentamos por el consumo extendido de la marihuana –y de las drogas, en general–. Problema que resulta abrumador por sus múltiples facetas y, ni quien lo dude, de difícil arreglo. Y con propuestas de solución controvertidas, por supuesto.",
    "Esta semana el máximo tribunal italiano absolvió de manera definitiva a cuatro geofísicos italianos y dos ingenieros sísmicos acusados de homicidio involuntario por la muerte de más de 300 personas durante el temblor de tierra ocurrido en abril de 2009 en la ciudad de L´Aquila en el centro de Italia. Los seis científicos, juntamente con Bernardo De Bernardinis, quien en esos momentos fungía como  subdirector del departamento de protección civil italiano, habían sido condenados en 2012 a seis años de cárcel por no haber informado de manera correcta a la población de L´Aquila de los riesgos que corrían por la posible ocurrencia de un temblor de tierra. Como se dio a conocer en su momento, los siete especialistas formaron parte de un comité que se reunió en L´Aquila el 30 de marzo de 2009 con el objeto de evaluar la situación sísmica de la ciudad, la cual había sufrido numerosos temblores de pequeña y mediana magnitud en los meses anteriores. Dicho comité fue convocado por el departamento  de protección civil italiano. De acuerdo con el juez que condenó a los científicos, los pobladores de L´Aquila se confiaron por las declaraciones que De Bernardinis hizo por televisión en el sentido de que no había motivo para alarmarse, y no tomaron las precauciones necesarias para protegerse de un posible terremoto de gran magnitud. Contrario a las declaraciones de De Bernardinis, sin embargo, la madrugada del 6 de abril de 2009, seis días después de la reunión del comité de expertos, un sismo golpeó a L´Aquila tomando por sorpresa a sus habitantes y matando a 309 de ellos.Los siete especialistas condenados en 2012 apelaron la sentencia, resultando absueltos en noviembre de 2014; con la excepción de De Bernardinis quien, sin embargo, vio reducida su sentencia de seis a dos años de cárcel. Al final, después de una impugnación del fiscal por la sentencia absolutoria, la Suprema Corte de Justicia de Italia absolvió esta semana a los científicos de manera definitiva.Si bien se sabe que los sismos son el resultado de esfuerzos que se generan entre dos placas tectónicas al deslizarse una contra la otra, la opinión prevaleciente entre los especialistas es que el conocimiento científico con que se cuenta hoy en día no es lo suficientemente detallado para anticiparlos con un cierto grado de precisión. La imposibilidad técnica de predecir un terremoto fue un argumento de defensa de los científicos acusados, pues si bien, según opiniones expertas, la ocurrencia de pequeños y continuos temblores podría aumentar la probabilidad de que sobrevenga un terremoto mayor, ésta es pequeña y es mas probable que dicho terremoto no ocurra en el corto plazo.En realidad, el juez que condenó a los especialistas en primera instancia no los acusó de no haber sido capaces de predecir el terremoto de L´Aquila, sino de haber fallado en la comunicación de sus opiniones expertas para alertar a la población sobre el riesgo al que estaban expuestos. En este contexto, la justicia italiana no exonera y encuentra culpable a De Bernardinis, quien manejó las relaciones públicas y declaró –incluso antes de la reunión del comité– que la serie de pequeños temblores que se habían sentido en L´Aquila en los meses anteriores en realidad eran un signo positivo pues ayudaban a aliviar la tensión acumulada en el subsuelo. De acuerdo con testimonios de habitantes de L´Aquila que vivieron el terremoto, esta declaración los tranquilizó e hizo que bajaran la guardia.  Con relación al último punto, un artículo publicado en la sección “Science Insider” de la revista “Science” el pasado 28 de octubre, refiere que un juez italiano decidió someter a juicio a Guido Bertolaso –médico de profesión–, quien fue el organizador de la reunión de científicos previa al terremoto. En esos momentos, Bertolaso fungía como director del departamento de protección civil italiano y De Bernardinis era su segundo a bordo. Según “Science”, el juicio a Bertolaso se basa en una llamada telefónica que hizo en la que se refiere a la reunión del comité en L´Aquila como “una operación mediática que él estaba organizando para callar a un técnico de un laboratorio cercano que estaba haciendo predicciones alarmistas que habían producido pánico entre la población”. Bertolaso también mencionó lo que el comité de expertos debía decir a la población: “que los pequeños temblores que estaban ocurriendo eran positivos pues descargaban la energía y hacían que un temblor mayor fuera menos probable”.En su conversación telefónica Bertolaso se habría referido a Giampaolo Guliani, quien había hecho predicciones sobre la inminente ocurrencia en L´Aquila de un terremoto de gran magnitud. Guiliani basaba sus predicciones en mediciones de niveles de gas radón, método que no es aceptado por la comunidad científica.  Todo lo anterior nos permite vislumbrar que el insólito caso de los juicios por el terremoto de L´Aquila se originó, en última instancia, por la decisión de un funcionario público de acallar –por medio de un comité de expertos– a un científico aficionado que alarmaba a la población con predicciones catastrofistas. Y posiblemente todo le hubiera resultado bien, pues la probabilidad de que ocurriera un terremoto mayor al corto plazo era pequeña. No tomó en cuenta, sin embargo, las opiniones de la madre naturaleza.",
    "Una de las víctimas –aparentemente no mortal– de los atentados terroristas del pasado viernes en Paris, Francia, es la conferencia sobre cambio climático de las Naciones  Unidas, la cual está programada para llevarse a cabo en esa ciudad del 30 de noviembre al 11 de diciembre próximos. Si bien existen dudas sobre su realización en las circunstancias actuales, el primer ministro francés, según la agencia Reuters, aseguró que la conferencia se llevará a cabo como se había planeado. Se tienen esperanzas que en la conferencia climática de París se logren acuerdos entre naciones que limiten la emisión de gases de invernadero a la atmósfera. Como sabemos, una mayoría de expertos climatólogos sostienen que el incremento en la concentración de dichos gases en la atmósfera, resultado del uso de combustibles fósiles, está llevando a una elevación de la temperatura promedio de la superficie de la Tierra y a un cambio climático que podría ser catastrófico de no ponerle freno.Como preámbulo a la cumbre climática, al inicio de esta semana la Organización Meteorológica Mundial de la ONU dio a conocer que en los primeros meses del presente año la concentración en la atmósfera de dióxido de carbono –el más significativo de los gases de invernadero– se mantuvo por primera vez por arriba de las 400 partes por millón. Esto representa un incremento de casi 50% con respecto al nivel que prevalecía antes de la Revolución Industrial. Concurrentemente, la oficina meteorológica del Reino Unido hizo públicos datos  que muestran que, de enero a septiembre pasados, el incremento de temperatura global promedio con respecto al periodo 1850-1900 fue mayor por vez primera a  un grado centígrado. Esto significa que estamos a medio camino de alcanzar el límite de incremento fijado por los expertos, que es de dos grados centígrados con respecto a los niveles preindustriales, por arriba del cual se produciría una catástrofe climática a nivel global.Uno de los efectos del incremento de la temperatura de la Tierra es la elevación del nivel de los océanos, debido a la fusión de las masas de hielo en los polos y a la expansión del agua por el aumento de su temperatura. Si bien, dado lo complejo de los fenómenos que lo determinan, es difícil precisar cuál será el incremento del nivel del mar en las próximas décadas, los expertos estiman que alcanzará hacia el final del siglo XXI un valor en el rango comprendido entre algunas decenas de centímetros y unos dos metros; lo anterior, según el grado en que se aminore la emisión de gases de invernadero a la atmósfera. No obstante, debido a que el aumento en la concentración de dióxido de carbono en la atmósfera se ha dado de manera extraordinariamente rápida, los especialistas consideran que más allá del siglo XXI podría ocurrir un aumento sustancialmente mayor a dos metros en el nivel de los océanos. Con relación a esto, hacen notar que el nivel del océano hace 125,000 años, cuando la temperatura de la Tierra era aproximadamente la misma que en la actualidad y la concentración de dióxido de carbono en la atmósfera era de 285 partes por millón, el nivel del mar estaba de seis a nueve metros por encima del nivel presente. Hace tres millones de años, por otro lado, cuando dicha concentración era aproximadamente igual a la actual y la temperatura terrestre era de uno a dos grados centígrados mayor que la temperatura hoy en día, el nivel del mar podría haber estado hasta unos veinte metros por encima del nivel presente.Lo anterior indicaría que la velocidad con que se ha dado el cambio climático no ha permitido que el planeta alcance un equilibrio, lo cual le tomaría cientos de años en lograr.Un incremento de seis metros en el nivel del mar, por otro lado, inundaría áreas costeras a lo largo de todo el mundo. Un mapa elaborado por la NASA y disponible en Internet nos indica, por ejemplo, que dado este incremento el sur de la Florida quedaría bajo el agua, lo mismo que las regiones costeras de nuestro país, particularmente las de la península de Yucatán. Y qué decir de las Islas Maldivas en el Océano Índico al sur de la India, que quedarían completamente sumergidas.    Afortunadamente para nuestra generación, el cambio climático global que estamos experimentando es un proceso relativamente lento, del que apenas somos conscientes y cuya conclusión final no alcanzaremos a atestiguar. Posiblemente no serán igualmente afortunadas las generaciones futuras, aunque es previsible que tendrán más conciencia del problema y contarán con mayores herramientas tecnológicas para resolverlo.  Por lo pronto, esperemos que los muy lamentables sucesos de París no den al traste con una reunión en la que se pretende atacar un problema grave para el mundo. Por más que, habría que reconocerlo, la experiencia de reuniones anteriores no da muchas esperanzas de que al problema del cambio climático se le de una solución efectiva en el mediano plazo.",
    "“Oye tú, Pimpis, que dizque la Ciudad Luz. ¿Dónde, digo yo? Ya quisieran tener la iluminación de Insurgentes para un día de fiesta”. Así se expresa  Junior, uno de los personajes de la novela “La región más transparente”, cuando se refiere a la iluminación de París en la década de los años cincuenta y la compara con la iluminación de la Ciudad de México en esa misma época. Cuando Carlos Fuentes publicó en 1958 “La región más transparente” habían pasado apenas unas ocho décadas desde que Edison y otros empresarios iniciaron la comercialización de las lámparas incandescentes. Como consecuencia de esta comercialización, la luz eléctrica rápidamente se expandió, en espacios tanto privados como públicos, a ciudades a lo largo de todo el mundo –incluyendo a Paris y a la Ciudad de México. En la actualidad, las lámparas incandescentes, que tanto cambiaron al mundo y que sobrevivieron por más de un siglo, están en vías de desaparición. Esto, por haber sido superadas por otros tipos de fuentes de luz, fundamentalmente por la lámpara LED – acrónimo del la expresión en inglés  “Light Emitting Diode” –, que en comparación es considerablemente más durable y eficiente. De manera más precisa, mientras que el tiempo de vida de una lámpara incandescente ronda a las 1,000 horas –un año, manteniendo la lámpara encendida un promedio de tres horas diarias–, el tiempo estimado de vida de una lámpara LED es de 10 a 50 veces más grande. Igualmente, mientras que menos de 5% de la energía eléctrica consumida en una lámpara incandescente se convierte en energía luminosa, en una lámpara LED  la cifra correspondiente es 6- 8 veces más grande.  No tiene de este modo la lámpara incandescente manera de competir con las nuevas fuentes LED en el campo de la iluminación –por más de que el costo  de estas últimas sea por el momento relativamente alto– y su destino es necesariamente el de la extinción. Hay que notar, además, que los usos de las lámparas LED no se limitan a la iluminación, sino que también incluyen aplicaciones tales como semáforos, lámparas para automóviles, pantallas de televisión, pantallas indicadoras y luces decorativas.Por otro lado, la tecnología de las lámparas LED es completamente diferente de aquella de las lámparas incandescentes, tanto en materiales de construcción como en su principio de funcionamiento.  El núcleo de un LED –en donde se genera la luz y cuya composición química determina el color emitido– contiene elementos químicos valiosos tales como el galio y el indio, que no forman parte de las lámparas incandescentes. En el caso de las lámparas LED blancas, además, dicho núcleo contiene elementos tales como el europio y el terbio.  Dado el valor de todos estos  materiales y el explosivo crecimiento que está experimentando la industria de las lámparas LED, los expertos se han dado a la tarea de buscar métodos novedosos para reciclar los materiales con los que están fabricadas.En relación a este último punto, en un comunicado de prensa emitido esta semana por el Instituto Fraunhofer en Alemania, se anuncia que investigadores de dicho Instituto han desarrollado técnicas para separar los diferentes componentes de una lámpara LED –en particular su núcleo– sin destruirlas. Una vez separados, los componentes podrán ser procesados de manera individual. Esto, esperan los especialistas, hará económicamente factible el reciclado de los materiales de un LED.  Por el momento, según los expertos, cuando la industria de las lámparas LED está aun en su fase inicial de crecimiento, el volumen de material a reciclar es todavía insuficiente para hacer económicamente viable el proceso de reuso. En estas condiciones,  las compañías recicladoras están solamente almacenando los desechos. En el futuro, sin embargo, en la medida en que la industria crezca y se sustituya una cada vez mayor proporción de lámparas incandescentes por lámparas LED, el proceso de reciclado será factible.Posiblemente no pasará mucho tiempo antes de que esto suceda. La velocidad con que está cambiando la luz artificial en nuestro planeta es incluso visible desde el espacio. A manera de ejemplo, en dos fotografías tomadas desde la Estación Espacial Internacional –que pueden ser consultadas en Internet– se muestra a la ciudad de Milán, Italia, en dos momentos recientes: en el año 2012, cuando el centro de la ciudad estaba iluminado con lámparas incandescentes, y en 2015, después de que se sustituyeron dichas lámparas por lámparas LED. Las imágenes que podemos ver son contrastantes: mientras que en 2012 el centro de Milán estaba iluminado con la luz rojiza característica de las lámparas incandescentes, en 2015 dicho color había cambiado al azulado propio de las lámparas LED. Además, en 2015 algunas áreas de la ciudad se muestran considerablemente más iluminadas que en 2012.   Las luces nocturnas de las ciudades del mundo han cambiado drásticamente a lo largo del último siglo y tal parece que lo seguirán haciendo.  Si bien no es claro si el cambio será para bien o para mal, los niveles de luz que tendremos en el futuro con seguridad irán a la alza. Y serán incluso más grandes que los de la Ciudad de México en la década de los años cincuenta.",
    "Como sabemos, el pasado domingo entró en vigor el horario de invierno y hubimos de atrasar nuestros relojes una hora. Contamos así con más tiempo de sueño, que, sin embargo, no todos aprovechamos, pues a algunos el cambio de horario nos provocó un cierto descontrol a lo largo del día que nos pareció más largo que de costumbre. Por lo demás, el cambio del horario de verano al de invierno provoca siempre menos molestias que el cambio en sentido inverso que se da en la primavera, cuando adelantamos una hora los relojes y tenemos que levantarnos más temprano. Si bien uno de los propósitos principales del horario de verano es el de ahorrar energía al contar con una hora más de luz natural hacia el final del día, hay quien opina que esto no sucede y que, por el contrario, el cambiar la hora tiene efectos negativos sobre nuestra salud. Se ha observado, por ejemplo, que en los días posteriores a la entrada del horario de verano se incrementa la tasa de mortalidad por problemas cardíacos, lo mismo que  por accidentes de tráfico.El estar cambiando nuestro ritmo circadiano de manera abrupta dos veces por año no parece ser entonces lo mejor para la salud y cabe preguntarse sobre la identidad del culpable de haber ideado esta práctica. Una búsqueda en Internet nos da la respuesta: el primero en proponer dicha práctica de manera formal fue un entomólogo neozelandés llamado George Hudson; lo hizo durante una presentación que realizó en una sesión de la Sociedad Filosófica de Wellington en 1895.¿Por qué un entomólogo pudo tener interés en hacer una propuesta para establecer un horario de verano? Al parecer Hudson, siguiendo su vocación de entomólogo, se dedicaba a cazar insectos al caer la tarde después de su jornada de trabajo. La falta de luz solar, sin embargo, le dificultaba la tarea y razonó que el problema podía solucionarse empezando el día oficialmente más temprano, por lo que propuso que los relojes se adelantaran dos horas durante el verano. Este adelanto, por otro lado, tendría beneficios no solamente para él, sino para todos aquellos interesados en realizar actividades al aire libre en horas cercanas a la puesta del sol. La propuesta de Hudson no fue bien recibida y fue motivo de numerosas críticas y hasta burlas de los presentes en la sesión de la sociedad, uno de los cuales sugirió que “los relojes podrían ser manejados teniendo diferentes manos”. Con el tiempo, no obstante, dicha propuesta fue vista con mejores ojos, al grado que Alemania la implantó en el año 1916, durante la Primera Guerra Mundial, con el objeto de ahorrar combustible. La medida fue posteriormente seguida por otros países, incluyendo a los Estados Unidos.La práctica de cambiar la hora dos veces por año cuenta con numerosos críticos y existen hoy en día propuestas para eliminarla. Rusia, por ejemplo, implantó en el año 2011 el horario de verano en forma permanente. Para justificar la medida, según el periódico “The Guardian”, el entonces presidente ruso argumentó: “Es irritante, las personas se levantan más temprano y no saben qué hacer con la hora extra. Por no mencionar  a las infelices vacas y a otros animales que no saben que los relojes han cambiado y no entienden por qué vienen a ordeñarlas en diferentes tiempos”. El horario de verano no fue bien recibido por los rusos y a partir del pasado año fue cambiado por el horario de invierno; también permanente, sin embargo.Existen en los Estados Unidos propuestas para eliminar los cambios de hora dos veces por año e implantar el horario de  verano en forma permanente. Una de las múltiples razones que se esgrimen para sustentar esta propuesta es que el horario de verano reduce la incidencia de robos y asaltos, particularmente durante el atardecer, cuando las personas regresan a sus casas desde sus centros de  trabajo. Este punto de vista está apoyado por un artículo por aparecer en la revista “The Review of Economics and Statistics”, publicado por Jennifer Doleac y Nicholas Sanders, de la University of Virginia y del “College of William and Mary”, en forma respectiva. En dicho artículo se presenta un estudio realizado con estadísticas criminales en los Estados Unidos, llevado a cabo con el propósito de determinar cómo el cambio de horario afecta a la actividad criminal. Encuentran que a partir del cambio al horario de verano la actividad de robos a lo largo del día se reduce en un 7%, mientras que en horas de la tarde, cuando hay más luz, este porcentaje es del 27%. La razón para lo anterior es que con mayores niveles de luz los malhechores se refrenan de cometer sus crímenes por el riesgo de ser identificados. Además, éstos no transfieren su actividad criminal a horas de la mañana cuando hay menos luz. Tal pareciera que prefieren seguir dormidos, aun a costa de sus utilidades.La anterior es pues un argumento adicional en contra de los cambios horarios, por si alguno faltara, y habría la posibilidad de que fuésemos testigos de su desaparición en un futuro no muy lejano. Esperemos que si así sucede, no se implante en nuestro país –por imitación– el horario de verano, que no pareciera ser el adecuado para la latitud en la que nos encontramos.",
    "A lo largo del último siglo se ha más que duplicado la esperanza de vida de la población a nivel global. Un factor que contribuyó a este incremento fue el desarrollo de los antibióticos, que estuvieron a disposición del público a partir de la segunda mitad de la década de los años cuarenta, al término de la Segunda Guerra Mundial. Mediante el uso de los antibióticos fue posible combatir bacterias que en el pasado causaron pandemias de grandes proporciones y que diezmaron a la población del mundo. Una bacteria de triste memoria en este sentido es la “Yersinia pestis”, causante de la peste bubónica, enfermedad que es trasmitida mediante la picadura de pulgas infectadas. La primera pandemia de la que se tiene noticia causada por la “Yersinia pestis” es conocida como Plaga de Justiniano, la cual asoló el Imperio Bizantino en el siglo VI de nuestra era y durante la cual habría muerto el 40% de los habitantes de Constantinopla. La plaga continuó con posterioridad de manera intermitente hasta aproximadamente el año 750 de nuestra era.Ochocientos años después la peste bubónica hizo una segunda aparición mayor en Europa; esta vez en la forma de la llamada Muerte Negra, con resultados aun más devastadores. Durante la Muerte Negra habrían muerto alrededor de 50 millones de personas. En los siglos que siguieron, la bacteria “Yersinia pestis” causó de manera intermitente otras epidemias, aunque de menores proporciones. Se le atribuye, por ejemplo, haber sido el agente infeccioso detrás de la “Gran Plaga de Londres” de los años 1666-1667, en la que habrían muerto 100,000 personas, un cuarto de la población londinense.Por cierto, a causa de dicha plaga la Universidad de Cambridge tuvo que ser cerrada de manera temporal. Esto obligó a Isaac Newton, quien era estudiante en la misma, a refugiarse en su casa familiar en el pueblo de Woolsthorpe. Su estancia forzada fuera de la universidad, sin embargo, no impidió a Newton proseguir con su trabajo de investigación. Lejos de eso, durante los dos años pasados en Woolsthrope Newton desarrolló algunos de sus más importantes descubrimientos, incluyendo la ley que gobierna el movimiento de los planetas. Regresando a acontecimientos más mundanos, la “Yersinia pestis” hizo un tercera aparición estelar en 1855, esta vez en China, extendiéndose después hacia otros lugares del mundo. La pandemia se prolongó hasta mediados del siglo XX y solo en China y la India mató a 12 millones de personas.  En la actualidad, si bien los antibióticos la mantienen bajo control, la “Yersinia pestis” aun se encuentra activa entre nosotros y puede ser trasmitida mediante el contacto con roedores infectados. Así, según el Centro de Control de Enfermedades de los Estados Unidos, en lo que va del año 15 personas en ese país han sido infectadas por dicha bacteria, en cuatro casos con resultados fatales. Estos números, sin embargo, son pequeños, de modo que la una vez temible bacteria no representa hoy en día mayor peligro.Por otro lado, si bien la “Yersinia pestis” ya no es de cuidado, sí puede todavía hacer noticia. Y en efecto, un artículo científico aparecido esta semana en la revista “Cell” afirma que esta bacteria ha vivido entre nosotros por bastante más tiempo del que se creía. Dicho artículo fue publicado por un grupo internacional de investigadores encabezado por Simon Rasmussen de la Technical University of Denmark, y en él se reportan los resultados de un estudio genético llevado a cabo con esqueletos humanos provenientes de diversos lugares de Europa y Asia, con antigüedades de varios miles de años.  El propósito del estudio fue el identificar rastros de la bacteria “Yersinia pestis” en los esqueletos, lo que indicaría que pertenecieron a individuos que murieron víctimas de la plaga. Estudiaron un total de 101 esqueletos, de los cuales 7 dieron resultados positivos. De éstos, en dos casos pudieron generar el genoma completo de la “Yersinia pestis”. Los restos más antiguos en los que pudo ser identificada la presencia de la bacteria pertenecieron a un individuo que murió hace 5,000 años. Es posible, sin embargo, que en esa época la bacteria no fuera tan contagiosa como lo fue durante las grandes pandemias. Esto es debido a que, de acuerdo a Rasmussen y colaboradores, carecía del gen que a la versión actual le permite alojarse en el tracto digestivo de las pulgas. En esas condiciones las pulgas no pudieron ser una vía de infección y la enfermedad tendría que haberse trasmitido mediante el contacto directo con la sangre o la saliva de la persona infectada, lo cual la hace menos contagiosa. Una vez infectada una persona, sin embargo, el índice de mortalidad era muy elevado.Así, de acuerdo con Rasmussen y colaboradores, hemos convivido –por lo general, de manera bastante incómoda– con la “Yersinia pestis” por varios miles de años. Y como resultado de dicha convivencia, según algunas fuentes,  habrían muerto unas 200 millones de personas. Hoy en día en que la bacteria no representa un problema mayor, lejanos nos parecen aquellos tiempos en los que podríamos haber muerto en el curso de unos pocos días a causa de una enfermedad cuyo origen quedaba en la más completa oscuridad. La experiencia tendría que haber sido escalofriante. Aunque al parecer no lo fue tanto para Isaac Newton.",
    "El finado director de cine Stanley Kubrick filmó en al año 1968  “2001: Odisea del espacio”, película que trata de una misión tripulada, ultra secreta, al planeta Júpiter. Dicha misión está a cargo de cinco astronautas, dos pilotos y tres que viajan en estado de hibernación a bordo de la nave espacial “Discovery One”. El control automático de dicha nave es llevado a cabo por “Hal”, una computadora de última generación, tan poderosa que podía razonar e incluso dialogar con los pilotos. Durante el viaje Hal se rebela y mata a cuatro de los astronautas, antes de morir al ser desconectada por el astronauta sobreviviente.“2001: Odisea del espacio” fue filmada en una época en la que los viajes espaciales eran una novedad. Hay que recordar que en esos momentos -1961–, hacía menos de una década que el soviético Yuri Gagarin se había convertido en el primer astronauta de la historia. Fue, además, una película que estableció un patrón de calidad en cuanto a efectos especiales en el cine, mismos que fueron diseñados con apego a las leyes de la física.En cuanto a la tecnología del futuro, Kubrick supuso que en el año 2001 serían posibles tanto los viajes tripulados interplanetarios como las computadoras inteligentes, y en ambos casos sus expectativas fueron, ahora lo sabemos, demasiado optimistas; aunque más cercanas a la realidad en el segundo caso. Por lo demás, el desarrollo tecnológico de las últimas décadas ha sido espectacular, y en algunos casos ha superado la visión del desarrollo futuro que Kubrick y sus contemporáneos tenían en los años sesentas.Así, si bien es cierto que las computadoras no han logrado alcanzar la inteligencia de Hal,  sí se han hecho cada vez más complejas e inteligentes, al grado que son capaces de competir exitosamente al más alto nivel como ajedrecistas, lo mismo que ganar concursos por televisión de gran sofisticación. Por otro lado, la tecnología ha seguido caminos de desarrollo que hubieran sido difíciles de predecir en la década de los sesenta. Pensemos, por ejemplo, en las computadoras personales, que contrastan en costo y tamaño con Hal que era todo menos un objeto personal. Menos aún podía haberse anticipado el desarrollo de la red Internet, de los teléfonos inteligentes y de las redes sociales, que le permiten mantener comunicación a distancia en forma permanente e instantánea a un grupo de personas.La tecnología es, ciertamente, un elemento que es cada vez más parte integrante de la vida moderna y en este contexto la manera como se están educando a las nuevas generaciones está bajo presión para que cambie.Un artículo publicado esta semana en la revista “Science” por investigadores de la Michigan State University (MSU), encabezados por Melanie Cooper,  aborda este tema. El argumento central de Cooper y colaboradores es que la educación en Ciencia, Tecnología, Ingeniería y Matemáticas (STEM, por sus siglas en inglés) debe tener como finalidad capacitar al estudiante en el uso de los conocimientos adquiridos a lo largo de su formación, en lugar de meramente buscar que el estudiante memorice conocimientos en campos cuya interconexión no alcanza a comprender. De acuerdo con Cooper: “Queremos que los estudiantes desarrollen una comprensión profunda de  ideas centrales sobre las que puedan construir y puedan poner en uso, no solamente hechos que puedan regurgitar”. Se deben, de este modo, definir tópicos centrales de estudio –para los dos primeros años de los programas de licenciatura– que el estudiante abordará a profundidad. Entre los tópicos que mencionan Cooper y colaboradores –definidos a través de discusiones interdisciplinarias en MSU– se encuentran: “evolución” en biología, “estructura y propiedades” en química”, y “las interacciones causan cambios en el movimiento” en física. Consideran igualmente el tópico “energía”, que abarca varios campos y que debe ser tratado de manera unificada en todos ellos. Según Cooper y colaboradores, la evaluación de los estudiantes debe hacerse no solamente sobre la base de sus conocimientos, sino también en base a como emplean dichos conocimientos. Como resultado de su entrenamiento, el estudiante debe ser capaz de resolver problemas a partir de los conocimientos adquiridos y empleando el método científico. Hace más de doscientos años Francisco de Goya pintó un pequeño cuadro al que puso por título “La letra con  sangre entra”. Dicho cuadro, que habría sido pintado por Goya como una crítica a los métodos educativos de su época, muestra a un niño con los pantalones bajados e inclinado hacia delante a punto de ser azotado con un látigo por su maestro. Si hemos de creerle a Goya, la efectividad pedagógica de prácticas como la que retrató en su cuadro –que habrían sido comunes en la época– era sumamente baja. Esto, a pesar de que los propósitos de la educación hace dos siglos eran limitados, mayormente a la memorización de textos y de hechos.     En la actualidad, si bien el adagio “La letra con sangre entra” ya no se toma tan a pecho, la educación universitaria, de acuerdo con Cooper y colaboradores, sigue teniendo una componente de memorización propia del pasado y poco acorde con la época y el previsible futuro inmediato. Que si bien puede quedarse corto en expectativas, puede igualmente reservarnos sorpresas tecnológicas.",
    "Siendo Marte el planeta del sistema solar más parecido a la Tierra –y el segundo más cercano en promedio –, ha sido frecuentemente motivo de especulaciones sobre la posibilidad de que pueda albergar alguna forma de vida, incluso inteligente. En este sentido, al inicio del siglo pasado el astrónomo norteamericano Percival Lowell estaba convencido  de que las líneas que el italiano Giovanni Schiaparelli había observado con su telescopio en 1877 sobre la superficie de Marte eran canales que los marcianos habrían construido para transportar agua desde los casquetes polares hacia las regiones desérticas en el ecuador del planeta. Siguiendo su creencia, Lowell dedicó mucho tiempo para tratar de confirmarla hasta que se convenció de lo contrario.Marte ha estado también presente en las novelas de ficción. Dicho planeta es, por ejemplo, el lugar de origen de los extraterrestres que invaden a la Tierra en la novela “La guerra de los mundos” del escritor británico H.G. Wells, quien se habría inspirado para escribirla en las ideas de Lowell. De la misma manera, el escritor de ciencia ficción Ray Bradbury en su libro “Crónicas Marcianas” da por asentado que en Marte hay vida inteligente.Sin embargo, a pesar de nuestra fascinación por Marte, las condiciones físicas que prevalecen en su superficie no son nada amigables para la vida superior tal como la conocemos. Así, las fotografías que nos hacen llegar las sondas de la NASA desde la superficie de Marte muestran un lugar extremadamente árido y pedregoso, sin el menor rastro de vida. Sabemos, además, que la atmósfera marciana es extremadamente delgada y compuesta fundamentalmente de bióxido de carbono y prácticamente nada de oxígeno. Sabemos también que, dada su distancia al Sol, la superficie del planeta es muy fría, con una temperatura promedio de menos 55 grados centígrados y una variación entre el día y la noche que puede alcanzar los 100 grados centígrados.   Un peligro adicional para la vida en Marte son las radiaciones alta energía provenientes del espacio que sabemos pueden provocar cáncer. A diferencia de la Tierra, Marte no posee un campo magnético que desvíe dichas radiaciones, las cuales continuamente bombardean su superficie. Lo delgado de la atmósfera marciana –posiblemente producto del bombardeo cósmico– no ayuda a atenuar dichas radiaciones, como tampoco lo hace con los rayos ultravioleta del Sol que son también cancerígenos.Por todo lo anterior, Marte no es un lugar hospitalario, al menos no como lo es nuestro planeta, y nadie espera que pueda albergar vida más allá de su versión microbiana. En este sentido, un estudio llevado a cabo por el explorador ”Curiosity” que se encuentra desde agosto de 2012 en la superficie de Marte, arrojó resultados alentadores. No ha descubierto “Curiosity” microbios en Marte, pero sí signos claros de que en algún momento en su historia geológica existieron ríos y lagos de agua estables en su superficie; y, como sabemos, el agua es un elemento esencial para el desarrollo de la vida. Dicho estudio fue publicado esta semana en la revista “Science” por un grupo internacional de investigadores encabezado por J.P Grotzinger del “California Institute of Technology”. Dicho grupo analizó fotografías enviadas por “Curiosity” de depósitos sedimentarios en el interior del cráter Gale en Marte que develaron la historia geológica de esa área del planeta. Los investigadores encontraron en el interior del cráter depósitos materiales que probablemente fueron arrastrados hasta ahí por ríos de agua, así como capas rocosas producto de la sedimentación de materiales en lo que habría sido el fondo de un lago.    De este modo, y de estar Gortzinger y colaboradores en lo correcto –lo mismo que otros investigadores que han llegado a la misma conclusión–, en el pasado Marte era un lugar considerablemente más húmedo en comparación con la actualidad, apuntando a la posibilidad de que en el pasado hubieran existido condiciones favorables para el desarrollo de la vida. El agua, por otro lado, es solo uno de los elementos esenciales para la vida. Adicionalmente, es también necesario contar con los elementos químicos que componen las moléculas de la vida y con las fuentes de energía para sintetizarlas , sean estas últimas de origen solar o geotérmico.  Y, por supuesto, es necesario contar con un medio ambiente que no le sea tóxico a la vida, como sí lo es en la actualidad la superficie marciana, bañada continuamente con radiaciones cósmicas de alta energía. Para el desarrollo de la vida es necesario que se conjunten varios factores y para  averiguar si éstos se dieron el pasado en Marte, lo mismo que para detectar la posible presencia de vida, pasada y presente, en ese planeta, la Agencia Espacial Europea tiene planeado enviar las misiones ExoMars a Marte en 2016 y 2018, mientras que La NASA planea enviar la misión Mars 202 con el mismo propósito.   No podemos por el momento saber si existe, o existió en un pasado remoto, vida en Marte. Es posible que, por otro lado, con un poco de suerte no tengamos que esperar mucho tiempo antes de averiguarlo de tener éxito las misiones europea y norteamericana planeadas para los próximos años. En todo caso, de encontrase alguna forma de vida marciana, ésta será simple, muy alejada de la vida inteligente imaginada por Lowell, Wells y  Bradbury.",
    "Hay un acuerdo generalizado entre los especialistas en que el ejercicio físico tiene efectos benéficos sobre la salud y que una actividad física sostenida de mediana intensidad ayuda a prevenir, e incluso a revertir, enfermedades modernas tales como la obesidad, la diabetes tipo 2 y los problemas cardiovasculares. Esto ha llevado a muchos científicos a plantear que el ejercicio físico es el método de preferencia para prevenir y tratar dichas enfermedades, producto precisamente del estilo sedentario de vida que llevamos en la actualidad.    Un obstáculo para esto último es que muchos pacientes a lo que se les ha prescrito un cierto régimen de ejercicio no lo llevan a cabo como debieran, sobre todo si tienen que realizarlo en su casa. Con relación a esto, de acuerdo con datos de diferentes fuentes, menos del 50% de los pacientes que inicialmente convienen en realizar un cierta rutina de ejercicios físicos mantiene su compromiso en el largo plazo. Ciertamente, realizar esfuerzos físicos puede ser agotador y por tanto tendemos a evitarlos. A menos que estemos firmemente convencidos de su utilidad.    Para los que no lo están –o son enemigos mortales del gimnasio– hay afortunadamente buenas noticias: en diferentes laboratorios en el mundo se están llevando a cabo investigaciones para desarrollar “píldoras para ejercitarse”, mismas que al ingerirlas se espera produzcan en el cuerpo los mismos efectos que el ejercicio físico. Así, si alguna vez llegaran a estar disponibles al publico, podremos quizá ir a una farmacia y comprar un frasco de píldoras equivalente a “5 horas semanales de caminata a paso acelerado”. En tal situación podrían tener condición de atleta aun aquellos que se pasan ocho horas en el trabajo sentados ante un escritorio –además de un tiempo considerable en su casa enfrente de la televisión– y que solo por excepción se ejercitan. Dado el caso, las píldoras del ejercicio serían de gran atractivo –y un gran negocio para quien las fabrique–, contribuyendo a incrementar la calidad de vida de aquellos contrarios al ejercicio.El que haya píldoras para ejercitase en desarrollo en varios laboratorios de investigación en el mundo es, ciertamente, una muy buena noticia para algunos. Al lado de ésta, sin embargo, hay también una mala: no se espera que las píldoras del ejercicio estén listas para su uso en el corto plazo, además de que los expertos no consideran que podrán sustituir por completo al ejercicio físico real.  Con relación a lo anterior, en un artículo que tiene como autores a Shunchang Li y a Ismail Laher,  este último de la “University of British Columbia”, Canadá, se examinan en forma crítica las varias sustancias químicas que son candidatas para desarrollar píldoras del ejercicio, y se discuten  los beneficios que podrían tener en comparación con el ejercicio real. Dicho artículo lleva por título “Las píldoras del ejercicio: En la línea de salida” y está por aparecer en la revista “Trends in Pharmacological Sciences”. De acuerdo con Laher, la necesidad que existe de contar con píldoras del ejercicio ha sido reconocido ya por algún tiempo y en estos momentos es una meta alcanzable. Las píldoras bajo estudio están en una etapa inicial de desarrollo y se enfocan en la obtención de músculos esqueléticos más fuertes y rápidos. Hace notar Laher, sin embargo, que los beneficios del ejercicio físico no se circunscriben al desarrollo de los músculos, sino que el mismo también impacta a la función cognitiva, al tiempo que contribuye a una mayor fortaleza de los huesos y a una mejora en la función cardiovascular. Así, según Laher, “no es realista esperar que las píldoras del ejercicio sean capaces de sustituir por completo al ejercicio físico, al menos no en el futuro inmediato”. Si bien en el momento en que estén disponibles dichas píldoras –asumiendo que sea el caso– beneficiarán a aquellos alérgicos al ejercicio, Laher prevé que tendrán su impacto más grande entre aquellos impedidos por alguna razón para realizar ejercicio físico. En particular, considera que las personas que sufren de parálisis por una lesión en la espina dorsal se encontrarán entre los más beneficiados, dado el deterioro en las funciones cardiovascular y muscular que su relativa inmovilidad les acarrea.De este modo, según Laher, en un futuro todavía no determinado, podremos tener algunos de los beneficios del ejercicio físico sin el ejercicio mismo, simplemente ingiriendo las píldoras adecuadas. Esto con seguridad suena atractivo para muchos. Para otros nos resulta un poco absurdo. Esto último dado que estaríamos empleando un producto de alta tecnología –las píldoras del ejercicio– para reemplazar una actividad –la de caminar y correr– que sabemos hacer muy bien desde tiempo inmemorial.",
    "El pasado viernes 18 de septiembre la oficina gubernamental encargada de la protección ambiental en los Estados Unidos (EPA) envió un comunicado a la compañía Volkswagen acusándola de haber vendido en ese país, desde el año 2009,  automóviles con motor diesel –incluyendo modelos de Jetta, Golf, Passat, y Beetle– que violan las leyes estadounidenses de protección ambiental. Esto, como sabemos, desató un escándalo de grandes proporciones y cobertura mediática que amenaza con sanciones de muchos miles de millones de dólares para VW.De manera específica, la EPA acusa a Volkswagen de haber introducido un “switch” en el software de control del motor de sus automóviles, el cual conmuta la operación de dicho motor entre modos de baja y alta emisión de contaminantes. Todo esto, con el propósito de pasar las pruebas que aplica la EPA para la venta de automóviles en los Estados Unidos. El software de control del motor diesel de los automóviles VW es lo suficientemente sofisticado para averiguar cuando el automóvil está sujeto a una prueba –por la posición del volante, la velocidad del automóvil, el tiempo que dura encendido el motor, e incluso por la presión barométrica– y conmutar al modo de baja emisión. Esto es importante, dado que en el modo alto los automóviles de la VW podrían emitir óxidos de nitrógeno en cantidades hasta 40 veces más altas que las permitidas por la norma ambiental. En contraste, en el modo bajo estos contaminantes estarían dentro de dicha norma.      Por supuesto, la pregunta obligada es ¿por qué, si los automóviles son capaces de operar en un modo de baja emisión de contaminantes y cumplir la norma ambiental, existe un segundo modo de operación con niveles de contaminación sustancialmente más altos? La respuesta es que en el modo bajo la eficiencia del motor diesel de VW es menor a la del modo alto. Así, de operar los automóviles en el modo menos contaminante se comprometería su competitividad en el mercado de los Estados Unidos. Como parte del escándalo mediático se han publicado artículos que incluyen opiniones acerca de cómo el mismo afectará a la compañía VW en particular y a la figura “Made in Germany” en lo general. En relación a esto, en el comunicado de la EPA del pasado 18 de septiembre se precisa que la multa por cada unidad comercializada es de $37,500 dólares. Si multiplicamos esta cantidad por el número de vehículos marca VW vendidos en los Estados Unidos desde 2009, que es alrededor de 500,000 –11 millones a nivel global–, resulta que VW tendría que pagar en multas alrededor de 18,000 millones de dólares. Además, podría enfrentar demandas por parte de los propietarios de sus automóviles debido a que los vehículos usados han bajado de precio a causa del escándalo. De aplicarse estas y otras sanciones, el impacto económico sobre la VW podría ser de grandes proporciones.En cuanto a afectación de la imagen de la marca VW, en un artículo publicado en el magazín “Marketing”, Sven Reinecke de la Universitat St. Gallen en Suiza considera que el escándalo por el que atraviesa la VW será superado dada su condición de compañía de gran prestigio, y que el despido de su director general hace algunos días es el primer paso en esta dirección. Solo que en un futuro cercano surjan otros escándalos semejantes es que la integridad de la compañía podría ser puesta en peligro.En cuanto al prestigio de la figura “Made in Germany”, Reinecke afirma que no hay ninguna posibilidad de que resulte afectada, dado que la imagen de una nación tiende a permanecer inalterada por largo tiempo. En este respecto, hace notar que Alemania no solo es asociada con la calidad, la cerveza, los automóviles, la limpieza y la puntualidad, sino también con Hitler y con la Segunda Guerra Mundial. Además, la VW no es la única compañía que sostiene la imagen de su país.  Por otro lado, por algún tiempo se ha sospechado que varias compañías fabricantes de automóviles diesel usan software como el empleado por la VW para lograr superar las normas ambientales. Así, un estudio publicado por la organización no gubernamental “Transport and Environment” indica que compañías como BMW, Citroen, Opel y Mercedes Benz, fabrican automóviles que exceden los limites permitidos de emisión de contaminantes. En  este respecto, la VW no es, entonces, un caso único.El que haya varios que lo hagan, por supuesto, no es ningún consuelo y no quita ninguna responsabilidad a la compañía VW por haber engañado a sus clientes vendiéndoles automóviles sin las características prometidas en cuanto a emisión de contaminantes –los cuales sabemos son dañinos para la salud–. Por lo demás, dado que los escándalos viven poco tiempo en los medios de comunicación, si hemos de creerle a Reinecke, no pasará mucho tiempo antes de que el escándalo que hoy nos ocupa quede en el olvido.",
    "Desde su introducción por Apple en 2007, el número de teléfonos inteligentes en los Estados Unidos creció de forma extremadamente acelerada. Según estadísticas del “Pew Research Center”, en octubre de 2014 el 64% de los adultos norteamericanos poseían uno de estos aparatos. Los teléfonos inteligentes son así el dispositivo que más rápidamente ha sido aceptado por la población y en este sentido, como entes inteligentes, podrían sentirse orgullosos del éxito alcanzado.En otro contexto, también en relación con productos de alta tecnología –aunque no tan inteligentes como los teléfonos–, la generación de energía eléctrica empleando paneles solares, la llamada energía fotovoltaica, ha crecido igualmente de forma impresionante. En efecto, si bien en números absolutos este tipo de energía juega todavía un papel menor –alrededor de 1% de la generación global de energía eléctrica tiene origen fotovoltaico–, en términos relativos el crecimiento que ha experimentado en los últimos años rivaliza con el de los teléfonos inteligentes. Por citar solo una cifra, mencionaremos que la capacidad de generación fotovoltaica en el mundo ha crecido por un factor de 100 en los últimos 14 años.En contraste con los teléfonos inteligentes, la celda solar de silicio –componente central del 90% de los paneles solares actualmente en uso– es un dispositivo que en su forma básica ha existido desde la década de los años cincuenta del siglo pasado. Su desarrollo original fue motivado por la entonces naciente era espacial y la necesidad de proveer de energía a los satélites artificiales. Desde entonces a la fecha, si bien las celdas solares de silicio han evolucionado, no lo han hecho en su estructura básica, sino en aspectos tales como la eficiencia de conversión de energía solar en energía eléctrica que es alrededor del 15% en los paneles comerciales.Dada la velocidad con que está incrementándose la capacidad de generación de electricidad fotovoltaica del mundo, los expertos vaticinan que en un corto tiempo se llegará al punto en el que que el 2% de la electricidad generada a nivel global tenga un origen solar. Sobre cuándo ocurrirá esto no hay un acuerdo generalizado y mientras que un experto vaticina que dicha marca se alcanzará en una fecha tan cercana como 2017, otro  es más conservador vaticinando que el 2% se alcanzará  en el año 2021. Cualquiera de los dos escenarios, sin embargo, implica un crecimiento de la electricidad solar extremadamente acelerado. Incluso más acelerado que en el pasado reciente, cuando tomó 14 años alcanzar un porcentaje de generación del 1%. De cumplirse las predicciones, llegar al segundo 1% tomará un tiempo considerablemente menor.Por otro lado, dado que la energía solar está muy diluida –en un metro cuadrado a pleno sol inciden aproximadamente 1,000 watts de potencia solar, de la cual es aprovechable apenas un 15%– la generación masiva de electricidad solar requerirá cubrir de paneles solares grandes extensiones de terreno. Esto implica que dichos paneles tendrán que ser fabricados con materiales que abunden en nuestro planeta. Afortunadamente, el silicio es después del oxígeno el elemento más abundante en la corteza terrestre. Sin embargo –no obstante su abundancia y sus muchas virtudes–, el silicio tiene también desventajas. Una de estas desventajas es su relativa transparencia a la radiación del sol. Esto obliga a que, con el fin de captar una proporción apreciable de dicha radiación, una celda solar deba tener un espesor de aproximadamente un medio milímetro; espesor que si bien no parece demasiado grande, está en relación directa con el volumen de silicio necesario para construir una celda solar y por lo tanto con su costo de fabricación. Así, dado que el costo de un panel solar es un factor  crítico para competir con otras fuentes de energía –fundamentalmente aquellas basadas en los combustibles fósiles–, el espesor de las celdas de solares de silicio resulta ser una característica desventajosa.    En estas circunstancias, un campo de investigación actual de gran actividad es el relativo al desarrollo de la celdas solares llamadas de “película delgada”, las cuales tienen espesores de aproximadamente un milésimo de milímetro. La investigación en esta dirección ha llevado al desarrollo de celdas de película delgada que compiten con las celdas de silicio. Los materiales empleados para construir dichas celdas, sin embargo, no tienen la abundancia del silicio. Parecería, entonces, que, a mediano plazo, la electricidad solar tendrá que depender en buena medida de las celdas de silicio.Ambos, teléfonos inteligentes y paneles solares han tenido un crecimiento acelerado en los últimos años. Los primeros han tenido un grado tal de penetración entre la población que, como bien nos consta, han modificado incluso algunas prácticas sociales. Dado su éxito, los teléfonos inteligentes podrían asumir incluso una actitud arrogante.  Los paneles solares, en contraste, no han tenido un impacto social equivalente y su visibilidad entre la población es solo moderada; en parte porque aun no son una fuente mayor de energía. Todo apunta, sin embargo, que a mediano plazo la energía solar asumirá el papel que le corresponde como la fuente natural de energía del mundo. Y cuando esto ocurra, aun los teléfonos inteligentes –o bien los dispositivos que los hayan sustituido– tendrán que reconocer que sin energía son nada.",
    "Hace quizá algunos millones de años –una fecha más precisa está todavía por definirse–, los cuerpos de un grupo de homininos –nuestros ancestros inmediatos y sus parientes cercanos– fueron depositados o arrojados al fondo de una caverna situada cerca de Johanesburgo, Sudáfrica, en una oquedad conocida como cámara Dinaledi. Dicha cámara está situada a una profundidad de unos treinta metros y a una distancia de ochenta metros en línea recta desde la entrada actual a la cueva. La cámara Dinaledi es de difícil acceso. Se accede a la misma en completa oscuridad, desde una antecámara situada a unos 21 metros de profundidad. Una vez en la antecámara, hay que subir 15 metros hasta lo alto de un escarpado bloque de piedra, y de ahí bajar hasta el fondo de la cámara Dinaledi, pasando por una abertura vertical de apenas 20 centímetros de ancho.No es claro cual fue la intención de aquellos que llevaron los cuerpos de los homininos hasta un el lugar oculto en el que fueron descubiertos. Lo que sí se sabe es que los restos encontrados pertenecen a una especie del género Homo hasta ahora desconocida. Esto al menos es lo que sostiene un grupo internacional de investigadores encabezado por Lee Berger de la “University of the Witwatersrand” en Sudáfrica en dos artículos publicados esta semana en la revista “eLife”. Berger y colaboradores fueron los descubridores de los restos fósiles de la cámara Dinaledi. El descubrimiento se llevó a cabo durante dos expediciones  llevadas a cabo en noviembre de 2013 y en marzo de 2104, mismas que produjeron 1,550 huesos fósiles que corresponden a cuando menos 15 individuos de todas las edades, desde neonatos hasta individuos de edad avanzada. El esqueleto reconstruido a partir de los fósiles encontrados muestra una rara mezcla de características modernas y primitivas. Indica, por ejemplo, que un macho adulto tenía una estatura de 1.50 metros y un cerebro del tamaño del de un gorila. Muestra, igualmente, que tenía manos adaptadas para el manejo de herramientas, al mismo tiempo que dedos curvos apropiados para trepar a los árboles. Esto indicaría que si bien los homininos encontrados en la cámara Dinaledi hacían uso de herramientas, también vivían parte de su tiempo en los árboles. Dadas estas características, Berger y colaboradores consideran que han descubierto una nueva especie de Homo al que han bautizado como “Homo naledi”.¿Cómo fue que los restos fósiles de los homininos fueron a parar al fondo de la cámara Dinaledi? Por el momento esto constituye un misterio. Así, si bien en el pasado pudieron existir otras entradas a la cámara de Dinaledi diferentes a la actual, de acuerdo con Berger y colaboradores es improbable que éstas no presentaran dificultades similares a las actuales para acceder a la misma. Descartan, por ejemplo, que pudiera haber existido una entrada vertical –por la que los homininos pudieran haber caído accidentalmente.  Si las dificultades para acceder a dicha cámara hubiesen siempre existido, incluyendo la de tener que recorrer un largo y angosto pasadizo en completa oscuridad, es difícil encontrar motivaciones por las que voluntariamente los homininos hubieran penetrado hasta la cámara de Dinaledi y ahí encontrado la muerte.   Así, los investigadores descartan que la cámara Dinaledi haya sido usada como vivienda por los homininos, quienes tendrían que haber penetrado a la misma empleando luz artificial –además de que no existen signos de ocupación de la cámara. –. Descartan, igualmente, que éstos hubieran sido llevados hasta ahí por animales depredadores, pues es también difícil entender lo que habría motivado al depredador a penetrar a una cueva en completa oscuridad y con muchos obstáculos, cargando además una presa de dimensiones apreciables.Como la explicación más probable, Berger y colaboradores considera que los homininos cuyos restos fueron encontrados en la cámara Dinaledi entraron para morir ahí o bien fueron llevados muertos a propósito como parte de un cierto ritual. De ser esto cierto, homininos tan primitivos como los Homo naledi habrían tenido patrones de comportamiento complejo que normalmente se asocian a individuos más evolucionados. Como es usual, no todos los especialistas están de acuerdo con las conclusiones de Berger y colaboradores y hay quien pone en duda que hayan descubierto una nueva especie del género Homo. Ponen en duda, igualmente, la hipótesis según la cual la nueva especie era capaz de comportamientos con un cierto grado de complejidad. Algunos incluso afirman que el “Homo naledi” y sus prácticas mortuorias están más dirigidas a los medios masivos de comunicación que a la comunidad científica.Y, como siempre, solo estudios y descubrimientos adicionales podrán arrojar luz sobre el asunto y eventualmente resolver la controversia. En particular, los fósiles de la cámara Dinaledi no han sido fechados y no sabemos si tienen una antigüedad de cientos de miles o de millones de años. Tendremos así que esperar por estos estudios adicionales para averiguar más acerca de los homininos enterrados en la cámara  Dinaledi. Aunque es posible que nunca lleguemos a desvelar sus secretos.",
    "Es bien conocido el apego que los perros tienen con su amo y hay un buen número de historias en las que este apego se manifiesta en grado extremo. Una de las más dramáticas es la de Hachikó, el perro japonés que esperó por nueve años el arribo de su amo muerto en una estación del metro de Tokio. Según la historia, Hachikó acostumbraba esperar todos los días la llegada de su amo, quien era profesor de la Universidad de Tokio, a la estación del metro al final del día de trabajo. Un día el profesor murió en la universidad de un derrame cerebral y el perro se quedó esperándolo en la estación. De manera sorprendente, Hachikó lo hizo por nueve años, a lo largo de los cuales acudió diariamente a esperar al tren en el que arribaría su amo. En recuerdo a la lealtad de Hachickó, los japoneses le  levantaron una estatua de bronce con su efigie a la entrada de la estación del metro en donde hizo guardia inútilmente por una buena parte de su vida.Dado los lazos afectivos que pueden desarrollarse entre los humanos y los perros, no es sorprendente que haya tantas personas que los adopten como mascotas –y en algunos casos incluso como miembros de la familia–. En cambio, llama la atención que  los gatos, que no han demostrado hacia los humanos la misma lealtad que los perros, superen a estos últimos en popularidad como mascotas, tanto en los Estados Unidos como en Europa.La aparente actitud de indiferencia –o al menos de menor atención– de los gatos  hacia nosotros en comparación con los perros, ha llevado a algunos a calificar a los primeros de  seres inútiles y malévolos, a lo que les gusta atormentar a animales más pequeños, y los que ni siquiera se preocuparían por alertar a los dueños de la casa en la que cohabitan en la eventualidad de que se hubiera introducido un ladrón –lo que sí es una virtud de los perros.  La creencia popular, según la cual con los gatos no nos unen el mismo tipo de lazos afectivos que nos unen con los perros, encuentra apoyo científico en un artículo publicado esta semana en la revista en línea PLOS ONE por Alice Potter y Daniel Simon Mill de la University of Lincoln en el Reino Unido. En efecto, empleando una metodología desarrollada por los sicólogos para determinar el grado con el que un niño pequeño está apegado a su madre, Potter y Mill encuentran que los gatos muestran independencia con respecto a aquellos que los cuidan.Los experimentos de Potter y Mill fueron llevados a cabo con 20 gatos y sus respectivos propietarios. Las pruebas consistieron en observar las reacciones de los animales ante la ausencia de su amo y/o la presencia de un extraño. Encontraron que si bien los gatos vocalizan más en la presencia de extraños, no muestran otros signos que revelen angustia por la ausencia del amo, con el que prefieren interactuar, pero al que no conciben como “un foco de seguridad y protección en un mundo peligroso”. Los gatos, en comparación con los perros y los bebés humanos, tienen un mayor grado de independencia con respecto a sus cuidadores, lo que estaría de acuerdo con su naturaleza de cazadores solitarios.Nuestra capacidad de comunicación con los gatos es, por supuesto, limitada y no tenemos la seguridad de que es lo que en realidad pensaron durante el experimento, cuando no estuvo presente el amo o cuando estuvieron en la compañía de un extraño. De hecho, en un estudio anterior al de Potter and Mill –citado como antecedente en su artículo– se llega a la conclusión que los gatos sí forman lazos afectivos con sus amos, lo que indica la dificultad de penetrar en su mente.No podemos, ciertamente, penetrar en el cerebro de los gatos. A menos que nos adentremos al mundo de la ficción, en donde podemos encontrar, por ejemplo, al libro “Yo soy un gato”, del escritor japonés Natsume Soseki “Yo soy un gato” es una novela escrita en la primera década del siglo XX que tiene como protagonista y narrador a un gato; un gato tenido en tan poco aprecio por sus dueños –y por todos aquellos que vivían en su vecindario– que ni siquiera tenía un nombre. A lo largo de la novela, el gato sin nombre describe su relación con los humanos –que podeos presumir no era del todo buena– y da una visión crítica de lo que ve a su alrededor. Si bien la novela de Natsume Soseki es fascinante, no podemos esperar que arroje luz alguna sobre el funcionamiento de la mente de los gatos  –y  sí, en cambio, sobre la sociedad japonesa cien años atrás–. Si se incluye aquí es solamente para mencionar un relato, con un gato como protagonista, que equilibre de alguna manera en nuestro ánimo los aspectos negativos de la aparente indiferencia de los gatos –según Potter y Mill– hacia los humanos.",
    "La máquina de vapor, perfeccionada en Escocia por James Watt, fue una pieza clave en el desarrollo de la Revolución Industrial que, como sabemos, nació en Inglaterra hace unos 250 años. Junto a la máquina de Watt, otra pieza fundamental para dicho desarrollo fue el carbón, usado como combustible para su funcionamiento. Con la máquina de Watt y el carbón se inició la era del uso de los combustibles fósiles, mismos que, con el correr de los años, se diversificaron para incluir al petróleo y al gas natural.   La Revolución Industrial ha sido considerada como el acontecimiento más importante para el desarrollo de la civilización después de la invención de la agricultura.  Al lado de sus muchos aspectos positivos, sin embargo la Revolución Industrial ha tenido también consecuencias negativas. Una de éstas tiene que ver con el medio ambiente. De manera específica, con la contaminación atmosférica con gases de invernadero resultante de la quema de combustibles fósiles.La semana que hoy termina fue pródiga en noticias relacionadas con los combustibles fósiles y la contaminación que de una u otra manera producen en el medio ambiente. Por un lado, durante su participación en el “National Clean Energy Summit” en Las Vegas, Nevada, el pasado lunes 24 de agosto, el presidente Barack Obama se manifestó por las energías renovables, en forma particular a la energía solar, como un medio para atacar el problema del calentamiento global.  Manifestó en su intervención el presidente Obama que, “Estamos aquí porque creemos que no hay ningún desafío que presente una amenaza mayor a nuestro futuro que el cambio climático”. Afirmó también que “La energía solar es más barata que la energía convencional que proporcionan las compañías proveedoras de servicios” y que no es tiempo de retirar el apoyo federal a los proyectos de energía renovable. Si bien hay quienes dudan que el calentamiento global sea un problema real, el caso es que la temperatura promedio del planeta está experimentando una tendencia a la alza –a pesar de la supuesta “pausa” que, según algunos estaría experimentando el clima de la Tierra–. Así, según la National Oceanic and Atmospheric Adminstration (NOAA)) de los Estados Unidos, 2014 fue el año más caliente en promedio desde que se mantienen récords y es posible que 2015 lo supere en este respecto. De la misma manera, también según la NOAA, el pasado mes de julio ha sido el más caliente desde 1880, con una temperatura promedio 0.81 grados centígrados por arriba de la temperatura promedio del mismo mes a lo largo del siglo XX.  La semana que hoy termina nos trajo también noticias relativas a los combustibles fósiles en otra dirección. De manera específica, en la forma de un artículo  publicado por especialistas de la “University of Missouri” en los Estados Unidos, encabezados por Susan Nagel, en el que se examinan los potenciales riesgos de salud asociados a la técnica de fractura hidráulica para la extracción de petróleo y gas natural. La técnica de fractura hidráulica requiere de la inyección al subsuelo de grandes cantidades de agua y sustancias químicas a altas presiones, con el objeto de fracturar la roca subterránea en la que se encuentra atrapado el petróleo o el gas natural,  que de esta manera fluye hacia la superficie. Esto posibilitó explotar yacimientos que antes se consideraban inaccesibles. En los Estados Unidos la fractura hidráulica ha tenido tal éxito que ese país obtiene mediante su empleo aproximadamente la mitad de  su producción actual de petróleo y gas natural.Existe, sin embargo, una controversia sobre los potenciales efectos a la salud humana que pueden tener las sustancias químicas inyectadas en el subsuelo. Los expertos consideran que existen varias vías por las que la población que vive en la vecindad de los pozos explotados por fractura hidráulica pueda se expuesta a dichas sustancias. Una posibilidad es que contaminen los mantos acuíferos pues parte de los fluidos inyectados reemergen a la superficie. Según Nagel y colaboradores, son alrededor de mil las sustancia químicas empleadas en la técnica de fractura hidráulica y algunas de éstas son adversas a la salud. En particular, estos investigadores están preocupados por aquellas sustancias que se sabe o se sospecha interfieren con el funcionamiento hormonal de cuerpo.  Con esto en mente, hicieron una revisión de más de cien artículos publicados en los que se estudian, por un lado, las posibles vías de dispersión de las sustancias contaminantes, y por el otro, los efectos que dichas sustancias tienen en el funcionamiento hormonal del cuerpo. Sobre las base de esta revisión Nagel y colaboradores concluyen que las actividades relacionadas con la fractura hidráulica “pueden potencialmente dispersar mezclas complejas de sustancias químicas que interfieren con el sistema endócrino y pueden potencialmente dañar el desarrollo humano y la reproducción”.Durante 250 años hemos hecho uso de la energía acumulada en los combustibles fósiles a lo largo de cientos de millones de años. Lo hemos hecho con tanto entusiasmo que terminamos por afectar a la salud planeta. Y ahora, de ser correctas las sospechas de Nagel y colaboradores, estaríamos poniendo en peligro incluso nuestra propia salud.",
    "El pasado jueves 20 de agosto, especialistas del Instituto Nacional de Antropología e Historia (INAH) dieron a conocer el hallazgo de los restos de lo que consideran fue el gran Tzompantli de Tenochtitlan. Dichos restos fueron localizados a una profundidad de dos metros en el centro de la Ciudad de México, a espaldas de la Catedral Metropolitana. La investigación que llevó al descubrimiento del gran Tzompantli, que estaba asociado al Templo Mayor, se prolongó de febrero a junio del presente año.Como sabemos, un Tzompantli –o zompantli, según la Real Academia Española– era una estructura en la que se exhibían cabezas de prisioneros de guerra víctimas de sacrificios rituales. Para fijar las cabezas se les practicaban orificios parietales y se ensartaban en vigas de madera sostenidas por postes verticales, a manera de formar una pared. Los restos del gran Tzompantli descubiertos consisten de una plataforma rectangular de 45 centímetros de altura, un ancho de seis metros y un largo que se estima puede llegar a los 34 metros. Asimismo, se encontraron en el núcleo de la plataforma 35 cráneos dispuestos en un semicírculo, unidos por una mezcla de cal, arena y grava de tezontle. Consideran los expertos que, en adición a los cráneos encontrados, hay muchos otros por descubrir. Además, si bien los elementos de madera del Tzompantli no resistieron el paso del tiempo y las condiciones húmedas del terreno, los investigadores descubrieron los orificios en los que se colocaron los postes que soportaban las vigas de la estructura. De acuerdo con los especialistas, la práctica de los Tzompantli, que estaba extendida a lo largo de Mesoamérica, obedecía a la particular cosmovisión de los pueblos prehispánicos que habitaban en esta región y tenía por tanto motivaciones religiosas. Según dicha cosmovisión, el sacrificio de humanos y de otros animales era esencial para mantener al mundo en operación. En palabras de Miguel León Portilla, comentando sobre la religiosidad de los pueblos prehispánicos: “…esa religiosidad implicaba la creencia en que la sangre de los sacrificados fortalecida la vida de los dioses, en particular del Sol. De este modo se propiciaba la perduración de la presente edad cósmica, diríamos que se redimía a los seres humanos de su destrucción cósmica”.   Si bien la práctica de los sacrificios humanos históricamente ocurrió en otras partes del mundo, la aparente –si bien controvertida– prevalencia de la misma en Mesoamérica ha conducido a algunos a  mantener posiciones maniqueas con respecto a las civilizaciones precolombinas –y a mitigar estas posiciones no ha ayudado, por supuesto, el que los sacrificios humanos estuvieran asociados a prácticas de canibalismo. En respuesta a las opiniones negativas con respecto a nuestros antecesores, León Portilla hace notar que, así como en las culturas precolombinas los humanos se redimían de su destrucción cósmica mediante el sacrificio humano, según la teología cristiana “un sacrificio humano y divino es el origen de la redención de todos los hombres y mujeres en la Tierra”.  Al margen de la calificación moral de la población mesoamericana, podemos decir hoy en día que si bien la memoria de los Tzompantli ha sobrevivido a través de las calaveras de azúcar que se elaboran alrededor del día de muertos –entre otras manifestaciones–, la cosmovisión de los mexicas ciertamente ha perdido credibilidad. Así, aunque las visiones mágicas del mundo no han desaparecido en el mundo, posiblemente se nos dificultaría encontrar a alguien que pensara que para ganar una batalla hay que sacrificar niños. Como presumiblemente habrían hecho los cholultecas cuando en Cholula pretendieron tender una trampa a Cortés –lo que, por cierto, a la postre les resultó en un desastre, pues fueron masacrados por los españoles y sus aliados tlaxcaltecas.   Hoy nos resulta claro que el sacrificio de un niño no puede condicionar el resultado de una batalla. Ni que el futuro del mundo dependa de realizar sacrificios de prisioneros de guerras floridas, no solamente abriéndoles el pecho con un cuchillo de obsidiana en lo alto de una pirámide, sino también por otros métodos, cual más variados. En realidad, aun hoy en día no es posible predecir con certidumbre el resultado de una batalla –a  menos que sea entre contendientes muy desiguales– o el futuro del mundo. No es posible ni aun utilizando métodos científicos –ya no digamos enfoques mágicos–. Pensemos, por ejemplo, en la incertidumbre que enfrenta el planeta en cuanto a su futuro climático se refiere: tenemos una certidumbre razonable, gracias a las investigaciones de los científicos, que el clima de la Tierra está cambiando y cambiará más en el futuro; en qué grado lo hará, no lo sabemos con seguridad.La cosmovisión de los mexicas, que esta semana fue noticia gracias al anuncio del descubrimiento del gran Tzompantli del Templo Mayor, era claramente errónea por más elaborada que haya sido. Por lo mismo, su poder de predicción era en gran medida nulo, como lo es el poder de predicción resultante de cualquier enfoque mágico del mundo. Aquellos que murieron sacrificados para dar su energía al Sol –algunos posiblemente de buena gana, pero otros seguramente bajo protesta–, lo hicieron entonces de manera estéril.",
    "La semana que hoy termina, el Consejo Nacional de Ciencia y Tecnología (CONACyT) hizo llegar a los miembros del Sistema Nacional de Investigadores (SNI)  un comunicado vía Internet en el que se dan a conocer las sanciones que el SNI impuso a los investigadores Rodrigo Núñez Arancibia y Juan Pascual Gay por fraude académico; específicamente, por haber plagiado material publicado  en diversos artículos y libros por otros investigadores. La sanción impuesta por el CONACyT a ambos investigadores fue la pérdida de la distinción como miembro del SNI y la imposibilidad de regresar al mismo por 20 años.Rodrigo Núñez Arancibia es de nacionalidad chilena. Después de graduarse con una maestría en Chile vino a nuestro país en donde en el año 2004 le fue conferido un grado doctoral por el Colegio de México. Este grado, sin embargo, le fue retirado hace unas semanas al comprobarse que su tesis doctoral era en realidad un plagio casi total del libro “La revolución empresarial chilena”  publicado en 1997 por la investigadora chilena Cecilia Montero Saavedra y por lo tanto carecía de la originalidad demandada por el Colegio de México. Antes de ser descubierto como plagiario, Núñez Arancibia había realizado una exitosa carrera académica que lo llevó a ser contratado como investigador por la Universidad Michoacana y a ser admitido en el SNI. Todo esto basado en una asombrosa e intensa actividad que se extendió por más de una década que lo ha llevado a ser caracterizado como un “plagiario serial”.  Para tener una perspectiva de la amplitud de dicha actividad, vale la pena leer el artículo de José Antonio Aguilar publicado el pasado mes de julio en la revista NEXOS, el cual lleva por título “El extraño caso del pirata Arancibia” y en el que se caricaturiza a Núñez Arancibia como un corsario en los mares de la academia, australes y septentrionales, los cuales “surcó durante más de una década saqueando a diestra y siniestra con absoluta impunidad”. El segundo caso mencionado por el comunicado del CONACyT es el de Juan Pascual Gay quien, en un artículo publicado el pasado 30 de junio en el diario El Universal fue acusado por el escritor Guillermo Sheridan de haber plagiado en un 99% un texto suyo publicado en 1993 en la revista “Vuelta”.  Una comparación del artículo de Pascual Gay –publicado en el año 2000 en la revista catalana “Arrabal”– con el original de Sheridan no deja lugar a duda de que se trata de un plagio burdo.  A raíz de esta acusación, Pascual Gay fue destituido de su puesto como investigador del Colegio de San Luis. Las acusaciones de plagio no son, por supuesto, nada nuevo y en el campo de la literatura ocurren de manera frecuente, involucrando incluso a autores muy connotados. Tan connotados como Alfonso Reyes, quien en 1954 fue acusado de plagiar un texto corto sobre Julio Verne del escritor George Kent, publicado en la revista “The Saturday Review”. Si bien el plagio fue más que evidente y Reyes tuvo que reconocerlo, se consideró que había cometido solamente un pecado menor. Que no de juventud, pues en esos momentos el escritor tenía 65 años –y ninguna necesidad, por supuesto, de plagiarle textos a nadie.El plagio de textos de ninguna manera se limita a la literatura o las ciencias sociales y se da en todos los campos científicos. Para averiguar la magnitud con que ocurre esto y cómo impacta al desarrollo científico, Daniel Citron y Paul Ginsparg de Cornell University llevaron a cabo un estudio sobre la práctica de duplicación de textos en diversos campos científicos. Dicho estudio, publicado en la revista “Proceedings of the National Academy of Sciences” el pasado mes de enero,  se realizó con la base de artículos conocida como arXiv.org que comprende casi un millón de artículos en áreas de la física, las matemáticas, la ciencia de la computación, y la biología, entre otras. Citron y Ginsparg   encontraron que, si bien la duplicación de textos en estas áreas es una práctica bastante extendida, ésta se da en gran medida en condiciones que pudieran considerarse aceptables. Es, por ejemplo, frecuente que un investigador reúse en nuevos artículos frases publicadas en artículos previos de su autoría –lo que técnicamente es un auto plagio–. No es tan frecuente, en contraste, que un investigador se apropie de un texto ajeno sin citar su origen. Según Citron y Ginsparg, además, el plagio sistemático se centra en un reducido grupo de investigadores, de modo que una gran mayoría de investigadores son ajenos a dicha práctica.  Y lo más importante, Citron y Ginsparg, encuentran que los artículos que contienen material reciclado de artículos anteriores son menos citados que otros artículos con material original. El número de citas por otros autores que recibe un artículo de investigación es una medida del impacto que ha tenido, de modo que los artículos con material plagiado tenderían a pasar sin ser notados por la comunidad científica y su influencia en el desarrollo de la ciencia sería reducido. Así, si hemos de creer a Citron y Ginsparg, la ciencia no sufre demasiado por el hecho de que haya quien haga del plagio su modo de vida, y posiblemente, por extensión, lo mismo podamos aplicar a la literatura. De lo anterior podemos concluir que si bien hacer del plagio un modo de vida puede ser peligroso como nos lo demuestran los acontecimientos recientes, existe un elemento de racionalidad que impulsa a algunos hacerlo, pues es al mismo tiempo redituable. Mas difícil de entender, en contraste, son las motivaciones por las que un escritor altamente apreciado decide plagiar un texto menor.",
    "Después del arribo de la sonda “New Horizons” a las inmediaciones de Plutón el último 14 de julio, la semana pasada nos enteramos por la prensa de dos novedades más relativas al espacio profundo. Por un lado, la NASA anunció el descubrimiento del planeta más parecido al nuestro del que tengamos noticia. Dicho planeta, que ha sido denominado Kepler-452b, se encuentra a una distancia de 1,400 años luz de la Tierra. Tiene un diámetro que es cerca de una y media veces el diámetro de nuestro planeta y orbita, con un periodo de 385 días, a una estrella similar al Sol. Además, según la NASA, existe una buena posibilidad de que por su tamaño el Kepler-452b sea un planeta rocoso. El descubrimiento fue publicado la pasada semana en la revista  “Astronomical Journal” por un equipo de investigadores encabezado por Jon Jenkins del NASA Ames Research Center. El planeta Kepler-452b recibe aproximadamente la misma cantidad de radiación solar que la Tierra, lo que lo coloca dentro de la “zona de habitabilidad” de su estrella. De acuerdo con Jenkins y colaboradores, esta zona se define como la región alrededor de una estrella en donde el agua, en un espacio abierto, puede existir en forma líquida en la superficie del planeta. A una distancia más allá de la zona de habitabilidad, alejándose de la estrella, la radiación solar se reduce y el agua se congela; dejando dicha zona en la otra dirección –hacia la estrella– el aumento de radiación solar hace que el agua se vaporice.  Dado que el agua líquida es esencial para sostener la vida tal como la conocemos, sólo en planetas dentro de la zona de habitabilidad de sus respectivas estrellas podríamos esperar que exista –o haya existido en algún tiempo remoto– vida con un cierto grado de complejidad. En el caso del planeta Kepler-452b, ha habido suficiente tiempo para que esto sucediera, pues Jenkins y colaboradores calculan que dicho planeta y su estrella se formaron hace unos 6,000 millones de años; es decir, unos 1,500 millones de años antes que nuestro sistema solar. La semana pasada nos trajo también la noticia que el magnate ruso Yuri Milner –quien hizo su fortuna con negocios de internet–, lanzó la iniciativa “Breakthrough Listen” que destinará 100 millones de dólares en diez años a la búsqueda de vida inteligente fuera de la Tierra. El anuncio fue hecho en una conferencia de prensa en la Royal Society en Londres. La iniciativa fue apoyada por diversas personalidades científicas, incluyendo al bien conocido físico Stephen Hawkins. De acuerdo con el sitio de internet de “Breakthrough Listen”, los radiotelescopios de Green Bank, Virginia y Parkes Observatory en Australia, buscarán señales de radio que denoten vida inteligente en un millón de estrellas de la Vía Láctea y en 100 galaxias cercanas. De manera adicional, el telescopio óptico del Lick Observatory, en San José, California, hará una búsqueda de posibles mensajes transmitidos a través de un rayo láser. Por más de cincuenta años se han llevado a cabo esfuerzos sistemáticos para detectar señales de radio que revelen la existencia de vida extraterrestres, con resultados negativos hasta el presente. De acuerdo con sus impulsores, la iniciativa “Breakthrough Listen” constituye el esfuerzo más grande jamás realizado para encontrar inteligencia extraterrestre y superará con mucho a lo hecho hasta ahora. Así, la iniciativa contempla escudriñar una región del cielo 10 veces mayor, con una velocidad 100 veces más grande. En cuanto a la sensibilidad de los instrumentos a emplear, de acuerdo al sitio electrónico de “Breakthrough Listen”, si una civilización en alguna de las 1,000 estrellas más cercanas emite un mensaje con una potencia equivalente a la que emplea el radar de un avión común y corriente, dicho mensaje podrá ser detectado por los radiotelescopios del proyecto. De la misma manera, podrá ser detectado un hipotético mensaje trasmitido  desde una estrella cercana empleando un rayo láser de apenas 100 watts –la potencia de uno de los focos incandescentes recientemente descontinuados.  Se espera que el proyecto arroje un torrente de datos e inunde a la pequeña comunidad de científicos actualmente ocupados en la búsqueda de inteligencia fuera de nuestro planeta; comunidad que recibirá en un solo día tanta información para analizar como la que recibía en un año en sus primeros tiempos. ¿Tendrá éxito el proyecto “Breakthrough Listen” con sus 100 millones de dólares de apoyo? No hay manera de anticiparlo pues, para empezar, no sabemos si existen los extraterrestres. El mismo Milner asume que es probable que terminen el proyecto con las manos vacías. Hawkings, sin embargo, afirma que en un universo infinito debe haber vida aparte de la nuestra y que debemos darnos a la tarea de buscarla. Que existen en el Universo mundos similares al nuestro que podrían haber originado vida inteligente es algo que es apoyado por hallazgos como el del planeta Kepler-452b. Sólo el futuro, sin embargo, mediante la detección de un mensaje interestelar, nos lo podría confirmar con seguridad.  Por otro lado, de un modo u otro, el proyecto “Breakthrough Listen” tendrá un impacto, si no interestelar, sí cuando menos a nivel de este mundo, pues la compra de tiempo de los radiotelescopios de Green Banks y Parkes para propósitos del proyecto los podrá salvar de un posible cierre por falta de fondos para su operación.",
    "En marzo de 1930 la prensa del mundo difundía el descubrimiento del largamente buscado noveno planeta del Sistema Solar. El descubrimiento fue llevado a cabo por el joven astrónomo norteamericano Clyde Tombaugh en el observatorio Lowell en Arizona, después de un año de paciente búsqueda. El nuevo planeta fue bautizado como Plutón –Pluto, para mayor precisión–, el dios romano del inframundo. El nombre fue sugerido por Venetia Phair, una niña inglesa de 11 años interesada en la mitología griega y romana. Si bien en un entrevista publicada en el año 2006 por la BBC, Venetia Phair menciona que no recuerda si cuando hizo su sugerencia estaba pensando en el “oscuro y amenazador Hades” –la versión griega de Plutón–, el nombre resultó adecuado para un planeta que se encuentra en una región fría y oscura del espacio, en los confines del Sistema Solar desde donde el Sol se ve apenas como una estrella muy brillante.    Plutón no se encuentra solo sino acompañado de un satélite, descubierto en 1978 por el astrónomo norteamericano James Christy. Este satélite, que es inusualmente grande en relación al tamaño de Plutón, circula al planeta cada seis días mostrándole siempre la misma cara. Para completar la imagen sombría de Plutón y sus alrededores, dicho satélite fue bautizado como Caronte, el barquero de Hades que se encargaba de transportar a los muertos al inframundo  a través del rio Aqueronte. La semana que hoy termina, 85 años después de su descubrimiento, Plutón vuelve a ser noticia. Esta vez por el arribo  de la sonda “New Horizons” de la NASA a sus inmediaciones. Plutón está unos 6,000 millones de kilómetros de la Tierra y tiene un tamaño relativamente pequeño –su volumen es apenas un 0.6% del volumen de la Tierra–, por lo que aun hoy no sabemos mucho acerca de él. En su máximo acercamiento, el “New Horizons” se situó a unos 12,000 kilómetros de Plutón, capturando imágenes y datos científicos que los  especialistas esperan amplíe considerablemente el conocimiento que tenemos acerca del planeta y de su satélite Caronte. Si bien se han dado a conocer pocas imágenes de Plutón y Caronte, en las que han sido liberadas es posible ver en la superficie de Plutón montañas de hielo con altitudes que alcanzan los 3,500 metros. En este respecto, hay que hacer notar que en la superficie de este planeta la temperatura media es de menos 230 grados centígrados y que a esta temperatura el hielo tiene una gran dureza. Las imágenes trasmitidas por “New Horizons” de la superficie de Plutón muestran también una extensa área brillante en forma de corazón. Un acercamiento de dicha área descubre una planicie helada de nitrógeno y metano, con áreas poligonales de unos veinte kilómetros de largo, bordeadas por canales en apariencia no demasiado profundos.   La NASA ha igualmente liberado una imagen de Caronte que muestra fracturas en su superficie que se extienden a lo largo de 1,000 kilómetros, así como un cañón que se estima tiene una profundidad entre 7 y 9 kilómetros.Algo que ha sorprendido a los especialistas es  la poca cantidad de cráteres de impacto observados en la superficie, tanto de Plutón como de Caronte. Esto implica que las huellas de los impactos ocurridos desde la formación de Sistema Solar han sido ocultadas por procesos geológicos. Tanto Plutón como Caronte estarían de este modo geológicamente activos. La fuente del calor en el interior de ambos cuerpos que sostiene esta actividad, sin embargo, es por el momento un misterio para los especialistas. En la primera mitad de siglo XIX Urban Le Verrier en Francia y John Couch Adams en Inglaterra predijeron de manera independiente la existencia de un octavo planeta en el Sistema Solar, con una órbita más allá de la órbita de Urano –el séptimo planeta–. Esta predicción estuvo basada en las perturbaciones observadas en la órbita de Urano que indicaban la presencia de un cuerpo masivo no descubierto hasta esa fecha. El octavo planeta –que recibió el nombre de Neptuno– fue descubierto por el astrónomo alemán Johann Gottfried Galle en 1846. Galle descubrió Neptuno guiado por Urban Le Verrier, quien le indicó el lugar del cielo en el qué debería buscar. De manera similar al octavo planeta, los astrónomos predecían la existencia de un noveno planeta,  el llamado Planeta X, basados en las aparentes irregularidades observadas en las órbitas de Urano y Neptuno, y el descubrimiento de Plutón en 1930 pareció darles la razón. Ahora es claro, sin embargo, que la masa de Plutón no es lo suficientemente grande para producir las supuestas perturbaciones de las órbitas de Urano y Neptuno; perturbaciones que, por otro lado, hoy sabemos no son reales. Así, Plutón, que en un principio se pensó era el Planeta X, en realidad es demasiado pequeño y no tiene nada que ver con el mismo. Aun más, ahora sabemos que Plutón ni siquiera es planeta, pues fue degradado de esta categoría por la Unión Astronómica Internacional en 2006 que lo clasifica ahora como un planeta enano.Aun degradado de categoría, no obstante y según los especialistas, Plutón es un objeto de estudio extremadamente interesante. Por lo pronto, es el planeta más lejano –enano o bien crecido– con el que hemos hecho contacto.",
    "En el curso de dos años, a lo largo de 1982-1984, el suelo del puerto de Pozzuoli en la costa oeste de Italia se elevó por casi dos metros. Entre otras consecuencias, este fenómeno hizo que las aguas del puerto no tuvieran la suficiente profundidad para aceptar embarcaciones de gran calado.  Si bien dicho fenómeno no resultó inusual, pues se sabe que el terreno en Pozzuoli ha sufrido elevaciones y descensos desde tiempos de los romanos, la velocidad con la que se dio –aunado a la ocurrencia de un temblor de tierra de magnitud 4– provocó alarma, forzando la evacuación de 40,000 personas en prevención de una posible erupción. No es difícil entender que haya habido alarma por lo que estaba sucediendo en Pozzuoli, si recordamos que esta ciudad se localiza en una zona rodeada de volcanes, y a poco más de cincuenta kilómetros del Monte Vesubio, el mismo que sepultó a las ciudades de Pompeya y Herculano cuando hizo erupción en el año 79 de nuestra era.    Por otro lado, si bien en el episodio de 1982-84 el suelo de Pozzuoli sufrió una gran deformación, éste resultó notablemente resistente a la fractura. El porqué fue así ha sido un misterio. Al menos lo fue hasta esta semana cuando se aventura una explicación en un artículo aparecido en la  revista “Science”. Dicho artículo fue publicado por Tiziana Vanorio y Waruntorn Kanitpanyacharoen de la Universidad Stanford en los Estados Unidos. De acuerdo con estos investigadores, el subsuelo de Pozzuoli contiene una capa de material resistente y dúctil similar al concreto, la cual le dio la fortaleza necesaria para soportar los esfuerzos sufridos en 1982-84.  Para llegar a esta conclusión, Vanorio y Kanitpanyacharoen estudiaron muestras del subsuelo en el área de Pozzuoli tomadas hasta una profundidad de 2.9 kilómetros. Encontraron que cerca de la superficie el subsuelo contiene una capa de cenizas volcánicas rica en óxidos de silicio y aluminio –conocida como pozolana–. A una gran profundidad, por otro lado, existe una capa de rocas que contienen carbonato de calcio y que reacciones químicas de los minerales en esta capa con otros constituyentes generan hidróxido de calcio –uno de los componentes del cemento– el cual fluye hacia la superficie.  Al reaccionar el hidróxido de calcio con la capa de pozolana se genera una capa de concreto y es esta capa la que le da su particular resistencia al subsuelo de Pozzuoli.  De acuerdo con Vanorio, el concreto así generado de manera natural, es similar al que fabricaban los romanos hace dos mil años empleando las mismas cenizas volcánicas de Pozzuoli –Puteoli para los romanos– aunque con una diferente fuente de calcio. Los romanos emplearon su concreto en construcciones tales como el Panteón romano y el Coliseo romano. La pozolana se usó, igualmente, en la construcción de puertos en las costas del Mar Mediterráneo, incluyendo a Alejandría y Cesarea.   Fue en Puteoli, según afirma Vanorio, en donde los romanos llevaron a cabo el descubrimiento del procedimiento para fabricar el concreto empleando las cenizas volcánicas de la localidad, y  sugiere que la inspiración para lograr esto les llegó a través de la observación del endurecimiento que la pozolana experimenta al contacto con el agua. Ya en el año 25 antes de nuestra era, el arquitecto romano Vitruvius recomienda pozolana para la preparación del mortero estructural.    El descubrimiento del concreto romano se habría dado entonces por la observación accidental de lo que ocurre a la pozolana al reaccionar con el agua y con otras sustancias. De acuerdo con esto, de no haber existido Pozzuoli lo romanos no habrían construido el Panteón romano, con su cúpula de más de cuarenta metros de diámetro que ha resistido por 19 siglos las inclemencias del tiempo, lo mismo que otras notables obras arquitectónicas. La tecnología romana del concreto es así fundamentalmente empírica, resultado de un descubrimiento accidental –y, por supuesto, de una gran capacidad de observación–, en contraste con las tecnologías actuales de base científica basadas en una búsqueda sistemática con la guía del conocimiento científico de los fenómenos naturales; sin dejar de depender, no obstante y en cierto grado, del descubrimiento accidental. De manera interesante, el artículo de Vanorio y Kanitpanyacharoen de alguna manera nos coloca en un mismo lugar de nuestro planeta, Pozzuoli,  en dos épocas separadas por 20 siglos. En lo que respecta a la época actual, a través de complejas y profundas exploraciones subterráneas y complicadas elucubraciones químicas, el artículo nos enseña cuál es la composición del subsuelo de Pozzuoli y cuáles son las posibles causas –la formación natural de una capa subterránea de concreto– por las que los fenómenos amenazadores que ocurren bajo tierra no hayan sido lo mortíferos que podríamos suponer. Por otro lado, nos recuerda que Pozzuoli habría sido el origen de la tecnología romana del concreto, desarrollada de manera sorprendente hace 20 siglos, y la cual produjo obras que aun hoy llaman la atención.",
    "En la primera mitad del siglo pasado los físicos descubrieron que la Tierra sufre un bombardeo continuo de partículas de alta energía. Y que estas partículas, a las que se les ha llamado rayos cósmicos, son generadas fuera del sistema solar, en lugares que no han sido aun identificados con claridad.  Las energías características de los rayos cósmicos son muy grandes e incompatibles con la vida tal como la conocemos. De hecho, estos rayos son uno de los obstáculos más grandes para la realización de la publicitada misión tripulada a Marte, durante la cual los astronautas estarían expuestos a un continuo bombardeo de energía. Los rayos cósmicos pueden ser incluso dañinos para objetos inanimados, como es el caso de los procesadores de las computadoras que controlan las misiones espaciales.En nuestro planeta estamos protegidos de los rayos cósmicos por el campo magnético de la Tierra, que los desvía hacia el espacio, lo mismo que por la atmósfera terrestre, que absorbe a aquellos que logran ingresar. Algunos rayos cósmicos, sin embargo, logran llegar a la superficie de nuestro planeta en forma de rayos secundarios generados cuando lo rayos primarios chocan con las moléculas de aire. A nivel del mar los rayos secundarios están formados mayormente por partículas llamadas muones, que resultan ser así parte del entorno natural en el que vivimos. Su flujo es continuo, aunque no demasiado grande, y en un metro cuadrado inciden unos 10,000 muones cada minuto. Los muones son partículas que pueden penetrar distancias relativamente grandes en la materia y esto ha dado origen a una interesante aplicación: la obtención de imágenes –tomografías– de diversos objetos en las que se revela su estructura interna. Dicha aplicación es el tema de un artículo aparecido esta semana en la revista “Science Advances”, publicado por un grupo de investigadores de los Estados Unidos, encabezado por J.M. Durham del Los Alamos National Laboratory.Como sabemos, por medio de rayos X podemos obtener una imagen del interior del cuerpo humano proyectada en dos dimensiones. Esto es posible debido a que las diferentes partes que componen nuestro cuerpo absorben de manera diferenciada los rayos X. De este modo, aquellas partes que absorben más los rayos X aparecen en la radiografía con una menor exposición en comparación con aquellas más transparentes. Los rayos X permiten también obtener tomografías, las cuales revelan la estructura interior de los objetos y no solamente su proyección en dos dimensiones.Al igual que con los rayos X, con los muones es posible obtener imágenes de dos dimensiones de objetos tridimensionales, con la diferencia que los muones penetran en la materia mucho más que los primeros. Esto permite estudiar objetos de grandes dimensiones, lo mismo que objetos de gran densidad. Así, los muones han sido empleados para investigar el interior de pirámides y volcanes, por citar dos casos.  De acuerdo con Durham y colaboradores, los muones pueden ser empleados también para obtener tomografías de objetos opacos, no susceptibles de ser estudiados por medio de rayos X. En su publicación de esta semana, los investigadores demuestran que la tomografía de muones permite, por ejemplo, obtener una imagen tridimensional del interior de una válvula metálica que indica si está en posición abierta o cerrada, o bien una imagen del interior de una tubería metálica que muestra problemas de corrosión. La tomografía de muones, de acuerdo con Durham y colaboradores, presenta ventajas sobre otras técnicas. Puede ser usada, por ejemplo, para evaluar el estado de corrosión de tuberías en centrales nucleares sin perturbar la operación de la planta, o para inspeccionar componentes embebidas en concreto. Otra ventaja es que los muones empleados para obtener imágenes son parte del entorno natural y, por lo tanto, no presentan un riesgo para la salud más allá del habitual. Esto, por supuesto, en contraste con los rayos X.En contraposición, dada la baja densidad de muones que inciden sobre la superficie terrestre, los tiempos requeridos para obtener una tomografía se miden en horas. Así, la tomografía de muones no podría ser usada para una evaluación rápida de una instalación. Otra desventaja de esta técnica es su pobre resolución –las imágenes obtenidas son borrosas– en comparación con las técnicas de rayos X o ultrasonido.Tenemos así que la tomografía de muones tiene ventajas con respecto a otras técnicas para la obtención de imágenes tridimensionales del interior de un objeto, ventajas que están basadas en la habilidad de los muones para penetrar y “ver” el interior de los objetos materiales y salir de los mismos antes de ser absorbidos. Tiene también desventajas, las cuales es posible se aminoren en el futuro en la medida en que se avance en el desarrollo de la tecnología.Como quiera que sea, es interesante señalar que la tomografía de muones, como todas las tecnologías avanzadas de nuestros días, está basada en descubrimientos científicos, que en su caso se llevaron a cabo en la primera mitad del siglo XX. Sin estos descubrimientos no habría tomografía de muones, y la mera idea de que por medio de rayos invisibles pudiéramos saber qué es lo que un objeto opaco tiene en su interior, hubiera sido desechada como imposible. O propia de alguna práctica oculta.",
    "Después de siete meses de hibernación, la sonda Philae de la Agencia Espacial Europea dio señales de vida el pasado fin de semana y con esto se reavivó la confianza en que las metas científicas planteadas para la misión Rosseta, de la cual Philae forma parte, puedan alcanzarse. La misión Rosseta tiene como objetivo estudiar en proximidad cercana a dicho cometa en la medida en que se acerca al Sol y entra en actividad. Con este propósito, la nave espacial Rosseta, con la sonda Philae a bordo, alcanzó al cometa el 6 de agosto del año pasado en un punto entre las órbitas de Marte y Júpiter, acompañándolo desde entonces en su viaje hacia el Sol.Además de lo anterior, como quizá recordemos, el pasado mes de noviembre Philae se desprendió de la nave Rosseta y enfiló rumbo a la superficie del cometa Churymov-Gerasimenko, con el propósito de permanecer ahí durante toda la misión y obtener, de primera mano, datos del cometa y de su “encendido” en su viaje hacia el Sol. No logró Philae, sin embargo, un descenso controlado y al tocar la superficie del cometa rebotó un par de veces terminando su viaje en una posición inclinada y a la sombra de una ladera. Esto limitó la cantidad de radiación solar que los paneles solares de la sonda podían recibir para recargar sus baterías, forzándola a entrar en un estado de hibernación.En estas condiciones, sólo quedó a los responsables de la misión Rossetta cruzar los dedos para que más adelante, cuando el cometa estuviera más cerca del Sol, la radiación solar tuviese la suficiente intensidad para recargar las baterías de la sonda y ésta pudiese salir de su estado de hibernación. Afortunadamente, esto fue lo que sucedió el pasado fin de semana, cuando la nave Rossetta pudo establecer comunicación con Philae por espacio de 85 segundos.   ¿Con qué propósito se envió una nave espacial para seguir al cometa Churymov-Gerasimenko en su trayectoria hacia el Sol? Con respecto a esto, los científicos consideran que la composición de los cometas refleja la composición de la nebulosa a partir de la cual se formaron el Sol y los planetas hace 4,600 millones de años y, según la Agencia Espacial Europea, el estudio, en proximidad cercana, del Churyumov-Gerasimenko podrá dar información sobre el origen y evolución del sistema solar.En particular, una de las preguntas que los científicos pretenden resolver con la misión Rossetta es el relativo al origen del agua en la Tierra. Como sabemos, el agua es un elemento de importancia fundamental para nosotros y sin el cual la vida tal como la conocemos no podría haberse desarrollado. Sabemos también que el agua en la Tierra tiene un ciclo mediante el cual primeramente se evapora de lagos y océanos, en seguida se condensa en la atmósfera formando nubes y, finalmente, se precipita en forma de lluvia iniciando nuevamente el ciclo. Todo esto nos lo enseñan en la escuela. Pocas veces, sin embargo, nos preguntamos sobre cómo se originó en primer término el agua en nuestro planeta.De acuerdo con el sitio de internet de la Agencia Espacial Europea, el agua habría existido como parte de la nebulosa a partir de la cual se formó la Tierra. Las altas temperaturas que en algún tiempo imperaron en la superficie de nuestro planeta, sin embargo, la habrían evaporado y enviado al espacio. De este modo, el agua actual en la Tierra debería haber llegado del exterior por alguna otra vía.Una hipótesis al respecto  es que el agua llegó transportada en los asteroides y cometas que han colisionado con nuestro planeta a través del tiempo. Una manera de comprobar esta hipótesis es comparando la proporción de agua pesada característica del agua terrestre, con la correspondiente proporción del agua de asteroides y cometas. –Hay que recordar que la molécula de agua está formada por dos átomos de hidrógeno y uno de oxígeno y esto es cierto tanto para el agua ordinaria como para el agua pesada; en este último caso, sin embargo, los átomos de hidrógeno que constituyen la molécula de agua son isótopos de hidrógeno más pesados que el hidrógeno ordinario.  De acuerdo con lo anterior, si el agua de lagos y océanos se hubiera originado en cometas como el Churyumov-Gerasimenko, el agua terrestre y el agua de dicho cometa deberían tener una proporción similar de agua pesada. Un primer resultado obtenido por Rossetta después de analizar el agua del cometa Churyumov-Gerasimenko es que su proporción de agua pesada es tres veces superior a la correspondiente proporción del agua en la Tierra. Esto constituye una evidencia en contra del origen cometario del agua terrestre.En los próximos meses, a medida que el cometa Churyumov-Gerasimenko se aproxime al  Sol, alcanzando su punto de máximo acercamiento el próximo 13 de agosto, Rosseta tendrá una visión privilegiada del proceso de “encendido” del cometa Churyumov-Gerasimenko a una distancia de un par de cientos de kilómetros. Con la información que obtengan, los expertos podrán contestar más preguntas acerca de la naturaleza de los cometas.Al mismo tiempo, tal parece que la sonda Philae podrá seguir el proceso de “encendido” del Churyumov-Gerasimenko todavía más cerca que Rossetta: ni más ni menos que a una distancia de cero kilómetros, desde la misma superficie del cometa. En los próximos dos meses sabremos qué resulta de todo esto, que en primera instancia parecen ser excelentes noticias.",
    "Sin duda estaríamos de acuerdo en que toparnos con un chimpancé que cocine sus alimentos sería muy sorprendente, pues, hasta donde tenemos conocimiento, los humanos somos la única especie en este planeta que lo hace. No lo hace ninguno de los grandes simios, ya no digamos otras especies más alejadas de nosotros. Esto indicaría que las habilidades cognitivas que son necesarias para desarrollar la práctica de cocinar alimentos –que hemos llevado a niveles de gran sofisticación– son exclusivamente humanas.Un artículo publicado esta semana en la revista “Proceedings of The Royal Society B” por Felix Warneken y Alexandra G. Rosat de la Universidad Harvard, sin embargo, indicaría que esto no es estrictamente cierto, y que los chimpancés, una especie cercana a la nuestra, comparten con nosotros, además del gusto por la comida cocida,  algunas de habilidades que nos han llevado a los humanos a desarrollar el arte de la cocina.De acuerdo con Warneken y Rosat, la práctica de cocinar alimentos desarrollada por nuestros lejanos antecesores requirió de varias habilidades cognitivas. Requirió, por ejemplo, de la capacidad para concebir que el pedazo de carne cruda que tenemos ante nosotros puede ser puesto sobre una parrilla caliente y con esto mejorar sustancialmente su sabor. Requirió también de la capacidad para abstenernos de comer la carne cruda en cuanto la tengamos a nuestro alcance –sobre todo si tenemos hambre– y esperar a que esté cocida para hacerlo. Y requirió, por supuesto, de la capacidad necesaria para controlar el fuego.Para averiguar en qué grado los chimpancés poseen estas habilidades, Warneken y Rosat llevaron a cabo una serie de once experimentos con chimpancés semi-cautivos de la República del Congo en el África Central. Estos experimentos buscaban, entre otros objetivos, comprobar si los chimpancés prefieren los alimentos cocidos o crudos, o bien, si teniendo un alimento en la mano, antes de ingerirlo estarían dispuestos a  desprenderse del mismo y colocarlo en una “estufa” para cocinarlo. La “estufa” en este caso consistió de un recipiente de plástico con fondo doble en el que se escondía una rodaja de papa cocida. Para “cocinar” una rodaja cruda, el experimentador la colocaba dentro de la “estufa” a la vista del chimpancé, sacudía el recipiente varias veces y, acto seguido, extraía la rodaja cocida oculta en el doble fondo.  Los experimentos confirmaron resultados obtenidos previamente por otros investigadores en el sentido que los chimpancés prefieren los alimentos cocidos a los crudos. También demostraron que los chimpancés son capaces de entender, con un mínimo de experiencia, el proceso de transformación que sufre un determinado alimento al colocarlo en la “estufa” y de generalizar este conocimiento a otros alimentos. Igualmente, fueron capaces de almacenar alimentos crudos para su cocción y consumo posterior, resistiendo la tentación de dar cuenta de ellos de inmediato. Si los chimpancés comparten con nosotros las habilidades psicológicas que nos han llevado a desarrollar el arte de la cocina, Warneken y Rosat se preguntan por qué aquellos no lograron otro tanto. Una respuesta obvia, señalan, es porque los chimpancés no controlan el fuego. Podría haber más razones, sin embargo. Una de éstas es que un alimento típico de los chimpancés son las frutas, para las cuales la cocción no presenta los mismos atractivos que para los tubérculos que consumían nuestros ancestros. Dado que los chimpancés tienen las habilidades psicológicas necesarias para el desarrollo de la cocina, y que éstas presumiblemente estaban presentes en nuestros lejanos ancestros, incluso antes de la adopción del fuego, Warneken y Rosat especulan que la cocción de los alimentos se dio rápidamente después de  esta adopción. Apuntan, incluso, a un uso oportunista del fuego natural para cocinar alimentos. En este respecto, señalan que es conocido que los chimpancés en su hábitat natural buscan activamente, para consumirlas, semillas que hayan sido tostadas por incendios forestales.  Tal pareciera, según Warneken y Rosat, que si bien los chimpancés poseen el conjunto de habilidades psicológicas necesarias para desarrollar el arte de la cocina –incluyendo la comprensión del proceso de transformación de los alimentos por la cocción, así como la paciencia para resistir la tentación de comerse una papa cruda antes de esperar el tiempo suficiente para cocinarla–, no lo han hecho quizá porque esto para ellos no presenta las suficientes ventajas. O, bien, quizá, porque no han descubierto como hacer un fuego y mantenerlo.De un modo u otro, los chimpancés no cocinan –con todo y los hallazgos de Warneken y Rosat–  y esto los habría relegado a un papel secundario en la evolución en comparación con nuestra especie. Esto, debido a que los alimentos cocidos nos habrían proporcionado mayores recursos energéticos para el desarrollo de un cerebro de mayor volumen. Así, la cocina habría hecho la diferencia.",
    "El chocolate es sin duda un alimento con una gran reputación y encanto y al que se le atribuyen, justificada o injustificadamente, una gran cantidad de propiedades y beneficios nutricionales. Quizá por esto fue recibido con gran entusiasmo el comunicado de prensa emitido el pasado mes de marzo en Alemania por el Instituto de Dieta y Salud, según el cual el consumo diario de chocolate amargo ayuda a controlar el peso. Este comunicado fue difundido de manera amplia por la prensa de todo el mundo, incluyendo al periódico alemán “Bild” que le dedicó una primera plana.En el comunicado referido se dieron a conocer los resultados de un estudio, encabezado por Johannes Bohannon, del Instituto de Dieta y Salud de Alemania, que tuvo como objetivo investigar los efectos que el consumo de chocolate tiene sobre 18 indicadores de salud, incluyendo el peso corporal, los niveles de colesterol y sodio, el nivel de proteína en la sangre, la calidad del sueño y el bienestar general. Para llevar a cabo el estudio, que se extendió por tres semanas, Bohannon y colaboradores dividieron a los voluntarios participantes en tres grupos. Dos de los grupos fueron sometidos a una dieta baja en carbohidratos y el tercero, que sirvió como grupo de control, siguió con su dieta acostumbrada. Adicionalmente, a uno de los grupos en régimen de dieta se le dieron a consumir 42 gramos de chocolate amargo todos los días.El peso de cada participante fue medido diariamente durante los 21 días que duró la prueba. Se encontró que aquellos que siguieron su dieta normal aumentaron y redujeron al azar su peso, sin modificarlo en promedio. Los que fueron sometidos a dieta, en contraste, perdieron en promedio 5 libras a lo largo del estudio. Además, los que consumieron chocolate lo hicieron un 10% más rápido, al mismo tiempo que mejoraron sus niveles de colesterol y su sensación de bienestar. La conclusión del estudio, publicado en la revista “International Archives of Medicine”, fue que el consumo de chocolate acelera la pérdida de peso entre aquellos sometidos a una dieta. Así, el chocolate, además de todas las virtudes que le atribuimos derivadas de su sabor, constituiría un alimento benéfico para nuestra salud. El resultado no podría ser mejor. No, si no fuera porque el artículo publicado por Bohanonn y colaboradores es un fraude, como él mismo lo hizo saber en un blog publicado el pasado miércoles. Johannes Bohannon es en realidad John Bohannon, quién trabaja como reportero para la revista “Science”. El mismo que hace dos años llevó a cabo un proyecto para exponer a aquellas editoriales científicas que publican artículos sin el rigor editorial adecuado mediante el pago de una cuota. En esa ocasión, Bohannon escribió versiones de un artículo sobre el descubrimiento de una nueva droga para tratar el cáncer, el cual adolecía de evidentes fallas científicas y que envió a 304 revistas en línea, de las cuales 157 lo aceptaron.El nuevo artículo-fraude de Bohannon sobre el chocolate, como él lo relata en su blog, fue resultado de una iniciativa de dos reporteros de televisión alemanes que pretendían elaborar un documental sobre la ciencia-basura de la industria de los alimentos. Con este propósito se acercaron a Bohannon –quien no es experto en nutrición sino en biología molecular– y juntos decidieron llevar a cabo el estudio reportado en el artículo referido. Este estudio se llevó se llevó a cabo con un grupo de 15 participantes, reclutados mediante un pago de 150 euros. El número de participantes en el estudio fue demasiado pequeño para que de los datos obtenidos se pueda llegar a una conclusión. De hecho, el estudio fue diseñado para que produjera conclusiones que en apariencia son sólidas pero que no tienen un fundamento estadístico. La receta para esto fue medir un gran número de parámetros –18 en total– en un grupo pequeño de participantes, lo que hacía probable que uno de estos parámetros –no necesariamente el peso corporal– resultase en apariencia correlacionado con el consumo de chocolate, sin que sea éste el caso. Esto último podría comprobarse llevando a cabo nuevamente el estudio, lo  que no se hizo, por supuesto.El artículo fue publicado en una revista que está en la lista de Bohannon de editoriales que publican artículos fraudulentos, sin que haya sido previamente revisada por expertos que con seguridad lo hubiesen rechazado por su falta de rigor científico. Una vez publicados el artículo y el comunicado de prensa, la información entró a los medios de comunicación y se expandió ampliamente por todo el mundo.¿Qué nos enseña todo esto? De acuerdo con Bohannon, entre otras cosas nos muestra la “increíble flojera” de algunos reporteros de ciencia que no se toman el trabajo de comprobar la veracidad de afirmaciones científicas; las cuales, por otro lado, pudieron haber sido publicadas en revistas poco serias que no se toman la molestia de llevar a cabo una revisión rigurosa por expertos del material que publican. Lo que hacen, por supuesto, mediante el pago de una cuota que en algunos casos es considerable.     Nos indica, además, que no porque están escritas las afirmaciones científicas son necesariamente ciertas. No lo son, por supuesto, si se originan en un fraude. Y podrían no serlo aun si provienen de una investigación sería, pues, por naturaleza, los resultados de la ciencia siempre están sujetos a revisión.",
    "En 1921, arqueólogos daneses descubrieron cerca del pueblo de Egtved en Dinamarca, en una tumba con una antigüedad de casi 3,400 años, un ataúd de madera de roble que contenía los restos de una adolescente con una edad entre los 16 y 18 años. La adolescente fue  bautizada como la “Joven de Egtved”, y si bien  de su cuerpo sólo sobrevivió el cabello, las uñas, los dientes, parte del cerebro y algo de piel –los huesos habían desaparecido, aparentemente por el ambiente ácido en el que se encontraba el cuerpo–, el hallazgo es notable por el grado de preservación de la ropa con que fue enterrada y que habría usado en vida.La adolescente fue inhumada vestida con una blusa de lana con mangas hasta los codos, que le dejaba el abdomen descubierto, y una falda corta de cuerdas que le daba dos vueltas a la cintura. Un cinturón de lana sostenía por el frente un medallón de bronce que, de acuerdo con los expertos, habría representado al Sol. Vestida de esta manera, la Joven de Egtved habría tenido una apariencia extrañamente moderna. La Joven de Egtved murió durante el verano del año 1,670 antes de nuestra era, y al fallecer tenía un estatura de 1.60 metros. Tenía una posición social alta y es posible que participara en danzas rituales por el medallón que portaba. No era originaria del lugar en el que fue enterrada y había viajado cientos de kilómetros meses antes de morir, posiblemente a su lugar de origen. Al fallecer tenía poco de haber regresado a Egtved, en donde vivía de manera permanente. ¿Cómo sabemos tanto de alguien que murió hace miles de años?  No es difícil concluir que la joven falleció en el verano, dado que fueron encontrados restos de flores dentro del ataúd. En cuanto a su estatura, si bien sus huesos se habían desintegrado, la silueta de la Joven de Egtved estaba claramente delineada en la piel de vaca sobre la que yacía en el ataúd, lo que nos permite saber con precisión cuál era su altura al morir.    Igualmente, sabemos con precisión el año en que murió a partir de un análisis de los anillos de la madera con la que fue fabricado el ataúd. Con respecto a esto, es conocido que cada uno de los anillos que podemos observar al cortar el tronco de un árbol corresponde a un año en la vida del mismo. Sabemos, además, que el grosor de un anillo depende de lo favorable o desfavorable que fue el clima de ese año para el crecimiento del tronco. Así, mediante el estudio del grosor de los anillos de un árbol es posible determinar la época en la que vivió.   Saber en qué lugares estuvo la Joven de Egtved en los meses previos a un muerte es un poco más complicado. Un grupo internacional de investigadores encabezado por Karin Frei del Museo Nacional de Dinamarca, sin embargo, logró averiguarlo, mediante el análisis del contenido relativo de dos  isótopos del elemento químico estroncio, en las muelas, las uñas y el cabello de la Joven de Egtved, haciendo uso del hecho que este contenido depende del lugar geográfico. Los resultados de la investigación fueron publicados el pasado 21 de mayo en la revista “Scientific Reports”.El estroncio es absorbido por los seres vivientes a través del agua y la comida. En particular, de acuerdo con Frei y colaboradores, el estroncio se fija durante los primeros 3-4 años de la vida en los molares, los cuales llevan así la información de lugar en donde la persona vivió en su primera infancia. Un análisis del contenido de estroncio de los molares de la Joven de Egtved puso en claro que no nació en el lugar en que murió, pues la composición relativa de isótopos de este elemento no corresponde a la de dicho lugar, y sí posiblemente a una región del sur de la actual  Alemania.    Basados en este descubrimiento, Frei y colaboradores especulan que posiblemente la Joven de Egtved habría nacido cientos de kilómetros al sur de donde fue encontrada su tumba, y que posiblemente haya terminado lejos de su lugar de origen al haber sido dada en matrimonio para establecer una alianza entre centros de poder en Dinamarca y en el sur de Alemania.Frei y colaboradores también analizaron el contenido de estroncio del cabello, el cual cortaron en cuatro segmentos con el fin de analizar los cambios ocurridos a lo largo de un periodo que podría abarcar hasta 23 meses. Encontraron que en el segmento más antiguo la composición de estroncio es consistente con la del sur de Alemania, indicando que la Joven de Egtved vivió por algún tiempo en esa área. En los segmentos intermedios 2 y 3, sin embargo, la composición de estroncio es similar a la de Dinamarca, de donde se concluye que la joven se desplazó desde el sur hasta su lugar de residencia. Finalmente, en el segmento de cabello más joven, que corresponde a unos pocos meses antes de su muerte, la composición de estroncio es nuevamente la que corresponde al sur de Alemania.De este modo, en el curso de unos dos años antes de su muerte, la Joven de  Egtved  se habría desplazado repetidas veces, por cientos de kilómetros, entre su lugar de residencia en Dinamarca y su lugar de origen en el Sur de Alemania. Así, de ser correctos los resultados de Frei y colaboradores, la Joven de Egtved resulta sorprendentemente moderna, no solamente en su manera de vestir, sino también por su gran movilidad.",
    "De acuerdo con una fantasía recurrente en el siglo pasado, los hombres de las cavernas tenían la costumbre de golpear a sus mujeres con un garrote y arrastrarlas por los cabellos hasta la cueva en la que habitaban. Todo esto teniendo como marco un paisaje de abundante vegetación, humeantes volcanes en la lejanía, y gigantescos dinosaurios con aletas dorsales, largas colas o afiliados dientes. ¿Qué tan cercana o lejana es esta fantasía de lo que realmente fue el mundo que habitaron nuestros antecesores? En lo que se refiere a que hayamos coincidido en el tiempo con los dinosaurios no hay nada más alejado de la verdad, pues éstos desaparecieron de la faz de la Tierra hace 65 millones de años, cuando en el planeta no había todavía ni el menor rastro del Homo sapiens. Con respecto a la cueva que servía de habitación, la especie no tiene visos de ser cierta si consideramos que los cavernarios habrían sido nómadas, cazadores-recolectores, que no habitaban en un lugar fijo –visto así, el llamarlos cavernarios es en cierto modo incorrecto–. En cuanto a lo del garrotazo y tirón de cabellos como método de seducción, un artículo aparecido esta semana en la revista “Science” arroja una sombra de duda sobre su veracidad. Dicho artículo fue publicado por un grupo de investigadores del University College London encabezado por Mark Dyble.   Si bien posiblemente nadie creería que de manera generalizada y cotidiana los cavernarios maltrataban a sus mujeres con golpes que podrían matarlas –lo que hubiera llevado a una escasez de mujeres y, por tanto, de nuevos nacimientos–, la fantasía del troglodita medio vestido con pieles arrastrando a una mujer por los cabellos refleja un estereotipo según el cual los hombres primitivos eran violentos y machistas.  Según Dyble y colaboradores, sin embargo, antes de la aparición de la agricultura y el sedentarismo, en las sociedades primitivas existía igualdad entre sexos, de modo que este estereotipo sería incorrecto.Los investigadores británicos llegaron a sus conclusiones después de estudiar a dos grupos modernos de cazadores-recolectores, los Palanan Agta de las Filipinas y los Mbendjele del Congo en África Central, al igual que a un grupo de agricultores, los Paranan, que son vecinos de los Agta. Un objetivo de la investigación fue el de determinar el grado de consanguinidad que existe entre los miembros de estos grupos, los cuales están formados aproximadamente por veinte individuos. Según Dyble y colaboradores, se sabe que los integrantes de un grupo social tienen la tendencia a admitir como nuevos miembros a personas con las que están emparentados. En los hechos, sin embargo, y en una aparente paradoja, los grupos incluyen también integrantes sin ningún lazo de sangre con sus compañeros. En consonancia con esto, Dyble y colaboradores encuentran que en los grupos de cazadores-recolectores estudiados un 14% de integrantes no están emparentados con los demás miembros del grupo. Los investigadores resuelven la paradoja notando que en los grupos de cazadores-recolectores la decisión de admitir a un nuevo miembro la pueden tomar tanto hombres como mujeres, lo cual aumenta la cantidad de personas no consanguíneas que podrían ser admitidas. Esto eventualmente lleva a la incorporación al grupo de miembros sin relación de parentesco aumentando así su heterogeneidad. La aparente paradoja es, de este modo, el resultado natural de la igualdad entre sexos en el momento de decidir la admisión de nuevos miembros al grupo social.   En contraste con los grupos de cazadores-recolectores, entre los grupos de agricultores estudiados el porcentaje de integrantes sin ninguna relación de parentesco con los demás miembros es de sólo el 4%; esto, como reflejo de una mayor desigualdad entre hombres y mujeres en el momento de decidir la admisión de un nuevo miembro.Dyble y colaboradores asumen que la igualdad entre sexos observada en los grupos nómadas de cazadores-recolectores estudiados era la norma entre los grupos primitivos y sugieren que dicha igualdad jugó un papel fundamental en el desarrollo social de la humanidad. En particular, al fomentar el desarrollo de grupos heterogéneos, la igualdad sexual promovió el contacto y la cooperación entre individuos más allá de las fronteras de la consanguinidad, así como  el intercambio y perpetuación de las innovaciones tecnológicas.   Si hemos de atender a las conclusiones de Dyble y colaboradores, la igualdad entre sexos no es un invento reciente. Lejos de esto, habría estado vigente en el pasado entre nuestros ancestros cazadores-recolectores, perdiéndose después por alguna razón. Las caricaturas de cavernarios armados con un garrote arrastrando a una mujer por los cabellos, además de lamentables, serían entonces solo eso: caricaturas sin mayor conexión con la realidad.",
    "En una artículo publicado el pasado 1 de mayo en la revista “Science Advances” por un grupo de investigadores encabezados por Vipan Parihar de la Universidad de California en Irvine, se reporta que ratones sujetos a radiaciones de alta energía, tales como aquellas a las que estarían expuestos futuros astronautas en un viaje interplanetario, muestran daños en el sistema nervioso central, al igual que una reducción de sus habilidades cognitivas. Esto último sería especialmente crítico para la tripulación de una nave espacial que tendría que tomar decisiones autónomas en situaciones no previstas, lo cual requeriría de toda su capacidad intelectual; sin dejar de considerar, por supuesto, que un daño al sistema nervioso central dejaría a los astronautas con secuelas de por vida.      Los resultados del estudio de Parihar y colaboradores añaden un obstáculo más a la realización de un viaje tripulado a Marte en un futuro cercano; en particular, a la misión que ha planteado la NASA para el año 2035, por no mencionar el proyecto “Mars One” de una compañía privada holandesa, que pretende colonizar a Marte en la próxima década.El sistema solar es un lugar peligroso, lleno de radiaciones de alta energía provenientes tanto del espacio profundo –rayos cósmicos–, como del Sol. Estas radiaciones son incompatibles con la vida, y si en la Tierra ésta ha prosperado, lo ha sido por la protección que le brinda la atmósfera, que absorbe las radiaciones, al igual que el campo magnético que rodea a nuestro planeta, que las desvía. Ya que ninguna de estas protecciones estaría disponible en el curso de un viaje a Marte, una nave espacial tripulada deberá contar con un blindaje anti-radiación adecuado.Blindar una nave espacial, sin embargo, implica añadirle peso, lo que a su vez incrementa el costo de lanzarla al espacio. En este respecto, hay que notar que en la actualidad, según la NASA,  el costo para colocar un objeto en órbita es de unos 10,000 dólares por kilogramo. Y, por supuesto, para hacerlo llegar a Marte dicho costo es todavía mayor. Esto refleja la gran cantidad de energía que hay que proporcionar a un objeto para que abandone nuestro planeta. Para que así suceda, es necesario acelerarlo hacia arriba hasta que alcance una velocidad tal que contrarreste la fuerza de atracción de la Tierra. Esto, sin embargo, no es fácil de lograr. Como bien sabemos, si lanzamos una piedra hacia arriba, ésta eventualmente regresara al suelo después de haber alcanzado una cierta altura. Esta altura será mayor en cuanto mayor sea la velocidad que imprimimos inicialmente a la piedra, y en este punto cabe preguntarse de qué tamaño tendría que ser dicha velocidad para que la piedra ya no regrese al suelo. La respuesta la sabe –o la debería de saber– todo estudiante que haya aprobado un curso de física elemental: la velocidad mínima para escapar a la fuerza de gravedad de nuestro planeta es de aproximadamente 40,000 kilómetros por hora.  Esta velocidad es extraordinariamente grande en comparación con las velocidades a las que estamos acostumbrados. En particular, es claro que ni remotamente podríamos lanzar con el brazo una piedra hacia arriba con la suficiente fuerza para que escapara de la Tierra. Es más, ni  Nolan Ryan, el pitcher de grandes ligas famoso por su bola rápida, lo hubiera podido hacer, aun en sus mejores tiempos, pues apenas era capaz de lanzar la pelota a unos modestos 160 kilómetros por hora –si  bien lo hacía de manera regular. En la práctica, para poner en órbita un objeto, o bien para enviarlo hasta otro planeta, más que darle un solo impulso inicial se le acelera por medio de cohetes a lo largo de un cierto tiempo, hasta que alcance la velocidad necesaria para escapar a la gravedad terrestre. El alto costo para poner un objeto en órbita está asociado al volumen de combustible de cohete necesario para vencer la atracción de la Tierra, el cual crece, por supuesto, con el peso de dicho objeto.  Los críticos de los planes de la NASA para realizar una misión tripulada a Marte en cosa de dos décadas señalan que el problema que representan la radiaciones de alta energía a las que estarían expuestos los astronautas en un viaje de ida y vuelta, cuya duración se mediría en años, no está resuelto por el alto costo que implica blindar a la nave espacial en contra de dichas radiaciones. Las mismas, además de su bien conocido potencial cancerígeno, pueden producir daños neurológicos a la tripulación, si hemos de atender a los resultados de la investigación de Parihar y colaboradores.Por lo anterior y por otros factores, el espacio es, ciertamente, un lugar peligroso e incompatible con la vida. Mucho más peligroso y fuera de nuestro alcance de lo que pudiéramos quizá pensar en primera instancia en función de los avances tecnológicos que han posibilitado misiones tripuladas a la Luna –que, en realidad, está en la vecindad de la Tierra en comparación con Marte–, así como el envío de robots controlados a distancia a la superficie marciana. En este contexto, es posible que, al menos en las próximas décadas, los viajes tripulados interplanetarios no serán sino fantasías, en el mejor de los casos en el campo de la ciencia ficción.",
    "Conforme pasan los días crece el número de víctimas fatales del terremoto ocurrido en Nepal el pasado sábado 25 de abril. Según el último reporte del gobierno nepalés, el número de muertos por el sismo sobrepasa los 6,800. Este número, sin embargo, con seguridad se incrementará en los próximos días en la medida en que se llegue a los poblados aislados en las montañas. En un comunicado emitido por el “United States Geological Survey”, 15 horas después de ocurrido el terremoto, se estimó que había un 52% de probabilidades de que el número de víctimas fatales fuera cuando menos de 10,000, cifra que, ahora sabemos, no estaba alejada de la realidad. De hecho, se queda corta, según otras estimaciones más pesimistas. Por ejemplo, el geofísico Max Wyss, afiliado al “International Center for Earth Simulation” en Ginebra, Suiza, estima en 57,700 el número de víctimas fatales. Esta estimación guarda proporción con la magnitud del terremoto, que alcanzó una magnitud 7.8 en la escala de Richter, lo mismo que con el número de pobladores en la región afectada. Guarda también proporción con la fragilidad de las casas que habitaban. De acuerdo con Wyss, en la región que experimentó sacudidas símicas con una intensidad en la escala de Mercalli igual o mayor que 6 –que habrían causado daños serios a casas pobremente construidas– habitan 22.8 millones de personas. Wyss igualmente encuentra que 7 millones de personas habitan en el área que experimentó una intensidad 8 en la misma escala.     El terremoto tuvo un epicentro en un punto localizado a unos 80 kilómetros al noroeste de la capital Katmandú. Como sabemos, el norte del territorio de Nepal es atravesado por la cordillera Himalaya, que cuenta con nueve de las diez montañas más altas de la Tierra. La cordillera Himalaya se formó por la colisión de las placas tectónicas de la India y de Eurasia que se inició hace unos 65 millones de años. Hoy en día, la placa de la India continúa su movimiento hacia el norte, deslizándose por debajo de la placa de Eurasia a una velocidad de dos centímetros por año y elevando con esto la cordillera Himalaya. El movimiento relativo de las placas tectónicas de la India y de Eurasia provoca tensiones entre ellas que, al acumularse con el tiempo y llegar a un punto crítico, se liberan provocando terremotos como el del pasado 25 de abril. Dicho sismo, además, tuvo la particularidad de ocurrir en un área conocida como brecha sísmica central, que se extiende por 700 kilómetros desde Katmandú en el este hasta el estado indio de Uttarakhand en el oeste, en la que no había tenido lugar un terremoto mayor en mucho tiempo. Según el geofísico francés Laurent Bollinger, la última vez que habría ocurrido un terremoto en dicha área fue en el año 1344, y si bien en lo que a esto respecta no hay un acuerdo unánime, se acepta que el tiempo ha transcurrido desde el anterior terremoto se mide en cientos de años.  Dada la ausencia por siglos de terremotos en la región de la brecha sísmica central y las consecuentes tensiones acumuladas a lo largo de este tiempo, los especialistas han considerado por décadas que la ocurrencia de un sismo de gran magnitud en dicha región era inevitable. En este respecto, por mencionar sólo un estudio reciente, un artículo publicado el pasado 12 de marzo por un grupo de investigadores en universidades en Australia y la India llega a la misma conclusión; esto, sobre la base de estudios geológicos llevados a cabo en Uttarakhand.   Y ahora, después de una larga pausa, el esperado terremoto finalmente llegó; aunque, según Laurent Bollinger, no con la suficiente intensidad –lo cual es, ciertamente, afortunado– para aliviar por completo la tensión acumulada por siglos. Así, Bollinger espera que se produzcan en el futuro sismos de gran magnitud; el cuándo estos ocurrirán, sin embargo, no puede ser determinado por la ciencia actual.Si no podemos predecir con certeza cuándo ocurrirá un sismo, sí podríamos en principio mitigar sus efectos. De acuerdo con los expertos, el alto número de fatalidades producto del sismo del pasado 25 de abril –y, típicamente, de otros que ocurren en países subdesarrollados– está asociado a las deficientes técnicas de construcción de vivienda, que incluso habrían empeorado por la escasez de madera para construcción que impera en Nepal. En un artículo publicado en 2013 en la revista “Science” por Roger Bilham de la “University of Colorado” y Vinod Gaur del “Centre for Mathematical Modelling and Computer Simulation” en Balgalore, India, hacen referencia a este problema. En su artículo –que de manera significativa titulan: “Edificios como armas de destrucción masiva”– Bilham y Gaur apuntan a los dos problemas fundamentales que están en contra de los esfuerzos para mitigar los efectos de un terremoto en los países subdesarrollados: la falta de recursos de buena parte de la población para la adquisición de materiales de construcción adecuados, y la falta de códigos de construcción anti sismos o bien su falta de aplicación por prácticas de corrupción.  Como bien sabemos, ambos problemas son de difícil solución. Para empezar a resolverlos, no obstante, quizá valdría la pena tomar conciencia que una casa o un edificio mal construido en una zona sísmica es, efectivamente, un arma mortal.",
    "Imagine que a voluntad pudiéramos hacernos invisibles. ¿Qué ventajas o desventajas nos representaría una habilidad como ésta? Una visión particular al respecto nos la proporciona el escritor británico H.G. Wells en su novela “El hombre invisible” –que podría no encontrarse entre las mejores de dicho autor– en la que explora algunos aspectos de la invisibilidad. El protagonista –invisible– de dicha novela, de nombre Griffin, es un científico que logra descubrir una sustancia que hace transparente a quien la ingiere y la emplea  en sí mismo. El poder aparente que le da la invisibilidad hace que Griffin pierda piso y pretenda conquistar al mundo por medio del terror. Al final el terror, no obstante, se vuelve contra él y tiene un fin desafortunado, muriendo golpeado por lo vecinos del pueblo. Si bien el método de Wells para alcanzar la invisibilidad es de improbable realización y no trasciende más allá del ámbito de la ciencia ficción, existen en la actualidad investigaciones científicas en curso que están buscando desarrollar tecnologías para hacerla realidad. Podemos ver un objeto en función de la luz que refleja y llega hasta nuestros ojos. De este modo, una manera de hacer un objeto invisible es evitando que refleje luz; es decir, manipulando la luz para que se abra y rodee al objeto y posteriormente  vuelva a unirse como si dicho objeto no hubiera existido. En un trabajo realizado hace un par de años, un grupo de científicos logró hacer invisibles a animales relativamente grandes –un pez dorado y un gato– empleando espejos para manipular la luz. Podemos entonces contemplar que en un futuro –todavía incierto, sin embargo– puedan desarrollarse técnicas para hacernos invisibles, tal como en la novela de H.G. Wells, pero empleando principios físicos muy diferentes. En estas circunstancias, hay quien se ha preguntado cuál será el efecto de una hipotética invisibilidad sobre nuestro comportamiento social.   Un artículo aparecido esta semana en la revista “Scientific Reports” ayuda a contestar esta pregunta. Dicho artículo fue publicado por investigadores suecos, adscritos al Instituto Karolinska, encabezados por Arvid Guterstam. Como un primer paso en su investigación, Guterstam y colaboradores desarrollaron una manera de inducir en personas sanas la ilusión de ser invisibles. Para llevar esto a cabo, los investigadores colocaron al voluntario de pie con la cabeza inclinada hacia abajo, como si viera hacia su cuerpo, pero con los ojos cubiertos por un visor. En dicho visor se proyectó la imagen de dos cámaras colocadas enfrente del voluntario, a  la altura de sus ojos, apuntando hacia abajo en la dirección de un cuerpo invisible. El propósito del arreglo fue el de engañar al voluntario para que tomara la imagen que veía en el visor, generada por las cámaras enfrente de él, como la imagen real que veían sus ojos.  La ilusión de invisibilidad se produjo por medio de estímulos táctiles en el cuerpo del voluntario, los cuales se aplicaron simultáneamente con estímulos virtuales idénticos en el cuerpo invisible, los cuales fueron captados por las cámaras. De este modo, el voluntario recibía los estímulos táctiles en su cuerpo, al mismo tiempo que veía a través del visor la imagen simultánea del mismo estímulo aplicado al cuerpo invisible. Creaba así la ilusión de que él era invisible. Una vez que Guterstam y colaboradores lograron crear en los voluntarios la ilusión de invisibilidad, pudieron estudiar la manera cómo ésta afecta la percepción que tenemos de nosotros mismos. Encontraron, en particular, que la invisibilidad reduce la ansiedad que experimentan algunas personal al hablar en público.     ¿Cómo afectaría la invisibilidad nuestro comportamiento social? En “El hombre invisible” el protagonista quedó tan perturbado con su cuerpo invisible que llegó a extremos de locura. No esperaríamos que un hombre invisible del futuro pretenda conquistar al mundo. Sin embargo, es probable que de la impunidad que seguramente llevaría aparejada la invisibilidad no resultaría nada positivo. Baste señalar la agresividad del lenguaje que emplean algunas personas en los intercambios de comentarios que se dan en internet, cobijadas, si no por una invisibilidad corporal, sí por la invisibilidad que les da el espacio virtual. Por otro lado, las tecnologías necesarias para alcanzar una invisibilidad equivalente a la que relata H.G. Wells serían de una complejidad tal que es improbable que se hagan realidad en el mediano plazo. De este modo, si bien el estudio de Guterstam y colaboradores seguramente es valioso  para los especialistas, sus conclusiones no tendrán afortunadamente que ser comprobadas en la práctica en los años por venir.",
    "¿Cómo, cuándo y dónde se originaron los perros domésticos que ahora ocupan un lugar tan especial entre nosotros? Si bien los expertos no tienen respuestas definitivas a estas preguntas, un estudio genético encuentra que el perro doméstico desciende de una especie de lobo gris, ahora extinta, que se habría originado en una época anterior al inicio de la agricultura, cuando nuestros ancestros eran todavía cazadores-recolectores.  El perro doméstico habría de este modo vivido entre nosotros por decenas de miles de años, periodo durante el cual habría sufrido una transformación drástica, del animal salvaje y feroz que habría sido en su orígenes, a uno noble y amigable –si bien no siempre, como a veces lo hemos constatado con temor– con el que fácilmente podemos establecer una relación cordial. Esto último incluso de una manera más expedita –o al menos más frecuente– que con otros animales más cercanos evolutivamente a nosotros, como es el caso del chimpancé y otros grandes simios.¿Cuál es la razón por la que tenemos los humanos una relación tan estrecha con los perros? Un grupo de investigadores japoneses, encabezados por Miho Nagasawa de la Universidad Azabu en Japón, creen haberla encontrado, según reportan en el número de esta semana de la revista “Science”, en una sección dedicada al perro doméstico. De acuerdo con los investigadores japoneses, la estrecha relación que existe entre un perro y su dueño tiene una base bioquímica y está asociada a la secreción de la sustancia oxitocina, en ambos, perro y dueño, como respuesta a su interacción mutua. Hay que hacer notar que la oxitocina es una sustancia asociada al establecimiento de lazos afectivos entre humanos, en particular los lazos madre-hijo que se forman durante los primeros meses después del nacimiento. De  este modo, de acuerdo con Nagazawa y colaboradores, el mecanismo responsable de los lazos de amistad entre un perro y su dueño está basado en la misma sustancia que se encuentra asociada al establecimiento de lazos afectivos entre humanos.   Los investigadores japoneses llegaron a esta conclusión después de observar a un grupo voluntario de 30 personas –24  mujeres y 6 hombres– interactuando con su perro mascota –14 machos y 15 hembras–. Nagasawa y colaboradores midieron los niveles de oxitocina en la orina de los dueños, tanto antes como después de interactuar visualmente con su perro, y encontraron que se elevaba como resultado del contacto. Los perros, de manera concurrente, experimentaron un incremento en su nivel de oxitocina. Un experimento similar llevado a cabo con lobos no produjo los mismos resultados, a pesar de que los dueños de los lobos mascota los habían criado desde que eran cachorros. En otro experimento, los investigadores japoneses administraron oxitocina a un grupo diferente de perros antes de establecer contacto con sus dueños y observaron un incremento en la extensión de los contactos visuales entre éstos y sus mascotas. Esto llevó a un incremento en el nivel de oxitocina en los primeros, aunque el efecto solo se observó con perros hembra. De este modo, los dueños de los perros experimentaron un incremento en su nivel de oxitocina como respuesta a aquella administrada a sus mascotas.Humanos y perros domésticos, ciertamente, hemos recorrido juntos un camino muy largo, cuya duración no conocemos con certidumbre pero que se mide en decenas de miles de años. A lo largo de este camino hemos desarrollado lazos afectivos sólidos a pesar de pertenecer a especies diferentes. Estos lazos y su antigüedad son evidentes en la fotografía incluida en el artículo publicado por David Grimm en el número de esta semana de la revista “Science” –en la sección dedicada a los perros– en la cual se muestran juntos los esqueletos de una persona y de un perro cachorro, los cuales tienen una antigüedad de 12,000 años. La particularidad del caso es que el esqueleto humano tiene al esqueleto del cachorro en sus manos.  Por lo demás, el artículo de Nagasawa y colaboradores nos muestra de manera fehaciente que los lazos que hemos desarrollado con los perros domésticos son sólidos y duraderos. Tanto que es incluso posible encontrarles una explicación racional.",
    "¿Volaría usted en un aeroplano piloteado por un robot? Probablemente su respuesta sería que no, independientemente de que le demostraran con estadísticas que esto no sería más peligroso que volar de la manera convencional. La explicación a su muy probable negativa quizá la tenga Mary Cummings, quien fue piloto militar y actualmente es profesora de la Universidad Duke, cuando afirma que “La razón por la que usted quiere un piloto en la cabina es porque comparte la misma suerte con usted, de modo que si el avión está a punto de desplomarse, se siente mejor sabiendo que hay un humano en el asiento delantero que hará todo lo posible por salvar su propia vida”. Esto, por supuesto, no se aplicaría a lo sucedido con el avión de la aerolínea alemana “Germanwings” que el pasado 24 de marzo, según la versión más extendida, fue estrellado deliberadamente por su copiloto –en ese momento en  los controles de mando–, matando a sus 144 pasajeros. Como sabemos, el copiloto Andreas Lubitz se habría encerrado en la cabina del avión, aprovechando que el piloto había salido al baño, para hacer que el avión perdiera altura y se estrellara en una montaña en el sur de Francia.       El incidente del avión de “Germanwings” se suma al sufrido por el vuelo de 370 de “Malaysia Airlines” que despareció el 8 de marzo de 2014, sin que hasta la fecha se sepa con certidumbre que fue lo que le sucedió, aunque existen indicios de que fue deliberadamente desviado de su ruta por los  pilotos.  Como resultado del desastre de “Germanwings”, varias aerolíneas alrededor del mundo han dispuesto que en la cabina de mando de sus aviones permanezcan dos miembros de la tripulación de manera permanente. De este modo, a menos que las dos personas en la cabina se pusieran de acuerdo, se reduciría considerablemente la probabilidad de que se repitiera el caso de “Germanwings”. Otra posibilidad que se ha manejado para evitar un desastre parecido es la  sustitución del copiloto  por un sistema de vuelo automático, o bien por medio de un operador remoto. La eliminación del copiloto –y a la larga del piloto– sería una continuación de la tendencia que se ha observado desde hace décadas de reducir el número de miembros de la tripulación en las cabinas de mando de los aviones comerciales en la medida en que se sofisticaron sus sistemas de comunicación y de control de vuelo. Así, mientras que en los primeros tiempos de la aviación comercial, además del piloto y del copiloto, se encontraban en la cabina de mando un ingeniero de vuelo, un navegante y un radio operador, los dos últimos desaparecieron desde la década de los años 40 del siglo pasado, mientras que el ingeniero de vuelo se hizo innecesario en los años 80. Según esta tendencia, el siguiente en desaparecer sería el copiloto.  En este respecto  y de acuerdo con un artículo publicado esta semana en el periódico New York Times, la agencia de investigación del Departamento de Defensa de los Estados Unidos empezará este año a probar un robot que será colocado en el lugar del copiloto y que podrá cumplir muchas de sus funciones. Entre otras cosas, podrá hablar con el piloto, manipular los controles del avión, e incluso podrá tomar control total del mismo y realizar maniobras de aterrizaje y despegue. Otra opción para sustituir al copiloto, explorada por la NASA, es su reemplazo por medio de un operador en tierra, el cual estaría a cargo de varios aviones en vuelo de manera simultánea. Si bien la automatización del tráfico aéreo resulta muy atractiva dada la dimensión de la industria de la aviación, no todo el mundo está de acuerdo que dicha automatización puede llevarse a cabo en el corto plazo. Así, según la Academia Nacional de Ciencias de los Estados Unidos “…la aviación civil está en la frontera de experimentar cambios revolucionarios en sus capacidades y en su operación asociadas con los sistemas automáticos. Estos sistemas, sin embargo, plantean serias preguntas que no tienen respuesta por ahora, acerca de cómo integrar todos los revolucionarios avances tecnológicos en un espacio aéreo nacional gobernado por reglas que pueden sólo ser cambiadas después de largas deliberaciones para alcanzar un consenso”.La Academia de Ciencias de los Estados Unidos considera que existen barreras de orden tecnológico y de reglamentación para incrementar el uso de sistemas automáticos en la aviación civil en los Estados Unidos. Considera, por ejemplo, que los sistemas automáticos están limitados en sus capacidades sensoriales, perceptivas y cognitivas para operar sin intervención humana. Igualmente, considera que la automatización del espacio aéreo requiere que los humanos y las máquinas trabajen juntos de maneras nuevas y diferentes que no han sido todavía identificadas.Pareciera, entonces, que los aviones sin piloto todavía no están a la vuelta de la esquina –a pesar de sus impulsores– y que por ahora no nos veremos enfrentados a la disyuntiva de abordar o no un avión pilotado por un robot. Que posiblemente no nos secuestre, pero al que, aun con sus buenas intenciones, no le tendríamos la suficiente confianza.",
    "Como es conocido, en los cuadros de la etapa posterior del Greco aparecen frecuentemente figuras humanas alargadas. Esto llevó a algunos a plantear la hipótesis de que  el pintor padecía de astigmatismo, defecto que, por supuestos lentes correctores pobremente diseñados, le habría hecho ver los objetos alargados en la dirección vertical. Esta hipótesis, no obstante, tiene una falla lógica, pues asumiendo que el Greco efectivamente tuviera este defecto visual, al pintar sus modelos tendría que haberlo hecho con las proporciones correctas, pues de otro modo las figuras en el cuadro le hubieran parecido doblemente alargadas. Así, podemos de entrada desechar la hipótesis del Greco astigmático. Esto, sin embargo, es estrictamente válido sólo si el Greco hubiera pintado a partir de modelos. Si, por el contrario, lo hubiera hecho de memoria, la hipótesis tendría que ser puesta a prueba. Esto fue lo que hace algunos años hizo Stuart Anstis de la Universidad de California en San Diego, quien llevó a cabo una serie de experimentos con voluntarios a los que les pidió dibujar un cuadrado viendo a través de un dispositivo óptico que les deformaba la visión y les hacía ver las cosas alargadas, tal como supuestamente  las habría visto el Greco. Publicó sus resultados en la revista “Leonardo” en el año 2000.  Anstis pidió a los voluntarios dibujar un cuadrado –viendo a través del dispositivo óptico– de dos maneras: primero copiando un cuadrado perfecto y en seguida de memoria. En el primer caso los voluntarios reprodujeron sin deformación el cuadrado que copiaron, mientras que en el segundo dibujaron un rectángulo alargado verticalmente. Esto último aparentemente apoya la hipótesis del astigmatismo del Greco. Experimentos adicionales, sin embargo, la desmintieron. En efecto, en una segunda serie de experimentos Anstis convenció a una de las voluntarias para que usara por varios días el dispositivo óptico deformador de la visión, al mismo tiempo que, de manera repetida, dibujara cuadrados de memoria. Como resultado, si bien inicialmente la voluntaria dibujó rectángulos verticalmente alargados, poco a poco corrigió este alargamiento y el curso de dos días estaba dibujando cuadrados perfectos superando el astigmatismo artificial. De este modo, aun si el Greco hubiera sido astigmático su pintura no lo hubiera revelado. La conclusión es que el Greco pintaba figuras alargadas por razones estéticas y no por un problema visual.     Lo que es cierto para El Greco no lo sería, sin embargo, para otros pintores y el último número de la revista de divulgación científica “Scientific American” contiene un artículo en el que se dan algunos ejemplos al respecto. Entre otros pintores con problemas visuales, en dicho artículo se menciona a Rembrandt, Monet y Degas. Un caso particularmente interesante es el del controvertido pintor británico del siglo XX Francis Bacon, quien es conocido por sus pinturas de rostros y cuerpos humanos mutilados o severamente deformados –en contraste con los objetos inanimados que aparecen en dichas pinturas sin deformación–. La reacción que el espectador común y corriente experimenta ante los cuadros de Bacon es el de rechazo. Así, Margaret Thatcher se refirió a Bacon como “ese hombre que pinta esos cuadros espantosos”. De hecho, según lo que explicaba Bacon, lo que él buscaba con sus pinturas es que el espectador experimentara un “choque visual”, lo que sin duda conseguía. En un artículo publicado en el año 2013 en la revista “Frontiers in Human Neuroscience” por Semir Zeki y Tomohiro Ishizu del University College London, encuentran que las pinturas de Bacon son tan perturbadoras porque apuntan directamente en contra del concepto que tenemos de un rostro o un cuerpo humano  y producen una respuesta neurológica significativamente diferente a la que se produce ante caras y cuerpos normales. Dicho concepto es heredado o bien adquirido en una etapa muy temprana de la vida y por tanto es independiente del bagaje cultural del espectador. El choque visual de los cuadros de Bacon sería entonces universal. Independientemente de la intención consciente de horrorizar al espectador, cabe preguntarse por las motivaciones o circunstancias inconscientes que llevaron a Bacon a pintar de esa manera tan poco convencional. En este respecto, un artículo publicado en agosto de 2014 en “Frontiers in Human Neuroscience”  por un grupo de investigadores de Francia y Suiza, encabezados por Avinoam Safran de la Universidad de la Sorbona, aventuran que Bacon sufría de un desorden de percepción visual conocida como dismorfopsia, que hacía que percibiera los objetos deformados.La conclusión de Safran y colaboradores se basa en testimonios de Bacon. En palabras del pintor, “Cuando lo observo a usted hablar veo una imagen que cambia constantemente: el movimiento de su boca, de su cabeza, de alguna manera; se mueve todo el tiempo. Yo intenté capturar esto en la pintura”.La pintura de Bacon es, ciertamente, de difícil digestión para los no iniciados. Esto no impide, sin embargo, que alcance en el mercado del arte precios estratosféricos. Baste señalar que el tríptico  “Tres estudios de Lucian Freund”, pintado por Bacon en 1969, fue vendido en subasta en 2013 en 142.5 millones de dólares. Esto coloca a la obra en el octavo lugar entre las pinturas que han alcanzado un precio más alto en subasta. Nada despreciable para una obra producto de un supuesto problema neurológico.",
    "Con la expansión del Imperio Azteca durante el siglo XV, el territorio que habitaban los tlaxcaltecas,  que nunca fueron dominados por los aztecas, terminó rodeado de enemigos.  Por otro lado, un material de amplio uso en Mesoamérica fue la obsidiana, con la que se fabricaban ornamentos, cuchillos  y puntas de flecha, entre otros utensilios para la vida diaria y armas para la guerra. Dado que los tlaxcaltecas estaban rodeados de pueblos no amigables, cabe preguntarse cómo obtenían la obsidiana para satisfacer sus necesidades, dado que en su territorio no existían depósitos de este mineral.Un artículo aparecido esta semana en la revista “Journal of Acheological Science” da contestación a esta pregunta. Dicho artículo fue publicado por un grupo internacional de investigadores encabezado por John Millhauser de la Universidad Estatal de Carolina del Norte en los Estados Unidos,  y en el que se incluyen especialistas del Centro de Investigación y de Estudios Avanzados.  Mérida, Yucatán y de El Colegio de Michoacán. De acuerdo con Millhauser y colaboradores, los tlaxcaltecas obtenían mayormente la obsidiana del yacimiento de El Paredón, localizado en el sur del hoy Estado de Hidalgo.   Millhauser y colaboradores basan sus conclusiones en un estudio llevado a cabo con objetos de obsidiana encontrados en Tlaxcala. De éstos, un 14% tiene un característico color verde que denota su procedencia de minas en la región de Pachuca. El origen del restante 86% no pudo ser determinado de manera visual. Para esto se empleó una técnica basada en rayos X, que sin ambigüedad delata el lugar de la mina de la que proceden.   El que sólo el 14% de la obsidiana empleada en Tlaxcala provenga de la zona de Pachuca contrasta con lo que ocurría con los aztecas, que obtenían el 90% de su obsidiana precisamente de dicha zona. Esto lleva a suponer a los investigadores que el comercio  precolombino de obsidiana estuvo afectado por la situación política en Mesoamérica, la cual habría hecho difícil que los tlaxcaltecas tuvieran acceso a las fuentes más comunes de obsidiana. Es interesante hacer notar también que el yacimiento de El Paredón estaba cerca de la frontera de la región dominada por los aztecas y que, aparentemente, los tlaxcaltecas habrían sido capaces de transportar la obsidiana de manera sistemática. En relación a esto, Millhauser se pregunta por qué los aztecas –que eran abiertamente hostiles a los tlaxcaltecas– no intervinieron para impedirlo.  Una posibilidad, según Millhauser, es que los aztecas hayan considerado que esto último significaba un esfuerzo que no valdría la pena hacer, dado que la obsidiana no era escasa y que de haber bloqueado a los tlaxcaltecas la mina de El paredón, éstos posiblemente hubieran encontrado alguna otra fuente de obsidiana.En todo caso, el que los tlaxcaltecas hubieran tenido a su disposición una fuente de materia prima con la que podían fabricar armas justo en la frontera del territorio enemigo, implicaría que, después de todo, los aztecas no eran tan poderosos como comúnmente se asume. En palabras de Millhauser “La concepción popular de un Imperio Azteca todopoderoso antes de la llegada de Cortez es exagerada. La región era un lugar política y culturalmente complicado”.El estudio de Millhauser y colaboradores nos enseña entonces que si bien la expansión del Imperio Azteca en el siglo XV terminó por rodear a los tlaxcaltecas y encapsularlos territorialmente, éstos de alguna manera se las arreglaron para mantener un flujo de obsidiana, la cual empleaban de manera amplia según han determinado los arqueólogos. Para esto emplearon fuentes no convencionales, diferentes a las que comúnmente empleaban los aztecas. La obsidiana, entre otras cosas, permitió a los tlaxcaltecas fabricar armas para defenderse de sus enemigos y mantener su independencia del Imperio Azteca. Armas que, por otro lado, emplearon cuando se aliaron con Cortez para derrotar a los mexicas. Aunque éstas de poco hubieran podido servirles para defenderse de las armas de fuego de los españoles, mucho más evolucionadas. O, dicho de otro modo, en lo que respecta a las armas de guerra, hierro mata obsidiana.",
    "De acuerdo con cifras de los Centros de Control y de Prevención de Enfermedades de los Estados Unidos, cada año se infectan con bacterias resistentes a los antibióticos dos millones de estadounidenses, de los cuales unos 23,000 mueren al no  lograr superar la enfermedad. La misma fuente estima que los correspondientes costos ascienden a 20,000 millones de dólares por año en cuidados médicos, además de 35,000 millones de dólares por pérdidas en productividad. Las bacterias resistentes a los antibióticos representan, así, un serio problema de salud pública para nuestro vecino del norte. Y, por supuesto, no solamente para nuestro vecino del norte, pues la resistencia a los antibióticos es por naturaleza un problema que no respeta fronteras, si bien su magnitud particular varía según el país considerado. El mecanismo que genera microbios resistentes a los antibióticos es una consecuencia natural del uso de los mismos, ya sea para la cura de enfermedades en humanos o bien en la industria avícola y ganadera. De este modo, al desarrollo de un nuevo antibiótico puede seguir la aparición de bacterias resistentes al mismo. En concordancia con lo anterior, los expertos han asociado el incremento de bacterias resistentes al uso excesivo de antibióticos en humanos. Un estudio publicado esta semana en la revista en línea PLOS ONE, sin embargo, concluye que, si bien el abuso de los antibióticos es una causa del desarrollo de bacterias resistentes, es la calidad del gobierno y la corrupción propia de cada país el factor más importante en este respecto. El estudio fue llevado a cabo con 28 países europeos por un grupo de investigadores en Australia, encabezados por  Peter Collignon de la Universidad Nacional de Australia. Los países con menor grado de resistencia microbiana y menor corrupción son Suecia y Dinamarca. En el otro extremo, los países más corruptos son Lituania, Letonia y Bulgaria, que se encuentran entre los de mayor grado de resistencia a los antibióticos.     ¿Cómo puede haber una relación entre bacterias resistentes a los antibióticos y el nivel de corrupción de un país, relación que en primera instancia se antoja improbable? De acuerdo con Collignon y colaboradores, el desorden debido a un gobierno de baja calidad produce, por un lado, un descontrol en el uso de los antibióticos –tanto en humanos como en animales, propagándose en este último caso a través de la cadena alimenticia–, mientras que al mismo tiempo resulta en un tratamiento deficiente de las enfermedades infecciosas. Igualmente, se afectan negativamente los procedimientos de purificación del agua y, en general, las condiciones de higiene de la población. Todo esto se combinaría para generar un ambiente propicio para el desarrollo de bacterias resistentes a los antibióticos.Sanyaya Senanayake, uno de los coautores del estudio de referencia, ofrece un ejemplo concreto al respecto: “La bacteria E-coli se encuentra comúnmente en humanos y en animales. Sabemos que en ciertos sectores agrícolas los antibióticos son empleados en animales y de esto resulta una resistencia a los mismos, y que dicha resistencia puede ser trasmitida a los humanos de varias formas, incluyendo la trasmisión a través de los productos alimenticios.  Nosotros pensamos que en los países con altos niveles de corrupción es menos probable que haya una supervisión estricta de cómo esos antibióticos son usados y la manera en que son desechados, habiendo la posibilidad de que ríos y otras partes del medio ambiente sean contaminados.”   Collignon y colaboradores encuentran también que no hay una correlación entre el porcentaje de personas en la población con una educación universitaria y la generación de bacterias resistentes a los antibióticos y, de manera sorprendente, que la medicina privada produce más bacterias resistentes que la medicina pública. No tienen una explicación clara para esto último, pero los investigadores ofrecen una hipótesis en el sentido que los médicos en el sector privado tienen menos restricciones para el uso de antibióticos, tanto en tipo como en volumen, que aquellos del sector público.Como sabemos –y comprobamos todos los días–, cuando se le compara con otros países del mundo nuestro país no sale bien parado en cuanto a índices de corrupción se refiere. En efecto, de acuerdo con Transparencia Internacional, México ocupa el lugar 103 entre 175 países en el Índice de Percepción de la Corrupción, con una puntuación de 35 puntos sobre 100. El primer lugar en dicho índice es ocupado por Dinamarca –que ocupa el segundo lugar en Europa en cuanto a la prevalencia de bacterias resistentes a los antibióticos– con 92 puntos, mientras que en el fondo se encuentran Somalia y Corea del Norte. De este modo, de ser posible extender a nuestro país los resultados de Collignon y colaboradores –obtenidos empleando datos estadísticos de países europeos–, México tendría el dudoso honor de ser un buen productor de bacterias resistentes a los antibióticos. Esta sería la mala noticia. La buena es que nuestro país podría en todo caso revertir la situación, pues de acuerdo con el artículo de referencia, las bacterias resistentes en el pasado no afectan la resistencia de las bacterias en el presente. Esto es, un posible alto nivel de resistencia microbiana en el presente no sería un obstáculo para mejorar la situación en el futuro.  Dado el caso, todo estaría en que nos lo propusiéramos.",
    "Tal parece que, si bien no es de este mundo, Ganímedes comparte algunas características con nosotros. Esto, de acuerdo a un artículo publicado en línea esta semana en la revista “Journal of Geophysical Research” por un grupo internacional de investigadores encabezado por J. Saur de la Universidad de Colonia en Alemania. Lo anterior probablemente requiera de algunas precisiones que ofrecemos en lo que sigue. Para empezar, haremos notar que por Ganímedes no hacemos referencia a Ganimedes, el troyano que fue raptado y llevado al Olimpo por Zeus según la mitología griega, sino al satélite joviano que fue descubierto 1610 por Galileo Galilei. Cuando en ese año Galileo apuntó su telescopio hacia Júpiter descubrió cuatro puntos luminosos orbitando al planeta –supo esto último porque los puntos cambiaban de posición de un día para otro alrededor de Júpiter–. Uno de esos puntos era Ganímedes; los otros tres correspondían a los también satélites jovianos Io, Europa y Calisto. De los cuatro satélites descubiertos por Galileo, Ganímedes es el más grande. De hecho, Ganímedes es el satélite más grande del Sistema Solar, con un volumen incluso superior al de Mercurio. Haremos notar, igualmente, que Ganímedes es un mundo inhóspito en el que no podríamos sobrevivir sin medios artificiales. Aun así, es posible que comparta con nuestro planeta una de las características que lo distinguen: las condiciones para el desarrollo de la vida tal como la conocemos.  Ganímedes fue tema del cuento corto publicado  por el escritor de ciencia ficción Isaac Asimov en 1940 que lleva por título “Navidad en Ganímedes”. Dicho cuento versa  sobre la explotación minera de este satélite por una compañía privada que utilizaba a los nativos ganimedeanos, llamados “astruces” –debido a que por su aspecto recordaban a los avestruces–,  como mano de obra semi-esclava. La compañía durante el relato está experimentando problemas pues uno de los mineros habló con los astruces acerca de Santa Claus y éstos, que poseían un cierto grado de inteligencia que incluso les permitía mal hablar el inglés, se negaban a trabajar a menos que los visitara Santa Claus en Navidad llevándoles regalos.    Los mineros pudieron arreglar el problema disfrazando de Santa Claus a aquel que había cometido la indiscreción de hablarles de la Navidad a los astruces. Hicieron, además, uso de un viejo trineo de madera que había sido llevado a Ganímedes mucho tiempo atrás cuando se pensaba que su superficie estaba cubierta de nieve, y de ocho animales nativos que vagamente recordaban a los renos. El problema aparentemente había sido resuelto, después de dificultades considerables para doblegar y amarrar al trineo a los supuestos renos, hasta que los mineros se dieron cuenta de que el año ganimediano –el tiempo que el satélite tarda en completar una órbita alrededor de Júpiter– es de sólo 7 días y que los astruces esperaban que Santa Claus los visitara con esta periodicidad.Las condiciones en la superficie de Ganímedes no son, por supuesto, adecuadas para que existan astruces o renos nativos ni tampoco para que los humanos podamos sobrevivir con apenas una máscara y un tanque de oxígeno como ocurre en el relato de Asimov. Por un lado, si bien aparentemente existe una atmósfera de oxígeno en Ganímedes, ésta es demasiado tenue. Por otro lado, dado que Júpiter está unas cinco veces más lejos del Sol que nuestro planeta, la radiación solar que recibe Ganímedes es muy débil y, de manera consecuente, la temperatura de su superficie es extremadamente baja, tanto que puede alcanzar los menos 180 grados centígrados. De este modo, el cuento de Asimov –que no es, por supuesto, sino un relato humorístico– no tiene sustento físico. Pone de  manifiesto, no obstante, que Ganímedes es un mundo extraño en el que las navidades tendrían que celebrarse con una frecuencia inusitada. Y, sin embargo, a pesar de sus diferencias con la Tierra, de acuerdo con el trabajo de Saur y colaboradores citado al principio de este artículo, Ganímedes tiene bajo una gruesa capa de hielo superficial un océano de agua salada que en volumen es mayor que toda el agua existente en la Tierra. Basan esta conclusión en el comportamiento de las auroras ganimedianas –similares a las auroras boreales que ocurren en nuestro planeta–, que están influenciadas por la rotación de Júpiter.  Un estudio por medio del telescopio espacial Hubble de los cambios que experimentan estas auroras en la medida en que rota Júpiter, muestra que debe existir un vasto océano de agua salada con una profundidad de 100 kilómetros bajo la superficie helada de Ganímedes    En forma coincidente con la comprobación de la existencia de un océano de agua salada en Ganímedes, un artículo publicado esta semana en la revista “Nature” llega a la conclusión que también en Encélado, el satélite de Saturno, hay un océano subterráneo en el que, de manera sorprendente, las temperaturas pueden alcanzar los 90 grados centígrados.  Así, tenemos que, de manera inesperada, dos mundos helados y lejanos tienen mayores probabilidades de mantener alguna forma de vida que la que tiene nuestro vecino Marte, aparentemente más parecido a la Tierra –al menos en las imágenes que nos han llegado desde su superficie–, pero extremadamente seco y con seguridad desolado. ¿Qué forma de vida existirá en Ganímedes en caso de que la hubiera? No lo sabremos hasta que lleguemos allá, pero probablemente no encontraríamos ni astruces ni remedos de renos.",
    "De acuerdo con el sitio de Google “Our mobile planet”, en el año 2013 el 56% de los norteamericanos contaban con un teléfono inteligente y el 67% lo usaban todos los días para acceder a internet. Según la misma fuente, el país con mayor penetración de teléfonos inteligentes en el mundo en ese año fueron los Emiratos Árabes Unidos, en donde un 74% de los habitantes contaba con uno de estos aparatos.  En cuanto a nuestro país, el 37% de los mexicanos en 2013 poseían un teléfono inteligente y el 73% lo usaba todos los días para acceder a internet. En este respecto, México ocupaba el primer lugar en Latinoamérica y el 28 en el mundo. De acuerdo con los expertos, para el año 2025 unas 5,000 millones de personas –más del 60% de los habitantes del planeta– harán uso de un teléfono inteligente para una serie de actividades, que van desde usarlo como teléfono –si bien no de manera predominante–, hasta emplearlo para averiguar si lloverá el día de mañana o para encontrar una dirección. En estas circunstancias, los especialistas conceptualizan al teléfono inteligente como una extensión de nuestro cerebro y se preguntan cómo esto ha modificado, o modificará en el futuro, nuestro comportamiento.En este respecto, un grupo de investigadores del Departamento de Psicología de la Universidad de Waterloo, Canadá, encabezados por Nathaniel Barr, publicaron esta semana en la revista “Computers in Human Behaviour” un artículo que lleva un título sugerente: “El cerebro en su bolsillo: evidencia de que los teléfonos inteligentes son usados para suplantar el razonamiento”. En dicho artículo los investigadores describen y discuten los resultados de un estudio que llevaron a cabo para determinar el grado en el que empleamos a los teléfonos inteligentes como una extensión de nuestro cerebro –de manera análoga a cuando anotamos un número en un papel en lugar de memorizarlo– para ahorrarnos así esfuerzos cognitivos o mentales.  Barr y colaboradores parten de la noción que las personas en una determinada circunstancia somos cognitivamente tacañas y tendemos a sustituir el razonamiento analítico –que requiere de un cierto esfuerzo– por una razonamiento intuitivo más ligero.  El contraste entre el pensamiento analítico y el intuitivo lo ilustran con el siguiente problema. Un bate y una pelota tienen un costo combinado de $1.10. El bate cuesta $1.00 más que la pelota. ¿Cuál es el costo de la pelota? La respuesta intuitiva inmediata es que el costo de ésta es 10 centavos, lo cual es incorrecto, pues de ser el caso el costo combinado del bate y la pelota sería $1.20 –es decir, el costo del bate de $1.10 más el costo de la pelota–. La respuesta correcta, obtenida después de hacer un esfuerzo mental, es que el costo de la pelota es 5 centavos.  De acuerdo con Barr y colaboradores, el porcentaje típico de respuestas acertadas al problema anterior es de sólo 33%. Es decir, son más aquellos que escogen una respuesta intuitiva inmediata, que los que hacen el esfuerzo mental requerido por el razonamiento analítico. Como lo anticipa el título del artículo de Barr y colaboradores, la conclusión es que algunas personas –descritas como más intuitivas que analíticas– usan a los teléfonos inteligentes como una extensión de su cerebro y como un medio para ahorrarse esfuerzos mentales. Es decir, que hay una tendencia a dejar que el teléfono inteligente piense por nosotros. Encontraron, además, que hay una correlación entre la tendencia a reaccionar de manera intuitiva con el tiempo dedicado a buscar información con el teléfono inteligente, aunque no la encuentran con el tiempo dedicado a las redes sociales o al uso del teléfono como un medio de entretenimiento.    Por otro lado, Barr y colaboradores no encuentran una relación entre el grado en que una persona reacciona de manera intuitiva con el hecho de que posea o no un teléfono inteligente. Esto indicaría que usar un teléfono de este tipo no es por sí  mismo un factor que influya en la disposición de una persona a pensar o no de manera intuitiva. Consideran, sin embargo, que esto bien pudiera no ser cierto entre aquellos que hacen un uso excesivo del teléfono inteligente, quienes sí pudieran resultar afectados en sus capacidades analíticas.¿Cómo nos está afectando el desarrollo acelerado de la red de internet y de todas sus aplicaciones, en particular los teléfonos inteligentes? Barr y colaboradores encuentran una correlación entre la habilidad a pensar de manera analítica con el uso del teléfono inteligente para buscar información. Si esta correlación puede ser explicada en términos de nuestra tacañería mental, que nos empujaría a dejar que los esfuerzos mentales los haga nuestra extensión cerebral –en forma de teléfono inteligente– en lugar de nuestro propio cerebro, es algo que deber ser confirmado por estudios adicionales, como Barr y colaboradores lo reconocen.En tanto esto sucede valdría la pena preguntarnos ¿en qué grado dependemos del teléfono inteligente para buscar información que bien pudiera estar guardada en alguna parte de nuestro cerebro, pero que nos da flojera tratar de recuperar?",
    "Titán, el satélite mayor de Saturno, comparte algunas características con nuestro planeta. A manera de ejemplo, podemos mencionar que la sonda Cassini, la cual ha estado orbitando a Saturno desde el año 2004, encontró que la superficie de Titán está sembrada de lagos líquidos, algunos de ellos de grandes dimensiones, incluso mayores que las de los Grandes Lagos en la frontera entre los Estados Unidos y Canadá.  Es posible también que sobre la superficie de Titán corran ríos –o que hayan corrido en alguna época remota– tal como sucede en la superficie de nuestro planeta. Otra coincidencia más es que Titán, al igual que la Tierra, tiene una atmósfera formada mayoritariamente por nitrógeno. Es posible, no obstante, que no encontremos muchas más coincidencias entre ambos mundos, Titán y la Tierra, separados por más de mil millones de kilómetros de espacio interplanetario. De hecho, los lagos de Titán no son de agua sino de metano y etano. No podrían ser de agua, pues la temperatura en la superficie de Titán es de menos 180 grados centígrados y a esta temperatura el agua no puede estar sino congelada. El metano, en cambio, se licua a una temperatura de menos 162 grados centígrados y puede existir en Titán en forma líquida.  Ciertamente, Titán es un mundo inhóspito, incompatible con la vida tal como la conocemos. Al mismo tiempo, no obstante, el descubrimiento de los lagos de Titán ha hecho especular a los expertos sobre la posibilidad de que exista en este satélite alguna forma de vida, si bien diferente a la  que conocemos. Si fuera el caso, los organismos titanianos de alguna manera tendrían con el metano líquido la misma relación que los organismos terrestres tenemos con el agua.Un artículo publicado esta semana en la revista en línea “Science Advances” especula en esta dirección empleando sofisticados argumentos. Dicho artículo fue publicado por James Stevenson y Paulette Clansy, de la Escuela de Ingeniería Química y  Biomolecular de la Universidad Cornell, en el Estado de Nueva York, y por Jonathan Lunine del Departamento de Astronomía de la misma universidad.  De acuerdo con Stevenson y colaboradores, en Titán podría haber evolucionado una forma de vida particular, adaptada tanto a su atmósfera –casi enteramente compuesta de nitrógeno–como a las extremadamente bajas temperaturas que ahí imperan. Esto, a diferencia de la vida en la Tierra, que está basada en el agua líquida y en temperaturas por arriba de los cero grados centígrados. Un elemento fundamental para la vida en la Tierra es la membrana que rodea a las células y que aísla a su interior del medio ambiente acuoso. Stevenson y colaboradores consideran que la aparición de esta membrana habría sido un primer paso evolutivo que dio origen a la vida en nuestro planeta. Una circunstancia similar podría haberse dado en Titán, y en este sentido estos investigadores  especulan que, sin bien una membrana similar a la de los organismos terrestres se congelaría y no podría funcionar en Titán, existen otras posibilidades viables para una membrana celular adaptada a las condiciones de bajas temperaturas ahí imperantes.En relación a esto, Stevenson y colaboradores propusieron una membrana basada en el nitrógeno, abundante en la atmósfera de Titán, en lugar del fósforo y el oxígeno que forman parte de las celulares de los organismos en la Tierra. Mediante complicados cálculos matemáticos, demostraron que la membrana propuesta se comportaría en las temperaturas de Titán de manera similar a las de las membranas de los organismos terrestres. Esto indicaría que, no obstante sus inhóspitas condiciones, una exótica forma de vida podría haber florecido en Titán.Esto, por supuesto, es sólo una especulación, si bien apoyada con argumentos sofisticados. En palabras de Stevenson y colaboradores: “La disponibilidad de moléculas con la habilidad de formar membranas celulares no demuestra por si misma que la vida es posible. Esto, sin embargo, dirige nuestra búsqueda de procesos químicos metabólicos y reproductivos que podrían  ser compatibles en condiciones de bajas temperaturas”.Para saber si hay, o alguna vez hubo vida en Titán posiblemente tengamos que ir hasta allá con las herramientas de búsqueda adecuadas. Dicho de manera más propia, posiblemente tengamos que enviar hasta allá una sonda con las capacidades analíticas que el caso amerita, pues, al menos en el mediano plazo, el que uno de nosotros pueda pisar el suelo de Titán después de viajar más de mil millones de kilómetros se antoja difícil, por no decir imposible.  Esto último, sin embargo, no impide que podamos echar a volar la imaginación y especular sobre exóticas formas de vida en igualmente exóticos mundos remotos. De hecho, como lo afirma Stevenson –quien es estudiante de posgrado en la Universidad Cornell– para llevar a cabo su estudio se inspiró en parte en el escritor de ciencia ficción Isaac Asimov quien, en una novela publicada en 1962, escribió, precisamente, acerca de la vida no basada en el agua.",
    "¿Qué características tienen en común la antigua Tenochtitlán y la actual Ciudad de México? Es posible que no muchas, si bien algunas habrá. La más obvia, por supuesto, es que Tenochtitlán ocupó un espacio que es actualmente parte de la Ciudad de México y en este sentido aquella es un antecedente de esta última. Una característica común adicional es que tanto Tenochtitlán como la Ciudad de México se cuentan entre las ciudades más grandes del mundo en el que les tocó desarrollarse. Por otro lado, en términos absolutos la Ciudad de México es una ciudad de proporciones gigantescas que no guarda ninguna proporción con Tenochtitlán, ni en población ni en superficie urbana. Tecnológicamente también existe una gran diferencia entre ambas ciudades. Así, por ejemplo, mientras que en la Ciudad de México existe en la actualidad un sistema masivo de transporte público movido por energía eléctrica, en la antigua Tenochtitlán el transporte se hacía por medio de canoas y canales de agua.     Pueden citarse muchas otras diferencias entre Tenochtitlán y la actual Ciudad de México, diferencias de orden tecnológico, social, económico, político o cultural. De hecho, se puede argüir que es un sinsentido intentar compararlas, pues son ciudades que pertenecen a dos mundos distintos –separados, además, por cinco siglos–, por más que una sea el antecedente de la otra. Y no obstante lo anterior, un artículo aparecido esta semana en la revista “Science Advances”, publicado por la “American Association for the Advancement of Science”, arguye que, al menos en un aspecto –la productividad–, los habitantes de la Ciudad de México se comportan de manera similar a como lo hacían sus antecesores de Tenochtitlán. El artículo fue publicado por un grupo de investigadores de universidades de los Estados Unidos, encabezados por Scott Ortman de la Universidad de Colorado y del Instituto de Santa Fe, Nuevo México. En realidad lo que arguyen Ortman y colaboradores es más general y se aplicaría a todos los conglomerados urbanos del pasado y del presente. Según estos investigadores,  la productividad de una ciudad puede ser descrita por una misma ley matemática, según la cual dicha productividad aumenta en la media que se incrementa el tamaño de la ciudad. Esta característica sería producto del tipo de interacción que establecen los habitantes de una ciudad y resultaría en gran medida independiente de las circunstancias particulares propias de cada núcleo urbano, incluyendo la época histórica en que se desarrolla.Ortman y colaboradores llegaron a esta conclusión a partir de un estudio de la productividad de diferentes poblaciones prehispánicas alrededor de Tenochtitlán. Dado que las posesiones de los habitantes de estas poblaciones desaparecieron hace ya largo tiempo, los investigadores tomaron como una medida de dicha productividad al volumen en metros cúbicos de edificios públicos construidos en un año, así como al tamaño de las casas habitación. Encontraron que la productividad en las poblaciones prehispánicas aumentaba con el tamaño de los conglomerados urbanos de la misma manera a como los hace la productividad urbana contemporánea.Esto es, ciertamente, de llamar la atención y en este respecto afirma Ortman: “Para mí, la idea que los mismos procesos que generaron un lugar como la ciudad de Nueva York estuvieron operando en pequeñas poblaciones en otras partes del mundo en la antigüedad resulta sorprendente”   Además, el que la productividad de una ciudad en la antigüedad dependa sólo de su tamaño tiene consecuencias interesantes para entender la relación entre la organización social y la productividad. Así, como apunta Ortman: “Nosotros crecimos alimentados por la idea que gracias al capitalismo, la industrialización y la democracia, el mundo moderno es radicalmente diferente del mundo en el pasado. Lo que encontramos aquí es que las fuerzas que determinan los patrones socioeconómicos en las ciudades modernas les preceden”. Tendríamos así que la productividad de la cual depende el bienestar relativo de una población urbana depende de su tamaño. El bienestar absoluto, no obstante, dependerá de otros factores. Así, si bien la ciudad de Nueva York y la Ciudad de México clasifican ambas en el grupo de las ciudades más grandes del mundo, sus productividades respectivas son claramente muy diferentes. Y esto, si hemos de creerle a Ortman y colaboradores, es independiente del grado de capitalismo, industrialización o democracia del país.En este respecto, cabe preguntarse por el nivel medio de vida en Tenochtitlán –sin tomar en cuenta a los prisioneros destinados a ser sacrificados en el Templo Mayor–. ¿Cómo se compara con el nivel medio de vida en la actual Ciudad de México?  ¿Ganó o perdió la ciudad con la conquista española?",
    "Según los climatólogos, Norteamérica experimentó en el periodo medieval, entre los años 900-1400 de nuestra era, una serie de sequías severas de larga duración. Sequías tan prolongadas que Richard Seager, Celine Herweijer y Ed Cook del Lamond-Doherty  Earth Science Laboratory de Columbia University, consideran que es más apropiado pensar que dicha época medieval simplemente fue más árida que la época que le siguió. Como un dato interesante, Seager y colaboradores hacen notar que el número de huesos de bisonte encontrados en la región de Norteamérica con una antigüedad de alrededor de un milenio es pequeño en comparación con aquellos que se encuentran con antigüedades mayores o menores. Una explicación a este hecho es que hace mil años la aridez del terreno era tal que no existían las condiciones para sustentar una población mayor de bisontes.Una evidencia de las mega-sequías que asolaron al sureste de los Estados Unidos en la época medieval son los restos de árboles de esa antigüedad –revelada mediante datación con Carbono 14– que es posible encontrar en la actualidad en lechos de ríos y lagos. En el periodo de sequía medieval dichos ríos o lagos estaban secos, lo que permitió que los árboles crecieran. Al terminar la sequía, no obstante, lagos y ríos fueron inundados y los árboles murieron.Una sequía prolongada habría tenido un fuerte impacto en la producción de alimentos y, según algunos especialistas, habría provocado el colapso de civilizaciones en el sureste de los Estados Unidos, lo mismo que de civilizaciones en Mesoamérica. En un artículo aparecido esta semana en el número inaugural de la revista en línea “Science Advances” –producida por la “American Association for the Advancement of Science” – publicado por un grupo de investigadores encabezado por Benjamin Cook del NASA Goddard Institute for Space Studies, se predice para la segunda mitad del presente siglo una sequía severa que afectará a la región de Norteamérica, incluyendo a nuestro país. La magnitud de esta sequía sería aun mayor que la del periodo medieval y no tendría parangón con ninguna otra desde esa época. Representaría, así, un cambio climático sin precedentes en los últimos mil años, con periodos de sequías severas que podrían durar décadas.Para hacer su predicción, Cook y colaboradores emplearon complejos modelos del clima de la Tierra. Hicieron, igualmente, uso de los datos del clima de nuestro planeta a lo largo de los últimos dos mil años. En este respecto, es posible saber acerca del clima en el pasado a partir del estudio de los anillos del tronco de un árbol, cada uno de los cuales se sabe a corresponde a un año de vida del mismo. Un año con clima favorable es revelado por un mayor espesor del anillo correspondiente, al mismo tiempo que un espesor más pequeño revela un año difícil.   La sequía por venir, a diferencia de la sequía medieval, no tendría causas naturales sino que sería  producto del calentamiento global que está experimentando nuestro planeta. Este calentamiento, a su vez, es producto de la emisión de gases de invernadero a la atmósfera por la quema sin control de combustibles fósiles, misma que estamos llevando a cabo de forma continua desde el inicio de la Revolución Industrial. Aunque Cook y colaboradores en su artículo sólo consideran de manera explícita al sureste de los Estados Unidos y a la región de las planicies centrales de este país al este de las Montañas Rocallosas, en la página de internet de la NASA es posible encontrar animaciones en donde se muestran las proyecciones de la sequía en Norteamérica a lo largo del presente siglo, según los datos del Goddard Space Flight Center. En dichas animaciones podemos ver que las predicciones de sequía para gran parte de nuestro país al final del siglo XXI –incluyendo al Estado de San Luís Potosí– son tan alarmantes como las correspondientes predicciones para las planicies centrales de los Estados Unidos.Podemos esperar entonces que, de ser correctos los cálculos de Cook y colaboradores, en las décadas por venir nuestro país sufrirá los efectos de una sequía que superaría en intensidad a todas aquellas ocurridas a lo largo de los últimos mil años. Esto incluso si hay una reducción en la emisión de gases de invernadero a la atmósfera a nivel global, la cual, si bien reduciría magnitud de la sequía, lo mismo que sus efectos adversos, no impediría que el presente siglo supere un récord que tiene cuando menos un milenio de existencia.",
    "Aunque no los podemos ver, sabemos que habitan en nuestro cuerpo una gran diversidad de microbios, en cantidades tan grandes que resultan sorprendentes. En efecto, según los especialistas, el número de células que de manera colectiva conforman estos microbios es de unos 100 millones de millones. Este número es tan enorme que escapa a nuestra comprensión; baste saber, no obstante, que es unas diez veces más grande que el número total de células que componen el cuerpo humano. Visto así, estamos en clara desventaja con respecto a nuestros huéspedes microscópicos, y resulta afortunado que podamos convivir con ellos de manera pacífica, e incluso que en algunos casos nos ayuden a mantener la salud. Sabemos, no obstante, que la convivencia no siempre es lo armoniosa que quisiéramos, y que puede suceder que algunos de los microbios que habitan en nuestro cuerpo, o que adquirimos por alguna vía, nos enfermen de manera más o menos grave.  Ciertamente, algunas veces nos enferman de manera muy grave, como sucedió en el curso de la pandemia de peste bubónica conocida como Muerte negra, que diezmó a la población de Europa en la primera mitad del siglo XIV. Se sabe que la Muerte negra fue producida por la bacteria “Yersinia pestis” que es trasmitida por pulgas de rata. Se sabe, igualmente, que fue primero introducida a Europa procedente de Asia Menor a través de un puerto en el sur de Italia. La Muerte negra fue una catástrofe en la que habría muerto entre el 30%  y el 60% de la población europea. A causa de la Muerte negra, la Europa del siglo XIV –que estaba a punto de salir de la Edad Media– no fue el lugar más agradable para vivir. Como no se tenía idea de qué la producía, la gente atribuía la peste bubónica a las más disímbolas causas, desde la calidad del aire, a  la influencia de los astros, o bien a las supuestas prácticas satánicas de judíos y brujas. El director de cine Ingmar Bergman en su película “El séptimo sello”  nos transporta a la Europa de esa oscura época. Si bien Bergman no tuvo la intención de dar una descripción fiel de dicha época –y, de hecho, hace coincidir a las cruzadas con la Muerte negra, eventos que no coincidieron en el tiempo–, si nos presenta un entorno en donde la muerte es una realidad cercana que todos enfrentan.      Afortunadamente, ahora sabemos cuál es la causa de la peste bubónica y no se espera que una catástrofe como la Muerte negra pueda volver a ocurrir. La “Yersinia pestis”, sin embargo, no ha desaparecido de ninguna manera. De hecho, en un artículo aparecido esta semana en la revista “Cell Systems”, se reporta haberla encontrado en un lugar que a primera vista nos resulta sorprendente: las estaciones de tren Metro de Nueva York. El artículo fue publicado por un grupo numeroso de expertos de universidades y centros de investigación de los Estados Unidos y de Irlanda, encabezados por Christopher Mason del Weill Cornell Medical College en la ciudad de Nueva York.      En dicho artículo se reportan los resultados de una investigación llevada a cabo con 1,457 muestras de material genético recogidas en 466 estaciones del metro neoyorquino con el objeto de estudiar su ADN. Dichas muestras fueron colectadas frotando hisopos de nailon sobre pasamanos, torniquetes, bancas, ventanillas expendedoras de boletos y otras superficies en las estaciones del tren, con las cuales hubieran tenido contacto los 5.5 millones de personas que diariamente las transitan. Igualmente, se recogieron muestras de bancas, pasamanos, postes y puertas en el interior de los carros del metro.En un resultado sorprendente, los investigadores encontraron que aproximadamente la mitad del ADN determinado no pudo ser identificado y correspondía a organismos desconocidos hasta antes de la investigación. Por otro lado, se encontró que un 47% del ADN identificado es de organismos bacterianos, y que si bien un 57% de las bacterias encontradas no han sido asociadas con enfermedades en humanos, un 31% de las mismas son bacterias oportunistas que pueden representar un riesgo de salud. El 12% de las bacterias restantes, entre las que se incluyen algunas resistentes a los antibióticos, están asociadas a enfermedades humanas.En este último grupo, Mason y colaboradores encontraron fragmentos de ADN de “Yersinia pestis”, así como fragmentos del bacilo del ántrax, ambos, sin embargo, en bajos niveles. Ya que no ha habido un caso de peste bubónica en Nueva York en los dos últimos años, los investigadores consideran que estas bacterias no están asociadas a enfermedades y son simplemente dos habitantes más de la población bacteriana del Metro de Nueva York. Apuntan, no obstante, que esto tiene que ser confirmado.De un modo u otro, aun si la bacteria de la peste bubónica vivera  entre nosotros –como sería el caso si hemos de generalizar los hallazgos del metro neoyorkino– y tuviera el potencial de causar una pandemia como la que devastó a la Europa Medieval, tendríamos esperanzas fundadas en que dicha pandemia no se haría una realidad en el futuro. La peste bubónica del Metro de Nueva York, no obstante, tiene la virtud de transportarnos cientos de años hacia el pasado y hacernos presente una época –recreada de manera magistral por Bergman– en la que la muerte era cosa de todos los días.",
    "Descubrir y rescatar los restos fósiles de un gran dinosaurio, de esos que puede uno admirar en los grandes museos de historia natural del mundo, no es algo simple y, ciertamente, constituye una tarea para expertos. Como sabemos, los dinosaurios vivieron hace muchísimo tiempo –de 65 a 230 millones de años– y si los restos de un espécimen particular han podido llegar hasta nosotros es porque, por casualidad, fueron enterrados rápidamente después de su muerte y se salvaron de ser destruidos por otros animales o por la acción de los elementos. Una vez bajo tierra, si bien los tejidos blandos comúnmente desaparecen con el tiempo, los huesos pueden preservarse por medio del proceso de fosilización, en cuyo caso viajan a través del tiempo embebidos en una matriz de roca. Durante la fosilización, los minerales que forman el hueso son reemplazados por otros minerales sin alterar su forma. El fósil es, de este modo, una especie de copia pirata del hueso, que refleja de manera de manera fiel, sin embargo, su morfología original. No en todos los lugares de la superficie de la tierra es posible encontrar restos de dinosaurios. Para esto es necesario que la roca expuesta tenga la antigüedad correcta; es decir, que tenga la  antigüedad  que corresponde al tiempo en el que los dinosaurios habitaron la Tierra. Es conocido, por ejemplo que la región del oeste de los Estados Unidos conocida como Formación Morrison, que se extiende desde el estado de Nuevo México hasta la frontera con Canadá, es rica es fósiles de dinosaurios que vivieron hace unos 150 millones de años.  Para descubrir fósiles de dinosaurios los paleontólogos deben buscar entonces en los lugares adecuados haciendo uso de su experiencia para encontrar signos de su posible presencia bajo la tierra. Una vez localizados, los huesos fósiles deben ser liberados de manera cuidadosa de la matriz de roca que los aprisiona y ensamblados, para su exhibición si fuera el caso, del modo en que los expertos piensan que el dinosaurio lució en vida. La anterior, llevado a cabo por paleontólogos profesionales trabajando para un museo, sería una vía para que dicho museo se haga de dinosaurios fósiles, ya sea para exhibición o para estudios científicos.  Hay vías adicionales, no obstante, como lo demuestra el fósil de estegosaurio  que ha estado en exhibición desde el pasado mes de diciembre pasado en el Museo de Historia Natural de Londres. Un artículo publicado la semana pasada en el periódico británico The Guardian por el profesor Paul Barrett del Museo de Historia Natural de Londres  nos cuenta la historia. De manera específica, la historia del derrotero que siguió dicho estegosaurio  hasta llegar al museo.De acuerdo con Barrett, durante la visita que en 2012 él y otros colegas hicieron a la Feria de Rocas y Minerales de Tucson –un evento anual que reúne a buscadores y traficantes profesionales de fósiles– dieron con el molde de tamaño real del cráneo de un estegosaurio y, para su sorpresa, se enteraron de que el fósil original, que incluía el esqueleto completo, estaba a la venta. Averiguando más, resultó que dicho fósil constituía el mejor esqueleto de un estegosaurio jamás encontrado. Dado lo interesante del fósil descubierto de manera tan sorprendente, Barrett y colaboradores decidieron proponer al Museo de Historia Natural de Londres adquirirlo para su colección y exhibición pública, propuesta que fue aceptada sin reparos. Los fondos necesarios fueron aportados por un donante principal y varios donantes menores, todos privados, y no se reveló el monto de la transacción. El fósil arribó al museo en diciembre de 2013, siendo bautizado como Sofía, en honor a la hija del donante principal –por más que sea imposible saber el sexo del dinosaurio.Su arribo fue mantenido en secreto por un año, aparentemente para hacer más espectacular su aparición pública, pero también para dar tiempo a que los científicos del museo lo estudiaran sin mayores perturbaciones.Los dinosaurios son animales que cautivan fuertemente nuestra atención, entre otras cosas porque si bien vivieron hace muchísimos millones de años, las huellas de su presencia en la Tierra –lo mismo que las de su extinción masiva– han llegado hasta nosotros de manera sorprendente. Nos atraen también porque algunos dinosaurios tuvieron enormes proporciones, mientras que otros fueron depredadores de una gran ferocidad. Los estegosaurios, en particular, fueron dinosaurios muy vistosos –como lo podemos constatar la página electrónica del Museo Smithsonian o en la Wikipedia, por ejemplo–, con una doble fila de placas a lo largo del lomo y agujas en el extremo de la cola que usaban posiblemente como defensa. En cuanto a la historia de Sofía y su arribo al Museo de Historia Natural de Londres, además del atractivo visual propio de los estegosaurios, llama la atención que traficantes de fósiles hayan comercializado un esqueleto de dinosaurio  que, según Barrett, tiene un alto valor científico y es el mejor descubierto hasta ahora.",
    "El número de la revista “Science” de esta semana publica una sección especial con una serie de artículos en los que se describen los primeros resultados alcanzados por los diferentes grupos de científicos que estudian al cometa Churyumov-Gerasimenko. Como en su momento fue dado a conocer por la Agencia Espacial Europea, la sonda Rosetta alcanzó en el mes de agosto pasado al cometa Churyumov-Gerasimenko en su órbita alrededor del Sol, entrando a su vez en órbita alrededor del cometa. Rosetta acompañará al Churyumov-Gerasimenko en su viaje hacia el Sol estudiando los cambios que sufre y se “enciende” en la medida en que la radiación solar que recibe se hace más intensa. El máximo acercamiento del cometa con el Sol ocurrirá en el mes de agosto del presente año. La Agencia Espacial Europea había sido objeto de críticas por su retraso en dar a conocer con prontitud los primeros resultados alcanzados con Rosetta. Este retraso fue motivado por la intención de los diferentes grupos de investigación que participan en el proyecto de mantener la prioridad sobre los resultados alcanzados y darlos a conocer solamente mediante artículos de investigación que les aseguren esta prioridad. Los artículos publicados esta semana en “Science” cumplen con esta función, de modo que los resultados del estudio del cometa Churyumov-Gerasimenko han alcanzado finalmente la luz pública, después de algunos meses. Dichos resultados son variados y comprenden desde el estudio de la morfología del cometa hasta la determinación de la composición química de su superficie –la  cual se encontró es rica en compuestos orgánicos–, lo mismo que la de los gases que deja escapar hacia el espacio. Se encontró, igualmente, que una nube de cien mil fragmentos con dimensiones mayores a los cinco centímetros orbita al cometa, los cuales, de acuerdo con los expertos, posiblemente resultaron de su último acercamiento al Sol.El cometa Churyumov-Gerasimenko tiene una forma peculiar con dos lóbulos de diferente tamaño unidos por un cuello. Los expertos ahora saben que el lóbulo pequeño mide 2.6x2.3x1.8 kilómetros y el lóbulo más grande 4.2x3.3x1.8 kilómetros, y que la masa del cometa es de unas diez mil millones de toneladas. Tiene, además, una densidad similar a la del corcho o de la madera, por lo que, de ser el caso, flotaría en agua. Los especialistas no tienen seguridad de cómo el cometa adquirió su forma. Una posibilidad es que la misma hubiera resultado de la colisión de dos cuerpos celestes. Una explicación alternativa es que el cometa hubiera llegado a su forma actual por la interacción con la radiación solar durante sus acercamientos al Sol.  Con relación a este punto, Rosetta ha revelado que la emisión de gases al espacio es particularmente activa en la región del cuello del cometa. Para los no especialistas, no obstante, quizá lo más atractivo sean las fotografías tomadas a una  distancia relativamente corta por las cámaras de alta resolución que Rosetta lleva a bordo y que muestran al cometa con asombroso detalle. Una de estas fotografías, tomada a una distancia de 8 kilómetros, aparece en la portada de la revista “Science” de esta semana y muestra un acantilado y un campo de arena con una resolución de 15 centímetros por pixel. Muestra, igualmente, una superficie oscura, notablemente más oscura que la superficie de la Tierra, o aun que la de la Luna.De acuerdo con la página electrónica de la Agencia Espacial Europea, se ha fotografiado con detalle aproximadamente el 70% de la superficie del cometa, la cual muestra una estructura irregular y variada. Se han identificado cinco tipos de terreno: campos de polvo, regiones de material quebradizo con pozos y estructuras circulares, depresiones a gran escala, áreas planas y áreas cubiertas de material rocoso. Las fotografías muestran también lo que podrían ser líneas de fisura en el cuello del cometa, lo que anticiparía su fractura en un futuro no determinado.Mucho queda por estudiar y aprender acerca del cometa Churyumov-Gerasimenko durante su tránsito hacia el Sol en los próximos meses y a lo largo de su posterior alejamiento siguiendo su órbita elíptica. De especial interés es la posibilidad de que la sonda Philae, que fue enviada hacia la superficie el cometa en noviembre pasado para estudiarlo a la distancia más corta posible, pueda retornar a la vida. Como se recordará, la sonda Philae tuvo un accidentado aterrizaje en el cometa que la dejó en posición difícil para recibir la luz del sol que necesita para recargar sus baterías y como resultado entró en un estado de hibernación. La esperanza es que, al acercarse el cometa al Sol, Philae pueda recibir suficiente radiación solar para despertar de su letargo, no obstante su posición desventajosa.Recibir noticias e imágenes de alta resolución de un mundo lejano, extraño y fascinante como lo es el cometa Churyumov-Gerasimenko es siempre gratificante. Aun si los responsables de difundir estas imágenes y noticias nos hacen esperar por algunos meses.",
    "En septiembre de 1991, una pareja de turistas alemanes descubrió de manera circunstancial en la región de los Alpes, en la frontera entre Austria e Italia, un cadáver humano semienterrado en hielo. Pensaron inicialmente que pertenecía a una persona fallecida recientemente. Lejos de esto y como después supimos, los restos humanos correspondían a los de un europeo de la Edad del Cobre, fallecido hace más de 5,000 años, que ha llegado a ser conocido como Otzi o el “Hombre del hielo”.  El cadáver de Otzi pudo ser conservado en buen estado debido a que después de su fallecimiento fue rápidamente cubierto de nieve. Así, enterrado en hielo, pudo conservarse en buen estado por miles de años hasta que, por la reciente pérdida de hielo alpino debido al calentamiento global, re-emergió a la superficie. La fusión de los hielos por el calentamiento global está descubriendo objetos del pasado lejano y en este sentido Otzi no es caso único. En el sitio de noticias científicas “Science Nordic”, por ejemplo, esta semana se publicó un artículo acerca del descubrimiento de 400 objetos de gran antigüedad en el condado de Opland, en la zona montañosa del sur de Noruega. Dichos objetos estuvieron enterrados en hielo por largo tiempo y emergieron a la superficie por la fusión de los hielos del glaciar Lendbreen. Entre los objetos encontrados se incluyen bastones para escalar y un cráneo de caballo del tiempo de los vikingos, lo mismo que un mango de flecha de la Edad de Piedra.  La disminución de volumen de hielo en los glaciares, que está llevando a la superficie  objetos enterrados por miles de años, no es sino una de las múltiples manifestaciones del calentamiento que está experimentando la superficie de nuestro planeta; manifestaciones que incluyen, entre otras, la disminución del volumen del hielo ártico y el incremento en el nivel de los océanos. Hay que señalar que desde 1880 la temperatura del planeta se ha elevado por casi un grado centígrado.Concurrentemente con el calentamiento global, la concentración de gases de invernadero en la atmósfera se ha incrementado por la quema de combustibles fósiles y por la destrucción de las superficies boscosas del planeta, y es a este incremento que los expertos atribuyen la elevación de la temperatura de la superficie de la Tierra.  Si bien una mayoría de expertos apoyan esta tesis, hay quién la pone en duda. Entre otros argumentos, los escépticos del cambio climático señalan que, si bien en la segunda mitad del siglo XX la temperatura del planeta se elevó con los años de manera sostenida, a lo largo de los últimos quince años la tasa de incremento de esta temperatura se ha moderado e incluso detenido. De este modo, puesto que la concentración de gases de invernadero en la atmósfera sí ha seguido aumentando, no sería esta concentración la causa del calentamiento global que más bien obedecería a causas naturales.Un hecho que no apoya los argumentos de los escépticos del cambio climático es que, de acuerdo a mediciones llevadas a cabo tanto por la NASA como por la “National Oceanic and Atmospheric Adminstration” de los Estados Unidos, 2014 ha sido, en promedio, el año más caliente desde 1880, año a partir del cual se cuenta con estadísticas globales de la  temperatura de la superficie terrestre. De manera específica, 2014 fue 0.7 grados centígrados más caliente que el promedio observado en los años 1951-1980. Hay que notar, además, que 2014 no fue afectado por el fenómeno de El Niño que, de manera oscilatoria, provoca incrementos en la temperatura promedio de la superficie de la Tierra.  Como puede observarse en la correspondiente página electrónica de la NASA, el incremento de temperatura no fue el mismo para toda la superficie del planeta y, de hecho, en algunas regiones se observaron temperaturas por abajo del promedio 1951-1980. Así, por ejemplo, los mayores incrementos en temperatura en el pasado mes de diciembre se dieron en Alaska y en la región central de Siberia, mientras que en el Atlántico norte las temperaturas fueron en realidad inferiores al promedio 1951-1980. Las regiones de la superficie del planeta en donde se dio un incremento de temperatura, no obstante, tuvieron una mayor contribución a la temperatura global que aquellas en donde se observaron temperaturas menores al promedio de largo plazo.Ciertamente, una mayoría de expertos considera que el cambio climático es un fenómeno real que está directamente asociado al incremento de la concentración de gases de invernadero en la atmósfera; esto último debido al consumo de combustibles fósiles en actividades industriales y de transporte, lo mismo que a la tala de árboles. A pesar de esto, dados los intereses que están en juego, no es claro que en un futuro inmediato puedan reducirse la emisión de gases de invernadero a la atmósfera y detener o paliar el calentamiento global.Si bien lo anterior resultará en beneficio para los arqueólogos que estarán de plácemes descubriendo más y más objetos antiguos en la medida en que desaparezcan los hielos de los glaciares, poco consuelo nos quedará al resto de los mortales –presentes y, sobre todo, futuros.",
    "¿Cómo vivirán nuestros descendientes dentro de 100 años? ¿Cómo lo harán en el año 3000 de nuestra era? ¿Vivirán en ciudades en el fondo del océano protegidas por un domo de cristal? ¿Habrán logrado superar la gravedad terrestre y habitarán en otros planetas? O bien, en un escenario propio de los tiempos de la Guerra Fría, como resultado de una guerra nuclear de proporciones globales ¿Habrá colapsado la civilización y nuestros descendientes vivirán en un estado primitivo de desarrollo social?Por alguna razón no determinada, a muchos de nosotros nos gusta soltar la imaginación y hacernos preguntas de este tipo. Preguntas que son, sin embargo, ociosas, pues no hay manera de responderlas con un grado de precisión razonable, dado que entre nuestras capacidades no está la de predecir el futuro. De este modo, las proyecciones y visiones del futuro quedan, en el mejor de los casos, en el ámbito de la ciencia ficción.Nuestro interés en la relación presente-futuro se da también en otro sentido. De manera específica, con las llamadas cápsulas del tiempo en las que se guardan objetos característicos de una época. La pretensión es que, en un futuro más o menos lejano, quienes abran la cápsula reciban un mensaje directo de una época pasada. El pasado mes de diciembre los medios de comunicación nos hicieron saber del hallazgo de una cápsula del tiempo oculta en una piedra de las paredes del capitolio de la ciudad de Boston. La cápsula –una caja de aleación de cobre de unos veinte centímetros de largo, 15 de ancho y 4 de alto– fue enterrada en 1795 por Paul Revere y Samuel Adams, dos de los héroes de la independencia norteamericana. Fue descubierta en el verano de 2014 por mediciones de radar durante un proyecto de mantenimiento del edificio por una fuga de agua. Con 220 años de antigüedad, la cápsula del tiempo enterrada en las paredes del capitolio de Boston es la de mayor antigüedad que se ha encontrado en los Estados Unidos. No ha estado oculta, sin embargo, por tanto tiempo, pues fue desenterrada en 1855 durante una operación de mantenimiento del edificio. En esa ocasión, después de limpiar los objetos que contenía, la cápsula fue nuevamente enterrada en el mismo lugar, añadiendo algunos objetos de la época.Después de trabajar por todo un día, Pam Hatchsfield del Museo de Bellas Artes de Boston, logró liberar la caja de la piedra que la aprisionaba. Con el objeto de explorar su contenido, la caja fue sujeta a análisis con rayos X. La apertura de la cápsula del tiempo  fue llevada a cabo por Hatchsfield de manera teatral el pasado día 6 de enero durante una rueda de prensa. Entre otros, se encontró que contenía los siguientes objetos: monedas de oro y plata fechadas entre 1652 y 1855, una placa  de plata que se piensa fue grabada por Paul Revere, una medalla de cobre con la imagen de George Washington, algunos periódicos y tarjetas de presentación.  Al igual que en 1855, en esta ocasión se piensa regresar la cápsula a su lugar en la pared del capitolio. No se ha decidido, sin embargo, si a su contenido se le añadirán objetos contemporáneos. La colocación de cápsulas de tiempo es una práctica muy extendida, Existe incluso una  Sociedad Internacional de Cápsulas del Tiempo, con sede en la Universidad Oglethorpe, Atlanta, Estados Unidos, que, entre otras cosas, pretende llevar un inventario de cápsulas en el mundo con el objeto de mantener visible su rastro a lo largo de los años. De acuerdo con esta sociedad hay unas 10,000 cápsulas de tiempo repartidas en el planeta.   Las cápsulas de tiempo pretenden enviar mensajes hacia el futuro desde una determinada época. El mensaje posiblemente esté contenido, tanto en los objetos dentro de la cápsula, como en la intención manifiesta de aquellos que enviaron dichos objetos hacia el futuro. El mensaje enviado, no obstante, depende de la colección de objetos contenidos en la cápsula y hay quien arguye que estos objetos no serán por lo común de gran utilidad para los arqueólogos del futuro.En particular, en el caso de la cápsula del tiempo de capitolio de Boston habría que esperar la opinión de los expertos en cuanto a cómo puede su descubrimiento contribuir a entender la historia de los Estados Unidos doscientos años atrás –más allá de lo que le ha servido al Museo de Bellas Artes de Boston para hacerse algo de publicidad.   De un modo o de otro, lo que sí no se puede negar es que, encontrarse con un mensaje que nos es enviado –con plena intención– directamente desde el pasado, es algo que no deja de ser fascinante.",
    "De acuerdo con la última encuesta de Gallup de diciembre pasado, 13 por ciento de los norteamericanos considera que el mayor problema de los Estados Unidos es el racismo y las relaciones entre razas. Este porcentaje es similar al de aquellos norteamericanos que colocan a la economía como el principal problema que enfrentan, y que sólo es superado por el 15% de quienes consideran que su principal problema se encuadra en la categoría de “Gobierno”. La preocupación de los norteamericanos por el racismo creció abruptamente –en noviembre pasado solamente el 1% de los encuestados lo destacaban en primer lugar– a raíz de los recientes disturbios ocasionados por la decisión de no procesar judicialmente a los policías blancos de dieron muerte a los afroamericanos Michael Brown y Eric Garner, en Misuri y Nueva York, de manera respectiva.   Como sabemos, los Estados Unidos es un país de inmigrantes, con una historia relativamente reciente, que tiene una población mayoritariamente de origen europeo pero que comprende también minorías de tamaño significativo. Entre éstas se encuentran la de los afroamericanos –resultado del tráfico de esclavos– y la de los latinos, que comprenden el 13% y el 17% de la población total del país, de manera respectiva.Si bien el racismo es un fenómeno real en los Estados Unidos –lo mismo que en otros países, incluido el nuestro–, el concepto de “raza” no puede definirse de manera precisa desde un punto de vista científico, dadas la mezclas inter-raciales que se han dado a lo largo de la historia. Un artículo publicado en el número de enero del presente año en la revista “The American Journal of Human Genetics”, arroja luz en este respecto. Dicho artículo es encabezado por Katarzina Bryc, adscrita a la “Harvard Medical School”  y a la compañía 23andMe, Mountain View, California, especializada en estudios genéticos. En dicho artículo se reportan los resultados de un estudio llevado a cabo para averiguar la herencia genética, mediante estudios de ADN, de un grupo de más de 160,000 norteamericanos. El estudio se llevó a cabo empleando el banco de datos de la compañía 23andMe que almacena datos genéticos de cientos de miles de personas. Alrededor de 150,000 de los participantes se autodefinieron como europeo-americanos, 5,000 como afroamericanos y 9,000 como latinos. Los resultados obtenidos muestran que el 3.5% de aquellos que se autodefinen como europeo-americanos tienen cuando menos un 1% de genes africanos. De la misma manera, el genoma promedio afroamericano resulta ser 73% africano y alrededor de 24% europeo.  En lo que se refiere al genoma promedio latino, éste es americano nativo en una proporción del 18%, europeo –fundamentalmente de la península ibérica– en un 65% y africano en 6%.Las composiciones genómicas anteriores, además, dependen del estado de la Unión Americana que se considere. Así, en Carolina del Sur un 12% de los europeo-americanos tienen un genoma que es africano cuando menos en un 1%. De la misma manera, en los estados del sur, limítrofes con México, el genoma latino tiene una mayor proporción nativo americana.De acuerdo con los resultados del estudio, existe también una dependencia de género en la composición genómica. Así, la probabilidad de que un europeo-americano tenga un ancestro africano hombre sería diez veces mayor a la probabilidad de que tenga un ancestro africano mujer. De  la misma manera, un afroamericano podría tener cuatro veces más ancestros indígenas americanos hombres que ancestros indígenas americanos mujeres.Las fronteras raciales en los Estados Unidos son, de este modo, imprecisas. Tenemos así que la población afroamericana es, en promedio, una mezcla europea-africana. De la misma manera, si bien de acuerdo al estudio de Bryc y colaboradores el genoma europeo-americano es, en promedio, europeo en un 98.6%, el 3.5% de la población europea-americana que tiene ancestros africanos –muchos millones de personas– se encuentra en una franja fronteriza entre dos categorías raciales.     En estas condiciones podríamos quizá esperar que los problemas raciales en los Estados Unidos tuviesen una virulencia sólo relativa. Las encuestas Gallup hasta la última realizada el pasado mes de diciembre así parecían indicarlo. En efecto, según estas encuestas, después de la década de los años sesenta, cuando la mitad de la población norteamericana llegó a considerar que los aspectos raciales eran el problema mayor que enfrentaban, la preocupación por este tema decayó de manera drástica –con la excepción del año 1992 con el caso de Rodney King que provocó, al igual que el pasado año, grandes manifestaciones públicas.De acuerdo a los acontecimientos del pasado año, sin embargo, no parece que la discriminación racial sea algo que pueda racionalizarse con argumentos científicos y desecharse por improcedente. Aunque esto último no es algo que deba sorprendernos como mexicanos, dado el estado de cosas en nuestro país, donde, ciertamente, existe discriminación racial por más que la población de nuestro país sea mestiza con todos los grados de mezcla imaginables.",
    "No han pasado todavía doscientos años desde que tuvimos acceso por vez primera a la luz eléctrica. Antes de eso, para iluminar las noches recurrimos a fogatas o antorchas y, con el transcurrir del tiempo y evolucionar la tecnología, a lámparas de aceite o de gas. Aun en este último caso, sin embargo, los niveles de iluminación que podían alcanzarse eran relativamente bajos, en comparación con los que es posible obtener con la luz eléctrica.De este modo, dado que nuestra especie evolucionó durante cientos de miles de años sin luz artificial, cabe preguntarse cómo nos afecta el hecho de que ahora estemos rodeados en las horas nocturnas de todo tipo de luz eléctrica que, ciertamente, acorta la noche cuando se supone que debemos dormir.Una tesis sorprendente en este sentido es la de Roger Ekrich, profesor de historia en el Instituto Politécnico de Virginia en los Estados Unidos, quien afirma que los últimos doscientos años cambiaron radicalmente nuestros hábitos de sueño. Actualmente se asume que debemos dormir ocho horas diarias de manera continua. De acuerdo con Ekrich, en contraste,  antes de la Revolución Industrial lo usual era dividir el sueño en dos periodos. Las personas iban a la cama un par de horas después de anochecer y dormían por unas cuatro horas. A esto seguía un periodo de vigilia de una a dos horas y un segundo periodo de sueño de cuatro horas.Ekrich llegó a esta concusión mediante la consulta de numerosos documentos de todo tipo en los que se hace referencia a un primer y un segundo periodos de sueño. Durante la vigilia entre estos dos periodos, sin bien la mayor parte de la gente permanecía en cama, algunos la aprovechaban para ir al baño o bien para meditar u orar, e incluso para visitar a los vecinos. Se afirmaba también que el periodo de vigilia era el más propicio para que las parejas concibieran un hijo.Según Ekrich, las referencias al sueño segmentado se vuelven más escasas hacia el final del siglo XVII, presumiblemente porque empezó a haber más vida nocturna, sobre todo entre aquellos con los recursos económicos suficientes para adquirir medios de iluminación. Con la aparición de la luz eléctrica la vida nocturna se extendió hacia todos los segmentos de población, y con esto empezaron a cambiar nuestros hábitos de sueño hasta llegar a la época actual, cuando dormir diariamente ocho horas continuas se considera normal. ¿Está en nuestra naturaleza dormir de manera segmentada en dos periodos separados por una o dos horas de vigilia en la madrugada como aventura Ekrich, de modo que dormir ocho horas continuas es una práctica artificialmente adquirida? Aunque esto último no es aceptado de manera generalizada por los especialistas, existe evidencia científica que lo apoya.  En 1992, el siquiatra Thomas Wehr llevó a cabo un experimento con un grupo de voluntarios a los que sujetó a periodos de 14 horas de oscuridad durante un mes, permitiéndoles dormir cuanto quisieran. Wehr encontró que, después de una fase de transición, los voluntarios terminaron durmiendo ocho diarias divididas en dos periodos, separados por una o dos horas de vigilia.Las aplicaciones de la luz artificial, por otro lado, no se limitan a la iluminación espacial sino que se extienden, por ejemplo, a las pantallas de las tabletas electrónicas, cuyo uso está, igualmente, cada vez más extendido. En este último caso, cabe preguntarse sobre los efectos que estos dispositivos electrónicos tienen sobre nuestra salud y en este respecto, un artículo publicado la presente semana en la revista “Proceedings of the National Academy of Sciences” describe los resultados de un estudio llevado a cabo para determinar el efecto que el uso de una tableta electrónica antes de dormir tiene sobre la calidad del sueño.La investigación fue encabezada por Anne-Marie Chang del Brigham Women´s Hospital en Boston, Estados Unidos, y durante la misma se estudió el efecto que sobre el sueño tiene leer, por cuatro horas antes de dormir, un libro electrónico en una iPad en comparación con uno impreso en papel. Los investigadores encontraron que leer un libro electrónico reduce la somnolencia antes de dormir y la incrementa al día siguiente; esto en comparación con el libro impreso. Encontraron, igualmente, que leer en un iPad reduce los niveles de melatonina –una hormona que juega un papel en la inducción del sueño– y retrasa el ritmo circadiano por más de una hora.  La luz artificial tal como la conocemos se inició con la lámpara incandescente de Edison en el último cuarto del siglo XIX y ha ido evolucionando de manera continua desde entonces hasta llegar a las actuales lámparas LED de luz blanca, cuyo uso para iluminar tanto espacios interiores como exteriores está cada vez más extendido. Una característica de los dispositivos LED es su alta eficiencia, lo cual presumiblemente incrementará los niveles de iluminación nocturna y con esto los posibles riesgos de salud.       De confirmarse los efectos adversos de la iluminación artificial sobre nuestros hábitos de sueño y potencialmente sobre nuestra salud –se le ha asociado, aunque de manera no conclusiva, a un incremento en el riesgo de desarrollar cáncer de pecho, lo mismo que otras enfermedades– habremos comprobado que en cuanto a la luz artificial se refiere, ni tanto que queme al santo ni tanto que no lo alumbre.",
    "Como sabemos, hace unos 66 millones de años los dinosaurios, que habían florecido por más de cien millones de años, desaparecieron de la faz de la Tierra de manera más o menos repentina –al menos en términos geológicos–. Al mismo tiempo, los mamíferos –nosotros a la larga incluidos–  progresaron y se diversificaron en las decenas de millones de años que siguieron hasta ocupar una posición dominante en la Tierra, ya sin la competencia de los dinosaurios. Así, lo que fue una catástrofe para estos últimos habría sido una bendición para los mamíferos.La causa más aceptada para explicar la extinción de los dinosaurios es la caída de un meteorito en un punto cercano a la costa de Yucatán, próximo al poblado de Chicxulub. Dicho meteorito, que habría tenido un diámetro de unos diez kilómetros, generó al impacto con la superficie terrestre un cráter, aun hoy visible, de 300 kilómetros de diámetro, lo mismo que una nube de polvo que se expandió por  la atmósfera a escala global. Siguiendo con la explicación, la nube de polvo permaneció suspendida en la atmósfera por largo tiempo, bloqueando la luz del sol e impidiendo la fotosíntesis. Habría sobrevenido así una escasez de alimentos que puso fin a la existencia de los dinosaurios en la Tierra.A pesar de lo llamativo de la hipótesis del meteorito, no todo el mundo está de acuerdo con la misma, pues si bien los especialistas cuentan con herramientas sofisticadas para investigar el pasado, determinar con precisión que es lo que sucedió hace decenas de millones de años no es de ninguna manera empresa sencilla. Así, hay quien opina que la verdadera causa de la extinción de los dinosaurios no fue la caída de un meteorito, sino las gigantescas erupciones volcánicas ocurridas hace 66 millones de años en la Meseta del Deccan, en el suroeste de la India.Como resultado de dichas erupciones se vertieron más de un millón de kilómetros cúbicos de lava,  que cubrieron una superficie equivalente a tres cuartas partes del territorio de nuestro país. Las erupciones arrojaron a la atmósfera cantidades masivas de dióxido de carbono y dióxido de azufre que habrían alterado gravemente las condiciones climáticas del planeta y llevado, o al menos contribuido de manera decisiva, a la extinción de especies. Esta última hipótesis ha sido defendida a lo largo de las últimas décadas por la paleontóloga Gerta Keller de la Universidad de Princeton en los  Estados Unidos, en contra de una opinión mayoritaria de los expertos. Un artículo publicado esta semana en la revista “Science” por un grupo de investigadores de los Estados Unidos, Suiza y la India, en el que participa Keller, ha dado impulso adicional a esta hipótesis. En dicho artículo se reportan los resultados de una investigación llevada a cabo para datar con una mayor precisión la ocurrencia de las erupciones del Deccan y establecer su conexión con la extinción de los dinosaurios. Los investigadores encuentran que dichas erupciones iniciaron 250,000 años antes que se produjera dicha extinción y que se prolongaron por 750,000 años. Concluyen que estos resultados apoyan la hipótesis que las erupciones constituyeron un factor decisivo que condujo a la extinción masiva de especies; quizá provocando un severo disturbio climático que dio inicio al proceso de extinción y que preparó el golpe final definitivo propinado por el meteorito de Chicxulub.  Los resultados de Keller y colaboradores revitalizan la discusión entre aquellos que atribuyen la extinción masiva de especies ocurrida hace 66 millones de años a un evento puntual –la caída de un meteorito–, y aquellos que hacen responsable de dicha extinción a erupciones volcánicas acontecidas a lo largo de cientos de miles de años. En el primer caso, ésta habría ocurrido en un tiempo corto en términos geológicos, mientras que en el segundo caso podría haber sido gradual.  En un futuro cercano con seguridad seremos testigos de opiniones a favor o en contra de cada una de las hipótesis contrarias, las cuales deberán ser respaldadas por nuevos datos. En particular, Keller y colaboradores han dado a conocer que llevarán a cabo mediciones adicionales con el objeto de datar con mayor precisión la ocurrencia de las erupciones del Deccan, lo que en su visión ayudará a clarificar el papel que jugaron en la extinción de los dinosaurios y en la irrupción de los mamíferos en la Tierra que le siguió.Por otro lado, hay que señalar que tal parece que, después de todo, el meteorito de Chicxulub –o las erupciones del Deccan, en su caso– no fue con los mamíferos lo amigable que se pensaba. Esto, al menos de acuerdo con un artículo publicado el pasado 17 de diciembre en la revista “ZooKeys” por investigadores de los Estados Unidos y el Reino Unido, según la cual hubo mamíferos que casi se extinguieron junto con los dinosaurios. Este es el caso de los mamíferos de los cuales descienden los actuales marsupiales, que nunca recuperaron la biodiversidad que tenían en el momento de la extinción.Con todo lo anterior, podríamos quizá concluir que nuestra especie bien pudo haber llegado al punto en que se encuentra por una intervención más allá de este mundo: por la fuerza de un meteorito literalmente venido del cielo. Aunque cabe también suponer que los resultados, por tantito, podrían haber sido muy diferentes.",
    "Uno de los problemas que aqueja al mundo en la actualidad es el de la contaminación atmosférica por la quema de combustibles fósiles. Al día respiramos decenas de miles de veces y en cada inspiración-espiración entra y sale de nuestros pulmones aproximadamente medio litro de aire. Con miles o decenas de miles de litros de aire circulando diariamente por nuestro cuerpo, los contaminantes químicos y las partículas suspendidas en la atmósfera son, ni quien lo dude, de primera importancia para nuestra salud. La contaminación atmosférica no es sólo relevante para los seres y también es un factor que determina la “salud” de algunos objetos inanimados. Este es el caso de uno de los monumentos más famosos del mundo, el Taj Mahal, localizado en la ciudad de Arga en el norte de la India, que está sufriendo por la contaminación del aire a su alrededor. Como sabemos, el Taj Mahal es un complejo de edificios y jardines construidos en el siglo XVII durante el Imperio mogol. Por su arquitectura, el Taj Mahal reviste una gran importancia, a tal grado que es reconocido por la UNESCO como Patrimonio de la humanidad. El edificio principal –y más identificable– del complejo es un mausoleo de mármol blanco con una cúpula acebollada de 35 metros de altura. En ese mausoleo se encuentran tanto los restos del emperador mogol Sha Jahan como los de su esposa favorita, en honor de la cual el emperador ordenó la construcción del monumento. Se tuvo la primera indicación de que había problemas con el Taj Mahal en la década de los años setenta cuando se observó que el mármol blanco del complejo había adquirido un color parduzco. El color original pudo ser restaurado mediante un tratamiento –equivalente a la aplicación de una mascarilla facial– en el cual la superficie del mármol es recubierta con una capa de barro que es removida una vez seca y finalmente lavada con agua. Para mantener el color original, sin embargo, es necesario aplicar este tratamiento de manera periódica con intervalos de algunos añosEl cambio de color del mármol del monumento se atribuyó desde un inicio a la contaminación del aire en las inmediaciones de monumento, y en intento de resolver el problema se restringió la circulación de vehículos dentro de un perímetro de un kilómetro alrededor del Taj Mahal. Se limitaron, así mismo, las emisiones contaminantes industriales en la ciudad de Agra. No se tuvo éxito, sin embargo.Para esto último es necesario primeramente encontrar la causa específica del problema. Si bien no han faltado hipótesis sobre la identidad de dichas causas –la reacción química de la superficie del mármol con el dióxido de azufre en el aire, por ejemplo– solo hasta fechas recientes fue que se llevó a cabo un estudio sistemático para determinarlas con precisión. Este estudio fue realizado por un grupo de investigadores de universidades en los Estados Unidos y en la India, encabezados por Michael Bergin del Instituto de Tecnología de Georgia. El estudio fue publicado en línea la semana pasada en la revista “Enviromental Science and Technology” y en el mismo se asegura haber finalmente descubierto la causa del cambio de coloración del Taj Mahal.  Bergin y colaboradores basan sus conclusiones en un estudio en el que, a lo largo de varios meses, recolectaron muestras de aire en los alrededores del Taj Mahal y las analizaron para determinar los contaminantes que contenían. Encontraron que el aire en Agra tiene en suspensión partículas de carbón y polvo. El carbón podría provenir de varias fuentes que incluyen la quema de combustibles, la fabricación de ladrillo y los vehículos automotores, entre otras. El polvo podría ser causado por actividades agrícolas o por el tráfico vehicular.  Para complementar el estudio anterior los investigadores colocaron muestras de mármol blanco en lugares cercanos a la cúpula del mausoleo y las expusieron al aire por un periodo de dos meses. Esto, con el objeto de determinar qué clase de partículas se adhirieran  a la superficie de las muestras, que resultaron ser, igualmente, carbón y polvo. Con estos resultados Bergin y colaboradores tenían fuertes indicios de que las partículas de carbón y polvo eran los culpables que buscaban. Para probarlo, no obstante, había que encontrar el mecanismo físico por medio del cual ocurría el cambio de color. Con este propósito notaron que ciertas partículas de carbón y de polvo reflejan más eficientemente la luz roja que la luz azul. Como sabemos, la luz del sol está compuesta de todos los colores y si de alguna manera le suprimimos el color azul la tonalidad de la luz cambia. Así, las partículas de carbón adheridas a la superficie del mármol, al reflejar más el rojo que el azul, cambian la tonalidad de la luz del sol reflejada por el mármol del Taj Mahal que adquiere una tonalidad parduzca.   Con esta información, habría que determinar la procedencia de las partículas de carbón y polvo suspendidas en el aire de Agra y buscar suprimirlas en su origen  –de ser tal cosa posible–. Con esto se protegería una obra de arquitectura que es patrimonio de la humanidad. Y de paso se protegería a los habitantes de Agra, pues si la contaminación del aire ahí es capaz de dañar a un edificio de piedra, nada bueno podríamos esperar para los humanos.",
    "En las últimas semanas la casa Christie´s ha llevado a cabo dos subastas públicas de objetos que, de una u otra manera, están relacionados con la ciencia. Por un lado, Christie´s subastó en noviembre pasado un fragmento de 27.9 gramos del meteorito “Black beauty”. Este meteorito fue expulsado del planeta Marte por el impacto de un asteroide y llegó a la Tierra después de viajar por cinco millones de años a través del espacio interplanetario.  Fue encontrado hace algunos años en el Desierto del Sahara por un nómada buscador de meteoritos. “Black beauty” alcanzó en la subasta un precio de $ 81,250 dólares, pagados por un coleccionista privado. Es decir, casi tres mil dólares por gramo.En días pasados, Christie´s subastó también la medalla Nobel de James Watson: Watson recibió en 1962 el premio Nobel de Fisiología o Medicina, conjuntamente con Francis Crick y Maurice Wilikins, por el descubrimiento de la estructura molecular del ADN. Este descubrimiento constituye uno de los avance científicos más trascendentes de todos los tiempos y ha establecido  las bases moleculares de la genética y las leyes de la herencia. Y con esto la promesa, entre muchas otras, de que el conocimiento del DNA de cada persona pueda llevar al desarrollo de terapias individualizadas para la cura de enfermedades.En 1968 Watson se convirtió en director del “Cold Spring Harbor Laboratory” (CSHL), una institución privada de investigación localizada en Staten Island en el Estado de Nueva York, que lleva a cabo investigaciones sobre cáncer, neurociencias y biología de plantas, entre otras áreas relacionadas con la genética. A partir de 1994 Watson actuó como presidente de esta institución y posteriormente como canciller. En 2007, a los 79 años de edad, fue relevado de sus responsabilidades administrativas en el CSHL, aunque  continuó ligado a la institución como canciller emérito. El relevo de Watson de su puesto como canciller fue motivado por declaraciones suyas hechas durante una entrevista que concedió en octubre de 2007 al diario británico “The Sunday Times”, en las que afirmó que los negros son menos inteligentes que los blancos. De manera específica, comentó que “estaba pesimista acerca de las perspectivas de África”, debido a que “todas nuestras políticas sociales están basadas en la suposición de que su inteligencia es las misma que la nuestra, mientras que todas las pruebas nos dicen que no es así”. Así mismo, expresó que “no hay una razón sólida para anticipar que la capacidad intelectual de grupos de personas separadas geográficamente en su evolución hayan evolucionado de manera idéntica” y que “Nuestro deseo de que iguales poderes de razonamiento sean una herencia universal de la humanidad no es suficiente para que esto sea una realidad”.   De acuerdo a la entrevista, Watson tenía la esperanza de que todo mundo fuera igual, pero “aquellos que tienen que lidiar con empleados negros saben que esto no es cierto”. Estas declaraciones, en un tema tan políticamente explosivo en los Estados Unidos y viniendo de alguien con la prominencia científica de Watson, fueron, por supuesto, de gran impacto y levantaron polvareda. En estas condiciones, fue que la dirección del CSHL se deslindó de Watson y le retiró de sus funciones administrativas en la institución.   En vista de la reacción que provocó, Watson hizo declaraciones posteriores en las que trato de suavizar su posición y afirmó que “A todos aquellos que han inferido de mis palabras que África como continente es genéticamente inferior, sólo puedo expresar mis disculpas sin reservas. Eso no fue lo que quise decir. De manera más importante, desde mi punto de vista no hay bases científicas para esta creencia”. A pesar de estas declaraciones, no obstante, también expresó que él no “es un racista en el sentido convencional”.Posiblemente en correspondencia con la fama que ha adquirido con sus declaraciones controvertidas –en otra ocasión defendió el potencial derecho que tendría una embarazada a interrumpir el parto si una prueba genética demostrara que su hijo por nacer era homosexual–, el precio que la medalla Nobel de Watson adquirió un alto precio en la subasta y fue vendida a un coleccionista anónimo en casi cinco millones de dólares. En la misma subasta Watson vendió el borrador del discurso que pronunció al aceptar el premio Nobel y el manuscrito de su conferencia Nobel. Por ambos obtuvo más de medio millón de dólares.El precio que alcanzó la medalla de Watson es considerablemente más alto que los 2.3 millones de dólares que obtuvo la medalla Nobel de Francis Crick cuando fue subastada en 2013. Crick, quien  murió en 2004, quien tenía un perfil público más moderado que el de Watson. Antes de 1980 las medallas Nobel se fabricaban en oro de 23 kilates y tenían un peso de 198 gramos. Por el oro que contiene, la medalla de Watson vendida en la subasta de Christie´s tiene entonces en la actualidad un precio que ronda los $10,000 dólares. Por lo que vemos, sin embargo, su valor simbólico –al que responden los coleccionistas– es considerablemente mayor. Sobre todo si, aparentemente, la medalla va acompañada de escándalos públicos,",
    "Hace cinco millones de años un asteroide hizo impacto con la superficie de Marte, lanzando rocas al espacio que escaparon a la gravedad del planeta. Una de estas rocas penetró hace algunos miles de años a la atmósfera de la Tierra y, por el rozamiento con la misma, se fragmentó en varios pedazos. Los fragmentos cayeron en el Sahara Occidental, en una región políticamente inestable bajo el control de facto de Marruecos; control que no es, sin embargo, reconocido por la ONU. El meteorito ha llegado  a ser famoso y es conocido como “Black Beauty” (belleza negra) por su color. Los buscadores y comerciantes de meteoritos han logrado recuperar fragmentos de “Black Beauty” que en total pesan unos dos kilogramos. De acuerdo con un artículo aparecido esta semana en la revista “Science”, dos tercios de estos dos kilogramos están en poder de Jay Piatek, un médico propietario de una clínica para adelgazar en Indianápolis, Indiana. Piatek habría adquirido dichos fragmentos de comerciantes marroquíes de meteoritos.    “Black Beauty” no es el primer meteorito marciano que ha encontrado el camino hasta nuestro planeta. Los expertos, no obstante, consideran que constituye un caso aparte, por la importancia científica que tiene para elucidar la historia geológica de Marte. “Black Beauty” está formado por pequeños guijarros aglutinados en una matriz de granos más finos. Munir Humayun, de la Universidad Estatal de Florida, arguye en un artículo publicado en la Revista “Nature” en noviembre de 2013, que la edad de “Black Beauty” –o al menos de algunas partes que lo forman– es de unos 4,400 millones de años. Esto implicaría que “Black Beauty” proviene de una época temprana en la historia geológica de Marte. Constituiría, además, el meteorito marciano más antiguo que ha arribado a nuestro planeta.La fama de “Black Beauty” ha rebasado incluso las fronteras científicas y un fragmento del mismo, con dimensiones 4.4x3.6x2 cm y un peso de 28 gramos, fue puesto este mes de noviembre en subasta por la casa Christie´s. Según esta casa de subastas, se espera que el meteorito se venda por una cantidad que oscile entre 75,000 y 100,000 dólares; es decir, podría alcanzar un valor de hasta unos 3,500 dólares por gramo, que es casi 100 veces el precio de un gramo de oro. Según algunas fuentes, no obstante, en el mercado de coleccionistas el precio por gramo de “Black Beauty” podría alcanzar los 10,000 dólares.¿Por qué están seguros los expertos que “Black Beauty” procede de Marte?  De no haber certeza en este respecto cabría la posibilidad de que aquel que adquiera en subasta el pedazo de meteorito en realidad se haga de una piedra sin valor  –aunque habría que reconocer que esto carece de importancia, excepto, por supuesto, para el afectado–.  Ciertamente, el meteorito no llegó a la Tierra con una etiqueta de hecho en Marte, planeta que se encuentra, por lo demás, a una distancia promedio de más de 200 millones de kilómetros. Para contestar la anterior pregunta tenemos que considerar primeramente que las afirmaciones científicas nunca son absolutas y que de ningún modo deben tomarse como definitivas. Todo lo contrario, las teorías e hipótesis científicas son modificables a la luz de nuevos resultados o mediciones. En el caso de “Black Beauty”, sin embargo, existen evidencias razonables de que tiene un origen marciano. Esto fue primero reconocido por Carl Agee de la Universidad de Nuevo México en un artículo publicado en la revista “Science” en febrero de 2013. Agee midió las concentraciones de manganeso y hierro en el meteorito y encontró que estas no coincidían con las concentraciones respectivas de las rocas terrestres, pero sí con las de las marcianas. Midió también el contenido de deuterio –una versión pesada del hidrógeno– en el agua atrapada en el meteorito y encontró que es similar al que se encuentra en el agua de Marte. Carl Agee había primeramente calculado que  la edad de “Black beauty” es de unos 2,200 millones de años. Esta cifra fue revisada por Munir Humayun quien la extendió hasta los 4,400 millones de años citados anteriormente. Agge había también postulado que el meteorito se originó en una erupción volcánica, lo cual fue también fue revisado por Humayun quién le atribuyó un origen sedimentario. Esto, de acuerdo con dicho investigador, es importante desde el punto de vista de la búsqueda de vida en Marte, pues dado su origen sedimentario, el meteorito pudo haberse formado en presencia de agua.De acuerdo con los expertos, vistas las coincidencias con las rocas marcianas investigadas por las sondas norteamericanas enviadas a la superficie de Marte, “Black Beauty” constituye un importante objeto de estudio que podría arrojar luz sobre el pasado geológico de Marte y eventualmente sobre la posible existencia de vida en un pasado remoto. A un costo, además, que es una fracción minúscula del costo que representa el envío de sondas para explorar  la superficie de nuestro planeta vecino.   Al mismo tiempo que asegura la inversión de aquellos que poseen un pedazo de Marte llegado a la Tierra en un viaje interplanetario de cinco millones de años.",
    "Cuando hace cientos de miles de años nuestros antecesores aprendieron a dominar el fuego, se hicieron de una fuente de energía que lo mismo les sirvió para cocinar alimentos que para iluminar sus noches o calentarse en los días fríos. No es difícil imaginar el gran cambio que esto les representó. Sobre todo porque la leña para prender un fuego no era, ciertamente, algo que fuera  escaso. Cuando quemamos madera liberamos energía que fue almacenada a lo largo de la vida del árbol del cual procede. El origen último de esta energía es la luz del Sol que las plantas utilizan para construir la materia orgánica por medio del proceso de fotosíntesis. Si bien la energía solar está muy diluida y la fotosíntesis no es un proceso de conversión de energía particularmente eficiente –solamente una fracción minúscula de la luz del Sol que incide sobre la planta es almacenada–, la planta acumula energía durante muchos años y al final ésta llega a ser considerable.  Vistas así las cosas, durante la combustión de la madera se libera súbitamente energía en forma de calor que, en contraste, fue acumulada lentamente por la planta durante largo tiempo. Lo anterior lo podemos aplicar a los combustibles fósiles –petróleo, carbón y gas natural–, cuya combustión también libera energía almacenada por plantas; sólo que en este caso se trata de plantas que vivieron hace decenas o centenas de millones de años –y por tanto son irreemplazables.   En las condiciones anteriores, de contraste entre la velocidad con que las plantas acumulan energía y la velocidad con que ésta es consumida, no resulta sorprendente que se hayan dado desequilibrios que han llevado a un cambio climático global en la medida en que se incrementó el consumo de combustibles fósiles.La energía solar que llega a la superficie de la Tierra está, efectivamente, muy diluida. No obstante, dado el enorme tamaño de nuestro planeta, la cantidad total de energía que recibimos es igualmente enorme. De manera más precisa, ésta es unas siete mil veces mayor que toda la energía que consumimos. Así, el aprovechamiento de sólo una pequeña fracción de la radiación solar que arriba a la Tierra sería suficiente para satisfacer nuestras necesidades energéticas.    Un dispositivo para aprovechar la energía del Sol es la celda solar, que absorbe la radiación solar y la convierte en energía eléctrica. La electricidad es una forma muy conveniente de energía, la cual puede ser transportada como tal a grandes distancias y que es, además,  fácilmente convertible a otras formas de energía –energía mecánica, por ejemplo, para el movimiento de automóviles, trenes y otros vehículos.Por medio de celdas solares podemos aprovechar la energía del Sol en la medida en que ésta llega a la Tierra sin los desequilibrios entre generación y consumo característicos de los combustibles fósiles. Hay que hacer notar, sin embargo, que las celdas solares por sí mismas no son capaces de almacenar la energía que producen y ésta debe ser consumida al instante. Esto, en contraste con los combustibles fósiles que pueden almacenar energía de manera indefinida –como lo han hecho desde tiempo inmemorial.Como quiera que sea, el aprovechamiento de la energía del Sol por medio de paneles solares es enormemente atractivo, entre otras cosas porque es una energía renovable –al menos hasta que se extinga el Sol, lo que ocurrirá en algunos miles de millones de años– y no contaminante del medio ambiente. Todo esto a diferencia de los combustibles fósiles.   Hacer afirmaciones absolutas, sin embargo, no deja de ser riesgoso y que los paneles solares sean no contaminantes es, en realidad, sólo una verdad a medias. En efecto, si bien una vez instalado y en funcionamiento un panel solar generará energía gratis no contaminante, para llegar a este punto hubo que invertir una cierta cantidad de energía para fabricar e instalar el panel mismo y, con seguridad, al menos parte de esta energía fue generada por medio de combustibles fósiles contaminantes.De este modo, es importante determinar cuánto tiempo necesitará un panel solar para generar la energía empleada en fabricarlo. Si este tiempo fuera mayor que el tiempo de vida útil del módulo, las celdas solares no constituirían una vía factible para mitigar los problemas de cambio climático que padece el mundo –es decir, saldría más caro el caldo que las albóndigas. Afortunadamente, una mayoría de expertos coincide en que los paneles solares regresan la energía con la que fueron fabricados en unos pocos años, y ya que el  tiempo proyectado de vida útil de un módulo solar es de unos 25 a 30 años, hay un balance energético altamente favorable.En la actualidad, la generación de energía empleando paneles solares está en rápido crecimiento y el mundo tiene instalada una capacidad de generación de energía solar equivalente a más de cien plantas nucleares como la de Laguna Verde en el Estado de Veracruz.   No se espera, sin embargo, que, al menos en las próximas décadas, la energía solar u otra forma de energía desplace por completo a los combustibles fósiles. Así, por un buen tiempo convivirán en el mundo modos sofisticados de generar energía con otros que, en esencia, no han variado desde hace cientos de miles de años, cuando el homo no era todavía sapiens.",
    "Tal como fue planeado, la madrugada del pasado  miércoles 12 de noviembre la sonda Philae de 100 kilogramos de peso de la Agencia Espacial Europea (AEE) se desprendió de la nave nodriza Rosetta con rumbo a la superficie del cometa Churyumov-Gerasimenko, con la que hizo contacto siete horas más tarde.Como informó en su momento la AEE, la nave Rosetta con la sonda Philae a bordo se encontró con el cometa Churyumov-Gerasimenko el 6 de agosto pasado después de un viaje de 10 años desde la Tierra. El encuentro ocurrió en un punto situado a 500 millones de kilómetros de nuestro planeta, entre las órbitas de Marte y Júpiter. A su arribo a la vecindad del cometa, Rosetta entró en órbita alrededor del mismo y se espera que lo acompañe en su viaje orbital alrededor del Sol por más de un año, hasta diciembre de 2015. Según el sitio de internet de la AEE, la misión de Rosetta tiene como objetivo determinar el origen y evolución del sistema solar. Los científicos consideran que la composición de los cometas refleja la composición de la nebulosa a partir de la cual se formaron el Sol y los planetas hace 4,600 millones de años, y de ahí la importancia de realizar un estudio a profundidad del cometa Churyumov-Gerasimenko. Este estudio incluía el envío de la sonda Philae a su superficie, lo que, desafortunadamente, no resultó como se había planeado, pues el aterrizaje de Philae no fue lo terso que se esperaba. Lejos de esto, al contacto con la superficie del cometa la sonda “rebotó”, elevándose por cerca de un kilómetro, para nuevamente caer –después de casi dos horas de vuelo– un kilómetro más allá del primer contacto.La trayectoria caótica de la sonda no paró ahí, y al segundo contacto con la superficie del cometa saltó nuevamente –aunque ahora a una menor altura– para finalmente caer de costado cerca de una pared de roca. En una fotografía enviada por Philae desde la superficie del cometa, hecha pública por la AEE, es posible ver una de sus tres patas, aparentemente en el “aire”, con rocas en el trasfondo. En comparación con la Tierra, el cometa Churyumov-Gerasimenko tiene una masa minúscula y su fuerza de atracción gravitatoria es correspondientemente pequeña, la cual se estima es apenas una diezmilésima parte de la fuerza de atracción que ejerce nuestro planeta. Con este valor, el peso de Philae sobre la superficie del cometa sería de apenas diez gramos, lo que explica que haya saltado un kilómetro hacia arriba, permaneciendo suspendido por casi dos horas. Para evitar lo que finalmente sucedió, la AEE había previsto que al contacto con la superficie del cometa se disparase un mecanismo de propulsión por aire que presionase a la sonda hacia abajo, al mismo tiempo que se disparasen arpones hacia el suelo para anclarla de manera firme. Sin embargo, ninguno de los dos mecanismos funcionó con los resultados conocidos. Una consecuencia desafortunada del accidentado aterrizaje de Philae es que terminó por parar en un sitio cerca de una pared de roca que obstruye en gran medida la radiación solar que deben captar sus paneles solares para recargar sus baterías. Sin esta radiación, la sonda sólo disponía de 60 horas de energía para cumplir sus funciones.Ante esta circunstancia, el equipo de control de la sonda aceleró los experimentos planeados con los instrumentos de medición a bordo de Philae, lo que incluyó la perforación del suelo del cometa para extraer muestras de material que los científicos consideran ha permanecido inalterado desde la formación del sistema solar.  Según el sitio de internet de la AEE, Philae tuvo suficiente energía para llevar a  cabo las mediciones planeadas y transmitir los resultados al orbitador Rosetta, que a su vez los ha enviado hasta nuestro planeta. En estos momentos la sonda agotó ya sus reservas de energía y ha entrado en un estado de hibernación, mismo que podría ser permanente, lo que implicaría el fin de la misión de Philae. Esto ocurrirá a menos que, en la medida que el cometa se acerque al Sol y la intensidad de la radiación solar aumente, los paneles solares capten suficiente energía para cargar las baterías: En tal caso la sonda podría despertar.   La AEE, por otro lado, ha sido objeto de críticas por el hecho de que no han hecho públicos los resultados de la misión con la suficiente celeridad. Un artículo publicado en el número de esta semana de la revista “Science”, por ejemplo, hace notar que no se han hecho públicas ninguna de las fotografías del cometa tomadas con las cámaras de alta resolución con las que cuenta Rosetta. Lo anterior obedece al hecho de que los científicos responsables de cada uno de los instrumentos a bordo de Rosetta quieren mantener la prioridad sobre los datos obtenidos, mismos que darán a conocer mediante artículos científicos de su autoría en su debido momento.Es ciertamente motivo de controversia que los resultados de una misión que ha costado 1,400 millones de Euros en fondos públicos no se hagan públicos en cuanto estén disponibles y eso es algo que podrán reclamar los ciudadanos de los países que pagaron la misión al cometa Churyumov-Gerasimenko. En lo que a nosotros respecta, mucho nos hubiera gustado haber tenido ya la oportunidad de admirar imágenes a colores y en alta resolución de un mundo por demás extraño. Y al mismo tiempo lamentamos que, en lo que al aterrizaje de Philae se refiere, no todo haya salido como se esperaba.",
    "Como sabemos, los cometas son objetos celestes que giran alrededor del Sol en órbitas excéntricas que los acercan y alejan de éste y que, por lo mismo, aparecen y desaparecen del cielo de manera periódica. Todo esto, sin embargo, no lo supimos con seguridad sino hasta fechas relativamente recientes. De manera específica, hasta que Edmund Halley predijo de manera acertada, con décadas de anticipación, la aparición en 1758 del cometa que lleva su nombre. Antes de Halley los cometas eran objetos que aparecían en el cielo al azar. Después de Halley su comportamiento es predecible, y en el caso particular del cometa nombrado en su honor, las apariciones ocurren cada 76 años.Antes de que se conociera su verdadera naturaleza, los cometas se asociaron con todo tipo de sucesos y presagios, comúnmente calamitosos –aunque no necesariamente– y de esto han surgido numerosas historias en las que los cometas han sido protagonistas estelares. Una de estas historias tiene que ver con el llamado Sitio de Belgrado, en el que se enfrentaron turcos y húngaros en el año 1458 en el marco de la expansión del Imperio Otomano hacia Europa. Viendo amenazada a la cristiandad y estando fresca la memoria de la caída de Constantinopla en poder de los turcos en 1543, el papa Calixto III emitió una bula para exorcizar el peligro turco en la que ordenaba a todas las iglesias a sonar una o más campanas tres veces a las doce del día, y a todos los fieles a rezar tres padrenuestros y tres avemarías, ente otros preceptos.  El Sitio de Belgrado se dio poco después de la aparición del cometa Halley en junio de 1458 y esto dio origen a una leyenda según la cual el papa Calixto III habría excomulgado al cometa al considerarlo objeto del demonio. Esta leyenda, que hoy se encuentra desacreditada, sobrevivió, sin embargo, hasta el siglo XIX, propagada, entre otros, por el físico y astrónomo francés Pierre-Simon Laplace.   Aun después de que se descubriera su verdadera naturaleza, los cometas han seguido siendo protagonistas de numerosas historias no tan favorables a su reputación. Durante la visita del cometa Halley en 1910, por ejemplo, la  cola del mismo, que contiene cianógeno –un gas tóxico– cruzó por nuestro planeta. Esto motivó alarma por la posibilidad de que la atmósfera pudiera ser contaminada con este gas, llevando al extermino de la vida sobre la Tierra. En realidad, la concentración de cianógeno en el cometa es tan pequeña que no represento peligro alguno –aunque sí constituyó una oportunidad de negocio para algunos vivales que se dedicaron a vender píldoras para, supuestamente, neutralizar los efectos del cianógeno. En un caso más reciente, la visita del cometa Hale-Bopp en 1997 provocó un suicidio en masa de miembros de la secta “Puerta del Cielo”, quienes por esta vía pretendían abordar una supuesta nave extraterrestre que se acercaba a la Tierra escondida detrás del cometa. Dicha nave habría sido descubierta en una fotografía del cometa tomada por un astrónomo aficionado. Según las creencias de la secta, la nave extraterrestre conduciría a los suicidas a un estadio superior de vida.Ciertamente muchas historias de cometas se han desarrollado a lo largo de la historia del mundo. Estas historias vienen a cuento en estos días porque el próximo 12 de noviembre la nave Rosetta de la Agencia Espacial Europea, que desde el pasado mes de agosto se encuentra en órbita alrededor del cometa Churyumov-Gerasimenko después de un viaje de diez años desde la Tierra, liberará la sonda Philae de 100 kilogramos de peso que se pretende se pose suavemente sobre su superficie y se afiance sobre la misma por medio de arpones. De acuerdo con su página web, la Agencia Espacial Europea trasmitirá en vivo el aterrizaje del Philae –con el retraso de tiempo obligado por la distancia de cientos de millones de kilómetros a la que se encuentra el cometa de la Tierra. Una vez sobre la superficie del cometa, Philae llevará a cabo mediciones para determinar su composición química y su estructura interna. Igualmente, seguirá, tan cercanamente como puede ser posible, la evolución de las actividades del cometa conforme se acerca al Sol.Rosetta y Philae llevan a bordo sofisticados instrumentos que permitirán obtener información científica sin precedente acerca de los cometas. Rosetta, lleva también a bordo, por supuesto, cámaras fotográficas que nos han permitido observar al cometa con un asombroso detalle. Las fotografías que podemos apreciar en la página web de la Agencia Espacial Europea nos muestran que el cometa Churyumov-Gerasimenko está formado por dos cuerpos unidos por un cuello. Cómo fue que el cometa adquirió esa forma es algo que tendrán que determinar los especialistas. Lo que sí es claro de las fotografías, tomadas a una distancia de decenas de kilómetros, es que el cometa viaja sólo, sin acompañantes, y que entre los descubrimientos que nos entregará Rosetta no se encontrará –con seguridad– el de alguna nave extraterrestre escondida detrás del cometa viajando sigilosamente hacia nuestro planeta. Ni tampoco encontrará evidencia alguna que justifique la negra reputación que tuvieron los cometas en tiempos pasados.   Por lo pronto, habría que cruzar los dedos para que el aterrizaje del próximo miércoles transcurra sin contratiempos.",
    "Los proyectos espaciales financiados con capital privado sufrieron en la semana que hoy termina dos fracasos importantes. Por un lado, el cohete Antares de la compañía Orbital Sciences Corporation explotó poco después de su lanzamiento el pasado martes por la noche desde una base de la NASA en el estado de Virginia, Estados Unidos. El cohete llevaba bajo contrato con la NASA una carga de más de dos toneladas de suministros, equipo y experimentos científicos con destino a la Estación Espacial Internacional. Los operadores del lanzamiento apretaron un botón de autodestrucción quince segundos después del despegue, cuando fue evidente que algo no estaba funcionando bien y para evitar que el cohete cayera en un área poblada.Si bien todavía no se han establecido oficialmente las causas del accidente, éste ha sido atribuido a un malfuncionamiento de los motores del cohete. Lo anterior según Bill Ketchum, un ingeniero aeroespacial retirado que trabajó para la compañía General Dynamics, quien fue citado por la revista Forbes. Forbes cita, igualmente, a Dennis Wingo, Director Ejecutivo de la compañía Skycorp Incorporated, quien afirma que dichos motores –desarrollados en la antigua Unión Soviética– fueron almacenados en 1975 en una bodega sin las condiciones climáticas adecuadas y que nadie quiso comprarlos hasta que Orbital Sciences lo hizo para Incorporarlos a su cohete Antares. De acuerdo con Wingo, la causa más probable del accidente del pasado martes fue el esfuerzo, la corrosión y las fisuras en la estructura de los motores del cohete, aunque considera que también pudo darse una fisura o fractura en las líneas de combustible.Un segundo accidente que involucró esta semana a la industria espacial privada ocurrió el viernes último cuando la nave espacial SpaceShipTwo de la compañía Virgin Galactic se estrelló en un vuelo de prueba en el Desierto de Mojave, California, muriendo uno de sus pilotos y resultando gravemente herido el otro. El SpaceShipTwo es un vehículo que Virgin Galatic pretende emplear para llevar a cabo vuelos turísticos al espacio. Los vuelos serían suborbitales, lo que significa que la nave no entraría en órbita alrededor de la Tierra. Se elevaría, no obstante, hasta una altura de unos 100 kilómetros desde donde se podrá observar la curvatura de la Tierra y lo oscuro del espacio. Los pasajeros podrán también experimentar la ingravidez por algunos pocos minutos y flotar dentro de la nave. Todo esto a un costo entre los doscientos mil y los doscientos cincuenta mil dólares por persona.Para alcanzar el espacio, el SpaceShipTwo –que puede transportar a seis turistas– es primero elevado hasta una altura de unos 15 kilómetros por un avión nodriza y luego liberado para que alcance los cien kilómetros de altura por sus propios medios. En el vuelo del viernes pasado todo ocurrió con normalidad hasta el momento que el avión nodriza liberó al SpaceShipTwo. Una vez liberado, no obstante, y al cabo de unos pocos segundos, ocurrieron “serias anomalías que llevaron a la pérdida de la aeronave”, según un comunicado de Virgin Galactic.No se sabe con certeza que fue lo que le sucedió al SpaceShipTwo, pero cabría la posibilidad de que el accidente estuviera relacionado con el tipo de combustible empleado, el cual no se había probado con una aeronave en vuelo, aunque sí en pruebas en tierra.El accidente hace preguntarse a los expertos sobre el futuro de la industria del turismo espacial.  No dudan que esta se desarrolle  algún día pero quizá no tan pronto como Richard Branson, dueño de Virgin Galactic, predijo en septiembre pasado, cuando afirmó que el primer vuelo comercial del SpaceShipTwo se llevaría a cabo en febrero o marzo de 2015. Esto que parecía inviable a los expertos antes del accidente del viernes pasado, es ahora claramente imposible.En realidad Branson ha hecho repetidos anuncios de la inminencia de vuelos comerciales del SpaceShipTwo y en la wikipedia podemos encontrar un recuento de los mismos. Primeramente, en julio de 2008 predijo que el vuelo inaugural se llevaría a cabo en 18 meses. En diciembre de 2009 aseguró ante 300 personas que habían pagado para apartar un lugar en el SpaceShipTwo, que los vuelos empezarían a llevarse a cabo en 2011. En abril de 2011, sin embargo, Branson declaró que tenía la esperanza de que en 18 meses “estaremos sentados en nuestra nave espacial con rumbo al espacio”. A pesar de esto, en mayo de 2013 Branson anunció en que él estaría a bordo del SpaceShipTwo en su vuelo inaugural, el cual se llevaría a cabo el 25 de diciembre de ese año, “quizá vestido de Santa Claus”.A la fecha, en la página electrónica de Virgin Galactic se asienta que la compañía ha recolectando 89 millones de dólares de 700 personas que han comprado un lugar en el SpaceShipTwo. Nada mal para una compañía que ofrece viajes al espacio pero que a la fecha no ha realizado ninguno –ni parece que lo vaya a hacer pronto.Como quiera que sea, al margen del buen o mal negocio, lo que sí resulta claro es que viajar al espacio –a más de medio siglo del inicio de la era espacial– sigue siendo una empresa difícil y peligrosa, como lo atestiguan los dos accidentes espaciales ocurridos en los últimos días. Y este es el consuelo que nos queda a aquellos que no pudimos pagar 250,000 dólares para apartar un viaje al espacio, que bien pudiera nunca llevarse a cabo.",
    "Hace dos semanas comentamos en este espacio la concesión del Premio Nobel de Física 2014 a los investigadores japoneses Isamu Akasaki, Hiroshi Amano y Shuji Nakamura –este último nacionalizado estadounidense–  por la invención de la lámpara LED de luz azul. El Premio Nobel de física fue otorgado esta vez por el desarrollo de un dispositivo práctico y no por un descubrimiento científico fundamental, como ha sido más frecuente. Es decir, el comité Nobel atendió más al impacto social y económico del descubrimiento que a su importancia científica. Más allá del criterio empleado por el comité Nobel, la decisión de otorgar el premio de este año a los investigadores japoneses ha motivado diversos comentarios. De acuerdo con Wally Rhines, por ejemplo, quien es director ejecutivo la compañía Mentor Graphics Corporation,  el comité Nobel cometió un error año al pasar por alto el trabajo de Herbert Maruska, a quien él considera el verdadero inventor del LED azul. En apoyo a este reclamo se puede mencionar que a Maruska le fue otorgada en 1974  una patente –en la que participa Rhines– para un LED azul fabricado con el mismo material que el LED desarrollado por los investigadores japoneses que les dio del Premio Nobel. En opinión de Rhines, en esta ocasión el comité Nobel premió a quienes perfeccionaron el LED azul y no a quienes los inventaron dos décadas antes.  Al margen de la controversia, es interesante mencionar que Maruska desarrolló el primer LED azul cuando era estudiante doctoral en la Universidad Stanford, en Palo Alto, California. Maruska al mismo tiempo era empleado de la compañía RCA que estaba interesada en el desarrollo de pantallas planas para televisión. El apoyo de RCA a Maruska para la realización de su doctorado era contingente a que su tesis doctoral versara sobre el desarrollo de un LED azul.En esa época –la primera mitad de la década de los años setenta– existían lámparas LED rojas y verdes. Para una pantalla de colores, sin embargo, se necesitaba también una lámpara azul y de ahí el interés de RCA en el trabajo de Maruska. El desarrollo del LED azul en la compañía RCA, no obstante, se vio interrumpido en 1974 cuando RCA entró en una etapa de problemas económicos que la llevaron al colapso en 1986.  Es igualmente interesante hacer una comparación entre el trabajo de Maruska y el de Shuji Nakamura, quien desarrolló su LED azul en la primera mitad de la década de los años noventa cuando trabajaba para una pequeña compañía, Nichia, en Tokushima, Japón. Al igual que Maruska, Nakamura empezó trabajar en dicho proyecto antes de recibir su doctorado. Esto no le impidió, como lo relata en su libro “El diodo laser azul: La historia completa”, recibir un considerable apoyo de Nichia para llevar a cabo su trabajo. Dicho apoyo se tradujo en una subvención de 3.3 millones de dólares, cantidad que representaba el 1.5% de total de ventas de la compañía.En un determinado momento, sin embargo, Nichia consideró que el proyecto estaba consumiendo demasiados recursos y suspendió el apoyo. Con esto, Nakamura tuvo que ingeniárselas para continuar con su trabajo. Finalmente, en 1993 tuvo éxito en su empeño y logró desarrollar un proceso para la fabricación de lámparas LED azules que resultó ser comercialmente viable.Nakamura realizó así lo que Edison llevó a cabo en su momento con las lámparas incandescentes. Ambos, Edison y Nakamura no fueron los inventores originales de sus respectivas lámparas, pero sí en cambio desarrollaron métodos de fabricación que posibilitaron su comercialización y amplia diseminación. De no haber suspendido RCA en 1974 sus proyectos sobre lámparas azules podría haber sido esta compañía y no Nichia la que se hubiera llevado el crédito por su desarrollo. Para ese entonces, sin embargo, Maruska había dejado la compañía. En palabras recogidas por la revista IEEE Spectrum –publicada por el Instituto de Ingenieros Eléctricos y Electrónicos de los Estados Unidos– Maruska afirma “Estoy seguro que no hubiera pasado mucho tiempo antes de que hubiera obtenido una lámpara LED brillante. Pero una vez que fui despedido no pude encontrar otro trabajo en donde continuar con el proyecto”.  Nakamura, por su lado, recibió de Nichia 180 dólares por la patente sobre el LED azul y, como resultado, demandó a la compañía. Después de un juicio aceptó a regañadientes nueve millones de dólares en compensación.Muchas circunstancias rodearon al desarrollo de las lámparas azules y mucho se podría escribir sobre el tema. Vale la pena, sin embargo, terminar este artículo reproduciendo uno de los argumentos que ofrece Nakamura para explicar por qué tuvo Nichia éxito cuando otros fracasaron. De acuerdo con Nakamura, “el manejo del proyecto estaba a cargo de solamente dos personas: él y el presidente de la compañía. No podría haber sido más simple: no comités, no supervisores, no jefes de departamento, no líderes de grupo etc., no comités revisores, no internacionalización, no coordinadores, no consorcios internacionales –solamente trabajo”.",
    "En 1865, el monje agustino Gregor Mendel publicó un artículo que tituló “Experimentos sobre hibridización de plantas”, en el cual describe los resultados de un estudio llevado a cabo con plantas de chícharos con el objeto de dilucidar las leyes que gobiernan la herencia de características físicas de padres a hijos. En sus experimentos, Mendel encontró –de manera inesperada– que las características físicas de las plantas estudiadas, como el color de sus semillas o el color de sus flores, se heredan de manera integral o bien simplemente no se heredan. Esto, en contraposición con la idea prevaleciente en la época según la cual dichas características serían el resultado de una mezcla de las correspondientes características de sus ascendientes inmediatos. Así, al cruzar una planta con semillas verdes con otra de semillas amarillas, Mendel encontró que se producía una planta con semillas amarillas y no una con semillas de un color combinado verde-amarillo. Con este hallazgo, Mendel dio origen al concepto de gen, partícula que contiene toda la información de un ser vivo –es decir, los planos necesarios para construirlo– y que puede ser heredada por sus descendientes.  Si bien el trabajo de Mendel no recibió mucha atención en un inicio, fue redescubierto al despuntar el siglo XX y desde entonces se constituyó como el punto de partida de la ciencia de la genética. Con posterioridad, durante la primera mitad del siglo XX, esta ciencia progresó rápidamente estableciéndose que la información genética está codificada en moléculas de ADN, de modo similar a como una pieza de música está codificada en la superficie de un disco compacto. La estructura molecular del ADN, que tiene la forma de una doble hélice, fue descubierta en los años 1952-1953 en la Universidad de Cambridge, Inglaterra, por Francis Crick y James Watson, y por Rosalind Franklin y Maurice Wilkins en el King´s College, Londres, Inglaterra. Por este descubrimiento, uno de los resultados científicos más notables del siglo XX, Crick, Watson y Wilikins recibieron el premio Nobel de Medicina o Fisiología en 1962. Rosalind Franklin, a pesar de haber sido una de las protagonistas principales del descubrimiento, no pudo recibir el Nobel debido a que había muerto de cáncer en 1958 y dicho premio no se otorga de manera póstuma.     La genética tiene en la actualidad un enorme rango de aplicaciones y lo mismo se emplea en la medicina para el desarrollo de terapias para curar enfermedades, que en la práctica forense para identificar restos humanos. Recientemente, la genética fue empleada para determinar que la epidemia de SIDA/VIH que asuela al continente africano se originó en la década de los años cincuenta del siglo pasado, en la ciudad de Leopolville en le entonces Congo Belga. La genética incluso se ha empleado para esclarecer asuntos que han ocurrido en un pasado remoto. Se ha empleado, por ejemplo, para dilucidar cuál ha sido el camino evolutivo que ha seguido la especie humana desde su salida de África hace cientos de miles de años.La información contenida en el ADN es única y característica de cada individuo, y el conocimiento de la estructura del material genético puede llevar a un diagnóstico y/o pronóstico de ciertas enfermedades humanas. Con el empleo de las modernas técnicas de la biología molecular es posible identificar microorganismos patógenos para el humano,  detectar mutaciones, comprobar paternidades, entre otras muchas aplicaciones que se llevan a cabo de una gran certeza y sensibilidad. Así, empleando la técnica de “Reacción en Cadena de la Polimerasa (PCR)” es posible obtener información del ADN empleando apenas 10 nanogramos –una cienmilésima parte de un miligramo– de material genético.   Las técnicas modernas de la biología molecular se realizan de manera rutinaria y están a disposición del público. Esto, no solamente en los países de primer mundo sino también en México, incluyendo a San Luis Potosí. La detección de enfermedades de transmisión sexual, del virus del papiloma humano y de mutaciones que generan enfermedades hematológicas, son algunas de las pruebas que ya se realizan por PCR en nuestra ciudad. En un futuro próximo podrá, además, llevarse a cabo la detección de mutaciones que producen cáncer de mama, cáncer de tiroides, tuberculosis e influenza. Al igual que en otras áreas de la ciencia y la tecnología, las aplicaciones de la genética quizá nos habrían parecido hace apenas algunas décadas un buen tema para una película de ciencia ficción. Estas aplicaciones son, sin embargo, reales y constituyen técnicas novedosas de diagnóstico que refuerzan las técnicas convencionales. En un futuro, podríamos quizá pensar en una medicina individualizada basada en la biología molecular y acorde con la información genética de cada persona. Cuando eso suceda veremos posiblemente un rápido incremento en la esperanza de vida de nuestra especie, tal como ocurrió a lo largo del siglo XX con la introducción de los antibióticos y las medidas de higiene.",
    "El pasado martes 7 de octubre se anunciaron los ganadores del premio Nobel de física 2014. La distinción en esta ocasión correspondió a los investigadores japoneses Isamu Akasaki de la Universidad Meiji, Hiroshi Amano de la Universidad de Nagoya, y Shuji Nakamura, nacionalizado estadounidense, de la Universidad de California, Santa Bárbara. El premio les fue concedido por el desarrollo de las lámparas LED de luz azul. Empleando estas lámparas es posible fabricar lámparas LED de luz blanca, las cuales prometen convertirse en las fuentes de iluminación del siglo XXI.  En 1879 Thomas Alva Edison desarrolló la primera lámpara incandescente que fue comercialmente viable y con esto inauguró la era de la luz eléctrica. No es difícil imaginar el impacto que la luz eléctrica tuvo en su momento en el mundo. Repentinamente, los altos niveles de  iluminación que pudieron conseguirse por medios artificiales posibilitaron la realización de actividades que antes eran exclusivas del día.  Así, la luz eléctrica trajo el día a la noche. Comúnmente se considera que la lámpara LED será para el siglo XXI lo que la lámpara incandescente fue para el siglo XX. La emergencia de la lámpara LED, no obstante, no ha sido tan espectacular como lo fue en su momento la aparición de la lámpara incandescente. Esto es debido a que, mientras que esta última sustituyó muy ventajosamente a fuentes de luz de tecnología añeja –como la vela de cera y la lámpara de gas o de aceite– la lámpara incandescente y la lámpara LED son ambas de naturaleza eléctrica. Así, el público en general no percibe a estos dos tipos de lámparas como fuentes de luz radicalmente diferentes.Y, sin embargo, lo son. En el caso de una lámpara incandescente la luz es generada haciendo circular una corriente eléctrica a través de un filamento metálico, mientras que en una lámpara LED la luz se genera haciendo pasar una corriente a través de un semiconductor, un material que es en varios sentidos similar al material con el que se fabrican los cerebros de las computadoras pero de una diferente composición química. En el caso de la lámpara incandescente la corriente eléctrica calienta el filamento y éste emite luz –se pone al rojo vivo o al rojo blanco según la temperatura que alcanza–. Un problema con la lámpara incandescente es que la mayor parte de la luz generada tiene la forma de radiación infrarroja que no podemos ver, y en consecuencia su eficiencia es notablemente baja. El LED, en contraste, no necesita que se eleve su temperatura para emitir luz –de hecho, se busca que durante su operación la temperatura se mantenga lo más baja posible– con lo que su eficiencia resulta varias veces más grande que la de su contraparte incandescente.Como sabemos, la pobre eficiencia de las lámparas incandescentes no fue obstáculo para su proliferación en el siglo XX. En la actualidad la situación ha cambiado, dados los problemas que enfrenta el mundo,  asociados al creciente consumo global de energía y a la contaminación ambiental que éste genera. En este punto hay que hacer notar que un cuarto de la energía eléctrica generada en el mundo se emplea en iluminación. La sustitución de fuentes de luz ineficientes por lámparas LED podría resultar entonces en un ahorro considerable de energía.    En el año de 1962 el investigador Nick Holonyak, que entonces fungía como científico asesor de la compañía General Electric, desarrolló lámparas LED de luz roja. Una década después otro grupo de investigación fabricó lámparas de luz verde. Una vez alcanzado este punto se buscó activamente el desarrollo de lámparas de color azul. Esto con el objeto de contar con fuentes de luz con los tres colores primarios, azul, verde y rojo, cuya mezcla produjera una lámpara de luz blanca. Las fuentes de luz azul, no obstante, eludieron la búsqueda de los científicos por tres décadas, hasta que los esfuerzos de Nakamura, Akasaki y Amano fueron recompensados con el éxito en la década de los años noventa.Si bien el otorgamiento del premio Nobel a los científicos japoneses ha sido ampliamente reconocido como merecido, Holonyak se siente ignorado por el comité Nobel en sus merecimientos como inventor de la primera lámpara LED de luz visible. En declaraciones a la prensa escrita, Holonyak se pregunta por qué el desarrollo del LED azul tiene valor Nobel mientras que el desarrollo del LED rojo, que ocurrió primero, no lo tiene.  Añade que el LED azul nunca hubiera sido desarrollado sin el trabajo que él y otros investigadores llevaron a cabo en los primeros años de la década de los sesenta.Aparentemente no habría razón para haber ignorado el trabajo de Holonyak y otros, llevado a cabo tres décadas antes que el desarrollo que produjo el premio Nobel. Aunque habría que conceder que si bien en muchos aspectos de la vida escoger entre el rojo y el azul es una cuestión de gustos, en el caso de las lámparas LED la cosa es un poco más complicada. Esto debido a que es posible fabricar una fuente de luz blanca empleando solamente un LED azul, lo que no se puede hacer con un LED rojo.Al margen de la controversia, lo que sí es muy probable es que el desarrollo de las lámparas LED azules tendrá un impacto público de gran magnitud, aunque quizá no tan espectacular como el que tuvo la lámpara incandescente hace más de un siglo.",
    "El pasado martes autoridades sanitarias de los Estados Unidos dieron a conocer el primer caso de infección por el virus del Ébola diagnosticado en los Estados Unidos. El diagnóstico correspondió a un hombre que había volado días antes de Liberia a Dallas, TexasComo sabemos, existe actualmente un brote epidémico de Ébola en varios países de África Occidental, incluidos Liberia, Guinea y Sierra Leona, que  ha resultado en más de 3,000 muertes. Se sabe que el virus de Ébola tuvo su origen en África y que es trasmitido de animales a humanos. Los primeros brotes detectados de Ébola ocurrieron en 1976 en Zaire (ahora República Democrática del Congo) y Sudán, y desde entonces se han dado con cierta regularidad.    La infección por el virus del Ébola tiene frecuentemente un desenlace fatal. Según la Organización Mundial de la Salud, en pasados brotes de Ébola murieron en promedio la mitad de los pacientes. Afortunadamente, el virus de esta enfermedad no es especialmente contagioso y para su transmisión se requiere de un contacto de persona a persona. Si el virus del Ébola pudiera llegar a transmitirse a través del aíre –como el virus de la gripe– sería ciertamente mucho más peligroso. No obstante, los expertos consideran que es improbable que este virus pueda mutar al grado de que se capaz de reproducirse en grandes cantidades en las vías respiratorias y posibilitar así su propagación a través del aire. No se espera, entonces, que el presente brote de Ébola se convierta en una pandemia que mate a millones de personas, como si lo fue la epidemia de gripe española que en el año 1918 provocó entre 50 y 100 millones de muertes. Y como también lo es la actual epidemia de SIDA/VIH, que surgió a la luz pública en la década de los años ochenta del siglo pasado y que ha afectado a 75 millones de personas en todo el mundo, habiendo muerto la mitad de ellas. Al igual que el virus de Ébola, el VIH se originó en África, y es precisamente en ese continente en donde vive alrededor del 70% del total de personas infectadas con este virus. En algunos países del sur de África la epidemia ha alcanzado proporciones dramáticas, como Botsuana y Zimbabue en donde el número de infectados con VIH supera el 25% del total de la población. Si bien el primer caso de SIDA diagnosticado como tal ocurrió en 1980, hace poco más de tres décadas, el inicio del proceso que llevó a la actual pandemia se remonta a un pasado más distante. ¿Qué tan distante?  Según un grupo internacional de investigadores encabezados por Oliver Pybus de la Universidad de Oxford en el Reino Unido y Philippe Lemey de la Universidad de Lovaina en Bélgica, la epidemia se inició hace unos noventa años en el África Central.  Las conclusiones anteriores están basadas en un estudio genético –publicado esta semana en la revista “Science” – de muestras de VIH procedentes de diferentes tiempos y lugares en la República Democrática del Congo (RDC) y en países vecinos en el centro del continente africano. Como parte de su estudio los investigadores siguieron las mutaciones  del virus a lo largo del tiempo y, yendo hacia atrás, determinaron su origen en tiempo y lugar, que resultaron ser la década de los años veinte del siglo pasado y la actual ciudad de Kinshasa, capital de la RDC.Un punto de interés para los epidemiólogos es determinar la causa o causas específicas que provocaron el inicio de  la epidemia de SIDA/VIH, precisamente en la ciudad de Kinshasa. En la década de los veintes, la RDC era una colonia belga. De acuerdo con Pybus, Lemey y colaboradores, un factor que contribuyó a disparar la epidemia fue el ferrocarril colonial que conectaba los principales centros urbanos del país con otras regiones de África y que permitió el desplazamiento de personas infectadas y consecuentemente la dispersión del virus. Consideran también que las condiciones de salud que imperaban en Kinshasa en la década de los años sesenta, así como el cambio en el comportamiento sexual de sus habitantes después de que el país alcanzó su independencia de Bélgica, pudieron igualmente haber influido en la propagación del VIH.Determinar las causas y condiciones que provocaron la epidemia de SIDA/VIH que padece el mundo –y que, sobre todo, padece el continente africano– es de interés fundamental para prevenir futuras epidemias. Las comunicaciones son obviamente un factor esencial para la dispersión de una infección a nivel global. Así, el enfermo de Ébola que viajó de Liberia a los Estados Unidos no podría haberlo hecho hace un siglo cuando el viaje por barco entre continentes tomaba una semana –se hubiera muerto en el intento, pues el Ébola es una enfermedad de rápida evolución, aunque probablemente antes de hacerlo hubiera infectado a otros pasajeros.Al margen de esto último, lo que no deja de sorprender es que Pybus, Lemey y colaboradores hayan logrado averiguar el lugar y el tiempo en que empezó la epidemia de SIDA/VIH, practicando una especie de arqueología genética.",
    "El pasado martes 23 de septiembre la India logró poner en órbita alrededor del planeta Marte un satélite artificial –bautizado como Mangalyaan–, algo que solamente los Estados Unidos, la antigua Unión Soviética y la Agencia Espacial Europea habían conseguido realizar. Otros dos países asiáticos, Japón y China, intentaron sin éxito realizar misiones a Marte –Japón en 2003 y China en 2011–, lo que añade notoriedad al logro de la India y pone a este país por delante de sus competidores asiáticos.Conseguir que una sonda lanzada desde la Tierra se inserte en una órbita alrededor de un planeta distante en promedio 225 millones de kilómetros –en una especie de tiro al blanco cósmico– no parece ser una tarea sencilla, aun para los ingenieros espaciales más capacitados. Y efectivamente no lo es, como lo prueba el hecho que la mitad de las misiones a Marte de una u otra manera  hayan terminado en fracaso. En los inicios de la exploración de Marte las misiones fallidas fueron frecuentes. Así, los Estados Unidos lograron en 1965  hacer que una nave, el Mariner 4,  sobrevolara Marte –a una altura de 10,000 kilómetros– sólo hasta su segundo intento. Por su parte, la Unión Soviética fracaso, en mayor o menor medida, en los ocho intentos que hizo para acercarse a Marte en la década de los años sesenta.    El Mariner 9 de la NASA fue la primera sonda que se logró poner en órbita alrededor de Marte. Esto ocurrió el 13 noviembre de 1971. La agencia espacial estadounidense tuvo éxito hasta su segundo intento, después de acumular una experiencia de cinco misiones de acercamiento a Marte, algunas exitosas y otras fallidas. Dos semanas después del logro estadounidense, la Unión Soviética hizo lo propio y colocó a la sonda Mars 2 en órbita marciana. Para esto tuvo que acumular la experiencia de 9 misiones a Marte, todas fallidas.   La India, en cambio, tuvo éxito con el Mangalyaan al primer intento. Y, por si algo faltara, la misión tuvo un costo de sólo 74 millones de dólares, que si bien es una cantidad de dinero nada despreciable, es apenas una fracción de lo que han costado otras misiones de acercamiento a Marte. La sonda norteamericana MAVEN, por ejemplo, que entró en órbita marciana casi simultáneamente con Mangalyaan, tuvo un costo de 671 millones de dólares; esto es, casi diez veces más alto que el de la sonda hindú. De la misma manera, la sonda europea Mars Express, que ha estado orbitando a Marte desde diciembre del 2003, tuvo un costo unas cinco veces superior al de Mangalyaan.Se ha señalado también de manera irónica que la misión Mangalyaan tuvo un costo menor al de la película “Gravity”, que contó con un presupuesto de 100 millones de dólares.¿Cómo hicieron los indios para llevar a cabo una misión de tanta complejidad con un presupuesto tan bajo? Según Kopillil Radhakrishnan, director de la agencia espacial india en entrevista a la revista Forbes, varios factores contribuyeron a que esto se diera. Uno de ellos es lo que llama una “acercamiento modular”, mediante el cual los indios desarrollaron la tecnología necesaria para la misión adaptando y mejorando tecnologías existentes. En el caso del motor del cohete de lanzamiento, por ejemplo, a partir de tecnología francesa que adquirieron en los años setentas. El mismo acercamiento modular fue empleando para el desarrollo de los instrumentos científicos a bordo de la sonda.Otra estrategia para reducir costos fue la de limitar pruebas en tierra, extrayendo el máximo de cada una de las llevadas a cabo. Igualmente, desarrollaron estrategias para reducir el consumo de combustible necesario para llevar la sonda desde la órbita terrestre hasta la órbita marciana.Señala también que la calendarización y ejecución de actividades se llevó a un extremo y que desde el anuncio de la misión por el Primer ministro indio y el inicio de la misma solamente transcurrieron 15 meses. Y en este punto menciona que mientras que en algunas partes de Europa, incluso los científicos espaciales trabajan semanas de 35 horas, para ellos eran comunes los días de 18 horas de actividad, además de que durante de periodo de lanzamiento muchos de sus científicos trabajaron 20 horas diarias.A pesar de su relativo bajo costo y espectacularidad, la misión india a Marte ha provocado críticas en el sentido que habría sido preferible que la India hubiera invertido los 74 millones de dólares que costó Mangalyaan en paliar los enormes problemas de pobreza que padece el país. Una editorial del periódico “The New Indian Express”, sin embargo, está en desacuerdo y afirma que “La construcción y lanzamiento de satélites es un negocio multimillonario el cual ha sido monopolizado hasta ahora por algunas naciones occidentales. La India ha demostrado su capacidad para construir satélites bajo demanda lo mismo que para ponerlos en el espacio a la altura requerida….La India es capaz de proporcionar estos servicios a una fracción de lo que costaría hacerlo en otros lados. El éxito del Mangalyaan ayudará a la India a obtener una mayor porción del negocio de construir y lanzar satélites al espacio”.El éxito de la sonda Mangalyaan muestra como la organización, el ingenio y la dedicación al trabajo, como sustitutos de cantidades masivas de dinero, pueden resolver problemas tecnológicos complejos. Muestra también que, de proponérselo, la India es un tirador de gran puntería que donde pone el ojo pone la bala.",
    "En el año 1960 vivían en el planeta alrededor de 3,000 millones de personas. Cuarenta años después esta cifra se había más que duplicado alcanzando casi 6,100 millones. Si bien a tasas menores que en la segunda mitad del siglo XX, el rápido crecimiento de la población del mundo se ha prolongado en lo que va de este siglo, y hoy en día habitamos este planeta unos 7,200 millones de seres humanos. La velocidad con que ha crecido la población mundial tuvo un máximo en los años sesentas, cuando alcanzo una tasa de alrededor de 2.2 % anual. A partir de entonces, dicha tasa ha disminuido paulatinamente con los años en la medida en que se ha difundido el uso de los anticonceptivos y se ha incrementado en nivel de educación de las mujeres. En la actualidad la tasa de crecimiento ronda al 1.1% anual. Dada la reducción del ritmo de crecimiento poblacional a lo largo de la segunda mitad del siglo XX, algunos expertos han considerado que la población global alcanzará un máximo al mediar el siglo XXI, punto a partir del cual descenderá de modo paulatino. Este máximo sería de alrededor de 9,000 millones de personas. Un artículo aparecido esta semana en la revista “Science”, sin embargo, está en desacuerdo y, por el contrario, concluye que el presente siglo probablemente no verá una estabilización en el crecimiento de la población que continuaría su ritmo ascendente por el resto de la centuria. Dicho artículo fue publicado por un grupo internacional de investigadores, encabezado por Patrick Gerland del Departamento de Asuntos Económicos y Sociales de las Naciones Unidas en Nueva York.    Gerland y colaboradores basan sus conclusiones en un análisis estadístico de datos poblacionales recientemente dados a conocer por las Naciones Unidas. Dicho análisis arroja que la población de mundo crecerá de manera paulatina en las próximas décadas hasta alcanzar una cifra entre 9,000  y 13,200 millones de personas en el año 2100, con una media de 10,900 millones. Este crecimiento, sin embargo, no será el mismo para todos los continentes. De hecho, Europa en realidad disminuirá su población, mientras que en Asia –que concentra en la actualidad más del 50% del total de habitantes del planeta– la población alcanzará su máximo alrededor del año 2050 y después empezará a descender. Igualmente, el número de habitantes de América Latina alcanzará un máximo en la segunda mitad del siglo XXI antes de iniciar su descenso. El continente que hace la diferencia es África, que tiene en la actualidad altas tasas de natalidad, comparables a las que tuvieron Asia y América Latina en los años sesentas. Estas tasas, si bien están declinando, no lo hacen a un ritmo suficientemente rápido. Gerland y colaboradores predicen que África elevará su población de los 1,000 millones de habitantes actuales a una cifra entre 3,100 y 5,700 millones en 2100De esta manera, en un siglo el continente africano podría competir con Asia en población. De hecho, Nigeria, que es el país africano más populoso, podría pasar, según Gerland y colaboradores, de los 160 millones actuales a 914 millones de habitantes en el año 2100, y aquí vale la pena señalar que Nigeria tiene una extensión territorial que es apenas la mitad de la extensión de México. De este modo, la población anticipada para Nigeria  se acerca a las actuales de China y la India, que son los países más poblados hoy en día.De resultar acertadas las predicciones de Gerland y colaboradores, el mundo se encontraría en el año 2100 con algunos miles de millones de habitantes más que los que se proyectan en la actualidad para ese año. Como apuntan los investigadores, esto tendría grandes implicaciones en materia económica, medioambiental, de salud pública, de alimentación y de educación, entre otras, y necesariamente requerirá de la implementación de políticas a nivel global para atenderlas.  Por otro lado, las proyecciones de Gerland y colaboradores son un intento para predecir el futuro y por tanto son inciertas y sujetas a debate. Hay que notar, no obstante –y guardando  la debida proporción–, que hay al menos un ámbito, la ciencia del clima, en los que en los últimos años hemos desarrollado una notable capacidad de predicción. En efecto, todavía no hace mucho tiempo las predicciones del clima eran motivo de burlas y chascarrillos. Hoy podemos saber con una certidumbre razonable si el día de mañana o pasado mañana –e incluso con mayor anticipación– tendremos un tiempo soleado o lluvioso. Habría que esperar para averiguar si las predicciones de los expertos en materia de población pueden llegar a ser tan acertadas como lo son nuestras predicciones del clima. En lo que sí parece que hay una mayor certidumbre es que en las próximas décadas la población europea se abatirá por un descenso en su tasa de natalidad. De alguna manera Europa sería así víctima de su propio éxito, habida cuenta que las bajas tasas de natalidad están aparejadas con un mayor bienestar económico y protagonismo de las mujeres en la sociedad.",
    "El pasado día 11 de septiembre, la NASA anunció que el explorador Curiosity había finalmente  alcanzado la base de la montaña marciana Aeolis Mons, de cinco kilómetros de altitud, después de un recorrido que le tomó dos años. Como recordamos, el explorador Curiosity –con un tamaño comparable al de un automóvil compacto y un peso cercano a una tonelada–  arribó al cráter Gale en Marte el 6 de agosto de 2012 mediante una espectacular maniobra que lo posó suavemente sobre la superficie del planeta. Curiosity tenía como una primera meta llegar hasta la montaña Aeolis Mons, distante unos 6 kilómetros de su punto de aterrizaje. El explorador lleva a bordo un conjunto de sofisticados instrumentos para el estudio científico de Marte, incluyendo cámaras fotográficas, equipos para análisis químicos, detectores de radiación y una estación meteorológica. Está igualmente equipado con un taladro para perforar rocas y extraer muestras con el fin de determinar su composición química. Entre sus objetivos declarados se encuentra el de averiguar si alguna vez existieron en Marte las condiciones necesarias para albergar vida.Con el arribo a la montaña Aeolis Mons, el explorador Curiosity completó la fase primaria de su misión que, entre otros resultados, arroja que en algún momento de su historia geológica en Marte existieron condiciones para la existencia de vida. En estos momentos, Curiosity se apresta a iniciar la segunda etapa de su misión en la que ascenderá 8 kilómetros por las laderas de la montaña, con el fin de escudriñar el pasado geológico del planeta y localizar hábitats en los que pudieran preservarse compuestos orgánicos.John P. Grotzinger, quien es el investigador principal del proyecto Curiosity, destaca los logros científicos que se han alcanzado en la primera etapa de la misión y se muestra entusiasmado con los resultados que espera se obtendrán en la segunda etapa. Un panel de  expertos contratados por la NASA, encabezados por Clive Neal de la Universidad de Notre Dame en Indiana, Estados Unidos, sin embargo, no comparte este entusiasmo. No lo comparte ni con respecto a los logros científicos alcanzados, ni con relación a los estudios planteados en la etapa de continuación de la misión.  El panel de expertos tuvo la encomienda de evaluar la extensión de un grupo de siete proyectos de la NASA que incluye, además de Curiosity, a la sonda Cassini, en órbita alrededor del planeta Saturno desde hace diez años, y al explorador Opportunity, que ha estado explorando la superficie de Marte desde su arribo a la misma en enero de 2004. Si bien el panel calificó a la misión Curiosity como “Muy buena/Buena”, esta calificación resulta engañosa si la comparamos con las calificaciones de “Excelente”  y ”Excelente/Muy bueno” otorgadas a Cassini y a Opportunity, de manera respectiva. De hecho, en calificación Curiosity solo superó al proyecto Mars Express de la Agencia Espacial Europea, en el que la NASA tiene una participación asociada. De acuerdo con el panel revisor, la actividad científica realizada por el Curiosity desde su arribo a la superficie marciana no ha sido lo intensa que pudiera haberse esperado. En particular, hace notar que en los dos años de operación desde su aterrizaje en Marte el Curiosity solamente ha realizado cinco perforaciones del suelo marciano y que en los próximos dos años sólo se plantea llevar a cabo ocho más. Así, los operadores del explorador estarían más preocupados en desplazar al Curiosity sobre el suelo de Marte que en la ciencia que pueda aportar. El panel encuentra, asimismo, que no es claro si los objetivos de la misión en su primera etapa has sido cumplidos y que incluso no es claro cuales fueron estos objetivos. De manera similar, el panel afirma que los objetivos fijados para la segunda etapa son imprecisos.Para hacer las cosas peores, Grotzinger, a pesar de ser el investigador principal de la misión, no asistió a las sesiones del panel para contestar preguntas y aclarar dudas. En estas circunstancias, en carta oficial dirigida a  James Green, Director de la División de Ciencia Planetaria de la NASA, Clive Neal escribe que la inasistencia de Grotzinger “dejó al panel con la impresión de que el equipo del Curiosity siente que ellos son demasiado grandes para fracasar y que era suficiente enviar a cualquier persona a las reuniones”. Como conclusión, el panel urge a la NASA a realizar las adecuaciones necesarias para que “el equipo del Curiosity se enfoque en maximizar la realización de ciencia de alta calidad que justifique la capacidad y la inversión de capital llevada a cabo en el explorador”. Asumiendo que a los miembros del panel de expertos les asiste la razón, no deja de sorprender que un proyecto en el que se invirtieron 2,500 millones de dólares  –y  que se inició con los mejores augurios– pueda ser motivo de cuestionamientos tan severos como los que enfrenta Curiosity. Y, sin embargo, a pesar de los duros calificativos que aplicó, el panel de expertos aprobó un presupuesto de 60 millones de dólares para la extensión de la misión por dos años más.",
    "En un artículo publicado el pasado 4 de septiembre en la revista en línea “Scientific Reports”, el cual fue comentado ampliamente por la prensa, se describe el hallazgo en la Patagonia argentina de los restos fósiles de un gigantesco dinosaurio herbívoro que vivió hace unos 66-84 millones de años. El artículo fue publicado por un grupo de investigadores de los Estados Unidos, Argentina e Inglaterra, encabezados por Kenneth Lacovara de Drexel University, Filadelfia, Pensilvania.  El recién descubierto dinosaurio medía unos 26 metros de cabeza a cola y pesaba alrededor de 60 toneladas. Para poner este último número en perspectiva, se puede mencionar que es unas nueve veces el peso del tiranosaurio rex, y que es mayor que el peso de un jet Boeing 737 con capacidad para más de 150 pasajeros. El dinosaurio ha sido bautizado “dreadnoughtus”, que se traduce como “sin temor”, en referencia a que su enorme tamaño lo hacía inatacable por cualquier predador.Lo más transcendente, sin embargo, es que los investigadores rescataron alrededor del 45% de los huesos del dinosaurio, lo que permite reconstruir con certidumbre un 70% del total de su esqueleto. Frecuentemente los paleontólogos han tenido que reconstruir el aspecto que tendría el dinosaurio sobre la base de unos pocos huesos fosilizados. Según Lacovara y colaboradores, el “dreadnoughtus” es el dinosaurio más grande jamás descubierto del que se puede tener certidumbre de su tamaño.Hoy sabemos que en el pasado remoto existieron animales, en particular dinosaurios de enorme tamaño como el “dreadnoughtus”, que se han extinguido con el paso del tiempo. Esto lo averiguamos gracias a la labor de investigadores que en los primeros tiempos trabajaron con muy poca información.Unos de los pioneros en el estudio de los dinosaurios fue el médico inglés Gideon Mantell, quien era además geólogo y paleontólogo aficionado. En 1822, juntamente con su esposa, descubrió cerca del pueblo de Cuckfield al sur de Londres, un diente similar a los de las iguanas pero considerablemente más grande. Con esta escasa evidencia Mantell conjeturó que el diente perteneció a un lagarto de grandes proporciones. Con la ayuda de restos fósiles encontrados posteriormente Mantell se convenció de que estaba en lo cierto y bautizo al lagarto con el nombre de iguanodón.Si bien Mantell logró convencer a los expertos sobre lo acertado de sus conclusiones, en la reconstrucción esquelética y artística del iguanodón, cometió un error curioso pues colocó el hueso del pulgar del dinosaurio sobre su nariz, con lo que el modelo adquirió un aire de rinoceronte. Este error se propagó a lo largo de varias décadas, incluso después de la muerte de Mantell. En efecto, en 1851, como parte de la exposición industrial mundial celebrada en el Crystal Palace de Londres, se preparó una exhibición sobre dinosaurios en la que se hizo uso de todo el conocimiento científico que sobre el tema se disponía en la época. Para este propósito se construyeron modelos de dinosaurios en cemento como se pensaba que lucían en vida y en particular el iguanodón apareció con su cuerno ficticio en la nariz. Sólo años después, cuando se encontraron fósiles de iguanodón más completos, fue que los paleontólogos se dieron cuenta del error cometido y el dinosaurio perdió su cuerno nasal en las discusiones científicas. No fue el caso, sin embargo, de los modelos de dinosaurios de Crystal Palace que fueron conservados con todos sus errores, tal como originalmente fueron concebidos. Hoy en día, dichos modelos pueden ser vistos en una exhibición permanente en el parque Crystal Palace en Londres.La historia del iguanodón nos muestra como progresa el conocimiento científico. Basado en la limitada información que tenía sobre su dinosaurio, Mantell aventuró una hipótesis en relación a su aspecto en vida, mismo que en su momento fue aceptado por los expertos. Al disponerse de mejores restos fósiles, no obstante, se puso en evidencia el error y se desechó el cuerno nasal, mismo que hoy en día  queda plasmado solamente en el recuerdo anecdótico y en esculturas que se han conservado por razones históricas; razones que nada tienen que ver con su exactitud científica.    Exactitud que es notable en el caso del “dreadnoughtus” recientemente descubierto en la Patagonia ya que se dispone de buena parte de su esqueleto. La cual, sin embargo, no es absoluta –como todo lo relativo al conocimiento científico– y por lo mismo puede llevar a errores de interpretación. Aunque seguramente no tan grandes que lleven a poner un hueso de la cola del dinosaurio en su nariz o viceversa.",
    "Pudimos enterarnos esta semana por la prensa que el próximo 12 de septiembre proveniente de la estación espacial internacional aterrizará en algún lugar de Kazajistán en Asia Central una muestra de whiskey escocés puesto en órbita hace tres años. El experimento es llevado a cabo por la destilería escocesa Ardbeg con el fin de estudiar cómo afectan las condiciones de micro gravedad del espacio al proceso de maduración del whiskey. Consultando la página de Ardbeg no se encuentran mayores detalles acerca del experimento, más allá de que se pretende estudiar la interacción de ciertas moléculas orgánicas con partículas de roble tostado y que los resultados obtenidos con la muestra en el espacio se compararán con los obtenidos con muestra similar que ha permanecido en tierra. De acuerdo con Bill Lumsden de la compañía Ardbeg, los experimentos tienen la pretensión de “desentrañar los misterios del proceso de maduración del whiskey”, mismos que, parafraseando a Neil Armstrong, “constituirán un pequeño paso para el hombre pero uno gigantesco para el whiskey”.     Las condiciones de micro gravedad que prevalecen en el espacio han inspirado experimentos de manufactura de diversos objetos y materiales. En ausencia de gravedad, por ejemplo, es posible fabricar esferas metálicas perfectas con diámetros uniformes para ser usadas como balines en rodamientos o baleros. Excepto en casos particulares, sin embargo, la manufactura en el espacio resulta demasiado cara y no ha resultado económicamente viable. Esto último es debido a los elevados costos para llevar un objeto a una órbita terrestre, el cual es de miles de dólares –cuando no decenas de miles de dólares– por kilogramo. Este costo refleja la energía necesaria para elevar un objeto desde la superficie de la Tierra hasta una órbita de algunos cientos de kilómetros en contra de la fuerza de gravitación de nuestro planeta. Para este propósito es necesario acelerar dicho objeto hasta una velocidad superior a los 25,000 kilómetros por hora. Esta situación podría cambiar en la medida que se reduzcan los costos para colocar cargas en órbita y hay quien anticipa que en el futuro veremos la instalación de fábricas en el espacio, a cientos de kilómetros sobre nuestras cabezas e incluso más allá. En este sentido vale comentar que en la carrera espacial, además de las agencias espaciales de varios países, participan también compañías privadas estadounidenses. Una de estas, SpaceX, anticipa que en el futuro podría reducir hasta unos 1000 dólares el costo por kilogramo de carga puesto en órbita.Otro factor que igualmente podría impulsar el desarrollo de fábricas en el espacio es la minería de asteroides. De acuerdo con los entusiastas de este campo, los asteroides que giran alrededor del sol en órbitas no demasiado alejadas de nuestro planeta son una potencial fuente de materias primas, incluyendo agua. Estas materias primas serían susceptibles de explotación y requerirían una energía relativamente pequeña para transportarlas desde el asteroide hasta una hipotética fábrica que gire en órbita alrededor de nuestro planeta es relativamente pequeña. Esto último debido a la débil fuerza gravitatoria del asteroide.   Además de lo anterior, el agua extraída de los asteroides podría ser descompuesta por medio de la luz solar en los elementos químicos hidrógeno y oxígeno, los cuales pueden ser empleados como combustible para impulsar el cohete que transporte a los minerales desde la mina hasta su destino final.Vista así, la minería de asteroides resulta ser una opción atractiva para proveer de materias primas a futuras fábricas en el espacio. Lo es excepto por un punto obvio: extraer minerales de una mina localizada a decenas de millones de kilómetros de nuestro planeta en condiciones hostiles es considerablemente más difícil que hacerlo en una mina terrestre. Con relación a esto, hay que recordar las grandes dificultades que experimentó la sonda japonesa Hayabusha para posarse sobre el asteroide Itokawa en el año 2005 y traer de regreso a la Tierra muestras de polvo del asteroide. Aun así, existen compañías privadas como Deep Space Industries y Planetary Resources interesadas en la minería de asteroides.  ¿Veremos algún día la explotación comercial de los asteroides? No todo el mundo está de acuerdo en que esto será factible. Martin Elvis de Harvard University, por ejemplo, en un artículo que será publicado en la revista “Planetary and Space Science” encuentra que hay muy pocos asteroides cuya explotación podría ser económicamente viable, conclusión con la que se mostró en desacuerdo, como es natural, el cofundador de Planetary Resources.A más de medio siglo del inicio de la era espacial la exploración del espacio conserva un atractivo público considerable que ha sido empleado para lanzar proyectos controvertidos, uno de los cuales es precisamente el de la minería de asteroides. El espacio ha servido igualmente, de manera poco elegante, como marco publicitario para anunciar alcohol. A menos que  Ardbeg nos de una sorpresa y por medio de sus experimentos produzca un whiskey con un sabor más allá de este mundo.",
    "Como ha sido ampliamente difundido por la prensa, el pasado 6 de agosto una represa de la mina Buenavista en Cananea, Sonora, derramó un volumen de 40,000 metros cúbicos de líquido en el río Bacanuchi, afluente del río Sonora. Dicho líquido era una mezcla de ácido sulfúrico y sulfato de cobre, además de otros metales pesados, incluyendo cadmio, mercurio y plomo. Con este derrame, los ríos adquirieron una coloración como de “salsa molcajeteada”, según fue descrito en la prensa en días pasados. A más de dos semanas del derrame el problema no se ha superado, y según un comunicado emitido el pasado viernes por la CONAGUA, existen todavía en el río Sonora concentraciones de arsénico, cadmio, cobre, cromo y mercurio en la parte media y baja de la cuenca, con picos que rebasan los límites máximos permisibles establecidos por la norma.  El cadmio –que es tóxico en muy bajas concentraciones– fue el causante de la enfermedad “Itai-itai” aparecida en las primeras décadas del siglo XX en la cuenca del río Jinzugawa en la prefectura de Toyama en el Japón. El cadmio fue vertido en el rio Takaharagawa, afluente del  Jinzugawa, por la mina Kamioka, operada por la compañía Mitsui Mining and Smelting. Una vez en el río, el cadmio fue ingerido por los peces que sirvieron de alimento a los habitantes de la región que así resultaron intoxicados. Una segunda vía de envenenamiento fue el arroz cultivado en las inmediaciones de los ríos contaminados y regado por medio del agua de los mismos. El envenenamiento por cadmio produce daños renales y ablandamiento de los huesos. Según la página web del “Museo de la Enfermedad Itai-itai” de la prefectura de Toyama, dicha enfermedad “empieza con un dolor en la espalda baja, los hombros y las rodillas. Cuando los síntomas llegan a ser más serios, se producen roturas repetidas de huesos hasta que la víctima es incapaz de moverse y es finalmente confinada a la cama con dolores atacando todo el cuerpo”. Estos dolores son los que han dado al padecimiento el nombre Itai-itai, que en japonés significa duele-duele.      Otro de los metales contenidos en el derrame de Cananea, el mercurio, no se queda atrás en cuanto a efectos tóxicos, particularmente en la forma de metilmercurio, compuesto que afecta al sistema nervioso central. Un caso famoso de envenenamiento masivo por mercurio ocurrió en la primera mitad del siglo pasado en Minamata, un pueblo de pesadores en la isla Kyushu del archipiélago japonés. El envenenamiento fue provocado por las descargas de mercurio por parte de la compañía Chisso en las aguas de la bahía de Minamata, en donde fue absorbido por peces y mariscos y eventualmente por los pescadores que los consumieron. Según la Wikipedia, hasta el año 2001 habían sido oficialmente reconocidas 2,265 víctimas de la enfermedad de Minamata.Sabemos que la contaminación del medio ambiente se ha acelerado a raíz del inicio de la Revolución industrial. En el caso de algunos contaminantes como el dióxido de carbono, tenemos información detallada de su evolución en la atmósfera y en los océanos a lo largo de los años. Esto, sin embargo, no es el caso general y en particular no lo es para el mercurio que es un contaminante de mucha relevancia para nosotros por su presencia en los productos alimenticios de origen marino que forman parte de nuestra dieta. En un artículo publicado el 7 de agosto pasado en la revista “Nature” se describe un estudio que tuvo por finalidad superar esta deficiencia. El artículo fue publicado por un grupo de investigadores en instituciones de los Estados Unidos y Europa, encabezado por Carl Lamborg de la Wood Hole Ocenaographic Institution. A través del estudio de muestras de agua recogidas a varias profundidades marinas, algunas por debajo de los 1000 metros, Lamborg y colaboradores llegaron a la conclusión de que la concentración de mercurio en aguas oceánicas a profundidades menores a 100 metros se ha triplicado desde el inicio de la Revolución industrial. Tomando a los océanos como un todo, dicha concentración se ha incrementado en 10% sobre los niveles pre-industriales. Así, la contaminación por mercurio, al igual que por otros contaminantes, ha crecido producto de nuestro desarrollo industrial.  A más de medio siglo de los desastres de Toyama y Minamata no esperaríamos que ocurrieran otros de similar magnitud; en particular, no se esperaría que esto ocurriera con el derrame actual del río Sonora. Lo cual no implica, por supuesto, que no estemos expuestos a contaminantes de todo tipo. Como tampoco implica que aquellas empresas generadoras de contaminantes se tomen todo el cuidado necesario para evitar nuestra exposición a los mismos, ni que asuman espontáneamente su responsabilidad en caso de un accidente. La compañía Chisso, por ejemplo, trató de ocultar de muchas maneras su involucramiento en el caso Minamata y, por declaraciones oficiales, tal parece que lo mismo ocurre en el caso Cananea.Y de alguna manera la estrategia funciona. Así, Chisso logró sortear la crisis y aun opera como fabricante de productos químicos. Incluso, en su página de internet se puede leer “Nosotros hemos operado la manufactura y la venta de productos químicos y contribuido a que las personas tengan un estilo de vida confortable desde nuestro establecimiento en 1906”.",
    "En su libro de viajes “The innocents abroad” publicado en 1869, el  escritor norteamericano Mark Twain escribe que durante su viaje a Egipto le hicieron saber que en ese país los trenes empleaban como combustible momias con una antigüedad de tres mil años, y que para este propósito las momias se compraban por toneladas o por cementerios completos.  Mark Twain afirma que esto se lo dieron por un hecho y que él estuvo dispuesto a creerlo. Lo anterior, si bien se toma como una broma del escritor, refleja la abundancia de momias egipcias, resultado de que en el Antiguo Egipto, hace miles de años, era costumbre momificar a los muertos con el fin de preservar sus cuerpos para la eternidad. Esto, con el objeto de que pudieran ser ocupados por el alma durante la vida después de la muerte terrenal. Dado que los procesos de momificación tenían un costo que podía ser elevado, el éxito alcanzado en esta empresa dependió de las posibilidades económicas de cada persona. Así la realeza y las clases altas lograron preservar sus cuerpos de manera asombrosa, mientras que aquellos con menores posibilidades económicas tuvieron que conformarse con procedimientos de preservación menos sofisticados.Aun así, el número de cuerpos momificados fue muy grande. En una entrevista al programa de televisión NOVA del “Public Brodcasting Services” de los Estados Unidos, la egiptóloga Salima Ikram de la Universidad Americana del Cairo considera que más de 70 millones de personas fueron momificadas en el Antiguo Egipto a lo largo de 3000 años.  En el Antiguo Egipto se momificaron también animales de manera masiva, incluyendo gatos, perros y aves, con el objeto de que acompañaran a sus dueños o les sirvieran de alimento en la otra vida. En la página electrónica del Museo Británico, por ejemplo, se asienta que unas 180,000 momias de gatos fueron llevadas a Gran Bretaña al final de siglo XIX. En este caso, para ser utilizados en la fabricación de fertilizantes.Para momificar un cuerpo de manera artificial los egipcios extraían primeramente los órganos internos, incluido el cerebro y con la excepción del corazón. El cuerpo desprovisto de órganos era subsecuentemente sometido a un proceso de deshidratación por varias semanas empleando una sal llamada natrón, al final del cual era lavado y cubierto de aceites aromáticos. Como paso final, el cuerpo era envuelto en vendajes impregnados de resinas con propiedades bactericidas. Con esto último, la momia adquiría la apariencia que nos es familiar.Los expertos han considerado que estas técnicas de momificación empezaron a desarrollarse en Egipto unos 2500 años antes de nuestra era, y que las momias anteriores a esta época que se han encontrado, se generaron de manera natural por la deshidratación de los cuerpos en contacto con la arena seca y cálida del desierto egipcio. Un artículo aparecido esta semana en la revista electrónica PlosOne, no obstante, desmiente esta creencia. Encuentra, por el contrario, que los orígenes del proceso de momificación –que alcanzarían su máximo de sofisticación durante el Imperio Nuevo entre los años 1500 y 1000 antes de nuestra era– tuvieron su origen mas de mil años antes de lo que se han asumido los expertos. El artículo fue publicado por un grupo de investigadores en Australia y el Reino Unido, encabezados por Jana Jones de Macquarie University en Sidney, Australia.  La investigación fue llevada a cabo con vendajes de momias que fueron encontrados en Egipto durante las primeras décadas del siglo XIX y que se encuentran almacenadas en el Bolton Museum en el Reino Unido. Mediante análisis químicos empleando técnicas de gran sofisticación, Jones y colaboradores encontraron resinas y otros componentes químicos en los vendajes, iguales y en proporciones similares a los que se emplearon en el cenit de la era de los faraones 3000 años después. El inicio de las técnicas de embalsamamiento en Egipto se remontaría así a unos 3500 años antes de nuestra era.La civilización egipcia y en particular las momias que produjo nos resultan de un gran atractivo.  Es interesante recordar, por ejemplo, que en la Inglaterra del siglo XIX se puso de moda organizar fiestas cuya principal atracción era desenvolver una momia traída desde Egipto, a lo cual seguían champaña y canapés. Hay que recordar, igualmente, que las momias han sido personajes centrales en películas de horror de gran éxito.   Pero quizá lo más impactante de una momia egipcia –más allá de entretenimientos frívolos– radica en su antigüedad, particularmente en la posibilidad de observar la cara y el cuerpo de una persona que vivió hace miles de años –por más que de manera deliberada hayamos destruido miles de momias, entre otras las que presumía Mark Twain–. Y aquí habremos de agradecer a los egipcios porque se preocuparon por desarrollar técnicas para la preservación de cuerpos momificados, si no para la eternidad como pretendían, sí para muchos años. Que después del trabajo de Jones y colaboradores, parece ser que son, hasta ahora, como 6500.",
    "Sabemos que los cometas se mueven alrededor del Sol con órbitas excéntricas que de manera periódica los acercan y alejan de nuestro planeta. Al acercarse a la Tierra un cometa puede llegar a ser visible a simple vista si su tamaño es lo suficientemente grande, ofreciendo en ocasiones espectáculos dignos de la mejor audiencia. Esto es frecuente con el cometa Halley, que aparece en el cielo cada 76 años en promedio, la última vez en 1986 –aunque hay que notar que, para decepción de nuestra generación, esta última visita resultó la peor en 2,000 años desde el punto de vista del espectáculo ofrecido. En tiempos antiguos la aparición de un cometa se veía como un suceso extraordinario que contrastaba con el movimiento periódico y predecible de los planetas. Esto llevó a considerar que los cometas eran precursores de acontecimientos igualmente extraordinarios, incluyendo guerras y epidemias, entre otras calamidades. No es sorprendente que la aparición de un cometa haya parecido un acontecimiento impredecible si consideramos que hay cometas con tiempos de revolución alrededor del Sol tan largos que han sido avistados solamente una vez a lo largo de la historia. Incluso en el caso del cometa Halley, que tiene un tiempo de revolución relativamente corto, su periodicidad no fue establecida sino hasta el siglo XVIII. En efecto, tocó a Edmund Halley –no confundir con el Haley del grupo de rock and roll de los años cincuenta “Bill Haley y sus Cometas”– descubrir que el cometa que lleva su nombre es un objeto que orbita al Sol describiendo una elipse. Haciendo uso de la teoría de gravitación universal que hacía poco había desarrollado Isaac Newton, lo mismo que de observaciones de las posiciones de los cometas avistados en 1531, 1607 y 1682, Halley llegó a la conclusión que éstos correspondían a un solo objeto y predijo un nuevo avistamiento en 1758. Esto último resultó acertado, y con esto los cometas perdieron su carácter mágico como precursores de desastres y calamidades, y entraron a la categoría de objetos naturales cuyas visitas sucesivas podían ser predichas con décadas de anticipación. Uno de los tantos cometas ahora conocidos, gracias al trabajo pionero de Halley, es el Churyumov-Gerasimenko descubierto en 1969, el cual tiene un periodo orbital de 6.5 años. Este cometa hizo noticia en días pasados debido a que será objeto de un estudio científico de un año de duración por medio de la sonda Rosetta de la European Space Agency (ESA). El estudio tendrá un alcance sin precedentes en cuanto al conocimiento que se espera obtener acerca de los cometas. La sonda Rosetta fue lanzada hace diez años y después de un largo viaje en el que dio cuatro vueltas al Sol, se encontró el pasado de 6 agosto con el cometa Churyumov-Gerasimenko en un punto a unos 400 millones de kilómetros de la Tierra, a medio camino entre las órbitas de  Marte y Júpiter. En estos momentos, el cometa y la sonda viajan  juntos –aunque separados por unos cien kilómetros– acercándose al Sol a una velocidad de 55,000 kilómetros por hora.Las fotografías del cometa que la sonda ha enviado a la Tierra muestran que éste tiene una forma muy irregular y rugosa, formado por dos partes bien diferenciadas unidas por un “cuello” que le dan el aspecto de un pato. De acuerdo con la página de internet de la ESA, Rosetta entrará en las próximas seis semanas en órbita alrededor del cometa y lo acompañará a lo largo de un año en la medida que se acerca al Sol. Durante  el viaje, Rosetta estudiará el proceso mediante el cual el cometa se hace más brillante y adquiere la característica “cola” de los cometas , en la medida en que es calentado por el Sol.En el mes de noviembre Rosetta enviará la sonda Philae de 100 kilogramos de peso, la cual se posará sobre la superficie del cometa y se fijará por medio de anclas. La misión de Philae es estudiar la composición y estructura interna del cometa. Han pasado 250 años desde que los cometas fueron desmitificados por Edmund Halley y desprovistos de su aura mágica y amenazadora. Lo más significativo para el desarrollo de la ciencia, sin embargo, fue que Halley pudo predecir la ocurrencia de un fenómeno natural empleando la teoría de gravitación que Isaac Newton había desarrollado poco tiempo antes, y con esto dio una demostración de las impresionantes capacidades de predicción del método científico.   Con seguridad la sonda Rosetta nos proporcionará información detallada sobre la naturaleza de los cometas. Será igualmente testigo cercano de cómo el sol “enciende” un cometa y lo convierte, de una masa fría sin un gran atractivo, en uno de los objetos más vistosos del Universo. Y todo esto será posible gracias a una tecnología – aeroespacial, electrónica, de cómputo, de telecomunicaciones, etc.– basada en el conocimiento científico acumulado desde los tiempos en que Isaac Newton llegó a una conclusión genial: que las leyes físicas que determinan los movimientos de los cuerpos celestes –incluidos los cometas– son las mismas que gobiernan a los objetos en la Tierra. De este modo, resulta que los cometas ayudaron –mediante el trabajo de Newton y Halley– a desarrollar una manera de ver al mundo en la que la magia no tiene lugar, manera que ahora emplearemos para tratar de averiguar sus secretos.",
    "Fotografías nocturnas de la Tierra tomadas desde el espacio –fácilmente localizables en internet– muestran señales claras de nuestra presencia en el planeta. Dependiendo del tiempo de exposición, dichas fotografías muestran manchas luminosas que corresponden a los grandes núcleos de población de la Tierra. Grandes manchas de luz se observan  en la costa este de los Estados Unidos, lo mismo que en la costa de California, en Europa Occidental y en Japón, entre otras muchas áreas urbanas. En nuestro país, la Ciudad de México, y aun San Luis Potosí, son claramente visibles desde el espacio.  Si bien hay regiones de la Tierra escasamente pobladas que aparecen con muy pocas luces o completamente en la oscuridad, las fotografías nocturnas de nuestro planeta nos indican hasta qué grado nos hemos extendido sobre su superficie. En número de habitantes hemos crecido hasta unos 7,000 millones, al mismo tiempo que nos hemos dispersado globalmente. Nuestra expansión ha sido tan grande que la Tierra nos ha resultado pequeña en muchos sentidos y todo esto ha llevado a cambios a escala global. La atmósfera terrestre, por ejemplo, ha sido incapaz de disponer de todos los gases de invernadero que generamos, resultando en el cambio climático de alcance global que está actualmente en curso. De la misma manera, la Tierra está perdiendo a ritmo acelerado su superficie boscosa, ya sea por la tala de árboles a cargo de la industria maderera, o bien por el desmonte de bosques para actividades agrícolas y ganaderas. De acuerdo con un artículo publicado el pasado mes de noviembre en la revista “Science”, en los trece primeros años del siglo XXI nuestro planeta perdió 2.3 millones de kilómetros cuadrados de bosques –un área que es superior a la superficie de nuestro país– y en contraste solo se crearon 0.8 millones de kilómetros cuadrados de nuevas superficies boscosas.En el número de la semana pasada de la revista “Science” se incluye una sección especial que trata de otro problema ecológico de magnitud global: el rápido declive en la población de animales silvestres que está experimentando nuestro planeta. Este proceso de declinación ha sido bautizado como “defaunación” por Rodolfo Dirzo de Stanford University, California, EUA, quien lo considera análogo al proceso de deforestación global aunque no tan visible –si bien con consecuencias igualmente profundas para el equilibrio ecológico.  Uno de los artículos incluidos en la sección especial de “Science” está encabezado precisamente por Dirzo –quién nació en México y recibió su educación de nivel de licenciatura en nuestro país– y lleva como coautores a  investigadores de los Estados Unidos, México, Brasil e Inglaterra. Dicho artículo, en el que se hace una revisión del conocimiento que los científicos tienen sobre el tema, lleva por título “Defaunación en el Antropoceno”, enfatizando así que los responsables del proceso de defaunación somos los humanos. Según Dirzo y colaboradores, “En los últimos 500 años los humanos han disparado una onda de extinción, amenaza y declive en poblaciones locales que puede ser comparable, tanto en velocidad como en magnitud, con las cinco previas extinciones masivas en la historia de la Tierra”.   En este respecto, el artículo de referencia cita algunas cifras. Menciona, por ejemplo, que desde el año 1500 se han extinguido 322 especies de vertebrados terrestres, y que de los  6 a 9 millones de especies animales que se estima existen en la Tierra, anualmente se pierden entre 11,000 y 58,000. También, desde el punto de vista del número de individuos, en las últimas cuatro décadas la población promedio de vertebrados ha disminuido en un 28% y hay grandes animales, entre ellos el elefante y el rinoceronte, que se encaminan hacia su extinción.Como hacen notar Dirzo y colaboradores, otras especies menos carismáticas están igualmente en curso de defaunación. Esto incluye a animales como nematodos, escarabajos y murciélagos, cuya disminución en población es considerablemente menos evidente que la de los grandes mamíferos y que, sin embargo, pudiera tener un impacto ecológico más grande.En la escala humana la Tierra es ciertamente enorme –su diámetro es casi diez millones de veces nuestra altura promedio– y en primera instancia no pensaríamos que con acciones nuestras pudiéramos sacarla de equilibrio. Y, no obstante, el acelerado crecimiento poblacional del mundo y su igualmente acelerado desarrollo industrial y económico han logrado lo que parecía difícil. Así, doscientos años de emisiones de gases de invernadero desde el inicio de la revolución industrial  han disparado un proceso de cambio climático el cual no es claro hasta donde nos llevará. Se ha generado igualmente un proceso de desforestación masiva que no abona a la reducción de la concentración de dichos gases en la atmósfera. Y, además, según expertos ecologistas, el mundo está en un curso de defaunación que, igualmente, afecta el equilibrio ecológico global. Dadas estas y otras  calamidades, tal parece que el planeta en el que nos tocó vivir nos quedó demasiado chico.",
    "Si usted vivió su adolescencia en los años sesenta posiblemente reconozca de inmediato al escucharlas algunas de las canciones que hicieron famosos a los Beatles. Entre estas tenemos a “Quieres Saber un Secreto”, “La noche de un Día Difícil”, “Help!” y “Yesterday”, por mencionar sólo algunas de las más conocidas. El sonido que inventaron los Beatles es ciertamente muy característico y fácilmente reconocible, sobre todo el de su primera época. Sabemos que los Beatles cambiaron su música de manera marcada durante su corta carrera de menos de una década de duración. Que el cambio musical que experimentaron los Beatles a lo largo de su existencia fue verdaderamente radical lo podemos fácilmente comprobar comparando canciones de su primer álbum “Please Please Me”, con algunas del periodo “psicodélico” de la segunda mitad de la década de los sesenta –“Magical Mistery Tour”, por ejemplo. Y sin embargo, si bien la distancia musical que alcanzaron los Beatles entre el primero y último álbum es considerable, entre dos álbumes sucesivos esta distancia es relativamente pequeña y difícil de apreciar para el común de los mortales. Así, a menos que uno sea un experto musical o un fanático de los Beatles, tendríamos problemas para decidir a partir de solo escucharlas cuál de las dos canciones “Quieres Saber un Secreto” o “La Noche de un Día Difícil” apareció primero. Investigando un poco encontramos que la primera canción forma parte del primer álbum de los Beatles “Please, Please Me” publicado en marzo de 1963,  mientras que la segunda está incluida en el álbum del mismo nombre lanzado 16 meses después, en julio de 1964.En este contexto, resulta interesante el artículo publicado en el número de agosto de la revista “Pattern Recognition Letters” en el que se reporta un análisis por computadora de la música de varios grupos de rock, incluidos los Beatles, Queen, ABBA y U2. Los autores de dicho artículo son Joe George y Lior Shamir de “Lawrence Technological University” en Southfield, Michigan, en los Estados Unidos.  El objetivo de George y Shamir fue el desarrollo de un programa de cómputo que permitiera analizar sin supervisión humana las similitudes entre álbumes producidos por un grupo de música popular.  Para este propósito, descompusieron los sonidos de las piezas musical estudiadas en sus diferentes frecuencias –desde las más graves hasta las más agudas– y elaboraron “mapas” de dos dimensiones en la pantalla de la computadora, las cuales mostraron como cambiaba cada una de estas frecuencias conforme se reproducía la canción.   De este modo, los investigadores tuvieron una “imagen” de la pieza musical bajo estudio y el análisis musical de la misma se convirtió en un ejercicio de análisis de imágenes llevado a cabo por  la computadora.En el caso de la música de los Beatles, George y Shamir analizaron los 13 álbumes publicados por el grupo, desde “Please Please Me” hasta “Let It Be”, lanzado este último en mayo de 1970. Basado en las características estudiadas, la computadora pudo determinar de manera correcta a cual álbum pertenece una determinada pieza en un 30% de los casos. Esto es relevante, pues si la computadora hubiera escogido un álbum enteramente al azar, habría acertado solamente en el 7.7% de los casos –puesto que había 13 opciones para escoger.La computadora fue también capaz de ordenar cronológicamente de manera correcta, excepto en un caso, los 13 álbumes. Pudo determinar, por ejemplo, que “Please Please Me” precedió a “Beatles for Sale”, publicado en diciembre de 1964, y que “Help!”, lanzado ocho meses después,  fue posterior a ambos. Igualmente, agrupó correctamente a los álbumes “psicodélicos” hacia el final, seguidos de “Let It Be” y “Abbey Road”, en ese orden. Esto último es incorrecto pues el último álbum de los Beatles no fue “Abbey Road” sino “Let It Be”.  Al respecto, Geroge y Shamir hacen notar que, no obstante su fecha de lanzamiento, algunas canciones de “Let It Be” fueron grabadas con anterioridad a la publicación de “Abbey Road”.El análisis produjo también un árbol filogénetico –similar a los que describen la evolución de los seres vivos– que muestra la evolución de la música de los Beatles e indica la distancia musical entre dos álbumes sucesivos. En este respecto, el programa de cómputo encuentra que se produjo un gran salto entre los álbumes “Beatles for Sale” y “Help!”, cuya publicación estuvo espaciada por  ocho meses.Los Beatles son un caso particular de marcada evolución musical en un corto periodo de tiempo y no se necesita ser un experto musical para apreciar la gran distancia que hay entre su música de los primeros tiempos y aquella de los años finales. Es, no obstante, sorprendente que un programa autónomo de cómputo sea capaz de distinguir álbumes que fueron producidos, en algunos casos, con sólo meses de diferencia. Y, por supuesto, es doblemente sorprendente que un grupo musical pueda ser capaz de evolucionar de manera tan radical y en tan corto tiempo. Por otro lado, es una lástima que la misma prisa que se dieron los Beatles para evolucionar musicalmente la hayan tenido cuando decidieron terminar su vida como grupo, la cual se extendió por escasamente una década.",
    "La copa mundial de futbol finalizada hace una semana –que resultó más ajetreada que de costumbre– tuvo un promedio de 2.67 goles anotados por partido, cifra que es significativamente más alta que la de Sudáfrica 2010, cuando se marcaron un promedio de 2.27 goles en cada encuentro. Para encontrar un porcentaje similar hay que retroceder 16 años hasta Francia 1998. Si consideramos solamente los partidos de la fase de  octavos de final en adelante, sin embargo, el promedio de goles cae drásticamente a sólo 2.19 goles por encuentro, que es menor que el promedio global de Sudáfrica 2010. Aun más, si eliminamos los 8 goles del juego entre Alemania y Brasil, cuyo resultado fue claramente anómalo, este último promedio cae hasta 1.8. Como consecuencia de esta sequía de goles, 12 de los 16 partidos posteriores a la fase de grupos, incluyendo el partido final, se resolvieron por una diferencia de un solo gol o bien por tandas de penales. En esta fase, sin contar los juegos en los que Brasil falló de manera sorprendente, los equipos estuvieron en general muy equilibrados y en buena medida se anularon mutuamente. En la definición del ganador y el paso a la siguiente ronda el azar jugó entonces  un papel importante, y esto vale también para el partido final que cualquiera de los dos contendientes pudo ganar.  Los partidos cerrados se dieron incluso entre equipos que nominalmente tienen niveles futbolísticos dispares, y aquí destaca lo hecho por Costa Rica, que demostró que las brechas futbolísticas entre países se están cerrando.  Así, resulta que muchas veces el equipo que gana un partido de futbol no es necesariamente el que tiene a los mejores jugadores, sino el que corre con la mejor suerte.¿Es injusto el futbol con los contendientes?  Un blog publicado la semana pasada por John Tierney en la sección de ciencia del periódico New York Times analiza esta cuestión. Ciertamente el futbol es injusto, pero no necesariamente más que otros deportes, en particular, aquellos que se practican en los Estados Unidos. La razón radica en que en ese país existen mecanismos para dar un mayor equilibrio en la calidad de los equipos que compiten en una determinada liga, de modo que los partidos tienden ser equilibrados y cerrados. Esto da al azar un papel importante en la definición del ganador. En el caso del futbol a nivel mundial, si bien se están ciertamente cerrado las brechas de calidad futbolística entre países, éstas siguen existiendo, como lo muestra el hecho que los cuatro semifinalistas del presente campeonato hayan sido países que tradicionalmente juegan papeles protagónicos en este tipo de competencias. Consecuentemente, el azar tiene en el futbol un papel relativamente menor –aunque no nulo– en la definición de un partido –no sería el caso, por supuesto, si los dos equipos que se enfrentan se equilibran en calidad, como sucedió en el partido final Argentina-Alemania del pasado domingo. Un partido de futbol equilibrado en el que los dos contendientes se anulen puede resultar tedioso. Al mismo tiempo, ya que los partidos espectaculares con muchos goles sólo se dan por excepción, se podría pensar que el futbol no constituye un espectáculo de masas. Lejos de esto, el futbol tiene un gran atractivo entre la población de muchos países –entre ellos el nuestro, como bien nos consta– y podríamos preguntarnos en qué radica este atractivo. Ciertamente no, al menos no de manera preponderante, en las virtudes del futbol como espectáculo. Por el contrario, posiblemente el futbol nos resulte atractivo en gran medida por la perspectiva de ver ganar a nuestro equipo, sea este la selección nacional o algún otro del campeonato local. El espectáculo futbolístico pasaría así a un segundo plano.De este modo, a pesar del soporífero partido de semifinales entre Argentina y Holanda, los argentinos llegaron en masa a Río de Janeiro para atestiguar la posible coronación de Argentina como campeón mundial, pagando en algunos casos, según los medios de comunicación, tarifas aéreas al triple de su precio normal. Para que un partido de futbol nos resulte más atractivo debe, entonces, existir una probabilidad razonable de que nuestro equipo salga triunfador.  En el peor de los casos, si el encuentro fuera entre un David y un Goliath,  los seguidores de David deben tener una esperanza –así sea leve– de que cuando menos le harán difícil la vida al gigante.Tenemos así una situación paradójica. Según Stefan Szymanski de la Universidad de Michigan, citado por Tierney en el artículo referido, si se hicieran más grandes las porterías para que haya un mayor número de goles se harían más acusadas las diferencias de calidad futbolística entre países y, de manera consecuente, se disminuiría el papel que el azar tiene en la definición de un partido. Una disminución del papel que el azar juega en la definición de un partido impactaría negativamente al negocio del futbol. En particular, en México se haría más difícil que cada cuatro años nos vendieran el espejismo de que podemos jugar un papel protagónico en la copa mundial, e incluso ganar la copa. Así, tendremos que resignarnos a que los partidos de futbol espectaculares y con muchos goles sólo se den por excepción.",
    "En tiempos antiguos, en Europa se creía que en tierras remotas existían monstruos humanoides con características físicas fantásticas. Un ejemplo de esto nos lo da el escritor romano Plinio el Viejo, quien en el primer siglo de nuestra era en su Historia Natural escribe que, según el historiador griego Ctesias, en la India “existe una raza de seres humanos, quienes son conocidos como Monocoli, los cuales tienen una sola pierna pero son capaces de saltar con asombrosa agilidad. Estas mismas gentes son también llamados sciapodos, porque tienen el hábito de acostarse de espaldas durante las horas de calor extremo, y protegerse del sol con la sombra del pie.” Los sciapodos no son los únicos monstruos de la India descritos por Plinio el Viejo. A estos se unen humanos con cabeza de perro, o bien con los pies volteados hacia atrás y ocho dedos en cada pie. Incluye también a humanos con un agujero por nariz y pies flexibles como una serpiente. Igualmente, en el extremo de la India localiza a una raza de hombres que no tienen boca y que subsisten solamente de los olores que inhalan.  Plinio el Viejo pudo escribir acerca de estas creaturas fantásticas con una cierta credibilidad por la lejanía de Roma de las tierras en las que supuestamente habitaban. En los tiempos actuales, en los que es posible viajar a cualquier lugar del mundo en menos de 24 horas, es más difícil creer que tales engendros pudieran existir en algún lugar del planeta.O casi en ningún lugar, si hemos de atender a leyendas como la del hombre de las nieves o yeti que habitaría en las montañas Himalaya. Como sabemos, el hombre de las nieves sería una creatura bípeda que camina ligeramente agachada, con una altura mayor a la típica humana y con el cuerpo cubierto de pelo. Quienes defienden su existencia, no obstante, muestran típicamente sólo evidencias indirectas, tal como huellas de pisadas en la nieve que aparentemente no pueden ser asociadas a ningún animal conocido. Otras veces muestran filmaciones en donde se ve al hombre de las nieves a lo lejos y de manera fugaz, y no una evidencia sólida como la presentación de un espécimen de hombre de las nieves, vivo o muerto. Las evidencias que apoyan la existencia del hombre del yeti son entonces  fundamentalmente anecdóticas y no resultado de una investigación formal. Para subsanar esto último, un grupo de investigadores de universidades y centros de investigación de Europa y los Estados Unidos, encabezados por Bryan Sykes de la Universidad de Oxford en el Reino Unido, llevaron a cabo un estudio genético con una serie de fragmentos de pelo atribuidos al yeti y a otros primates anómalos, incluyendo al llamado pie grande que ha sido avistado en los Estados Unidos y al almasty del Asia Central. El estudio fue publicado este mes en la revista “Proceedings of the Royal Society B”. Para llevar a cabo el estudio, los investigadores solicitaron muestras de pelo a museos y a colecciones particulares en diferentes partes del mundo. Recibieron un total de 57 muestras, una de las cuales resultó ser una fibra vegetal y  otra una fibra de vidrio. Después de una primera revisión escogieron 37 muestras, las cuales fueron sometidas a un análisis genético para determinar su procedencia. Solamente en 30 de los casos estudiados se pudo determinar esta procedencia, que en ningún caso correspondió a una especie desconocida. En todos los casos, excepto uno cuyo análisis genético demostró que se trataba de una muestra de pelo humano, los fragmentos de pelo correspondieron a una especie animal conocida, incluyendo osos negros, osos pardos, caballos, vacas, mapaches, e incluso un puercoespín, entre otros animales.El animal al que corresponde cada fragmento, además, habita en el lugar en el que fue encontrado dicho fragmento. Esto es cierto en todos los casos excepto para dos muestras encontradas en el Himalaya en los que la información genética coincide con la de un fósil de oso polar que vivió hace 40,000 años –más no a la de un oso polar actual–. Para explicar este  hallazgo, Sykes y colaboradores sugieren que los animales a los cuales pertenecen los fragmentos son una variante del oso polar que no había sido descubierta hasta ahora, o bien un híbrido de oso polar con oso pardo.  Esto podría ayudar a explicar, según los investigadores, el origen de la leyenda del yeti, que no correspondería de este modo a un primate desconocido, sino una variante de oso polar no reconocida hasta ahora. El estudio de Sykes y colaboradores prueba que ninguno de los casos estudiados corresponde a los de un primate desconocido y da una indicación de la fragilidad de las evidencias ofrecidas en favor de su existencia. No prueba, sin embargo, que el yeti o el pie grande no existan. Pudieran existir, pero hasta ahora no hay pruebas sólidas que apoyen esta posibilidad.Como tampoco hay pruebas de que los sciapodos existan o hayan existido en tiempos de Plinio el Viejo, si bien no podamos negarlo categóricamente. Y, sin embargo, hay que reconocer que, dado el caso, con seguridad apostaríamos doble contra sencillo a que nadie, así buscara por mar, cielo y tierra, podría alguna vez descubrir a un sciapodo recostado bajo los rayos del sol haciéndose sombra con el pie.",
    "Habiendo sido testigos de la manera acelerada como  ha aumentado el número de automotores en la ciudad de San Luis Potosí en las últimas décadas, que hace cada vez más y más difícil la circulación, cabe preguntarnos que nos espera en los tiempos por venir. Por un lado, la industria de automóviles en México está en crecimiento, produciendo actualmente alrededor de 3 millones de vehículos anuales, y si bien el 87% de esta producción se exporta, en 2013 se quedaron en México alrededor de medio millón de automotores que se sumaron a aquellos en circulación. Un segundo factor que ha contribuido al incremento del parque vehicular en México es la importación de automóviles nuevos. Sumando vehículos fabricados localmente y vehículos importados, en 2013 se vendieron en México más de un millón de unidades nuevas. A estas últimas hay que sumar los automóviles usados importados legal o ilegalmente, que en 2013 fueron unos 650,000, según la industria automotriz de nuestro país. Como resultado, el parque de vehículos automotores en México creció en más de un millón setecientos mil unidades en 2012. Todo lo anterior se ha combinado para producir el vertiginoso incremento de automóviles del cual somos testigos. Según datos del INEGI, entre los años 1980 y 2012 el número de automotores en México creció por un factor de seis, hasta alcanzar una cifra alrededor de los 35 millones. En el estado de San Luis Potosí este incremento ha sido todavía más acelerado. En efecto, según el INEGI, aquí el número de automotores ha pasado de aproximadamente 80,000 en 1980 a casi 900,000 en 2012, es decir ha crecido por un factor cercano a 11, casi dos veces el promedio nacional. En lo que se refiere a la conurbación San Luis Potosí-Soledad de Graciano Sánchez, el número de automóviles se incrementó por un factor cercano a diez entre los años 1980 y 2012, según los mismos datos del INEGI. Los números del INEGI nos muestran que en la ciudad de San Luis Potosí en las últimas tres décadas el parque vehicular se ha duplicado aproximadamente cada 9 años. De seguir esta tendencia, en el curso de una década se incrementará de manera sustancial el número de automóviles en nuestra ciudad, con el previsible incremento de problemas de tráfico urbano. La predicción de los volúmenes futuros de automotores en San Luis Potosí es, por supuesto, materia de especialistas. Hay que notar, no obstante que en México hay aproximadamente unos 300 automotores por cada mil habitantes y que este número es pequeño en comparación con los que se observan en otros países. En los Estados Unidos, España y Bulgaria, por ejemplo, hay unos 800, 600 y  400 automóviles por cada mil habitantes, de manera respectiva. En estas circunstancias, posiblemente no podríamos descartar un aumento sustancial en el número de automotores en México, y en particular en San Luis Potosí, en los próximos años.El acelerado crecimiento observado en el número de automóviles, no obstante, debe auto limitarse en algún momento y tender a una saturación. Esto se observa en los países desarrollados en donde dicho número aumenta a velocidades relativamente pequeñas. En el Distrito Federal, igualmente, el número de automotores creció sólo por un factor de 2.5 en el periodo 1980-2012, cifra significativamente menor que el  promedio nacional. Esto aun con la activación del programa Hoy no circula, que fomentó la adquisición de más automóviles por parte de los habitantes de la capital. Queda a los especialistas determinar si esta baja tasa de crecimiento es debida a las dificultades crecientes para circular en la Ciudad de México, lo cual no sería sorprendente.La industria de los automóviles en México contribuye sustancialmente al producto interno bruto del país así como a su volumen de exportaciones. Al mismo tiempo, no obstante, el aumento creciente de automóviles en nuestras ciudades hace cada vez más difícil el tráfico urbano, al grado de que algunos de nosotros en ocasiones llegamos a ver al automóvil al mismo tiempo como una ventaja y como una plaga.Es una ventaja dados los deficientes sistemas de transporte urbano en nuestras ciudades. Al mismo tiempo, sin embargo, el crecimiento acelerado del número de automóviles requiere de un crecimiento concomitante de la infraestructura de vialidad urbana, que nunca alcanza a ser suficiente para equilibrar al crecimiento de vehículos circulantes.Los automóviles, sin duda, ocupan un lugar en el espacio y la experiencia en México es que cuando su número crece sin medida hay un punto en que saturan todo el espacio disponible. Llegado a este punto, para introducir más automóviles es necesario crear espacios adicionales. En la Ciudad de México, por ejemplo, cuando el programa Hoy no Circula fue insuficiente, se construyeron segundos pisos para aliviar la congestión de los primeros.  Todo esto, por supuesto, resulta absurdo y pone de manifiesto la inconveniencia de un modelo de transporte urbano basado parcialmente en el automóvil particular, en contraposición con un modelo que se apoye en un transporte público eficiente.Hay que tomar en cuenta que, a pesar del refrán popular, llegado a un cierto punto no todo cabe en un jarrito, por más esfuerzos que hagamos para acomodarlo.",
    "¿Quién fue el descubridor de América? De acuerdo con la versión más extendida, fue Cristobal Colón, quién, como sabemos, después de un viaje trasatlántico de varias semanas desde las Islas Canarias  arribó el 12 de octubre de 1492 a isla de San Salvador en las Bahamas. Hay quien afirma, sin embargo, que el descubridor de América es el vikingo Leif Eriksson, quién, en el año 1,000 de nuestra era, navegando por el Atlántico norte llegó hasta la isla de Terranova estableciendo ahí una colonia, aunque ésta no fue permanente. Otros afirman que los verdaderos descubridores de América fueron sus primeros pobladores, siberianos que cruzaron desde Asia hacia el contienen americano a través de Beringia hace más de diez mil años.   Según el color del cristal con que se mira, cualquiera de los puntos de vista anteriores podría ser correcto. Si, por ejemplo, demandamos que el descubridor del continente americano sea una persona concreta, entonces los siberianos anónimos quedan descartados. Si, además, atendemos al aspecto cronológico, tendremos que inclinarnos por Leif Eriksson. Si, por otro lado, pasamos por alto la cronología y nos enfocamos en los efectos a largo plazo que resultaron del descubrimiento en cuestión, entonces el descubridor de América es Cristobal Colón. Cristobal Colón, no obstante, subestimó el tamaño de la Tierra y en un inicio no fue consciente de que había arribado a un nuevo continente, si bien en viajes posteriores habría llegado a esta conclusión. Como quiera que sea, al nuevo continente no se le nombró en honor a Colón –sino en honor a Américo Vespucio– lo que significa que sus contemporáneos no lo consideraron como su descubridor.   El puesto de honor en el descubrimiento del Nuevo Mundo está entonces disputado y en este contexto es interesante comentar el artículo de divulgación aparecido esta semana en la revista “Science”, en el que se añade un nuevo candidato a la lista de aspirantes al título de descubridor de nuestro continente. El nuevo candidato es Abu Rayhan al Biruni, quien fue un intelectual persa que vivió en el siglo X en Asia Central, al sur de Mar de Aral en lo que hoy en Uzbekistan. Al Biruni fue un genio universal que se especializó en matemáticas, física, astronomía, filosofía e historia, y es considerado uno de los mayores  intelectuales de la edad de oro islámica.  Una de las aportaciones de al Biruni fue la medición de la circunferencia de la Tierra. Para esto, desde lo alto de una montaña de altura conocida –medida por él con anterioridad empleando métodos de trigonometría– midió el ángulo al que se encontraba el horizonte y de ahí pudo deducir la curvatura de la Tierra y en consecuencia su circunferencia. El valor que obtuvo está asombrosamente cerca del valor aceptado en la actualidad.Al Biruni sabia de la existencia de tres continentes, Asia, Europa y África, pero no tenía noticia alguna, por supuesto, del continente americano. No obstante, ya que sabía cuál era la circunferencia de la Tierra y la extensión de las tierras conocidas, pudo estimar que éstas ocupaban solamente dos quintas partes del total de la superficie terrestre. A partir de aquí, dedujo que no había razón para que las tres quintas partes inexploradas de la superficie de la Tierra fueran diferentes de las dos quintas partes conocidas, puesto que ambas fueron moldeadas por las mismas fuerzas. En consecuencia, las primeras debían contener tierras desconocidas hasta entonces. Esto ha llevado a S. Frederick Starr de la Universidad Johns Hopkins en Washington, D.C., a considerar a al Biruni como el descubridor de América. Un descubridor y un descubrimiento muy peculiares, pero que antecedieron a Cristobal Colón por cinco siglos.El descubrimiento de al Biruni es, ciertamente, de naturaleza muy diferente al de Colón y esto ha llevado a algunos a criticar las conclusiones de Starr. El artículo de “Science” anteriormente referido, por ejemplo, cita a Nathan Sidoli de la Universidad Waseda, Tokio, Japón, quien afirma, “Nosotros no decimos que Copérnico descubrió que la Tierra se mueve alrededor del Sol simplemente porque él así lo asumió de manera hipotética, así que no veo por qué debamos decir que al Biruni descubrió el continente americano”.Al margen de la controversia, resulta sorprendente la sofisticación intelectual que se dio en Asia Central hace un milenio y al respecto Starr hace notar que en esa época, “Los chinos se sorprendían que niños de ocho años estuvieran aprendiendo matemáticas en Samarcanda”. En particular, en asombroso el tipo de razonamientos empleados por al Biruni para inferir –correctamente– la existencia de un continente o continentes desconocidos. Esto contrasta con el oscurantismo intelectual de la Europa de la época, que no fue superado sino hasta siglos después.    Habría que admitir, no obstante, que entre la gente común y corriente posiblemente pocos considerarían natural cambiar a Cristobal Colon por Abu Rayhan al Biruni como descubridor de América, lo mismo que al 12 de octubre por otra fecha. Fecha que, por otro lado, sería difícil de determinar.",
    "Así como no todo lo que brilla es oro, no todo lo que está escrito es verdad, y esto es especialmente cierto cuando se trata de artículos científicos. De hecho, nunca podremos tener la seguridad absoluta de que una determinada teoría o hipótesis científica constituya una verdad absoluta  –por más evidencias que existan en su favor– y siempre estará sujeta a que sea refutada por nuevos descubrimientos. De este modo, al hacerse público, el resultado de una cierta investigación científica se expone al escrutinio de otros especialistas, quienes podrán refutarlo o confirmarlo de acuerdo a sus propias investigaciones. Sólo después de que un pretendido descubrimiento ha pasado por esta prueba de ácido es que es aceptado por la comunidad científica –aunque muy comúnmente no de manera unánime.Si bien el debate científico se da por lo general en el seno de las revistas o congresos científicos, en algunos casos, por su impacto trasciende a los medios masivos de comunicación. Este es el caso de un artículo publicado la semana pasada en la revista de la “Proceedings of the National Academy of Sciences” por un grupo de investigadores de universidades norteamericanas encabezados por Kinju Jung del Departamento de Administración de Negocios de la Universidad de Illinois. En dicho artículo, Jung y colaboradores encuentran que, de manera sorprendente, los huracanes con nombres femeninos resultan en promedio más mortíferos que aquellos con nombres masculinos.  Como sabemos, a lo largo de la historia los huracanes en el Atlántico norte han producido grandes devastaciones y mortandades, tanto en la Antillas como en Centro América, México y los Estados Unidos. A partir de 1954, y con el objeto de evitar confusiones y facilitar las medidas preventivas para mitigar sus efectos, los huracanes fueron identificados con un nombre propio. Inicialmente se escogieron exclusivamente nombres femeninos. Esto, según algunos, como una referencia al hecho de que los huracanes son fenómenos naturales impredecibles. En estas circunstancias y como era natural, con el tiempo se consideró que dar a los huracanes nombres exclusivamente femeninos tenía un carácter sexista y, a partir de 1979, éstos fueron bautizados alternadamente con nombres masculinos y femeninos. Por ejemplo, los primeros cuatro huracanes de la temporada 2014 del Atlántico norte que acaba de comenzar oficialmente el pasado 1 de junio son: Arthur, Bertha, Cristobal y Dolly. Si hemos de atender a los resultados del trabajo de Jung y colaboradores, sin embargo, aun con los cambios de 1979 la nomenclatura de los huracanes mantiene connotaciones sexistas, aunque en un sentido diferente a las originales. En efecto, según el artículo de referencia, el grado de feminidad del nombre de un huracán, tal como es percibido de manera subjetiva, determina el grado de peligrosidad con el que dicho huracán es juzgado. Así, un huracán con el nombre de Victoria sería percibido como menos peligroso que otro bautizado como Víctor, aunque ambos fueran de la misma magnitud y peligrosidad.Lo anterior obedecería a nuestros prejuicios culturales según los cuales la masculinidad está asociada a la fuerza y a la agresividad, en contraste con el comportamiento pasivo que atribuimos a la feminidad.  De este modo, habría una tendencia a relajar nuestro estado de alerta ante un huracán con un nombre femenino, el cual tendemos a percibir con un grado de peligro menor al real. Esto explicaría el mayor número de fatalidades asociadas a los huracanes femeninos en comparación con los masculinos.Estas conclusiones, sin embargo, no son compartidas por otros investigadores y se ha producido al respecto un debate público en el espacio de internet. Por ejemplo, Jeff Lazo del National Center for Atmospheric Research de los Estados Unidos considera que no hay una evidencia estadística suficientemente fuerte de que los huracanes femeninos sean efectivamente más mortíferos que los masculinos. Hace notar, además, que el estudio de Jung y colaboradores se llevó a cabo con huracanes en el periodo de 1950-2012, que incluye el periodo cuando los huracanes tenían nombre exclusivamente femeninos. Esto sería importante porque en esa época los huracanes producían en promedio más víctimas.Lazo argumenta también que Jung y colaboradores incluyen en su conteo de víctimas fatales aquellas que murieron después del meteoro –por la caída de un poste de luz, por ejemplo– muertes en las que, obviamente, no tuvo nada que ver el género del huracán.¿Son más mortíferos los huracanes del género femenino? El nombre que le tocó en suerte a un huracán dado no tiene, por supuesto, nada que ver con su grado de peligrosidad. Según Jung y colaboradores, sin embargo, los huracanes femeninos resultan más mortíferos que los masculinos por nuestros prejuicios sobre los respectivos atributos de ambos géneros. De ser esto cierto nos podríamos quizá hacer una idea a priori sobre la intensidad y peligrosidad de los primeros cuatro huracanes del Atlántico de la presente temporada: Arthur, Bertha, Cristobal y Dolly ¿cuál considera que será el más peligroso?",
    "¿Pueden predecirse los terremotos? Dado lo destructores y mortíferos que han sido a lo largo de la historia, con seguridad todos estamos de acuerdo en que sería altamente deseable contar con medios para predecirlos con días de anticipación, con el fin de tomar las medidas pertinentes para protegernos de sus efectos. Desgraciadamente, los especialistas no han logrado todavía desentrañar con claridad las señales físicas que preceden a un terremoto y que les permitirían predecirlo. La predicción acertada de sismos tendrá entonces que esperar por algún tiempo difícil de precisar.  Hasta la fecha solamente ha habido un caso de predicción correcta de un terremoto: el que golpeó a la ciudad de Haicheng  en el noreste de China en febrero de 1975.  Este terremoto, que tuvo una magnitud de 7.3 grados en la escala de Richter, fue precedido por una serie de temblores de pequeña magnitud que, aunados a la historia sísmica de la región, indujeron a las autoridades chinas a ordenar la evacuación de la ciudad de Haicheng un día antes de la ocurrencia del sismo. Esta decisión salvó numerosas vidas en dicha ciudad, que a la sazón rondaba el millón de habitantes.Expertos fuera de China, sin embargo, consideran que la predicción del sismo de Haicheng no fue llevada a cabo con suficiente rigor científico y dudan de su validez. De acuerdo con estas opiniones, la predicción correcta del sismo fue sólo producto de la casualidad, lo que es consistente con el hecho que constituye un caso único que no se ha vuelto a repetir.Para predecir un sismo de gran magnitud de manera efectiva, los especialistas tienen que identificar señales o precursores de algún tipo que anuncien su ocurrencia con la suficiente anticipación. Estas señales podrían consistir en un cambio en la actividad sismológica de baja intensidad –como ocurrió en Haicheng–, en un cambio en la elevación o inclinación del suelo, o bien en un cambio de algún o algunos de los parámetros físicos del terreno. Como precursores de un sismo se han considerado incluso cambios en el comportamiento de los animales.  De acuerdo con algunos investigadores, previo a la ocurrencia de un movimiento de tierra se han observado  pulsos magnéticos que podrían usarse como señales de su inminencia. La magnitud de estos pulsos es alrededor de un milésimo del campo magnético en la superficie de la Tierra. No está claro, sin embargo, cual es el origen de dichos pulsos y si realmente están asociados al sismo o tienen otro origen. Un artículo escrito por un grupo de investigadores en California y Perú, encabezados por John Scoville de San Jose State University, intenta clarificar este punto. El artículo está todavía en etapa de revisión para su publicación y ha sido colocado en el servidor de pre-impresos arXiv de Cornell University.De acuerdo con Scoville y colaboradores, los pulsos magnéticos previos a un sismo son debidos a corrientes eléctricas generadas en las rocas situadas por debajo de la superficie de la tierra. Las corrientes eléctricas son a su vez debidas a los enormes esfuerzos a las que son sujetas las rocas en la medida en que se están creando las condiciones para iniciar el sismo. La detección de dichos pulsos en un determinado momento constituiría de este modo una indicación de la inminencia de un terremoto. Podría, además, determinarse el epicentro del mismo determinado la dirección en que se generan los pulsos magnéticos vistos desde dos localizaciones. Scoville y colaboradores comprobaron lo anterior llevando a cabo mediciones a lo largo de varios días de los pulsos magnéticos generados antes de la ocurrencia de un sismo cerca de Lima, Perú. Para esto emplearon dos medidores separados por una distancia de 25 kilómetros con el fin de determinar el epicentro del futuro sismo. Éste ocurrió dos semanas después de las mediciones en el sitio predicho. Hay quien duda, sin embargo, de los resultados del artículo en cuestión, e incluso considera la posibilidad de que los pulsos magnéticos medidos hayan tenido un origen diferente al de los procesos sísmicos.Al margen de lo anterior, no obstante, confiemos que en un futuro –que ojalá no sea muy lejano–, ya sea mediante la detección de pulsos magnéticos o de cualquier otro precursor, podremos librarnos de la amenaza de los terremotos. Esperemos que en algunas pocas décadas, por poner un ejemplo, en los noticieros matutinos trasmitidos en las áreas de mayor riesgo sísmico se incluya, junto a los pronósticos del tiempo, el pronóstico de los movimientos de tierra. Algo así como: “Para el día de hoy se esperan lluvias moderadas y un máximo de temperatura de 20 grados centígrados, por lo que se recomienda llevar paraguas al salir. Se espera también un sismo de 4 grados Richter que no representará mayor peligro.”",
    "Los océanos de nuestro planeta tienen un área combinada de alrededor de 350 millones de kilómetros cuadrados, lo que representa el 70% de la superficie total de la Tierra. Su profundidad media es de poco menos de cuatro kilómetros, aunque hay abismos considerablemente más profundos. Dada esta inmensidad, no es sorprendente que a lo largo de la historia se hayan dado numerosos casos de barcos desaparecidos en el mar sin dejar rastro alguno. Así sucedió, por ejemplo, con el USS Cyclops, navío de carga norteamericano que desapareció en el año 1918 con 306 tripulantes y pasajeros en el área conocida como el Triángulo de las Bermudas. Cuando se esfumó, el Cyclops navegaba hacia Baltimore procedente de Brasil con un cargamento de 11,000 toneladas de manganeso. Se ha especulado que pudo haber sido hundido por un submarino alemán o bien por una tormenta inusualmente fuerte, pero no se sabe con certeza cuál fue su verdadera suerte. Curiosamente, otros dos navíos hermanos del Cyclops, el Proteus y el Nereus, desaparecieron en la misma área durante la Segunda Guerra Mundial, igualmente sin dejar huella. Otro episodio famoso de desaparición en el mar es el caso del velero Mary Celeste ocurrida en el año 1872. Cabe mencionar que en este caso no se trató de la desaparición de un buque, sino la de todos sus tripulantes y pasajeros. El Mary Celeste partió en noviembre de 1872 de la ciudad de Nueva York con destino a Génova, Italia, con un cargamento de alcohol industrial. En el barco navegaban el capitán, su esposa, una hija pequeña, y una tripulación de siete hombres. A la altura de las islas Azores, el Mary Celeste  fue avistado por el buque De Gratia. Al acercarse al Mary Celeste, el capitán del De Gratia pudo cerciorarse de que la tripulación lo había abandonado. Fuera de esto, todo estaba en orden a bordo del Mary Celeste, sin indicios de que hubieran ocurrido hechos de violencia, sólo faltaba del bote salvavidas y algunos instrumentos de navegación. A la fecha no hay explicación para este hecho, que fue tema de una de las primeras novelas de Arthur Conan Doyle, el creador de Sherlock Holmes.El mar es tan grande que incluso podían haber ocurrido desapariciones de islas enteras. Este es el caso de la Isla Bermeja que según mapas del siglo XVI debería estar localizada a unos 100 kilómetros al noroeste de la península de Yucatán. Según la Wikipedia, esta  isla aparecía en mapas oficiales del Gobierno de México hasta fechas tan tardías como 1946 –curiosamente, aparece en Google Maps como Islote Bermeja–. Expediciones recientes al supuesto sitio de la Isla Bermeja no han arrojado resultados positivos y la posición oficial actual es que dicha isla no existe. La supuesta isla habría sido el resultado de un error cartográfico que se propagó a lo largo de los siglos.Si bien la Isla Bermeja por sus dimensiones no debería tener mayor importancia, su existencia implicaría la extensión de la soberanía de México hacía una región del Golfo de México rica en petróleo. Algunos afirman que la Isla Bermeja si existió, pero que desapareció, ya sea por causas naturales, o bien por una acción deliberada para limitar la soberanía de nuestro país sobre las reservas petroleras del Golfo de México.El que un velero pudiera haber desaparecido en la inmensidad del mar sin dejar rastro hace 150 años, cuando los viajes transoceánicos tomaban semanas, no es algo que nos sorprenda demasiado. No nos sorprende tampoco que buques de principios del siglo XX se hayan perdido sin dejar rastro en medio de una tormenta. Lo que no hubiésemos quizá esperado es que un moderno avión comercial se hubiese esfumado en pleno siglo XXI con 339 pasajeros y tripulantes durante un vuelo de rutina, tal como le sucedió al vuelo MH370 de Malaysia Airlines el pasado mes de marzo. Y que hoy en día, a casi tres meses del episodio, no tengamos certeza de que fue lo que le sucedió al avión malayo, a pesar de la avanzada tecnología de localización de que disponemos.Como recordamos, el pasado 8 de marzo, durante el viaje entre Kuala Lumpur y Beijing, los pilotos del vuelo 370 aparentemente desactivaron los sistemas de comunicación del avión y desviaron su ruta en varias ocasiones, hasta enfilarlo hacia el sur en dirección de la costa occidental de Australia. Esto se dedujo porque el avión, a pesar de llevar apagado su sistema de comunicación, emitió señales de su posición que fueron captadas por un satélite británico. De acuerdo con un análisis de estas señales, el avión habría caído al mar en el Océano Índico al oeste de la costa Australiana. No se han encontrado, sin embargo, evidencias físicas del accidente que corroboren el hipotético lugar de la caída. Así, lo sucedido al vuelo 370 continúa siendo un misterio.Y con seguridad lo seguirá siendo por años, pues aún si se ubicara con precisión el lugar en el que el vuelo 370 se precipitó al mar, explorar el fondo del océano, que en ese punto está a una profundidad superior a los 5 kilómetros, será una empresa que tomará una considerable cantidad de tiempo. Esto si hay suerte, pues existe la posibilidad de que el vuelo 370 se añada al arcón de misterios marinos de desapariciones de barcos, tripulaciones e islas.",
    "Hoy en día es ampliamente aceptado que la ciencia y la tecnología –que se deriva de aquella– juegan un papel fundamental en el desarrollo económico de un país. Una diferencia evidente entre los países industrializados y los que están en vías de serlo es, ciertamente, la de sus respectivos desarrollos científicos. Como parte de su política de crecimiento económico, los países en desarrollo están obligados entonces a impulsar la creación de una infraestructura científica propia y a buscar que ésta impacte a la actividad económica. A México la ciencia y la tecnología arribaron con un considerable retraso. En efecto, fue sólo hasta el año 1971, con la creación del CONACyT, cuando el país puso a la ciencia y la tecnología en un primer plano –segundo, en ocasiones– como factor para el desarrollo económico del país. A partir de ese año, el CONACyT estableció un amplio programa de becas para la realización de estudios de posgrado en México y en el extranjero –con escasos paralelos en el mundo– que ha llevado a la formación de numerosos  investigadores, muchos de los cuales se han incorporado a centros de investigación y universidades en México.El CONACyT ha establecido igualmente programas de apoyo a proyectos de investigación llevados a cabo en instituciones del país, aunque la magnitud de estos apoyos ha variado grandemente con las diferentes administraciones federales. En la presente administración, el CONACyT, encabezado por Dr. Enrique Cabrero Mendoza –quien estuvo de visita en la UASLP esta semana–, ha hecho público el propósito de elevar el gasto nacional en ciencia y tecnología hasta alcanzar el 1% del producto interno bruto del país –desde la cifra actual inferior al 0.5%–. En consecuencia con este propósito, el CONACyT ha emitido nuevas convocatorias de todo tipo dirigidas a las instituciones de investigación en México, lo mismo que al sector privado. Dado que nunca habrá suficientes recursos para apoyar a todos los campos científicos por igual, surge la pregunta de cómo se debe priorizar el apoyo a la ciencia en un país con recursos limitados con el fin de optimizar su impacto. En primera instancia, se podría pensar que la estrategia correcta es la de apoyar de manera prioritaria a aquellos campos científicos que estén más cerca de las aplicaciones industriales, sin descuidar aquellas dirigidas a resolver problemas de carácter social.No sería éste el caso, sin embargo, si hemos de atender a un artículo publicado en la revista en línea “Plos One” el pasado año por un grupo de investigadores de la Universidad Simón Bolívar en Caracas, Venezuela, encabezados por Kalus Jaffe. En dicho artículo se describe una investigación llevada a cabo con el propósito de determinar qué campos científicos tienen un impacto mayor sobre el desarrollo económico de un país.  Para este propósito, los investigadores venezolanos compararon datos del Banco Mundial referentes al crecimiento del producto interno bruto per cápita de un centenar de países, con su productividad científica en varios campos básicos y aplicados. Dicha productividad  fue determinada tomando como base al número de artículos científicos publicados en un campo específico. Hay que aclarar, no obstante, que este número varía grandemente entre países desarrollados y no desarrollados, de modo que en lugar de considerar números absolutos los investigadores tomaron números relativos a cada unos de los países estudiados.  Del estudio se deriva un resultado sorprendente: la productividad científica de un país en áreas de física y química en un determinado momento está correlacionada con el crecimiento económico del país en los siguientes cinco años. La productividad científica en física y química es entonces un indicador confiable del crecimiento económico que experimentará un país en el futuro cercano. En contraste, la productividad científica en otros campos de ciencia aplicada, como la medicina y la farmacología, no tiene la misma correlación con el crecimiento económico. El que exista una correlación entre dos parámetros, en este caso productividad científica y crecimiento económico, no significa que uno sea causa del otro. De hecho, Jaffe y colaboradores no encuentran ninguna relación causal entre productividad en física y crecimiento económico. Sugieren en lugar de esto que la decisión de apoyar a la ciencia básica “es un indicador confiable de la existencia de una atmósfera que favorece la toma de decisiones racionales que, dadas las circunstancias adecuadas, promueve el crecimiento económico”. Concluyen Jaffe y colaboradores que “Ningún país con una inversión preferencial en tecnología, sin una inversión concurrente en ciencia básica, ha alcanzado un relativo alto crecimiento económico. De este modo, tecnología sin ciencia es improbable que sea sostenible”.La tecnología moderna es hija de la ciencia y ha podido desarrollarse adecuadamente sólo en países en donde esta última tiene carta de naturalización. No es posible, entonces, saltarse etapas, y si México ha de desarrollarse tecnológicamente tendrá que crear una base científica robusta. Al menos de acuerdo a los hallazgos de Jaffe y colaboradores.",
    "Es ampliamente aceptado que los primeros pobladores del continente americano llegaron de Asia, cruzando desde el extremo oriental de Siberia hasta Alaska. Lo hicieron a través del puente de Beringia, que en su momento unió a América con Asia. Hoy en día dicho puente no existe debido a que la fusión de los hielos polares al finalizar la última glaciación hizo subir el nivel del mar hasta alcanzar su posición actual –lo hizo subir unos 120 metros desde el máximo de la última glaciación hace unos 20,000 años. Si bien se asume que los primeros americanos llegaron del continente asiático, no hay acuerdo unánime entre los expertos en la fecha en que esto ocurrió. Igualmente, no es claro si hubo una sola migración o varias migraciones procedentes de diversas regiones de Asia. En particular, dado que los indígenas norteamericanos modernos tienen características morfológicas diferentes a las de los restos fósiles humanos de más antigüedad encontrados en el continente americano, algunos especialistas se preguntan si aquellos y estos últimos tienen origen distinto.  Un artículo aparecido esta semana en la revista “Science” busca encontrar una respuesta a esta pregunta. Dicho artículo fue publicado por un grupo de investigadores de los Estados Unidos, México, Canadá y Dinamarca, encabezado por James Chatters de “Pennsylvania State University”.El artículo reporta el trabajo llevado a cabo en torno al esqueleto –casi completo, incluyendo su dentadura– encontrado por buzos profesionales en una cueva sumergida en Yucatán en el año 2007. A dicha cueva se accede por un cenote y un pasaje –también inundado– de 600 metros de longitud. El esqueleto perteneció a una niña de 15-16 años de edad y una altura aproximada de 1.5 metros, quien habría vivido hace 12,000-13,000 años. Fue bautizada como Naia, haciendo referencia a Náyade, ninfa acuática en la mitología griega.Aparentemente, Naia habría penetrado a uno de los pasajes que llevan a la caverna, de unos 30-40 metros de profundidad –a la que se ha bautizado como Hoyo negro– y habría caído accidentalmente al fondo de la misma. Esto explicaría el que se le haya encontrado con la pelvis rota.En la misma caverna se encontraron restos de animales ya extintos, incluyendo tigres dientes de sable y gonfoterios –parecidos a los elefantes–, lo que indica que el Hoyo negro, efectivamente, hacia las veces de trampa mortal.Sobre la manera en que Naia pudo ingresar a la caverna, hay que recordar que la inundación que sumergió a Beringia también tuvo grandes efectos en Yucatán que es una península de tierras bajas. De este modo, la caverna y sus pasajes de acceso, que hoy en día están inundados y por abajo del nivel del mar, hace 13,000 estaban secos –con la excepción del fondo de la caverna– y tenían conexión expedita con la superficie.  Pero regresando al estudio de referencia, Chatters y colaboradores llevaron a cabo un análisis morfológico del cráneo de la niña con el fin de compararlo con los rasgos característicos de los indígenas norteamericanos modernos. No tuvieron, sin embargo, un contacto directo con el cráneo que se ha mantenido en el lugar en el que fue encontrado, así que el estudio se realizo por medio de un modelo tridimensional de computadora. Para construir el modelo, el cráneo fue colocado en una plataforma rotatoria en el fondo de la caverna y fue fotografiado desde diferentes ángulos. Esto fue llevado a cabo, por supuesto, por buzos profesionales, que son los únicos capacitados acceder a la caverna. Mediante el análisis del cráneo, los investigadores encontraron que las características morfológicas del mismo corresponden a los de otros restos humanos en América con una antigüedad equivalente, y no a las de los indígenas norteamericanos modernos. En contraste, un análisis genético de la dentadura de la niña muestra características coincidentes con los estos indígenas, lo que sugiere un origen común.   La investigación concluye que los primeros pobladores de América tienen antecesores comunes, quienes después de arribar de Siberia se habrían establecido en Beringia y posteriormente penetrado hacia el sur del continente americano. Las diferencias morfológicas entre los pobladores primitivos y los indígenas norteamericanos actuales serían resultado de la evolución de éstos últimos después de penetrar en el continente.Como es frecuente en asuntos científicos, no todo mundo está de acuerdo con las conclusiones y las mismas estarán bajo el escrutinio de otros investigadores quienes aportarán pruebas a favor o en contra hasta lograr un consenso.Cualquiera que este consenso resulte ser, es interesante notar que Yucatán está jugando un papel importante en la discusión. Y que no es la primera vez que algo similar sucede. Recordemos que fue en Yucatán, de manera precisa en Chicxulub, en donde se descubrió hace algunos años los restos de la caída de un meteorito de grandes dimensiones que muchos especialistas consideran fue la causa de la extinción de los dinosaurios hace 65 millones de años. Y que en ambos casos, Naia y Chicxulub, han participado investigadores radicados en instituciones de nuestro país.",
    "Como sabemos, el primer viaje de Cristóbal Colón a nuestro continente tuvo una duración de poco más de dos meses. Hoy en día un viaje similar en un avión jet comercial puede ser llevado a cabo en un tiempo que se mide en horas. Este contraste de tiempos puede ser visto desde diferentes puntos de vista. Uno de estos es el de la cantidad de energía por pasajero necesaria para cada medio de transporte, la cual es sustancialmente más grande para un avión a reacción que para un barco de vela. Así, empleando medios de transporte  modernos podemos desplazarnos más rápido que hace cinco siglos, pero a un costo energético considerablemente mayor.Desde la época en que éramos cazadores recolectores hace unos diez mil años el consumo promedio de energía per cápita se ha incrementado casi cien veces. El incremento ha sido gradual pero se ha acelerado en los últimos 200 años a partir de la Revolución industrial. Y esto ha sido mediante la explotación de los combustibles fósiles, carbón, petróleo y gas natural. Por otro lado, en la primera mitad del siglo XX se llevaron a cabo descubrimientos fundamentales sobre la naturaleza de las estructura del átomo, lo que puso de manifiesto la enorme cantidad de energía almacenada en el interior del mismo. Con el conocimiento adquirido, científicos e ingenieros idearon maneras de liberar esta energía, ya sea de manera incontrolada –como en una bomba atómica– o de forma controlada por medio de reactores nucleares. Dada la enorme magnitud de la energía almacenada en el interior de los átomos, resultaba natural pensar que en la energía nuclear estaba el futuro energético del mundo. Con el transcurrir de los años, sin embargo, esta energía no tuvo el desarrollo que hubiera podido pensarse. Así, si bien hay países que obtienen su energía eléctrica mayoritariamente de la energía nuclear –notablemente Francia, con el 75% de su consumo total– la energía nuclear juega un papel relativamente menor en comparación con los combustibles fósiles y a nivel global solamente de 13% de la energía eléctrica que consume el mundo es de origen nuclear.  Un problema con la energía del átomo es que su explotación genera productos radiactivos altamente tóxicos. Tres accidentes nucleares de gran magnitud nos recuerdan los peligros de la energía nuclear: los accidentes de “Three Mile Island”, ocurrido en Pensilvania, Estados Unidos, en 1979,  de Chernobil, Ucrania en 1986, y de Fukushima, Japón, en 2011. El accidente de Chernobil, como resultado del cual explotó la vasija del reactor, dispersó material radiactivo sobre todo el continente europeo y obligó a la evacuación de más de cien mil personas del área alrededor de la central nuclear. El gobierno soviético estableció una zona de exclusión de 30 kilómetros de radio alrededor del sitio del reactor, la cual está fuertemente contaminada, y que se ha mantenido hasta la actualidad.Se sabe que la radiación de alta energía producida por la desintegración de materiales radiactivos puede producir daños a nivel celular que pueden llevar al desarrollo de cánceres, a la alteración del material genético, e incluso a la muerte de las células mismas, y se han llevado a cabo estudios del efecto de la contaminación radiactiva en Chernobil sobre las especies que viven en el área, que se ha convertido en un laboratorio involuntario para este tipo de estudios.Uno de estos estudios fue realizado por un grupo de investigadores de Europa, los Estados Unidos y Japón, encabezado por Ismael Galván del Consejo Superior de Investigaciones Científicas de España, el cual fue publicado en línea el pasado mes de abril en la revista “Functional Ecology”.El estudio fue llevado a cabo con varias especies de aves, tanto dentro como fuera de la zona de exclusión, y arrojó un resultado sorprendente: encontró que aves que viven en zonas altamente contaminadas parecen haberse adaptado a los efectos de la radiación. Esto, por una especie de “selección innatural”, según uno de los autores de estudio. Los investigadores encontraron también un efecto inverso entre el daño genético y el nivel de contaminación, de modo que aves que viven en zonas menos contaminadas están peor adaptadas que aquellas que habitan zonas con más contaminación. No todos los especialistas, sin embargo, están de acuerdo con las conclusiones de Galván y colaboradores. En particular, un experto afirma que es difícil demostrar que la radiación en la zona de exclusión pueda haber tenido en efecto apreciable sobre las aves estudiadas. En todo caso, sean o no sean acertadas las conclusiones del artículo en cuestión, la energía nuclear no pierde su carácter de alta peligrosidad, incompatible con la vida, y no es un sustituto viable para los combustibles fósiles. Aun si algunos organismos fueran capaces de adaptarse a niveles de radiactividad más allá de los propios del mundo natural. Por un proceso de selección que no es de este mundo.",
    "Mucho ruido ha causado el incidente ocurrido el pasado fin de semana cuando un asistente al partido Barcelona-Villareal de la liga española de futbol le lanzó un plátano al jugador brasileño Dani Alves. En una acción inédita, el jugador lo levantó del suelo y se lo comió de un bocado, para después proceder a lanzar un tiro de esquina. El suceso se amplificó de manera sorprendente a través de internet a favor de Alves, al grado que al autor del insulto racista tiene problemas con la policía. Este tipo de hechos no son raros en los estadios de futbol europeos. Hay que reconocer, no obstante, que los prejuicios racistas en Europa, al menos en las apariencias, han disminuido en las últimas décadas. Sobre todo si recordamos que todavía no ha pasado mucho tiempo desde que en zoológicos y ferias mundiales se organizaban exhibiciones de personas de raza distinta a la europea –los llamados zoológicos humanos.La Exposición Mundial de París de 1889, por ejemplo –para la cual se construyó la Torre Eiffel–, incluyó entre sus exhibiciones a un “Pueblo Negro” con 400 indígenas.  De la misma manera, el pigmeo africano Ota Benga fue exhibido junto con otros pigmeos del Congo Belga en la Exposición Internacional de St. Louis en 1904. Posteriormente, Benga fue exhibido en  1906 en el zoológico de Bronx en Nueva York encerrado en una jaula junto con un perico y un orangután.Pero no es nuestro propósito dedicar este artículo a la discriminación racial, tan extendida en el mundo, sino tratar un tema que está de algún modo relacionado con la misma. Este tema se refiere al artículo aparecido esta semana en la revista en línea PLOS ONE en el que se revalúa a la especie Neandertal, la cual comúnmente se describe como inferior a nuestra especie. El artículo lleva como autores a Paola Villa del University of Colorado Museum, en los Estados Unidos y a Wil Roebroeks de la Leiden University, Holanda. Los expertos saben que los neandertales vivieron en Europa y en el Cercano Oriente desde hace 350,000 años, hasta hace unos 40,000 años cuando se extinguieron como especie. Esta extinción coincidió con la llegada del Homo sapiens proveniente de África. Según una opinión extendida, la extinción fue resultado de la superioridad de nuestra especie con respecto al Neandertal.  De acuerdo a los especialistas, el Homo sapiens se originó en África y de allí emigró hace unos 50,000 años hacia Europa y hacia otros lugares del mundo. Durante nuestra permanencia en África habríamos desarrollado una serie de capacidades cognitivas, de lenguaje, de fabricación de armas, de técnicas de caza, diversidad de alimentación y de organización social que no poseían los neandertales y en estas condiciones de gran desventaja, estos últimos no habrían resistido el choque con el hombre moderno sobreviniendo su extinción.Después de hacer una investigación de las evidencias arqueológicas existentes, sin embargo, Villa y Roebroeks discrepan de este punto de vista. Concluyen que las habilidades que comúnmente se asocian al hombre moderno también las poseían los neandertales.  Así, encuentran evidencias, por ejemplo, que los neandertales cazaban bisontes arrinconándolos y haciéndolos caer en pozos. Igualmente, hay evidencia de que tenían una dieta variada. Se han encontrado también pigmentos en sitios relacionados con neandertales que pudieran haber usado para pintarse el cuerpo y otros ornamentos. Esto indicaría que los neandertales practicaban rituales y tenía capacidad de pensar de manera simbólica.De acuerdo con Villa y Roebroeks, la supuesta inferioridad de los neandertales con respecto a nuestra especie se origina en la práctica de los investigadores de comparar las habilidades de neandertales que vivieron en el paleolítico medio, con las habilidades de hombres modernos del paleolítico superior.  En palabras de Villa “es como comparar una Ford modelo T con un Ferrari moderno y concluir que Henry Ford tenía habilidades congnitivas inferiores a las de Enzo Ferrari”.Al margen de lo anterior, es interesante preguntarse qué tanto influyeron nuestros prejuicios raciales para llegar a una conclusión (¿precipitada?) sobre la inferioridad cognitiva de los neandertales. Ciertamente los neandertales constituyen una especie diferente a la nuestra. Al mismo tiempo, no obstante, visualmente no nos parecen demasiado lejanos y esto puede estimular nuestros prejuicios raciales. Si bien los neandertales de extinguieron hace mucho tiempo y no es fácil determinar cuáles eran    sus habilidades cognitivas, investigaciones futuras con seguridad nos darán más datos al respecto. Un punto interesante que sí sabemos es que neandertales y humanos modernos se cruzaron y desde este punto de vista podemos decir que los primeros en realidad no se han extinguido. O viéndolo de otro modo, que todos somos un poco Neandertal.Un último comentario. Si aquel que lazó un plátano en el partido de futbol hubiera sabido que sus ancestros salieron de África hace 50,000 años, se habría dado cuenta que si a Dani Alves le gusta la fruta –y tal parece que es el caso– no es probablemente por su parentesco con los monos.  Que en realidad si lo tiene, como lo tenemos todos y en igual grado –incluyendo al barbaján–, pero bastante lejano.",
    "Es un hecho que a lo largo de la última década la velocidad con que está creciendo la temperatura global de nuestro planeta se ha reducido considerablemente. Esto ha dado armas a aquellos escépticos del calentamiento global para afirmar que dicho calentamiento se ha “pausado”; y que la catástrofe ambiental predicha hace algunas décadas –que no ha ocurrido– nunca sucederá porque el calentamiento global en realidad no existe. En contraste, para una gran mayoría de los estudiosos del clima de la Tierra, incluyendo a los expertos del Panel Intergubernamental de Cambio Climático, el calentamiento global es un hecho incuestionable y la reducción observada en el incremento de temperatura en años recientes es sólo temporal. Eventualmente, aseguran, la temperatura de la Tierra reanudará su tendencia ascendente.El debate sobre la realidad del calentamiento global lo tienen que dar, por supuesto, los expertos del clima. Como espectadores no especialistas, sin embargo, no podemos dejar de echar un vistazo a los datos de la temperatura de la Tierra en el pasado. Estos datos se pueden encontrar fácilmente en internet. En Wikipedia, por ejemplo, podemos consultar un artículo en el que se muestra una gráfica, con datos de la NASA, de la temperatura media de la Tierra desde 1880 a la fecha. Viendo dicha gráfica nos percatamos que de un año a otro la temperatura global experimentó cambios al azar, tanto positivos como negativos, que en algunos casos superan a un décimo de grado centígrado. Estos cambios caóticos son un reflejo de los múltiples factores que regulan la temperatura del planeta. A largo plazo, sin embargo, la gráfica nos dice que la temperatura de la Tierra ha tenido un crecimiento neto, que en el curso del último siglo fue de unos 0.8 grados centígrados. Este crecimiento no ha sido uniforme. Así, tenemos que entre 1900 y 1910, en contra de la tendencia a largo plazo, la temperatura terrestre se redujo cerca de 0.2 grados centígrados. Después de esto hubo un periodo de calentamiento entre 1920 y 1940, seguido  de un periodo que abarcó tres décadas de poco incremento e incluso de disminución de temperatura. A partir de 1970, la temperatura global reanudó su crecimiento, situación que se prolongó hasta final del siglo XX cuando dicha temperatura entró en una fase de estabilización aparente que se ha prolongado hasta nuestros días. Es esta última fase de 10-15 años la que usan como argumento algunos escépticos para negar la existencia de un calentamiento global. Con la misma lógica, no obstante, podíamos haber escogido el periodo entre 1900 y 1910 y concluir que la Tierra se está enfriando.  El hecho descarnado es que la temperatura global de la Tierra se incrementó en casi un grado centígrado en los últimos cien años y que esto produjo, por considerar sólo uno de sus efectos más visibles, la disminución del volumen de hielo en el Océano Ártico.Cabría la posibilidad de que el incremento en la temperatura global fuera debido a causas naturales. Si tal fuera el caso, con seguridad no existirían tantos argumentos en contra del calentamiento global y se aceptaría como un hecho comprobado que la Tierra se está calentando. El debate sobre sus causas específicas se circunscribiría enteramente al ámbito científico y pasaría en buena medida desapercibido por el público.  Una mayoría de expertos, sin embargo, considera que el calentamiento global no obedece a causas naturales sino que está asociado a la emisión de gases de invernadero a la atmósfera, principalmente dióxido de carbono generado por la quema de combustibles fósiles. El asunto tiene entonces una considerable importancia económica, lo que agita el debate. Como un ejemplo, Richard Rahn –quien, de acuerdo con Wikipedia, fue Vicepresidente y Principal Economista de la Cámara de Comercio de los Estados Unidos durante la administración de Ronald Reagan– publicó la semana que hoy termina en el periódico Washington Times un artículo que intituló “El Apocalipsis del calentamiento global que no sucedió”, en el que, aceptando que el clima de la tierra está cambiando por causa nuestra, recomienda que nos adaptemos al cambio climático que “ha traído más beneficios que perjuicios”. Afirma también que “más dióxido de carbono, calentamiento global y lluvias beneficiarán a la agricultura”.Si bien los expertos tienen todavía que encontrar una explicación sólida para la “pausa” actual del calentamiento global –lo que refleja la complejidad de los fenómenos que determinan el clima de la Tierra– a los no iniciados en la ciencia climática nos parece que si el incremento en la temperatura de la Tierra desde el inicio de la Revolución Industrial no tiene precedente en miles de años, y que si esto coincide en tiempo con un incremento en la concentración de dióxido de carbono sin precedente en los últimos cientos de miles de años, entonces ambos fenómenos deben estar conectados. Y que si no se ha producido una Apocalipsis hasta ahora, no significa que en el largo plazo no ocurran desastres climáticos. Es decir, si el animal tiene piel de conejo, orejas de conejo y salta como conejo, entonces debe tratarse de un conejo, y más vale que pongamos remedio si no queremos que nos destruya el jardín.",
    "Como comentamos en este espacio hace una semana, con la partida de Gustavo Del Castillo y Candelario Pérez de San Luis Potosí mediando el siglo pasado, se cerró una segunda y brillante época de construcción de instrumentos científicos en la UASLP, la cual se prolongó de 1955 a 1966. Con anterioridad, a lo largo del último cuarto del siglo XIX,  Francisco Javier Estrada, en su calidad de encargado de la cátedra del Instituto Científico y Literario –antecesor inmediato de la UASLP– desarrolló una carrera notable como constructor de los más diversos aparatos y dispositivos y con esto dio vida a una primera época de construcción de instrumentos en la Universidad.   Claramente, Gustavo Del Castillo y Candelario Pérez –por no decir Francisco Javier Estrada– se adelantaron a su tiempo y, como todos los pioneros, no la tuvieron fácil para llevar a cabo la empresa que se propusieron. A mediados del siglo XX el país en general y la Universidad en particular no estaban lo suficientemente maduros para acoger una iniciativa con estas características –hay que recordar que la Escuela de Física de la UASLP fue la tercera en México, mientras que el Instituto de Física fue el segundo en el país–. No obstante, el trabajo pionero de Gustavo del Castillo y Candelario Pérez  plantó una semilla en la UASLP que con el tiempo fructificó y convirtió a San Luis Potosí en uno de los principales centros de investigación en física en México.De haber existido mejores condiciones para el proyecto de Del Castillo y Candelario Pérez, la UASLP se hubiera hecho de un proyecto de investigación pionero en el país y la investigación en física hubiera echado sin duda raíces más tempranas en la UASLP. Y con un bono adicional: la capacidad de construcción de instrumental científico, que no es algo que se pueda cultivar en maceta Un escollo que enfrentaron Gustavo del Castillo y Candelario Pérez fueron los bajos salarios que podía ofrecer la Universidad.  De hecho, fue solamente hasta el año 1978 –12  años después de la partida de Candelario Pérez– que la UASLP otorgó los primeros nombramientos de profesor-investigador de tiempo completo. En principio, estos nombramientos posibilitaron que un profesor pudiera dedicar parte de su tiempo al desarrollo de un proyecto de investigación. En la práctica, los salarios que ofrecían las universidades en México –con muy pocas excepciones– no eran competitivos y esto dio origen a una “fuga de cerebros”, tanto externa, como interna –por no hablar de “multichambismo”. Dos programas federales solventaron en buena medida esta situación: el Sistema Nacional de Investigadores, creado en 1984, y el programa de estímulos al trabajo académico. Al mismo tiempo, el CONACyT –fundado en 1970– estableció un programa de becas para estudios de posgrado y programas de apoyos a proyectos de investigación. De este modo, a unas décadas de la segunda incursión de la física en San Luis Potosí, se dieron las condiciones adecuadas para el advenimiento de una tercera época de construcción de instrumentos en la UASLP, vigente hoy en día, que esperamos se prolongue de manera indefinida.Con la ayuda del Dr. José Refugio Martínez Mendoza y de la Dirección de Imagen y Promoción Institucional de la UASLP, durante la semana académica del Instituto de Investigación en Comunicación Óptica de la UASLP llevada a cabo al inicio del presente mes de abril, se montó una exposición de instrumentos construidos en la UASLP. Esta exposición buscó recrear las tres épocas, bien definidas, de construcción de instrumental científico en la UASLP que han sido motivo de los tres últimos artículos, incluyendo el actual, publicados en este espacio: el último cuarto de siglo XIX, los años 1955-1966 y la época actual. En referencia a esta última época, y como dos ejemplos de los muchos instrumentos actuales construidos en la UASLP, se expuso un microscopio que emplea una delgada punta de vidrio para obtener una imagen de la superficie de un material –e incluso, en algunos casos, de detalles que encuentren por debajo de dicha superficie–, y un instrumento óptico para uso en procesos de síntesis de materiales para electrónica. Ambos instrumentos están actualmente en operación en el Instituto de Investigación en Comunicación Óptica de la UASLP y de alguna manera, así sea imperfecta, reflejan el grado de desarrollo actual del campo de construcción de instrumentos en la Universidad.Y, sobre todo, hacen justicia a los pioneros de este campo en la UASLP, quienes trabajaron en condiciones desventajosas en un entorno nacional que no impulsaba el desarrollo de una ciencia y tecnología propias, en particular el desarrollo de instrumentos. Y, a pesar de todo, medio siglo después, su legado está hoy más que nunca presente en la UASLP.La exposición de referencia será montada nuevamente en el Centro Universitario de las Artes de la UASLP y podrá ser vista en fecha próxima.",
    "Como escribimos hace una semana en este mismo espacio, poco después de la muerte en 1905 de Francisco Javier Estrada –farmacéutico de profesión pero que desarrolló en San Luis Potosí en el último cuarto de siglo XIX numerosos aparatos y dispositivos de la más diversa índole– el país entró en un periodo de guerras civiles y levantamientos militares poco propicio para el desarrollo de actividades creativas, y en particular para la construcción de instrumentos científicos. En 1935, habiendo México alcanzado una calma relativa, el presidente Lázaro Cárdenas creó el Consejo Nacional de Educación Superior y de la Investigación Científica (CONESIC). Esto significó el inicio de un cambio en el país –si bien demasiado lento y pausado– en lo que refiere al impulso gubernamental a la ciencia, muy desprestigiada durante el conflicto revolucionario por aquello de los “científicos” porfiristas. Si bien el CONESIC enfrentó muchos problemas políticos y tuvo una vida efímera, en 1942 fue creada por el presidente Ávila Camacho la Comisión Impulsora y Coordinadora de la Investigación Científica. Esta, a su vez, fue sustituida en 1950 por el Instituto Nacional de la Investigación Científica (INIC), antecesor inmediato del CONACyT.  En estas condiciones relativamente favorables –aunque muy lejos de ser ideales– se dio un renacimiento en la construcción de instrumentos en la UASLP.  Este renacimiento fue debido a los esfuerzos tanto de Gustavo Del Castillo y Gama como de Candelario Pérez Rosales. Gustavo Del Castillo, potosino y químico de formación, se interesó por la física y decidió en 1951 viajar a los Estados Unidos en búsqueda de un doctorado en esta especialidad. Para este propósito ingresó a la Purdue University en el estado de Indiana. Ahí conoció a Candelario Pérez Rosales, también potosino, quien era igualmente estudiante de física pero en el nivel de licenciatura. Como lo relata Candelario Pérez en su libro Física al Amanecer, fue en el verano de 1954, cuando Del Castillo preparaba su regreso a México, que hicieron planes para la creación de una escuela de física en la UASLP. Esta se materializó a finales de 1955 cuando el Consejo Directivo Universitario aprobó la creación de la Escuela de Física, la cual inició operaciones el 5 de marzo de 1956, con Gustavo Del Castillo y Candelario Pérez como profesores de física y matemáticas, de manera respectiva.En paralelo a la creación de la Escuela de Física, Gustavo Del Castillo, quien era un formidable constructor de instrumentos, inició la construcción de una “Cámara de niebla” para la detección de las partículas resultantes al interactuar los rayos cósmicos provenientes del espacio exterior con una placa de plomo. La construcción de esta cámara en un tiempo récord de un año y cuatro meses es uno de los ejemplos más notables de la construcción de instrumentos en el país. En su libro Física al Amanecer, Candelario Pérez nos describe la operación de la cámara: “Era impresionante contemplar en acción ese incansable autómata que fotografiaba, día y noche, trayectorias de partículas que eran producto de la radiación cósmica. En medio de una densa y sofocante oscuridad, salpicada por el parpadeo de los focos indicadores del control automático, se escuchaba la explosión estruendosa que producía el aire al salir repentinamente de la cámara de expansión; luego, un destello cegador iluminaba el interior de la cámara de niebla, al tiempo que la cámara fotográfica registraba los eventos nucleares; después se oía el corrimiento de la película fotográfica y la cámara quedaba en espera del siguiente disparo”.  Candelario Pérez Rosales siguió con la tradición y se abocó a la construcción de un espectrómetro de centelleo, mismo que estuvo operativo en 1960.  Por este desarrollo el Gobierno del Estado de San Luis Potosí le otorgó el Premio en Ciencias “Francisco Javier Estrada”.Hoy en día, 60 años después de que Gustavo Del Castillo construyera su cámara de niebla no puede uno sino sorprenderse de lo insólito de su logro. No solamente por la poco común destreza técnica de la que hizo gala, sino también por el hecho de tuvo que buscar apoyos, tanto para adquirir los materiales necesarios para construir la cámara, como para complementar su salario como profesor de la Universidad –hay que recordar que la UASLP estableció el puesto de profesor-investigador sólo hasta el año 1978–. En su empresa, Del Castillo fue apoyado tanto por el INIC como por PEMEX. Los restos de la cámara de niebla pudieron ser admirados los primeros días del presente mes de abril en la exposición de instrumentos organizada como parte de la semana académica del Instituto de Investigación en Comunicación Óptica de la UASLP.Las condiciones que tanto Gustavo Del Castillo como Candelario Pérez encontraron en San Luis Potosí para el desarrollo de sus proyectos, sin embargo, no fueron las adecuadas y con el tiempo se vieron forzados a emigrar. Primeramente lo hizo Del Castillo en 1959 y 7 años después Candelario Pérez. Con la partida de ambos se dio fin a una segunda y brillante época de construcción de instrumentos en la UASLP. Hubieron de pasar varias décadas para que las condiciones del país posibilitaran una tercera época, vigente hoy en día, que esperamos se prolongue de manera indefinida. De esta última época nos ocuparemos el próximo domingo.",
    "Si bien es cierto que el sentido de la vista nos proporciona una asombrosa cantidad de información acerca del mundo que nos rodea, también lo es que tiene sus limitaciones. Podemos, por ejemplo, percibir luz en una gama de colores que se extiende desde el rojo hasta el violeta, pasando por el anaranjado, el amarillo,  el verde y el azul. No podemos, sin embargo, ver ni el infrarrojo ni el ultravioleta, que se encuentran más allá de los colores rojo y violeta, de manera respectiva. Igualmente, somos capaces de ver a simple vista una hormiga e incluso a una pulga, pero definitivamente no a un microbio, por no mencionar moléculas o átomos.Lo que es válido para la vista lo es también para los otros sentidos: todos tienen una gran sensibilidad  que, sin embargo, está al mismo tiempo severamente limitada. En este sentido, si dependiésemos solamente de nuestros sentidos el conocimiento que pudiéramos alcanzar del mundo físico y biológico sería sólo parcial. Afortunadamente, hemos desarrollado instrumentos que nos han permitido observar al mundo más allá de nuestras limitaciones. Galileo Galilei, por ejemplo,  empleando el telescopio pudo observar que alrededor del planeta Júpiter –que a simple vista se percibe sólo como una estrella brillante– orbitan cuatro lunas. Igualmente, el holandés Antonie van Leeuwenhoek descubrió, empleando microscopios de su manufactura, la existencia de organismos unicelulares invisibles a simple vista.Empleando instrumentos científicos hemos averiguado una enorme cantidad de cosas acerca del mundo físico y biológico, desde la composición química de las estrellas hasta la estructura atómica del ADN, y del conocimiento científico alcanzado se ha derivado la tecnología moderna, que lo mismo nos ha dado medios para curar enfermedades que desarrollado una vasta red global de comunicaciones que ha cambiado nuestro estilo de vida. Podemos quizá decir que una medida de la capacidad científica y tecnológica de un país nos la da su capacidad de construcción de instrumentos.En México, dada la poca importancia que se ha concedido históricamente a la ciencia, el desarrollo de instrumentos científicos ha sido igualmente limitado. Tenemos, no obstante, algunos ejemplos notables. Con relación a esto, y como parte de las actividades de la semana académica del Instituto de Investigación en Comunicación Óptica de la UASLP, se organizó en la semana que hoy termina una exposición que llevó por título “La instrumentación científica en la historia de la UASLP”, la cual buscó recrear tres épocas del desarrollo de instrumentos en la universidad potosina: el último cuarto del siglo XIX, el periodo que abarca de 1955 a 1966 y la época actual.   La primera época recreada se centra en Francisco Javier Estrada, nacido en 1838, quien si bien no tuvo una educación formal en física o ingeniería eléctrica, sí tuvo un fuerte interés en estos campos y se abocó al desarrollo de diversos instrumentos como responsable de la cátedra de Física del Instituto Científico y Literario de San Luis Potosí –antecesor inmediato la UASLP–. El nombramiento como catedrático le fue concedido a Estrada poco después de la reapertura del Instituto en 1867, una vez restaurada la República. En un país que acababa de superar una invasión extranjera y que arrastraba más de medio siglo de inestabilidad política, Estrada debió tener dificultades considerables para salir adelante en su empeño como constructor de aparatos e instrumentos. Y a pesar de esto, tuvo un insólito desempeño, desarrollando diversos dispositivos notables para su tiempo. Entre éstos se cuentan, por mencionar sólo algunos, un teléfono para largas distancias, un fonógrafo y  un sistema para la comunicación de un tren de ferrocarril en movimiento con las oficinas telegráficas. Cabe destacar que, con relación a este último invento, el presidente Porfirio Díaz concedió a Estrada en 1886 un privilegio exclusivo por diez años. Para poner este logro en contexto, hay que recordar que la primera trasmisión pública de telegrafía sin hilos fue realizada por Guglielmo Marconi  en Londres, Inglaterra, hasta el 27 de julio de 1896. Estrada se vio frenado en su actividad creativa no solamente por la situación general del país sino igualmente por cuestiones de salud, y al parecer también por problemas de índole política al interior del Instituto Científico y Literario. Así, en 1886 perdió su puesto como catedrático de Física; la razón: su imposibilidad física para asistir a impartir cátedra, debiendo los alumnos acudir a la casa de Estrada a recibir instrucción. El despido del profesor provocó el enojo de los estudiantes, quienes protestaron enérgicamente en una carta abierta dirigida al entonces gobernador del estado Carlos Diez Gutiérrez. La carta fue publicada en el periódico “El Estandarte”, el 8 de febrero de 1886.Francisco Javier Estrada murió en la Ciudad de México en el año de 1905, ciego y sin haber sacado mayor provecho de sus inventos. Como sabemos, poco después de la muerte de Estrada el país entró nuevamente en un periodo de guerras y conflictos, poco propicio para el desarrollo de actividades creativas. Así, la construcción de instrumentos en la UASLP hubo de esperar medio siglo para tener un renacimiento. De esto hablaremos el próximo domingo.",
    "De acuerdo con la Wikipedia, la primera fotografía de una persona fue realizada por Louis Daguerre en el año 1838. Dicha fotografía, que requirió de un tiempo de exposición de unos diez minutos, muestra el “Boulevard du Temple” de Paris en un día soleado. En un primer plano, en la parte inferior derecha de la imagen podemos ver un edificio de tres pisos, detrás del cual sube en diagonal el bulevar bordeado por edificios. El efecto visual que produce la fotografía es peculiar, pues si bien muestra claramente el efecto del paso de los años –lo que le da un aspecto que incluso podríamos calificar de fantasmagórico–, al mismo tiempo las imágenes de los edificios y de los árboles a lo largo de la avenida están claramente delineadas. El tiempo de exposición que fue empleado por Daguerre no permitió registrar personas en movimiento –si  es que las hubiera habido–. En la parte inferior izquierda, sin embargo, podemos ver la silueta de una persona lustrándose los zapatos, la cual debió permanecer relativamente inmóvil durante la toma. Dada la distancia a la que se encontraba, sin embargo, fuera de su silueta la fotografía no arroja ninguna otra información acerca de su persona.    Como sabemos, Daguerre y los daguerrotipos fueron solamente el inicio de una revolución que en el curso de medio siglo llevó al desarrollo de cámaras fotográficas que pudieron ser manejadas por las personas comunes y corrientes, y no solamente por los especialistas. Revolución que finalmente nos ha llevado a la época actual en la que tomar una fotografía puede ser tan simple como apretar un botón –virtual o real– de un dispositivo –un teléfono celular– que irónicamente fue diseñado con otro propósito.  La aparición de la fotografía permitió por vez primera registrar imágenes para la posteridad. Antes de la fotografía, la imagen de una persona podía preservase, por ejemplo, en una pintura de estudio. En tal caso, sin embargo, no hay seguridad del grado de fidelidad de la pintura con respecto a la persona representada. En contraste, si descartamos aquellas fotografías manipuladas a propósito, una fotografía es una imagen de una persona real, resultado de la luz que refleja la persona fotografiada, y en ese sentido las fotografías antiguas son una ventana hacia el pasado.Un ejemplo de esto nos lo da el sitio de internet denominado “Mirror of Race” en el que se exhiben fotografías de la segunda mitad del siglo XIX alrededor del tema del racismo y la esclavitud en los Estados Unidos. Este sitio fue motivo de sendos artículos publicados hace unos días por “The New York Times” y el periódico británico “The Guardian”. De acuerdo con sus creadores, “Mirror of Race” es un “lugar para la reflexión sobre el significado de raza en los Estados Unidos, tanto en el pasado, como en el presente y futuro”.  El sitio contiene fotografías de un periodo que va desde la década de 1840 hasta finales del siglo XIX. Incluye por lo tanto imágenes obtenidas antes y después de la abolición de la esclavitud en los Estados Unidos ocurrida en 1865. Aunque algunas de las personas fotografiadas están identificadas, la mayoría son anónimas o están identificadas sólo por su nombre propio.  Cada fotografía se presenta inicialmente con una información escueta con el objeto de que quién la vea se forme su propia opinión sin interferencia externa. Se presenta enseguida la información factual que se posee acerca de la fotografía y, finalmente, se proporciona una interpretación experta en el marco conceptual del sitio. Algunas de las fotografías de “Mirror of Race” son representativas de la violencia asociada a la esclavitud. Incluyen, por ejemplo, la fotografía de una persona de raza blanca identificada como “Abraham, amo de esclavos”, de aspecto siniestro –apropiado para una película de terror de los años treinta–, con un látigo de cuero en las manos. Esta fotografía, de finales de los años 1850, nos transporta –quizá más que ninguna otra– al mundo de violencia esclavista del siglo XIX. El sitio incluye también varias fotografías de esclavos liberados que fueron empleadas en una campaña para abolir la esclavitud. Entre las personas fotografiadas se incluye a Wilson Chinn, quien muestra las letras VMB en la frente que le fueron herradas por su dueño Volsey B. Marmillon. De la misma manera, el sitio incluye fotografías de niños en apariencia de raza blanca, hijos de padres esclavos y por tanto también esclavos.No alcanza el espacio de esta columna para describir con palabras una muestra de fotografías en extremo interesante que nos acerca a un pasado en cierto modo olvidado pero no tan distante; ni en el tiempo ni, desgraciadamente, en cuanto a haber superado los prejuicios del racismo. “Mirror of Race”  es ciertamente un sitio fascinante al que es necesario ingresar para poder apreciar. Hay que recordar que una imagen dice más que mil palabras. Y que una fotografía del pasado nos dice más de mil palabras…..del pasado.",
    "Los desechos espaciales que se encuentran en órbita alrededor de la Tierra –producto de más de medio siglo de actividades en el espacio– representan en opinión de los expertos un peligro creciente para satélites y naves espaciales. Según la NASA, circulan en órbita alrededor de la Tierra unos 500,000 objetos de todo tipo – partes de cohetes, tornillos, guantes, etc. – que viajan a velocidades que pueden alcanzar los 28,000 kilómetros por hora. Si bien la mayor parte de estos objetos son de pequeñas dimensiones –del orden del tamaño de una canica– unos 20,000 son más grandes que una pelota de softbol. No es difícil de imaginar las consecuencias que tendría chocar de uno de estos objetos viajando a tales velocidades. Hay que notar, no obstante que aun los objetos pequeños pueden ser letales para un satélite o nave espacial en caso de una colisión, dada la enorme velocidad que llevan en órbita.La basura espacial alcanzó notoriedad en los últimos meses por ser el tema de la película “Gravity” que recibió una enorme publicidad. Si bien algunas cosas que se ven en esta película resultan inverosímiles y con poco sustento científico, los choques catastróficos de satélites en órbita con desechos espaciales sí son reales. En febrero de 2009, por ejemplo, un satélite comercial norteamericano colisionó con un satélite ruso ya fuera de funcionamiento, resultando en más de 2,000 fragmentos que se añadieron a la basura espacial.  De la misma manera, en 1996 un satélite francés fue dañado al colisionar con desechos de un cohete también francés que había explotado diez años antes. En el año 2007, en un suceso que posiblemente sirvió de inspiración para el director de “Gravity”, China destruyó en el espacio –como parte de un experimento– un satélite meteorológico en desuso que se desintegró en 3,000 fragmentos que se sumaron al cúmulo de desechos espaciales.   Dada su peligrosidad, la NASA mantiene un inventario de los fragmentos más grandes de basura espacial, incluyendo las trayectorias que siguen. Esto, con el fin de anticipar posibles choques con naves o satélites en órbita y aplicar las medidas pertinentes si fuera el caso. En días pasados, por ejemplo, nos enteramos por los medios de comunicación que la NASA movió media milla la posición de la estación espacial internacional para prevenir un posible impacto con un fragmento de un satélite ruso lanzado en 1979.  En la medida en que aumente el número de fragmentos en órbita, sin embargo, será cada vez más difícil evitarlos y la única solución factible será la de su destrucción. De otro modo, consideran los especialistas, se colapsaría el sistema de satélites del planeta y se imposibilitarán los viajes espaciales.  Un fragmento espacial puede ser destruido provocando su reingreso a la atmósfera en donde se consumiría por el rozamiento con el aire. Para llevar a cabo este reingreso es necesario desestabilizar la órbita del fragmento modificando su velocidad por algún medio.  En días pasados un grupo de investigadores de la Universidad Nacional de Australia dio a conocer un proyecto en colaboración con la NASA para lograrlo empleando un láser de alta potencia. El láser estará colocado en el observatorio de “Mount Stromlo” en Australia y se hará incidir sobre el fragmento de basura a destruir, al que “golpeará” modificando su velocidad e induciendo su reingreso a la atmósfera.Hacer blanco con un láser en un objeto que se encuentra a cientos de kilómetros de distancia no parece, por supuesto, algo simple de realizar. Sobre todo si consideramos que el láser tiene que viajar a través de la atmósfera, la cual sufre perturbaciones continuas. Para lograrlo, los investigadores australianos emplearán técnicas desarrolladas por los astrónomos para observar objetos lejanos a través de la atmósfera, desde telescopios colocados sobre la superficie de la Tierra.Tendremos que esperar para averiguar si los investigadores australianos tienen éxito con su proyecto y logran destruir, aunque sea parcialmente, la basura espacial acumulada a lo largo de medio siglo. A finales de la década de los años cincuenta, cuando dio inicio la era espacial con el lanzamiento del primer satélite artificial, el “Sputnik 1”, hubiera sido quizá difícil anticipar que el espacio en las inmediaciones de la Tierra llegaría un día a saturarse con desechos espaciales, pues en la escala humana este espacio en inmenso.  El tiempo nos ha demostrado, sin embargo, que medio siglo de actividades espaciales descuidadas hicieron posible lo que parecía improbable.",
    "La revolución científica que se produjo en Europa a lo largo de los siglos XVI-XVIII tuvo un enorme impacto intelectual que cambió radicalmente nuestra concepción del mundo físico y natural. Con el transcurrir de los siglos, la nueva manera de ver al mundo nos llevó a desarrollos tecnológicos en los que la ciencia jugó un papel fundamental, que lo mismo alargaron nuestra esperanza de vida que nos proporcionaron nuevas formas de transportación y de comunicación a distancia, por mencionar sólo algunos ejemplos. En el mundo actual la ciencia y la tecnología de base científica tienen, además, un impacto económico de gran magnitud. De acuerdo con cifras publicadas por la “National Science Foundation” (NSF) de los Estados Unidos, la industria de alta tecnología y los servicios especializados basados en el conocimiento contribuyeron en el año 2012 con un 27% al producto interno bruto a nivel global.Dado que la ciencia se originó en Europa, fue en ese continente donde floreció inicialmente y en donde se desarrollaron sus primeras aplicaciones. El surgimiento de los Estados Unidos en el siglo XX como una potencia económica, no obstante, cambió este panorama, al grado que este país se convirtió en la segunda mitad del siglo XX en el principal centro científico y tecnológico del planeta.El mundo, no obstante, se mantiene en cambio continuo y los indicadores de ciencia y tecnología 2014 que publicó el pasado mes de febrero la NSF muestran que los Estados Unidos está perdiendo su posición preponderante en ciencia y tecnología –si bien todavía conserva el liderazgo mundial–. Esto a favor de los países del este, sudeste y sur de Asía, particularmente China, principal competidor de los Estados Unidos como potencia científica y tecnológica. Los números que ofrece la NSF en su informe son ilustrativos. Por un lado, tenemos que el 40% del producto interno bruto de los Estados Unidos es generado por servicios especializados e industrias de alta tecnología. Este porcentaje se compara favorablemente con el 29-30% de otros países desarrollados –países de la Unión Europea, Japón y Corea del Sur– y en ese respecto los Estados Unidos es el país de grandes dimensiones con el porcentaje más alto. Además, entre 1999 y 2012 dicho porcentaje se incrementó en varios puntos. En algunos países en desarrollo, sin embargo, las manufacturas de alta tecnología están creciendo muy rápidamente, notablemente en China, en donde se han multiplicado por un factor de cinco entre 2003 y 2012. En términos de porcentaje de producción global de estas manufacturas, China saltó de un 8% a un 24% en ese periodo. El porcentaje respectivo de los Estados Unidos en el año 2012 fue de 27%, de modo que aún está por arriba de China. Sin embargo, de seguir la tendencia de los pasados años, China rebasará pronto a los Estados Unidos como principal productor de manufacturas de alta tecnología.En lo que se refiere a los servicios especializados, éstos aun se concentran fundamentalmente en los países desarrollados. Así, el 32% de estos servicios a nivel global se generan en los Estados Unidos y el 23% en la Unión Europea, mientras solamente el 8% se fabrican en China.En inversión en investigación y desarrollo los Estados Unidos en 2011 se mantuvieron  a la cabeza con un total 429 miles de millones de dólares, seguido de China y Japón con 208 y 147 miles de millones de dólares, de manera respectiva. Con respecto a la inversión global, sin embargo, los Estados Unidos vieron reducida su participación de 37% en 2001 a 30% en 2012. En el mismo periodo, las economías de los países del este, sudeste y sur de Asia –entre los que se incluyen China, India, Japón, Malasia, Singapur, Corea de Sur y Taiwan– incrementaron dicha participación de 25% a 34%.Una medida de la actividad de investigación de un país lo da el número de sus publicaciones científicas. De acuerdo con la NSF, en 2011 se publicaron en el mundo 828,000 artículos científicos. De éstos el 24% correspondió a los Estados Unidos que, como país, se mantiene a la cabeza. Este porcentaje, no obstante, ha venido disminuyendo a una tasa de 4 puntos anuales desde el año 2001. Un fenómeno similar ocurre, en los países de la Unión Europea y en Japón. Todo debido, fundamentalmente, al incremento en las publicaciones de países asiáticos. Otros países aparte de asiáticos están igualmente incrementando su capacidad científica y tecnológica, como es el caso de Brasil y México en nuestro continente, pero su peso específico es menor al de aquellos.   Las cifras de la NSF Indican que, efectivamente, el centro de gravedad científico y tecnológico del mundo se está desplazando hacia el este de Asia. Un cambio que hubiera sido difícil de imaginar en Europa, cuna de la revolución científica, hace apenas cien años.",
    "No es ningún secreto que en cuanto a la manipulación de objetos electrónicos novedosos de alta tecnología se refiere –una televisión, una computadora,  un reloj digital, etc.– los niños y jóvenes muestran una habilidad digna de envidia: la de manipular con destreza artilugios que no les son familiares. Como adultos tal parece que hemos perdido en buena medida esta habilidad, lo cual no deja de ser lamentable en ciertas situaciones. Como cuando estamos entrampados tratando de cambiar la televisión de modo TV a modo de video, en cuyo caso más nos vale pedir la ayuda de algún joven –o niño– a nuestro alrededor.   Con la miniaturización de los circuitos electrónicos que se dio a lo largo del último medio siglo los aparatos electrónicos adquirieron una complejidad inimaginable hasta hace algunas décadas. Con los teléfonos celulares, por ejemplo –y según el chiste de moda–, además de poder tomar fotografías y videos, consultar internet, y  mandar y recibir mensajes, podemos también hacer llamadas telefónicas.   Una característica de los nuevos objetos electrónicos es que tienen múltiples modos de operación. Esta característica es desconcertante y difícil de manejar para quienes no fueron expuestos cuando jóvenes o niños a la tecnología digital –lo es, cuando menos, para quien esto escribe–. Antes del advenimiento de la microelectrónica los objetos eran inmutables en el sentido que tenían un solo modo de funcionamiento. Los botones de una televisión en el pasado tenían así una función única: prender o apagar el aparato, cambiar de canal, cambiar el brillo de la pantalla, etc. Hoy un  botón tiene una función múltiple que depende del modo de operación en que se encuentre el televisor y esto es algo difícil de manejar para aquellos ya no somos tan jóvenes.   ¿La facilidad con que los niños, en comparación con los adultos, entienden el funcionamiento de un aparato que les es desconocido indica que tienen una manera diferente de razonar? En un artículo publicado en el último número de la revista “Cognition”, investigadores de la University of Edinburgh en el Reino Unido y de la University of California, Berkeley, en los Estados Unidos, responden afirmativamente a esta pregunta. En particular, encuentran que en ciertos casos lo niños de edad preescolar tienen una mayor habilidad que los adultos para entender el funcionamiento de aparatos poco comunes. La conclusión anterior fue alcanzada por medio de un estudio con 106 niños de 4 y 5 años de edad, y con un grupo de 170 estudiantes universitarios. Durante el estudio se les presentó a los participantes una caja de música –con una tapa de color rojo– la cual se ilumina y se pone a funcionar como respuesta a colocar un objeto correcto o una cierta combinación de objetos de formas definidas –cilindros, pirámides, cubos, etc. –  sobre su tapa. El objeto o combinación de objetos que activa la caja de música cambia durante el experimento y los participantes tienen que determinar cuál objeto o combinación es el correcta.Los investigadores encontraron que con ciertas combinaciones poco usuales de objetos los niños superan a los adultos en el momento de determinar la combinación correcta. Esto es debido a  que los niños tienen una mayor flexibilidad para establecer una relación de causa-efecto en una situación novedosa en comparación con la flexibilidad de un adulto que, por su experiencia previa, puede tener ideas preconcebidas acerca de  dicha relación.    De acuerdo con los autores del artículo referido, los resultados obtenidos sugieren que “la tecnología y la innovación se pueden beneficiar de las habilidades exploratorias y de razonamiento que les son naturales a los niños pequeños, muchos de los cuales están aprendiendo a usar un teléfono inteligente aun antes de aprender a abrocharse las cintas de los zapatos”. Nuestra falta de habilidad para manipular artilugios electrónicos que nos son poco familiares podría tener entonces una explicación objetiva. Así, en la medida que crecemos adquirimos experiencia, la cual es ciertamente valiosa y usamos en nuestro beneficio. Al mismo tiempo, sin embargo, esa misma experiencia nos rigidiza y nos dificulta la adaptación a situaciones que requieren de soluciones novedosas. En particular, nos dificultaría entender cómo funciona el control remoto de una televisión a quienes no somos parte de la generación digital.Una explicación objetiva de nuestras limitaciones, no obstante, es de poco consuelo cuando intentamos ver una película en la televisión y no atinamos a apretar el botón adecuado del control del aparato. En esos momentos constatamos que los niños y jóvenes tienen en verdad una habilidad envidiable, que los adultos desgraciadamente hemos perdido en no poca medida.",
    "Difícilmente encontraríamos hoy en día a alguien que afirme que la Tierra es plana. Por más que muy pocos –aquellos que han viajado al espacio–hayan podido comprobar con sus propios ojos que ésta es esférica. La generalidad de los mortales lo creemos porque así nos lo enseñaron en la escuela y porque hay muchas evidencias indirectas en su favor, incluyendolas fotografías de nuestro planeta desde el espacio que otros han tomado por nosotros.De hecho, mucho antes de que pudiéramos viajar al espacio ya sabíamos que la Tierra esesférica. Lo sabíamos incluso antes del viaje de circunnavegación de Magallanes y Elcano cuando esto se comprobó de manera práctica. Fueron los griegos, hace más de dos mil años, quienes llegaron a la conclusión que la Tierra es esférica. Conclusión que alcanzaron observando, por ejemplo, cómo cambiaba la altura de las estrellas sobre el horizonte cuando se viajaba hacia el norte o hacia el sur; obien al observar la sombra circular que la Tierra proyecta sobre la luna durante un eclipse.Estas conclusioneslas alcanzaron pensadores como Platón y Artistóteles, que se ocupaban de elucubrar sobre el mundo, y no tantola gente común y corriente que tenía otras preocupaciones. De hecho, para el profano era –y aun lo es en cierta medida–más natural pensar que la Tierra es plana, pues esto es lo que nos parece a primera vista. Los expertos, no obstante, encontraron tantos argumentos en favor de una Tierra esférica, que la redondez de nuestro hábitat terminó por aceptarse de forma general.¿Cree usted en el calentamiento global, tal como lo hace en la esfericidad de la Tierra? Según el Centro de Investigación Pew, en 2013solamente el 67% de los estadounidenses creía que la Tierra se está calentando. Este porcentaje, además,no ha permanecido constante en los últimos años ymientras que en 2006 era de 77%, en 2009 se redujo hasta un 57%. Nuestra creencia en el calentamiento global no es de este modo tan sólidacomo nuestra fe en la redondez de la Tierra, al grado que incluso varía significativamente –a la alza y a la baja– en el curso de unos pocos años.¿Se está realmente calentando el planeta? Para el profano esto no siempre resulta obvio –el pasado mes de enero, por ejemplo, más bien parecía que se estaba enfriando–.Así, después de un invierno crudo es más difícil creer en el calentamiento global y lo contrario sucede en medio de unverano tórrido. Una respuesta objetiva a la pregunta solamente nos la pueden dar los expertos que se dedican a estudiar el clima y en este sentido el consenso es que sí hay un calentamiento global, y que la temperatura promedio de la superficie de la Tierra es hoy alrededor de 0.8 grados centígrados más alta que en el año 1900.Esto último es lo que de manera explícita se afirma en un documento aparecido la semana que hoy termina, publicado en forma conjunta por la “NationalAcademy of Sciences” de los Estados Unidos y la “Royal Society” del Reino Unido. Un punto significativo del documento es el hecho que haya sido suscrito por dos organizaciones de sólida autoridad científica, lo que lo hace digno de la mayor credibilidad. El documento afirma también que con una muy alta probabilidad el aumento de temperatura ha sido causado por actividades humanas. Particularmente por la emisión de gases de invernadero a la atmósferadebido al uso de combustibles fósiles. De estos gases, el que tiene más impacto es el dióxido de carbono cuya concentración en la atmósfera es en la actualidad 40% más alta que en el año 1800. Adicionalmente, más de la mitad de este incremento ocurrió desde 1970.De no modificarse la emisión de gases de invernadero, la temperatura de la Tierra en el año 2100 será de 2.6 a 4.8 grados centígrados más alta que en la actualidad. Esto provocará cambios climáticos que harán más húmedas algunas regiones húmedas y más secas algunas áreas secas, y posiblemente lleven a un aumento en la frecuencia de eventos climáticosextremos. Habrá, igualmente, un incremento en el nivel de los océanos, que ya es unos 20 centímetros más alto que hace cien años. Si bien el calentamiento global produce cambios lentos que son difíciles de apreciar para el no especialista, el consenso científico es que éste es real.Así, aun en medio del más crudo de los inviernos tendríamos los profanos que creer en que la Tierra se está calentando lentamente, del mismo modo como creemos en la esfericidad de la Tierra aun en contra de nuestra intuición.Es posible, sin embargo, que no todo sea tan simple pues hay otras razones no objetivas para negar el calentamiento global. En particular, porque creer en él implica combatirlo reduciendo la emisión de gases de invernadero y estolo convierte en un asunto económico y político.Así, el Centro Pew reporta quede aquellos en los Estados Unidos afiliados al partido demócrata, el 88% considera que hay una evidencia sólida del calentamiento global. En contraste, en el caso de los republicanos este porcentaje es de sólo el 50%.Pareciera ser que dar o no credibilidad al calentamiento de la Tierra es un asunto más complicado que decidir entre una Tierra plana y una esférica.",
    "El proyecto “Mars One”, que hizo noticia esta semana, es sin duda un proyecto muy peculiar –por decir lo menos–. Es peculiar no solamente porque pretende realizar una misión tripulada a Marte –que no tiene precedentes– con fondos privados, sino porque el viaje planeado será solamente de ida, pues se busca que los astronautas se queden a colonizar el planeta.  El proyecto, sin embargo, no parece tener viabilidad técnica, pues Marte se encuentra a una distancia muy grande que varía, según su alineación con la Tierra, entre los 59 y 102 millones de kilómetros. La distancia en realidad no resulta por sí misma un obstáculo insuperable para viajar hasta Marte. De hecho, toma solamente de cinco a diez meses llegar hasta allá, pues las naves interplanetarias viajan a una velocidad considerablemente alta. Un problema más grande lo constituye la radiación de alta energía que permea el espacio y a la que estarían expuestos los astronautas a lo largo del viaje. Una parte de esta radiación proviene de fuentes de energía más allá del sistema solar; otra parte, la más impredecible y peligrosa, se origina en el sol. En la superficie de la Tierra estamos protegidos de estas radiaciones por la atmósfera y por el campo magnético que rodea a nuestro planeta. En la travesía a Marte los viajeros estarían sin esta protección y absorberían radiaciones capaces de enfermarlos o matarlos.¿Qué tan altos son los niveles de radiación en el espacio? Un artículo publicado el pasado mes de diciembre en la revista “Science” nos da la respuesta: en un viaje con una duración de 180 días los astronautas recibirían una dosis de radiación que es aproximadamente un tercio del límite de seguridad establecido por la NASA. Estos datos, que fueron obtenidos por la sonda “Curiosity” en su viaje al planeta Marte, no toman en cuenta, sin embargo, que la actividad del Sol puede aumentar de forma imprevista elevando considerablemente su nivel de radiación emitida. Con relación a esto, se puede leer en la página electrónica de la NASA que en el periodo de tiempo que medió entre las misiones Apollo 16 y Apollo 17, que llevaron astronautas a la superficie de la Luna, las radiaciones del Sol se incrementaron de manera extraordinaria, y que de haber ocurrido este evento durante el tránsito de una de las naves a nuestro satélite, los astronautas habrían recibido dosis letales en unas diez horas. De acuerdo con la NASA, para futuras misiones tripuladas interplanetarias, uno de los problemas clave a resolver es precisamente el relativo a la protección de las tripulaciones en contra de las radiaciones del espacio.    Al margen de lo anterior, los astronautas del proyecto “Mars One” eventualmente recibirían dosis letales de radiación en la superficie de Marte, pues este planeta tiene una atmósfera demasiado tenue y no cuenta con un campo magnético protector como el de la Tierra.No parece, entonces, que el proyecto “Mars One” tenga factibilidad técnica. No parece tampoco que tenga viabilidad financiera si tomamos en cuenta que enviar un hombre a la Luna –que está considerablemente más cerca de la Tierra– costó unos 170,000 millones de dólares. Y, sin embargo, el proyecto es tomado de manera seria –al menos en apariencia– por numerosas personas. Mínimamente por las 200,000 que, según sus promotores, se inscribieron –pagando una cuota– para buscar uno de los 24 sitios en alguna de las naves que viajarían con dirección a Marte a partir de 2024. Cabe preguntarse quién puede estar interesado en realizar un viaje tan peligroso y, además, sin regreso. Una revisión en internet entre los semifinalistas nos arroja algunos nombres. Encontramos, por ejemplo, que Ken Sullivan, quien vive en Utah y es padre de cuatro hijos, está interesado en viajar a Marte –por más que haya sido amenazado por su esposa con el divorcio si continúa con sus planes–. Sullivan trabajó como contratista en Iraq arreglando helicópteros. Nuria Tapias de 30 años, del pueblo de Calafell en Cataluña, está también interesada en viajar a Marte. Nuria estudió filosofía y trabaja en una inmobiliaria. Es apasionada de la astronomía y espera vivir 40 años en la Tierra y el resto en Marte. No sabe en que se basaron para escogerla, pues solo le pedían que fuera mayor de 18 años y midiera entre 1.60 y 1.90 metros de altura.Entre los 1058 semifinalistas destaca Andrés Eloy Martínez Rojas, Secretario de la Comisión de Ciencia y Tecnología de la Cámara de Diputados. Martínez Rojas es astrónomo aficionado y considera que un punto que tiene a su favor para lograr estar entre los finalmente elegidos es que bautizó a un cráter que se encuentra en Marte con el nombre de Jojutla –Martínez es diputado por el Estado de Morelos–. Una de sus motivaciones para el viaje es precisamente visitar este cráter.     No es claro que tan firmes son las intenciones de viajar a Marte por parte de los más de mil preseleccionados y si dado el momento y se encuentran entre los finalistas de verdad se atreven a abandonar este planeta. Ciertamente, si bien es cierto que de todo hay en la viña del señor, también lo es que no hay loco que coma lumbre. En todo caso, es posible que nunca tengan que tomar la decisión y que solo se trate de un cuento chino sobre Marte que, sin duda, dejará ganancias a sus promotores.",
    "Comparado con el oro –e incluso con la plata– el cobre es ciertamente un metal que ocupa un lugar secundario en muestraescala de valores. Así, decimos que alguien “enseña el cobre” cuando al querer aparentar ser de oro muestra una faceta deleznable que asumimos es más propia del cobre. La menor jerarquíaque para nosotros tieneeste metal en comparación con el oro queda de manifiesto a lo largo de las dos semanas que duran los juegos olímpicos, cuando se entregan medallas de oro, plata y bronce al primero, segundo y tercer lugares de una competición. Si bien las medallas de oro sólo lo son nominalmente, pues tienen muy poco de este metal, las de bronce si son mayoritariamente de cobre.Hay razones para que le tengamos al oro un mayor aprecio que el que le tenemos al cobre. Una de ellas es que el primero esen buena medida químicamente inerte y no pierde su lustre con el tiempo, como sí sucede con el cobre. Otra razón tiene que ver con la escasez del oro en el mundoen comparación con el cobre. Si damos un vistazo a las estadísticas nos daremos cuenta que en 2012en todo el planeta se extrajeron 2,700 toneladas de oro. Como este elemento es muy pesado,esta producción es equivalente a un cubo de oro sólido de apenas unos 5 metros por lado. En contraste, en el año 2006 seprodujeron 16 millones de toneladas de cobre, que representa en peso una cantidad varios miles de veces mayor que la producción de oro. De este modo, dado el poco oro que hay en el mundo, su posesión es símbolo de estatus social y no sorprende que lo tengamos en una alta estima.La situación, no obstante, es engañosa pues en realidad el cobre ha sido considerablemente más importante que el oro en la historia humana. Así, por ejemplo, tenemosque los arqueólogos distinguen en la historia de la civilización a una Edad del Cobre pero no a una Edad del Oro. El cobre, junto con el oro y la plata fueron los primeros metales trabajados por nuestros antepasadosmiles de años antes de nuestra era. El oro es un material dúctil que  se empleó para fabricar joyería y objetos suntuarios. El cobre por su lado, si bien es relativamente suave, puede tratarse o mezclarse con estaño o zinc para aumentar sudureza, lo que posibilitó su uso para la fabricación de armas y herramientas. Es interesante notar que Otzi, la momia de 5000 años de antigüedad que fue descubierta en 1991 enterrada en hielo en los Alpes, en la frontera entre Austria e Italia, tenía entre sus pertenencias un hacha de cobre.Posteriormente, el cobre y sus aleaciones perdieron ventajas con el desarrollo de la metalurgia del hierro, que resultó ser un material más duro y ligero.Con el advenimiento de la electricidad en el siglo XIX, sin embargo, el cobre cobró nueva vida y –por segunda vez– se convirtió en un elemento esencial para el mundo. En efecto, es a través de cables hechos de cobre que se mueven los electrones que encienden una lámpara eléctrica, hacen funcionar una televisión, mueven el motor de una bomba de agua o accionan la marcha de un automóvil. Así, la producción mundial de cobreha ido en aumento continuo a lo largo de los últimos cien años –con la excepción de los periodos que corresponden a la Gran Depresión y a la Segunda Guerra Mundial–para satisfacer la creciente demanda.Hay quienes, no obstante, auguran que este crecimiento llegará pronto a su fin y que la producción mundial de cobre, al agotarse las reservas, alcanzará en unas décadas un máximo a partir del cual decaerá, de manera similar a como se ha proyectado para otros minerales, notablemente el petróleo. Richard Kerr, reportero científico de la revista “Science”, escribe sobre el tema en el número de esta semana de la misma. Las proyecciones, llevadas a cabo por especialistas en Australia, indican que si bien en las próximas dos o tres décadasla producción de cobre seguirá incrementándose para satisfacer la demanda,alrededor de 2040 dicha producciónse desplomará hasta alcanzar valores sustancialmente menores que los actuales -un punto interesante en este sentidoes que la producción de cobre de México no disminuirá sino que incluso aumentará.Como con toda proyección, sin embargo, no todo mundo está de acuerdo. Se hace notar, por ejemplo, que a diferencia del petróleo, el cobre se recicla en alrededor de un 50%, un porcentaje que podría incrementarse en los años por venir.Al margen de los que nos depare el futuro –o mejor dicho, les depare a los que logren llegar hasta esas fechas–, el que se presente una discusión sobre la posibilidad de que una materia prima –sea petróleo, carbón, cobre, oro o cualquier otra– esté en posibilidad de agotarse significa que algo estamos haciendo mal. Desde que empezamos a usar el cobre hace diez mil años siempre estuvo a nuestra disposición. Ahora, según algunos, existe la posibilidad de que después de todo este tiempo termine por escasear. En cuyo caso posiblemente lo lleguemos a apreciar más que al oro y se decida, por ejemplo, que a un campeón olímpico se le premie con una medalla decobre –o, dicho con más propiedad, de oro recubierto con cobre.",
    "El 7 de febrero de 1964 arribó al aeropuerto John F. Kennedy de la ciudad de Nueva York el grupo británico The Beatles. Dos días después –el día de hoy hace justamente medio siglo– se presentaron en la televisión de los Estados Unidos ante una audiencia de 73 millones de televidentes encendiendo una beatlemanía que se extendió por todo el mundo. A los Beatles les siguieron en su viaje a los Estados Unidos muchosotros grupos británicos de rock–incluyendo a los RollingStones, los sobrevivientes más célebres de la época– lo que a la larga tuvo un impacto profundo en el desarrollo de la música popular en el mundo.La década de los años sesenta es una de las más recordadas del siglo XX por la gran cantidad de acontecimientos de repercusión mundial ocurridos en ese periodo. Además de la aparición de los Beatles, entre otros sucesos el siglo XX nos trajo: el asesinato del Presidente Kennedy –ocurrido dos meses y medio antes de la llegada de los Beatles a Nueva York–, la guerra de Vietnam, el movimiento hippie, la píldora anticonceptiva y el escalamiento de las tensiones entre los Estados Unidos y la Unión Soviética. Ésta última llevó a la crisis de los misiles de Cuba la cual, según algunos estudiosos, estuvo a punto de desencadenar una guerra nuclear. El peligro de una guerra de aniquilación total entre las dos superpotencias fue tratada de manera magistral por el director de cine Stanley Kubrick en su memorable película de humor negro “Dr. Strangelove o: cómo aprendí a dejar de preocuparme y amar a la bomba”, película cuyo estreno, por cierto, tuvo lugar casi simultáneamente con el arribo de los Beatles a los Estados Unidos. De acuerdo con Kubrick, si bien en esos momentos el peligro de una guerra nuclear era real, dadoel caso no podríamoshacer nada por evitarla y, por tanto, más valíarelajarse y tomar el asunto con calma.Las tensiones entre los Estados Unidos y la Unión Soviética durante la década de los años sesenta tuvieron, no obstante, también un lado amable –y relajante–, lejos del peligro de una catástrofe nuclear. En efecto, en abril de 1961 la Unión Soviética puso en órbita a Yuri Gagarin, el primer humano en el espacio, adelantándose a los Estados Unidos quienes hicieron lo propio sólo hasta febrero del siguiente año.Los soviéticos adelantabanasí por corto margen a los Estados Unidos en materia espacial.Esto motivó que en mayo de 1961 el presidente Kennedy anunciara el propósito de los Estados Unidos de llevar una misión tripulada a la superficie de la Luna y traerla de regreso a la Tierra antes de terminar la década.Si bien Kennedy no pudo ver cumplido su objetivo, como sabemos la NASA por medio del programa Apollologró realizar un viaje tripulado de ida y vuelta a la superficie de la Luna en julio de 1969, unos meses antes del límite previsto, colocando a los Estados Unidos a la cabeza de la exploración espacial. Lo hizo la NASA, no obstante, a un costo estratosférico de $170,000 millones de dólares, en dólares de 2005.Como quiera que haya sido, la agencia espacial estadounidense se hizo de un enorme prestigio popular cuando unos 500 millones de personas en todo el mundo pudieron ver por televisión cuando Neil Armstrong bajó por la escalera del módulo lunar y dio los primeros pasos sobre la superficie de la Luna. A partir de ahí, la NASA fue popularmente concebida como el centro de investigación científica y tecnológica por excelencia.Después de conquistar la Luna, sin embargo, a la NASA le resultó difícil encontrar otro objetivo practicable que pudiera resultar tan atractivo y con logros tan espectaculares como los que obtuvo con el vuelo tripulado a la Luna. Después de nuestro satélite cualquier otro destino espacial está demasiado lejos. Marte, el más cercano,dependiendo su posición relativa con nuestro planeta se encuentra a una distancia que varía entre 59 y 102 millones de kilómetros. Según la ruta a seguir, el viaje a Marte toma de 5 a 10 meses, comparado con los tres días que le tomó a los astronautas del Apollo llegar hasta la Luna. Una misión tripulada a Marte tendría entonces un costo considerablemente más alto que el viaje a nuestro satélite natural, además de que los astronautas se verían amenazados por las radiaciones de alta energía que encontrarían en el espacio fuera de la protección de la Tierra.De este modo, algunos ven a la NASA como una agencia con problemas para conseguir apoyo público para sus proyectos. El encontrar signos de vida en Marte ha sido uno de los argumentos que se han empleado para justificar las actuales misiones de exploración en ese planeta. Las sondas que en estos momentos se encuentran en Marte, sin embargo, no tienen la capacidad para detectar actividad biológica.En un suceso desafortunado, la agencia espacial hace algunos años patrocinó un estudio que aseguraba haber descubierto una nueva forma de vida basada en el arsénico en lugar de fósforo, lo que resultó un fiasco. Así, la NASA, al igual que los RollingStones, es un sobreviviente de una década que transformó al mundo. Y al igual que aquellos, la agencia muestra los estragos del paso del tiempo.",
    "Ya viene el solYa viene el sol, y digoTodo está bienThe Beatles, 1969No hay mal que dure cien años –afortunadamente– y si bien todavía nos quedan un buen número de frentes fríos por venir antes de que termine la temporada invernal, tal parece que el tiempo empieza a mejorar y nos devuelve la confianza en el sol como sostén de la vida en el planeta . Este invierno se ha caracterizado no solamente por las bajas temperaturas que hemos sufrido sino también por los muchos cambios climáticos repentinos. Y, sin embargo, posiblemente no pase a la historia como un invierno particularmente crudo. Al menos no como sucedió con el año 1816, que es conocido como el Año que no tuvo verano. 1816 fue, en efecto, un año con un clima excepcionalmente frío –en Inglaterra la temperatura promedio descendió 2 grados centígrados–. Se piensa fue producto de la erupción del volcán Tambora en Indonesia en abril de 1815. De acuerdo con Alan Robock de la Universidad de Maryland, citado por la revista “New Scientist”, dicha erupción habría arrojado a la atmósfera entre 100 y 200 kilómetros cúbicos de piedra y ceniza –el equivalente a una masa de roca en forma de un de cubo de 10 a 15 kilómetros por lado–. La erupción de Tambora, que pudo oírse a 2,000 kilómetros de distancia, habría sido la mayor en la historia escrita.  Las cenizas de la explosión se dispersaron en la atmósfera, bloqueando la luz del sol, arruinando cosechas y provocando hambrunas. Inspirado por un día oscuro en el que “los candiles se encendieron como si fuera la medianoche”, el poeta inglés Lord Byron escribió en el Año que no tuvo verano: “Tuve un sueño que no era del todo un sueño/El brillante sol se apagaba, y los astros/vagaban diluyéndose en el espacio eterno,/sin rayos, sin senderos, y la helada tierra/oscilaba ciega y oscureciéndose en el aire sin luna;/ la mañana llegó, y se fue, y llegó, y no trajo/consigo el día” (Oscuridad, 1816).El Año que no tuvo verano no es, sin embargo, caso único en la historia. Un evento mucho más grave ocurrió en el año 536 d.C. Escribió al respecto Procopio de Cesarea, cronista de Justiniano, emperador del Imperio Bizantino: “Y sucedió durante este año que ocurrió un presagio terrorífico. El sol iluminó sin brillo, como la luna, durante todo el año, de la misma manera que el sol en un eclipse, y los rayos que envió no fueron claros ni como los acostumbra enviar”. El enfriamiento resultante por la disminución de radiación solar trajo consigo malas cosechas, hambrunas y caos social. Se considera también que fue causante de la epidemia conocida como Plaga de Justiniano, que se desencadenó en el Imperio Bizantino a partir del año 542 y que, según Procopio, causaba 10,000 muertes diarias. Los expertos creen que la Plaga de Justiniano fue una epidemia de peste bubónica que antecedió a la epidemia del Siglo XIV en Europa conocida como la Muerte negra, en la que también se piensa el clima jugó un papel importante.Aparte  de las crónicas de la época, los científicos tienen pruebas de que, efectivamente, alrededor del año 536 ocurrió un enfriamiento a nivel global. Michael Baillie, de la Queen´s University  Belfast en el Reino Unido, realizó estudios con anillos del tronco de árboles y encontró que el crecimiento de los anillos más delgados ocurrió alrededor del año 536, lo que es indicativo de la prevalencia de bajas temperaturas. Como sabemos, los anillos de los árboles crecen anualmente con espesores que dependen de las condiciones climáticas imperantes. Si estas son buenas el grosor de los anillos es mayor y lo contrario ocurre si son desfavorables.   No es claro, sin embargo, que fue lo que causó el bloqueo de la radiación solar. Según los especialistas, hay dos posibilidades. Una de ellas es la de una erupción volcánica masiva, similar a la que dio origen al Año que no tuvo verano. Otra posibilidad tendría que ver con el impacto de restos de cometas con la Tierra. Se ha aventurado que fragmentos desprendidos del cometa Halley, que se acercó al sol en el año 430, pudieron haber sido interceptados por nuestro planeta algunos años después y arrojando nubes de polvo a la atmósfera.Regresando al tiempo presente, los problemas que hemos sufrido en las últimas semanas por el crudo invierno son nada comparados con los sufridos por nuestros antecesores –con todo y la preocupante amenaza de la influenza que crece día a día según datos oficiales–, lo que desmiente aquello de que todo tiempo pasado fue mejor. Esto, por supuesto, de poco consuelo nos sirve y nos mueve a pedirle al sol que, en lo sucesivo, no se aleje demasiado.",
    "De acuerdo con John Cannarella y Joshua A. Spechler, estudiantes doctorales de la Universidad de Princeton en los Estados Unidos,  participar en una red social es en cierto sentido equivalente a estar contagiado por una enfermedad. Para mayor precisión, en un artículo escrito por estos dos investigadores –que está aun sin publicar pero que a partir de esta semana puede ser consultado electrónicamente en el sitio arXiv de la Universidad Cornell– se hace un paralelismo entre el crecimiento inicial y posterior declive de una epidemia, por un lado,  y el nacimiento y desaparición de una red social, por el otro.En su estudio, Cannarella y Spechler establecen una correspondencia entre una persona infectada y un miembro de una red social. Establecen, igualmente, un paralelismo entre una persona susceptible de contraer la enfermedad y el usuario potencial de una red social, lo mismo que entre una persona curada o inmune a la enfermedad y una que se opone a las redes sociales. Una red social exitosa correspondería de esta manera a la prevalencia de la epidemia, y puesto que las epidemias por más virulentas que sean a la larga desaparecen, las redes sociales tendrían irremediablemente un ciclo paralelo de nacimiento, vida y muerte.El artículo de Cannarella y Spechler probablemente no hubiera tenido la trascendencia que ha  alcanzado de no haber hecho a Facebook el motivo de su estudio. Y, sobre todo, de no haber pronosticado su desaparición en muy pocos años. De acuerdo con estos investigadores, Facebook –con más de mil millones de usuarios– llegó ya a su cima de crecimiento –máximo de la epidemia– y empezará a declinar rápidamente al grado de que perderá al 80% de sus usuarios en algún momento entre los años 2015 y 2017.    Cannarella y Spechler basan su predicción en datos obtenidos de Google sobre la frecuencia con que se ha tecleado la palabra Facebook a lo largo de los últimos años, la cual encuentran que ha ido a la baja a partir de 2012. Esto lo interpretan como el inicio de una pérdida de interés en Facebook por parte de los usuarios. Para predecir qué tan rápido progresará esta falta de interés, los investigadores utilizaron herramientas matemáticas que se emplean para investigar la propagación de epidemias. Durante una epidemia la enfermedad se transmite por contagio directo, de modo que entre más contactos tenga una persona con otras enfermas más alta será la probabilidad de que resulte infectada. Traducido al caso de una red social, diríamos que entre más usuarios tenga la red más alta será la probabilidad de que una persona dada, en contacto con los usuarios, se convierta en un nuevo usuario. Por otro lado, del mismo modo que en una epidemia los enfermos pueden sanar o morir, un usuario de una red social puede dejar de serlo y superar la “enfermedad”. El balance entre los crecimientos de infectados y curados determina el curso de la epidemia. Las drásticas predicciones de los investigadores de Princeton les provocaron críticas negativas por los métodos que emplearon para alcanzar sus conclusiones, mismas que algunos consideraron son infundadas. Entre otras cosas, los críticos argumentan que los 1,100 millones de usuarios de Facebook no desaparecerán de la noche a la mañana. Adicionalmente, un artículo sarcástico intitulado “Desprestigiando a Princeton”, escrito por científicos que trabajan para Facebook, apareció el jueves pasado en la página de internet de esta compañía. El artículo implícitamente descalifica a los métodos usados por Cannarella y Spechler y afirma que las conclusiones que alcanzaron no tienen sentido. Basan sus conclusiones en una “investigación” dirigida a determinar la frecuencia con que la palabra Princeton es usada en búsquedas en Google y encontraron que cada vez menos usuarios la teclean, lo que indica que está decreciendo el interés en esa Universidad. Concluyen que la Universidad de Princeton se quedará sin alumnos en el año 2021. Devolvieron así el golpe a quién aparentemente ni la debía ni la temía.  Al mismo tiempo, sin embargo, otros consideraron que tanto el enfoque que Cannarella y Spechler dieron a su investigación, como el método que usaron para resolver el problema que plantearon, son muy interesantes. En particular, les resulta atractiva la comparación que hacen entre una red social y una epidemia.  En pocos años sabremos quién tiene la razón. Si la tienen los críticos de Cannarella y Spechler, y Facebook continua con su impresionante paso que lo ha llevado a tener como usuarios al 15% de los pobladores del planeta. O si bien, como una de tantas epidemias que han asolado al mundo, se achica rápidamente hasta prácticamente desaparecer en el curso de unos pocos años. Tal como le sucedió a su red antecesora Myspace, que nació, llegó a su máxima extensión –75 millones de usuarios– y declinó drásticamente en menos de una década.  En tanto lo averiguamos, tendremos que reconocer que cuando menos en algo Cannarella y Spechler tienen razón: las redes sociales son como una epidemia muy contagiosa que induce en los “enfermos” comportamientos impensados hasta hace apenas unos pocos años. Como el de estar continuamente pendientes de la computadora o del teléfono celular, recibiendo y contestando mensajes.",
    "En un artículo publicado el pasado 9 de diciembre en el diario británico “The Guardian”, Randy Schekman, ganador del premio Nobel de Fisiología o Medicina y profesor de la Universidad de California, Berkeley, afirma que su grupo de investigación ya no publicará artículos de investigación en revistas como Nature y Science, a las que se refiere como revistas de lujo. Hablando en términos automovilísticos, Nature y Science son los Rolls Royce de las revistas científicas –entre las cuales también hay volkswagens, toyotas, nissans y vochos–, y se supone que solamente publican resultados de la más alta calidad y trascendencia científica. Sheckman, no obstante, dice de Nature y Science: “Si bien publican muchos artículos sobresalientes, no publican solamente artículos sobresalientes. Ni son las únicas editoriales que publican resultados de investigación sobresalientes. Esas revistas de manera agresiva manejan su reputación, más para vender suscripciones que para estimular la investigación más importante”. Nature y Science de este modo estarían más preocupados en lo llamativo –o “sexy” como lo llama Scheckman– de los artículos que publican que en su calidad científica.Por supuesto, estas fuertes opiniones generaron comentarios, algunos adversos. Se señaló, por ejemplo, que Sheckman no tenía una posición imparcial pues es editor de la revista eLife que compite contra Science y Nature. La revista eLife se publica electrónicamente en internet y pertenece a la categoría de las llamadas de revistas de acceso libre que pueden ser consultadas gratis en línea por cualquier persona. Los costos de publicación de estas revistas se cargan al autor que publica y pueden superar los 2,000 dólares por artículo. Science y Nature, en contraste, se financian vendiendo suscripciones, ya sea para la versión impresa de las revistas o para su consulta a través de internet.Se ha criticado igualmente que Sheckman haya expresado sus opiniones con respecto a Science y Nature hasta después de que se le concediera el premio Nobel, mismo que ganó por trabajo que publico en dichas revistas. Y que lo haya hecho precisamente la misma semana en que le fue entregado dicho premio, cuando pudo atraer una máxima atención. Así, no faltan suspicaces que ven en esto una estrategia para ganarle terreno a las revistas de lujo.  Sheckman se dice sorprendido, no el por hecho de que haya habido respuestas expresando un amplio espectro de opiniones, sino por la cantidad de las mismas. Esto, considera, es una clara prueba de que el asunto debe ser discutido. En respuesta a las críticas por haber esperado tanto para emitir sus opiniones, Sheckman afirma que en su posición como ganador de un premio Nobel puede expresar algo que otros creen, pero que no dicen por temor a perjudicar su carrera científica.  En la discusión que se ha dado, sin embargo, hay algo en lo que parece haber menos discrepancia. Esto se refiere al sistema de revisión al que es sometido un artículo que es enviado a una revista para su publicación, el cual muchos consideran debe ser de alguna manera cambiado. Con el objeto de formarse una opinión acerca de la calidad científica de un determinado artículo, el editor de la revista lo envía a uno o varios expertos pidiéndoles que expresen su opinión al respecto. Sobre la base de los informes recibidos, el editor decide aceptarlo o rechazarlo.Esta práctica en principio aseguraría la calidad científica de los artículos publicados. No siempre es así, sin embargo, ni aun en las revistas de lujo. Un ejemplo muy socorrido en este sentido es el trabajo publicado en diciembre de 2010, precisamente en la revista Science, en la que se reporta el descubrimiento de una bacteria que sustituye al fósforo por arsénico en su material biológico. De haber resultado cierto, esto hubiera constituido el descubrimiento de una nueva forma de vida y hubiera sido un resultado, no solamente vistoso, sino sensacional, con una enorme trascendencia científica. El artículo, no obstante, fue  inmediatamente criticado de manera casi unánime por los expertos y hoy ha quedado en el más completo descrédito. Los críticos de las revistas de lujo, por supuesto, lo ven como un ejemplo de lo que ha señalado Sheckman en su artículo de “The Guardian”. Al igual que en muchos aspectos de nuestra vida,  la emergencia del internet ha traído profundos cambios en la manera como se comunican los resultados científicos y el “affaire Sheckman” fue posibilitado por estos cambios. Con el internet han aparecido las revistas científicas de acceso libre y esto representa un desafío para las revistas tradicionales que se sostienen vendiendo suscripciones. No en balde, hace algunos meses fuimos testigos del “affaire Bohannon” –comentado en su momento en este espacio–, promovido por Science en contra de las revistas de acceso libre, aunque no dirigido en contra de las revistas serias, como es el caso de eLife.Al margen de la discusión, habría que coincidir con Sheckman en que, en lo que a los artículos publicados en Nature y Science se refiere, ni son todos los que están ni están todos los que son, y así como hay automóviles más rápidos que los Rolls Royce, hay otras revistas que pueden publicar artículos tan trascendentes, como los que aparecen en las revistas de lujo.",
    "Durante los intensos fríos que sufrimos en días pasados, en los que el cielo se mantuvo cubierto por una gruesa capa de nubes, nos fue más que evidente –por si acaso lo hubiéramos olvidado– lo importante que es el Sol para nosotros. De hecho, sabemos que nuestra estrella es la fuente de toda la vida que existe en la Tierra, pues sin la luz del Sol no podría darse el proceso de fotosíntesis mediante el cual las plantas crean la materia orgánica. Así, sin el concurso de la radiación solar nuestro planeta sería una masa de roca inerte, sin el menor rastro de vida. El Sol tiene igualmente la función de mantener la superficie de la Tierra a una temperatura adecuada para el desarrollo y supervivencia de los seres vivos –por más que en días pasados nos hayan surgido dudas al respecto. Y, por supuesto, es el Sol la fuente última de energía para producir los alimentos que necesitamos para no morir de hambre.  En la actualidad el Sol es para nosotros importante también en otro sentido, que va más allá de nuestra mera supervivencia. En efecto, desde hace un par de siglos hemos requerido de cantidades crecientes de energía para mantener e incrementar nuestra capacidad industrial, lo mismo que para cubrir nuestras necesidades de transporte y vivienda. Hasta hoy, la energía ha sido mayoritariamente obtenida de combustibles fósiles –carbón, petróleo y gas natural–. Éstos, no obstante, además de contaminantes de la atmósfera son no renovables y en algún momento se agotarán. Afortunadamente, nos queda el recurso de la energía solar, que de manera sobrada podría proveernos de la energía que necesitemos, cuando en el futuro nos quedemos sin combustibles fósiles. Desafortunadamente, si bien tenemos energía solar en abundancia, ésta tiene un problema básico: es intermitente. De este modo, si en buena medida hemos de depender de esta energía en el futuro, habrá que desarrollar medios para almacenarla durante el día para usarla en la noche –o bien en las horas cuando la radiación solar disminuya por alguna causa.¿Qué medios se emplean en la actualidad, o se proyecta emplear en el futuro, para almacenar la energía solar? Existen varios. Se puede, por ejemplo, generar electricidad por medio de módulos solares y emplearla para accionar bombas para comprimir aire. El aire bajo presión se almacena en cavernas subterráneas para su posterior uso. O bien se puede bombear agua y almacenarla en un lugar elevado como energía gravitacional. En otro esquema, se puede usar electricidad solar para generar hidrógeno mediante la descomposición de agua. El hidrógeno así generado se almacena para su uso posterior como combustible. Hay ciertamente un buen número de medios empleados para almacenar la energía solar –lo mismo que la energía del viento, que es igualmente intermitente–, pero quizá el primero que se nos viene a la cabeza es la batería de plomo-ácido, como la que empleamos en los automóviles para ponerlos en marcha, o bien la batería de litio de los teléfonos celulares. Este tipo de baterías, no obstante, no es el más adecuado para almacenar energía solar o eólica.Una mejor opción es la batería de flujo, la cual genera electricidad a partir de fluidos que se hacen circular por la misma. De este modo, la cantidad de energía almacenada depende, no del tamaño o potencia de la batería, sino de la cantidad de fluidos almacenados. La capacidad de almacenamiento de energía puede ser entonces incrementada aumentando el volumen de fluidos sin necesidad de aumentar su potencia. Esto contrasta con las baterías convencionales en donde la potencia y la capacidad de almacenamiento de energía están en relación directa. Un uso más amplio de las baterías de flujo, sin embargo, ha sido obstaculizado por los altos costos de los materiales –en particular el metal vanadio– que componen los fluidos para operarlas.En un artículo publicado esta semana en la revista británica “Nature” un grupo de investigadores de “Harvard University” en los Estados Unidos y de “Eindhoven University of Technology” en Holanda reportaron el desarrollo de una batería de flujo que emplea moléculas orgánicas de bajo costo. De acuerdo con Michael Aziz, uno de los autores del artículo, la nueva batería puede resolver el problema de almacenamiento de energía. En el caso de una casa-habitación, por ejemplo, plantea que un tanque de 2,000 litros podría bastar para almacenar suficiente energía solar durante el día para satisfacer las necesidades nocturnas.No todo luce tan color de rosa, sin embargo, pues la nueva batería emplea bromo y ácido bromhídrico que pueden causar problemas severos de contaminación en caso de que ocurra una fuga. De hecho, de acuerdo con Aziz, el bromo es el talón de Aquiles de dicha batería.Si la nueva batería efectivamente resolverá el problema de almacenamiento de energía es algo que nos lo dirá el futuro. De un modo u otro, no obstante, se tendrá que encontrar una solución si hemos de asegurar el futuro energético del mundo en base a la energía solar. De otro modo, no será motivo de orgullo que, dada la cantidad ingente de energía que el Sol pone a nuestra disposición, no la podamos aprovechar de manera plena.",
    "No podríamos quizá esperar que quien se apreste a realizar un viaje al planeta Júpiter tenga una preocupación demasiado grande por la comida que llevará para la travesía. Ciertamente, no porque considere que la comida es superflua –a menos que tenga la intención de suicidarse o de viajar en estado de hibernación, como en la famosa película de Stanley Kubrick de los años sesentas–, sino porque con seguridad tendrá la mente ocupada en un sin número de otros asuntos, propios de un viaje a un lugar tan lejano e inhóspito. Viajar a Júpiter no es una empresa sencilla. De hecho, no se avizora que lo hagamos en las próximas décadas, pues está muy por encima de nuestras capacidades tecnológicas y económicas actuales –además de que no tenemos una motivación real para hacerlo–. Hay quien se preocupa, sin embargo, por los alimentos que consumirán los astronautas del futuro en viajes de larga duración, alimentos que se pretende no sean demasiado diferentes a los que acostumbramos en nuestro planeta. Esto, con el objeto de disminuir la presión sicológica sobre los viajeros espaciales, ya de por sí muy grande.Un ejemplo de lo anterior es el artículo publicado la semana que hoy termina en la revista  “Food Research International” por investigadores químicos de la Universidad Aristóteles de Tesalónica en Grecia. En dicho artículo se reportan los resultados de un estudio, financiado por la agencia espacial europea, llevado a cabo para determinar el efecto que una fuerza de gravedad mayor a la terrestre –como la de Júpiter– tendría sobre la preparación de papas fritas. Se sabe que el grado de dorado de una papa depende del calor que es transportado por convección desde el fondo de la freidora a la superficie de la papa. La convección es el fenómeno por medio del cual un volumen de líquido caliente en el fondo de un recipiente experimenta una fuerza de flotación y sube a la superficie desplazando a otro volumen similar del líquido a menor temperatura– como cuando calentamos agua en la estufa–. La fuerza de flotación que produce la convección, por otro lado, depende de la fuerza de gravedad, de modo que la convección será más grande en la medida que ésta última aumente.En pocas palabras, podríamos esperar que en la superficie de Júpiter se puedan preparar papas fritas más crujientes, como lo señala la revista “Science” en su último número al comentar el artículo de los investigadores griegos.    Si bien los viajes de exploración espacial son cualitativamente diferentes a los viajes de exploración de los siglos XV y XVI que llevaron a la colonización europea del mundo, es interesante hacer una comparación entre los alimentos de los viajeros en ambos casos. Como sabemos, el interés de España en encontrar una ruta por el oeste a las indias orientales llevó primero a  Cristóbal Colón a “descubrir” el Nuevo Mundo y años después a la circunnavegación del globo terrestre por una expedición que inició en 1519 Fernando de Magallanes y finalizó Sebastián Elcano tres años después. Con el transcurrir de los siglos, los viajes de exploración iniciados por españoles y portugueses llevaron a prácticamente todos los rincones de la superficie terrestre. En comparación, los resultados de los viajes espaciales han sido modestos y el único destino extraterrestre que ha alcanzado un viaje tripulado es la Luna.    Esto último por supuesto, es resultado de las dificultades técnicas que presentan los viajes espaciales, que han demandado de grandes desarrollos tecnológicos en muchos campos. En este contexto, no es sorprendente que la comida de los astronautas sea igualmente foco de una atención detallada; mucho mayor que la que en su momento se le dio a los alimentos que consumieron los marinos exploradores hace cinco siglos.Uno de los relatores del viaje de circunnavegación de Magallanes y Elcano fue Antonio Pigafetta. Después de salir del estrecho de Magallanes y penetrar en el Océano Pacífico, Pigafetta relata en su diario: “El miércoles 28 de noviembre de 1520 salimos de dicho estrecho y entramos en el mar pacífico en donde permanecimos por tres meses y 20 días sin llevar alimentos frescos, y solamente comimos galletas reducidas a un polvo lleno de larvas y apestando por la suciedad que las ratas habían hecho cuando se comieron las galletas buenas, y bebíamos agua que era amarillenta y apestosa. También comimos cuero de buey....que era muy duro y que tuvimos que poner cuatro o cinco días en el mar y después en las brasas…..Además de todas las calamidades anteriores, el infortunio que mencionaré fue el peor, éste fue que las encías superiores e inferiores de la mayor parte de nuestros hombres se hincharon tanto que no pudieron comer, y sufrieron tanto que 19 murieron”.  Como ahora sabemos, las encías de los marineros se hincharon por causa del escorbuto, que a su vez es causado por una falta de vitamina C en la alimentación. El cuidado que los exploradores marinos hace cinco siglos ponían en su alimentación fue evidentemente muy diferente del que gozarán los futuros exploradores del sistema solar, quienes es posible que incluso disfruten de mejores papas fritas en compensación por todas las molestias del viaje. En el caso, por supuesto, de que los viajes interplanetarios sean algún día factibles.",
    "En las primeras décadas del siglo XX la compañía General Electric lanzó imágenes publicitarias que presentaban a la luz eléctrica como rival del Sol, tanto en empaques de lámparas incandescentes como en otro tipo de materiales. En una de estas imágenes, impresa en  la cara superior de un papel secante de tinta del año 1914  –que  es posible encontrar en el sitio internet del “Smithsonian Museum”–,  aparece el Sol emergiendo detrás de la Tierra, al lado de una lámpara incandescente emitiendo rayos de luz. El Sol mira a la lámpara, su único rival según reza el cartel, con una expresión entre complaciente e intrigada.Aunque válida para propósitos publicitarios, es una exageración, por supuesto, presentar al foco eléctrico como un rival del Sol. No obstante, en la noche, cuando el Sol se oculta, la luz eléctrica ciertamente tiene ventajas sobre un rival inexistente en esos momentos, con el resultado de que la lámpara incandescente ha cambiado de manera drástica nuestros hábitos nocturnos.¿Cómo era la vida nocturna antes de la llegada de la luz eléctrica? Hay ciertamente actividades que no necesitan de iluminación para llevarse a cabo y para éstas la luz eléctrica no ha significado gran cosa. En otros casos –los más– la luz eléctrica es muy ventajosa, pues los niveles de iluminación que nos proporciona son muy superiores a aquellos que es posible obtener, por ejemplo, con velas de cera.El director de cine Stanley Kubrick en su película Barry Lyndon de 1975 describe visualmente el ambiente de Inglaterra y Europa del siglo XVIII, incluyendo el ambiente de noche. Para esto grabó escenas nocturnas de interiores sin iluminación eléctrica, empleando solamente la luz proveniente de velas de cera. Incluye en la película escenas de interiores lujosos profusamente iluminados –en la medida de lo posible– por medio de candiles con numerosas fuentes de luz. Incluye también escenas escasamente iluminadas con algunas pocas velas. Aun en el primer caso, los niveles de iluminación resultan notoriamente inferiores a los que se acostumbran en la actualidad y en cierto modo resultan hasta tenebrosas.El desarrollo de la luz eléctrica se atribuye a Thomas Alva Edison, quien inventó lámparas incandescentes que demostraron ser comercialmente viables. Edison igualmente estableció el modelo actual de generación de energía en instalaciones centrales y de distribución de la energía generada a los consumidores. En sus inicios el foco incandescente tuvo que competir con la iluminación por medio de gas. Sin embargo, las ventajas de la electricidad sobre el gas, que resulta peligroso como combustible, pronto hicieron que la lámpara incandescente ganara la carrera y se estableciera como el medio común de iluminación. El foco incandescente con todas sus ventajas es, sin embargo, notablemente ineficiente para generar luz y el noventa por ciento de la energía que consume la convierte en calor. En sus inicios, en una época en la que los costos de los combustibles fósiles necesarios para generar energía eléctrica –en centrales termoeléctricas– eran bajos, las lámparas incandescentes no encontraron obstáculos para su desarrollo, aun con su marcada ineficiencia. En contraste, a partir de la crisis del petróleo en la década de los años setentas esta ineficiencia se convirtió en un parámetro a considerar, a tal grado que en la actualidad la lámpara incandescente tiene sus días contados.En efecto, alrededor del mundo las lámparas incandescentes –en su versión original– están en proceso de desaparecer. En los Estados Unidos, a partir del próximo primero de enero no se podrán fabricar ni importar lámparas incandescentes de 60 y 40 watts y los establecimientos comerciales solamente podrán vender sus existencias hasta agotarlas. Una suerte similar la sufrieron en ese país a lo largo de los dos últimos años los focos de 75 y 100 watts. En México desaparecieron del mercado en 2012 y 2013 las lámparas de 75 y 100 watts. Los focos de 60 y 40 watts habrían de desaparecer en nuestro país por norma oficial a partir del 1 de enero de 2014. La Comisión Nacional para el Uso Eficiente de la Energía, no obstante, ha retrasado la aplicación de la norma hasta el 1 de enero de 2015. Las lámparas incandescentes en nuestro país tendrán de este modo un año adicional de vida.   Más temprano que tarde, sin embargo, los focos incandescentes convencionales desaparecerán como fuentes de iluminación, excepto en algunas aplicaciones especiales. Cuando esto llegue se habrá ido una época, que duró más de un siglo, en la que las noches en nuestro planeta paulatinamente se fueron iluminando, hasta un grado tal que en la actualidad las luces nocturnas de la Tierra son visibles desde el espacio. Irremediablemente desaparecerán las lámparas incandescentes originales más no así la iluminación nocturna, que provendrá en el futuro de lámparas más eficientes. Quizá incluso pudiera ser que un Stanley Kubrick del futuro consiga que un museo le facilite por unos días algunas lámparas incandescentes para filmar imágenes que describan visualmente cómo se vivía en las noches del siglo XX.",
    "Si bien hoy en día un objeto hecho de plástico se considera frecuentemente una imitación barata y poco elegante de su equivalente fabricado con materiales naturales, esto no siempre ha sido así. En efecto, y a manera de ejemplo, podemos mencionar que al final de la Segunda Guerra Mundial se dio en los Estados Unidos lo que se conoce como los “tumultos del nailon”, en los que grupos de mujeres pelearon en los almacenes de ropa por conseguir un par de medias fabricadas empleando este material. Muchas de ellas sin alcanzar éxito.El nailon es una fibra desarrollada en 1935 por el químico Wallace Carothers, quien trabajaba para la compañía DuPont en los Estados Unidos. Esta compañía vio en el nailon un sustituto atractivo de la seda para fabricar medias; y no se equivocó, pues las medias de nailon gozaron de un éxito instantáneo cuando se pusieron a la venta. De acuerdo con Emily Spivak en un artículo publicado en el Smithsonian Magazine, el 16 de mayo de 1940 se pusieron a la venta en los Estados Unidos 4 millones de pares de medias de nailon con un precio de 1.15 dólares por unidad, con tan buen tino que se agotaron en apenas dos días.Al entrar los Estados Unidos como combatiente en la Segunda Guerra Mundial las medias de nailon desaparecieron de los anaqueles cuando la producción de la fibra fue dedicada a la fabricación de paracaídas y otros implementos militares. Solamente al final de la guerra volvieron a estar disponibles, causando los mencionados tumultos hasta que DuPont logró aumentar su volumen de producción para satisfacer la demanda.El nailon es un material artificial –es decir, que no se encuentra de manera natural– formado por largas cadenas de átomos llamadas polímeros. Los químicos habían descubierto en las primeras décadas del siglo XX la manera como se unen los átomos para formar un compuesto químico y este conocimiento fue clave para la fabricación del nailon. El nailon es así un desarrollo tecnológico en el que la ciencia jugó un papel central.No es el único ejemplo, por supuesto. Lejos de esto, las tecnologías modernas, que se caracterizan por una gran sofisticación, son el resultado de la aplicación del conocimiento científico. Otro ejemplo en este respecto es el silicio –igualmente un material que no existe como tal en la naturaleza–, que es empleado para la fabricación de los cerebros y memorias de computadora. Sin los descubrimientos científicos realizados a lo largo de la primera mitad del siglo XX, referentes a la estructura interna –atómica– de los materiales, no existiría el silicio y por consecuencia no habría computadoras tal como las conocemos –y, por supuesto, tampoco internet ni redes sociales.Uno de los campos de investigación más activos en la Universidad Autónoma de San Luis Potosí es el de los materiales y en este respecto hay que destacar el Premio Nacional de Ciencias que recientemente le fue entregado por el Presidente de la República a Magdaleno Medina Noyola, investigador del Instituto de Física de la Universidad. Medina Noyola es especialista en los llamados materiales vítreos –o simplemente vidrios–, que se distinguen de otros materiales como el silicio por la manera  como se ordenan los átomos en su interior.En el silicio de calidad suficiente para fabricar “chips” de computadora  los átomos están espaciados regularmente, excepto por algunos defectos que en algunos casos son intencionalmente introducidos y en otros son inevitables. Tenemos, no obstante, que si bien los átomos de silicio tienen una tendencia natural a acomodarse de manera regular, lograr que efectivamente lo hagan no es de ninguna manera sencillo. Para esto se requiere elevar la temperatura del material hasta fundirlo y enseguida enfriarlo muy lentamente. Esto con el fin de dar a los átomos la posibilidad de moverse en la búsqueda de su posición preferente, antes de que la masa de silicio se solidifique y queden inmóviles.En contraste, en los materiales vítreos aun con todos los cuidados no es posible lograr que los átomos terminen acomodados de manera regular. Un ejemplo de éstos es precisamente el vidrio común, en cuyo interior los átomos están colocados de manera desordenada. Como lo explica Medina Noyola, el comportamiento de los materiales vítreos constituye un misterio cuya develación ha resistido los esfuerzos de los especialistas a lo largo de casi cien años.Ciertamente, los expertos han desarrollado teorías para explicar el comportamiento peculiar de los vidrios. Éstas, no obstante, han resultado controvertidas y no han logrado un consenso generalizado. En este respecto, Medina Noyola piensa que tiene la respuesta y que una teoría que ha desarrollado en la UASLP puede explicar cómo al enfriar un material vítreo sus átomos en movimiento no logran alcanzar el estado final ordenado –que si alcanzan otros materiales como el silicio– y se inmovilizan en el camino en un estado intermedio desordenado.Resolver el problema de los materiales vítreos es posible que tuviera un interés solamente académico de no ser, como comenta Medina Noyola, porque la gran mayoría de las sustancias con las que tenemos contacto en nuestra vida diaria –incluyendo los alimentos y las medicinas– pertenecen a esta categoría de materiales. Así, entender el comportamiento de los materiales vítreos podría tener incluso un impacto más grande que el nailon, que en su momento provocó hasta tumultos.",
    "¿Recuerda usted cuándo usó por vez primera un procesador de textos o bien supo de su existencia? Para quien esto escribe el recuerdo del primer contacto con una de estas herramientas, que tanto ha facilitado la escritura de todo tipo de textos, se pierde en la noche de los tiempos. Esto, ciertamente, resulta una expresión exagerada, en tanto que el horizonte temporal de la noche de los tiempos para los procesadores de textos no pasa de ser apenas de unas tres décadas. Tres décadas que, sin embargo, han sido suficientes para hacernos olvidar que existió un tiempo en que estos procesadores no existían y que escribir un texto, y sobre todo hacerle correcciones, era complicado, al menos  en comparación con los tiempos actuales. Esto, por otro lado, no resulta sorprendente, pues a lo bueno es fácil acostumbrarse. Como nos hemos acostumbrado –igualmente en poco tiempo– a la medicina moderna, la cual nos ha librado de la amenaza de numerosas enfermedades que en el pasado asolaron a la población del mundo. Un hito en este sentido fue, como sabemos, el descubrimiento de la penicilina por Alexander Fleming en 1928, y su aplicación para combatir infecciones bacterianas llevada a cabo por Howard Florey, Ernst Chain y Norman Heatley en la Universidad de Oxford. La penicilina fue usada por primera vez por Florey y colaboradores con fines terapéuticos en la persona de Albert Alexander –de 43 años de edad y de  profesión policía–, quien se había rasguñado en la boca con la espina de una rosa cuando trabajaba en su jardín. Como resultado del rasguño, a Alexander le vino una infección severa que lo llevó a perder un ojo y lo puso al borde de la muerte al extendérsele a los pulmones. Dado que el caso era desesperado, Alexander se convirtió en candidato para experimentar con la penicilina, lo cual era arriesgado pues no se conocían los efectos secundarios que podría tener en los humanos. Después de que se le administró una primera dosis, el paciente experimentó una mejoría y le cedió la fiebre. Desafortunadamente, una dosis no fue suficiente y dado que no disponía de más penicilina, la infección regresó y Alexander murió a los pocos días. Hoy en día, gracias a los antibióticos, no esperaríamos morir por el rasguño de la espina de una rosa; hace siete décadas esto no estaba garantizado, como lo pudo atestiguar Albert Alexander. Los antibióticos nos han dado un margen de seguridad contra las infecciones bacterianas que han ocasionado numerosas epidemias y cobrado numerosas víctimas a lo largo de la historia del mundo. Una de las más famosas, durante la cual habría muerto un tercio de la población de Europa, es la epidemia de peste bubónica llamada Muerte Negra ocurrida en el siglo XIV. Según los expertos, la epidemia penetró al continente europeo por el sur de Italia, transportada desde Asia por barcos mercantes. En el número de esta semana de la revista “Science” se incluye un artículo en el que se describe un estudio que está llevando a cabo un grupo de investigadores encabezado por el antropólogo Clark Spencer Larsen de la Universidad Estatal de Ohio en los Estados Unidos, en la Abadía de San Pedro, cerca de Pisa, Italia. La investigación se centra en las numerosas tumbas que se encuentran alrededor de esta abadía,  cuya antigüedad abarca un periodo de mil años, entre los siglos XI y XIX. El estudio tiene como objetivo averiguar tanto como sea posible acerca de las víctimas, incluyendo su lugar de procedencia, los alimentos que consumían, las diferencias de alimentación entre clases sociales, las enfermedades que sufrían y las causas de su muerte. Entre las tumbas, los investigadores esperan encontrar algunas pertenecientes a víctimas de la Muerte Negra. Un estudio de los esqueletos por medio de diferentes técnicas, tales como la tomografía computarizada tridimensional y el análisis de isótopos de los dientes, les permitirá determinar la cepa del patógeno con que fueron infectados. Averiguarán también su estado de nutrición y si de manera concurrente sufrían de otra enfermedad como la tuberculosis, lo que les hubiera debilitado y hecho más susceptibles al ataque del bacilo de la peste bubónica.  Además, según los investigadores, dado que los muertos enterrados en la Abadía de San Pedro se habrían encontrado entre las primeras víctimas de la epidemia, por encontrarse cerca del punto de entrada del bacilo al continente, una comparación del patógeno que les provocó la muerte, con cepas encontradas en otras partes del norte de Europa, les dará indicios de los modos de propagación de la epidemia.\tLos expertos no tienen la certeza de que fue lo que hizo tan mortífero al bacilo de la Muerte Negra. Cualquiera que haya sido, la razón se esconde –esa sí– en la noche de los tiempos. Confiemos, no obstante, en que las técnicas analíticas de investigación moderna podrán en algún momento penetrar la oscuridad nocturna y desvelar un misterio de siete siglos.En tanto esto sucede, confiemos también en que ya nunca más alguien pueda de manera irremediable morir por un rasguño de una espina de rosa y que este recuerdo se pierda definitivamente en la noche de los tiempos. Como se han perdido, por cierto, las máquinas de escribir.",
    "Cuando usamos la hornilla de la estufa para cocinar nuestros alimentos, lo mismo que cuando viajamos en un automóvil impulsado por un motor de gasolina, liberamos energía que estuvo almacenada en combustibles fósiles –gas o petróleo– por un largo periodo de tiempo. En efecto, dichos combustibles son producto de la descomposición de materia orgánica enterrada bajo gruesas capas de sedimento hace millones de años, y desde entonces habían permanecido bajo la superficie de la Tierra. Esto, hasta fechas recientes –unos dos siglos–, cuando los empezamos a  extraer de manera sistemática.Como fuentes de energía, los combustibles fósiles han tenido mucho éxito. Lo han tenido a tal grado que hemos dado ya buena cuenta de ellos –pensemos, por ejemplo, en el yacimiento de Cantarell en la Sonda de Campeche–. No es difícil identificar las causas de este éxito: los combustibles fósiles contienen energía concentrada que es fácilmente liberada al quemarlos. Además, por su misma alta concentración de energía, resulta económico transportarlos desde su sitio de origen hasta su lugar de consumo.     Con todas sus virtudes, sin embargo, los combustibles fósiles tienen un defecto obvio: no son renovables –si asumimos que esperar millones de años para su reemplazo no es una opción viable– y en un futuro probablemente habrán de agotarse. Otro inconveniente –que no defecto, pues en este respecto los combustibles fósiles son los menos culpables– es que la quema acelerada de carbón, petróleo y gas que se ha dado a partir de la revolución industrial está cambiando la composición de nuestra atmósfera. Dado que toda materia orgánica se forma mediante el proceso de fotosíntesis con la ayuda de la luz del Sol, los combustibles fósiles tienen su origen último en la radiación solar que llega  a la superficie de nuestro planeta. Este origen es compartido por otras fuentes de energía, entre las que se incluyen los biocombustibles, fabricados a partir del maíz o de la caña de azúcar, y la energía del viento, producto del calentamiento de la superficie de nuestro planeta por los rayos solares.Así, a través de los combustibles fósiles, de los biocombustibles y del viento, que actúan como intermediarios, aprovechamos la energía del Sol de una manera relativamente simple. En primera instancia podríamos quizá pensar que si eliminamos a estos intermediarios tendríamos acceso a la energía solar de una forma todavía más simple. Algunas veces así resulta, como cuando la usamos para calentar el agua del baño. No es siempre el caso, sin embargo. En particular, usar la energía del Sol para generar electricidad ha resultado más complicado que usarla simplemente para calentar agua. Esto ha sido frustrante en cierto modo, pues la electricidad es una forma muy útil de energía a la que podemos manipular y transformar de manera relativamente simple, lo mismo que transportar a largas distancias. Y no es que no haya existido la tecnología necesaria para convertir en electricidad la energía del Sol de manera eficiente, sino que el costo de la misma no ha sido en el pasado lo suficientemente bajo para hacerla económicamente viable.No obstante lo anterior, la electricidad solar ha avanzado rápidamente en los últimos años. Alemania, por ejemplo –uno  de los países en donde más se ha impulsado el uso de la energía del Sol–, tiene actualmente instalada una capacidad de generación de 25 GW de electricidad solar. Esta cifra es, en números redondos, igual a la mitad de la capacidad total de generación de energía eléctrica de México, que en un 75% proviene de los combustibles fósiles. Además, según los expertos, se espera que para el año 2050 el 16% de toda la energía eléctrica que se genere en el mundo sea de origen solar.  Un problema evidente con la radiación solar es que es intermitente y que está disponible solamente durante el día; y por supuesto, aun en la horas de sol dicha radiación puede ser parcialmente bloqueada por las nubes. Así, una instalación solar tiene, o bien que complementarse con otra instalación de generación de electricidad que no dependa del Sol de manera directa, o bien debe contar con un medio de almacenamiento de energía para usarse en las horas de ausencia o disminución de la radiación solar.Según se comentó ampliamente en la prensa en días pasados, la compañía norteamericana “SolarCity”, que tiene como negocio la instalación y renta de paneles fotovoltaicos en negocios y casas particulares, anunció esta semana que el próximo año ofrecerá en California instalaciones solares con sistemas de baterías de respaldo. Dichas baterías proveerán energía en caso de una falla en el suministro de energía eléctrica o, de manera más significativa para los clientes comerciales, cuando se presente una demanda alta de energía que normalmente implica un incremento sustancial en la tarifa eléctrica. Las baterías de respaldo tendrán de este modo una doble función.Dado los rápidos desarrollos que está experimentando la electricidad solar, es posible que los próximos años veamos su consolidación definitiva. Seremos así capaces de aprovechar la energía del Sol sin intermediarios, lo que sin duda será ventajoso. Esperemos que lo sea tanto para nosotros como para el medio ambiente.",
    "El conocimiento científico que tenemos acerca del mundo se ha acumulado a pasos agigantados desde hace unos cuatro siglos con el nacimiento de la ciencia moderna. Este conocimiento nos ha dado un dominio creciente sobre las fuerzas de la naturaleza y del mismo ha nacido la tecnología moderna, que ha tenido un impacto profundo sobre nuestra civilización, particularmente a lo largo de los dos últimos siglos.Si bien en un principio el método científico se empleó para investigar al mundo natural, objeto de estudio de ciencias tales como la Física, la Química y la Biología, a lo largo de los dos últimos siglos dicho método se extendió a la investigación de los fenómenos sociales. Este es, por ejemplo, el caso del estudio llevado a cabo por el antropólogo Jamshid Tehrani de la Universidad de Durham en la Gran Bretaña, que tuvo como objetivo averiguar el origen del cuento Caperucita Roja. Los resultados de dicho estudio aparecieron en el número de esta semana de la revista en línea “PLOS ONE”.    La versión más conocida de este cuento es la que los hermanos Grimm publicaron en 1812, basados en un cuento del siglo XVII de Charles Perrault. Este último, sin embargo, no es el inventor original de la historia. De hecho, Tehrani señala que se acepta que Perrault la tomó de una antigua leyenda, El Cuento de la Abuela, que ha sobrevivido como una tradición oral en zonas rurales de Francia, Austria y el norte de Italia. En muchas de las versiones de este cuento, la niña –que no usa una capa roja– es atrapada por un lobo. Logra, no obstante, escapar cuando convence a su captor de que la deje salir para ir al baño. Como precaución el lobo ató una cuerda a su pie para evitar que escapara. Cosa que la niña logra cortando la cuerda y atándola a un árbol.Tehrani menciona el redescubrimiento de un poema belga del siglo XI que describe una historia similar a la de Caperucita Roja. El poema, que circulaba por Europa Occidental en la Edad Media, habla de una niña que deambulaba por el bosque con una capa de bautismo de color rojo que le había sido regalada por su padrino. En su deambular encuentra a un lobo que la secuestra y la lleva a su madriguera. La niña, no obstante, logra escapar de su cautiverio domesticando a los cachorros del lobo.Otras historias similares a Caperucita Roja existen como tradiciones orales en otros países no europeos, según Tehrani. Entre estas se encuentra La Abuela Tigre, historia que es popular en China, Corea y Japón.  En este cuento un grupo de hermanos duermen con un tigre que finge ser su abuela. Cuando el tigre devora a los hermanos más jóvenes los mayores lo convencen de que los deje salir para ir al baño, lo que aprovechan para escapar. En África, igualmente, circula un cuento acerca de una niña que es devorada por un ogro que la engañó imitando la voz de su hermano. La niña logra escapar viva una vez que le abren la panza a su victimario.  En Europa y en el Medio Oriente existe una historia conocida como El lobo y los Cabritos en la que una cabra instruye a sus hijos a no abrir la puerta mientras ella no esté en la casa. Cuando se ausenta, el lobo llama a la puerta simulando ser la madre y cuando le abren devora a los cabritos. La investigación de Tehrani tuvo como objetivos encontrar la relación que existe entre estas historias, que guardan entre ellas elementos de similitud, y si acaso todas tuvieron un mismo origen. Para este propósito empleó una metodología similar a la que emplean los biólogos para determinar el camino evolutivo que han seguido las especies biológicas. Es decir, consideró que las historias populares se trasmiten de generación en generación con pequeños cambios, análogos a las mutaciones que sufren los seres vivos.Así, identificó primero los elementos característicos de las diferentes historias. Consideró, por ejemplo, si los protagonistas eran o no eran humanos, si era una o eran varias las víctimas del villano, la clase de villano –ogro, lobo o  tigre–, los métodos que usó éste para engañar a su víctima, y si ésta fue devorada, escapó o fue rescatada. Con estos elementos –72 en total– mediante un procedimiento matemático Tehrani comparó las diferentes historias y determinó que Caperucita Roja tuvo como antecesor a El Lobo y los Cabritos. Este último cuento ya existía en Europa al inicio de nuestra era y habría dado origen a Caperucita Roja en el siglo XI. Las versiones africanas y del Lejano Oriente del cuento habrían tenido igualmente su origen en El lobo y los Cabritos.   Podría quizá parecer superfluo el conocimiento que ahora tenemos sobre el origen de Caperucita Roja y una pérdida de dinero los recursos que se hayan empleado en investigarla. De acuerdo con Tehrani, no obstante, “los cuentos folclóricos involucran fantasías, experiencias y miedos, y son realmente un buen medio para averiguar, a través de los productos de nuestra imaginación, qué es realmente lo que nos importa”. Al margen de esto, cualquier nuevo conocimiento que adquiramos sobre el mundo es importante, pues no podemos descartar que algún día tenga utilidad. Como lo han sido tantos resultados de la investigación científica.",
    "Algunos piensan que las impresoras 3D (impresoras en tres dimensiones) revolucionarán la industria tal como la conocemos. Es probable que esto ocurra en el futuro, aunque, ciertamente, para competir con la industria tradicional las impresoras 3D –con la suficiente precisión– tendrán primero que bajar de precio, aumentar su velocidad de impresión,  ampliar el rango de materiales que manejan y, en ciertos casos, aumentar su tamaño. Por lo pronto, sin embargo, las impresoras 3D nos han dado nuevas maneras de fabricar objetos. El trabajo artesanal es por naturaleza poco preciso y produce objetos imperfectos, en una medida que depende de la habilidad del artesano o de la prisa con que trabaje. Igualmente, dos objetos artesanales nominalmente iguales no lo serán necesariamente, incluso si fueron fabricados por la misma persona. En contraste, la revolución industrial iniciada en Inglaterra hace más de dos siglos trajo, entre otras cosas, técnicas de fabricación automatizada que producen en masa y a gran velocidad objetos con menos imperfecciones y variaciones que aquellas típicas del trabajo artesanal.A manera de ejemplo, comparemos monedas que se acuñaron hace dos mil años con las actuales. Si bien estas últimas no están necesariamente libres de defectos, si lo están en un grado considerablemente mayor que aquellas que se acuñaron, por ejemplo, durante el Imperio Romano. Las imperfecciones de las monedas antiguas las encontramos no solamente en su forma, sino también en los motivos troquelados en sus caras. Incluso en algunos casos es posible ver que el troquelado está fuera de centro, debido posiblemente a la prisa con que trabajó el artesano.  A su vez, las impresoras 3D han traído una nueva sofisticación en las técnicas de fabricación y, entre otras cosas, han posibilitado la duplicación de objetos con una forma codificada en un programa de computadora. Así, cualquiera sería capaz fabricar un objeto con una forma complicada empleando un código de computadora que posiblemente podría conseguir en un sitio de internet. Podría además reproducir el objeto las veces que quisiera, obteniendo cada vez un duplicado exacto y en gran medida libre de imperfecciones. Y todo esto lo podrá llevar a cabo alguien sin la menor habilidad como artesano –aunque sí con la correspondiente habilidad para manejar una computadora. Todo esto que resulta sorprendente nos lleva a otra sorpresa que suena a ciencia ficción, pues recientemente se ha planteado la posibilidad de construir impresoras 4D a partir de la tecnología 3D. Habría que aclarar, no obstante, que no se trata de máquinas que impriman objetos en un hiperespacio de cuatro dimensiones ni nada que se le parezca y que la cuarta dimensión se refiere en realidad al tiempo. Así, se pretende que las impresoras 4D produzcan, no objetos rígidos, sino objetos con la capacidad de transformarse en respuesta a condiciones medioambientales cambiantes. Se plantea, por ejemplo, la fabricación de tubos conductores de agua con materiales inteligentes, cuyo diámetro aumente o disminuya en función de flujo de agua a conducir, o bien que se cierren por completo para hacer la función de una válvula. Podrían igualmente contraerse y expandirse de manera periódica para impulsar el agua, tal como ocurre en el tracto digestivo para hacer avanzar los alimentos. Los materiales inteligentes podrían también emplearse para fabricar llantas de automóvil que cambien sus características para adaptarse a las condiciones del camino. Sin embargo, si bien todo esto suena excepcionalmente atractivo, cabe hacer notar que en la actualidad las impresoras 4D y sus aplicaciones con materiales inteligentes están apenas en una etapa incipiente y en algunos casos no son más que ideas.En el número de la semana pasada de la revista de divulgación científica “New Scientist” se publica una entrevista con Skylar Tibbits del  Instituto Tecnológico de Massachusetts (MIT, por sus siglas en inglés). Tibitt es un arquitecto y especialista en computadoras que está llevando a cabo investigaciones sobre el uso de impresoras 4D y materiales inteligentes en el área de diseño y en aplicaciones en infraestructura. No ve Tibbits a las impresoras 4D y a los materiales inteligentes compitiendo con la industria actual y encuentra su aplicación más bien en áreas periféricas en donde la fabricación es actualmente  imposible, por cara, peligrosa o inaccesible, como es el caso de la fabricación de objetos en el espacio.Tibbits ejemplifica sus ideas con objetos fabricados empleando un material que se expande en contacto con el agua. Muestra, por ejemplo, una vara que al sumergirse en el líquido se transforma en un objeto que simula a un cubo, o bien otro caso en el que la vara se dobla hasta formar las letras MIT. Es posible encontrar fácilmente en internet videos en los que se muestran estos efectos.    Cuando las impresoras 4D se hagan realidad –esperemos que en un futuro no muy lejano–, mucha agua habrá corrido bajo el puente desde los tiempos aquellos, hace cientos de miles de años, cuando nuestros ancestros desarrollaron técnicas artesanales para fabricar hachas y puntas de piedra. Mucho tiempo habrá ciertamente transcurrido, pero es notable que sea en los últimos doscientos años cuando hayan ocurrido tantos y tan profundos cambios en nuestras técnicas de manufactura.",
    "De acuerdo con un artículo publicado esta semana en la revista “Science”, en los trece primeros años del siglo XXI nuestro planeta perdió 2.3 millones de kilómetros cuadrados de bosques, un área que es superior a la superficie de nuestro país. Este número implica, además, que cada 1.2 segundos se desforesta en algún lugar de la Tierra un área equivalente a un campo de futbol. En contraste, en este mismo periodo de trece años solamente se tuvo una ganancia global de 0.8 millones de kilómetros cuadrados de nuevos bosques. El artículo de referencia fue publicado por un grupo de 15 investigadores de universidades y centros de investigación en los Estados Unidos, encabezado por Matthew Hansen de la Universidad de Maryland. De acuerdo con Hansen, a pesar de que los bosques juegan un papel fundamental en aspectos tales como la regulación del clima, la biodiversidad y el suministro de agua, hasta ahora no había sido posible obtener datos precisos y detallados acerca del los cambios que está sufriendo la cubierta boscosa de nuestro planeta. La cuantificación de estos cambios fue llevada a cabo mediante un análisis de 650,000 imágenes de satélite de la superficie terrestre, descargadas de archivos del “US Geological Survey”. La resolución de los mapas es de solamente 30 metros, de manera que muestran un gran detalle.Hansen y colaboradores obtuvieron mapas del total de la superficie del planeta, con la excepción de continente antártico, en los que se muestran los cambios que han experimentado los bosques a lo largo de periodo 2000-2012. Se reportan tanto áreas que han sufrido pérdida de bosques, como áreas que han tenido una ganancia. De dichos mapas, que pueden ser consultados en internet, se desprende que las regiones ecuatoriales han contribuido con un 32% de la pérdida global de superficie boscosa y que de este porcentaje la mitad ha ocurrido en la selva amazónica. Es, sin embargo, en la región de Chaco, en Argentina, Bolivia y Paraguay, en donde la desforestación está avanzando a paso más acelerado.De acuerdo con el artículo de referencia, en la Amazonia brasileña la pérdida de área boscosa fue frenada de manera sustancial, desde un máximo de 40,000 kilómetros cuadrados por año en 2003-2004, hasta 20,000 kilómetros cuadrados por año en 2011-2012. Recientemente, sin embargo, debido a cambios en la legislación brasileña, esta pérdida se incrementó en un 28% con respecto a la observada en 2011-2012, como lo ha reconocido el propio gobierno de Brasil.  Por lo que respecta a nuestro país, nos encontramos en el lugar 15 a nivel mundial en cuanto a pérdida de áreas boscosas, con una disminución aproximada de 24,000 kilómetros cuadrados entre los años 2000 y 2012, que es aproximadamente un 40% de la superficie del Estado de San Luis Potosí. En contraposición, en ese mismo periodo en México solamente se generaron poco más de 6,000 kilómetros cuadrados de nuevas áreas de bosque. Con relación a nuestro estado, como habría de esperarse, el área que ha sufrido una mayor merma en bosques es la región Huasteca.  Un caso notable es nuestro vecino Guatemala, que con una superficie de apenas el 5.5% de la superficie de nuestro país ha perdido en los primeros trece años del siglo XXI casi 9,000 kilómetros de bosques; es decir, casi el 40% de lo que ha perdido México. Una cantidad similar ha perdido Nicaragua, país igualmente de dimensiones  reducidas.En el otro extremo del mundo, Indonesia tiene el más alto crecimiento en pérdida de áreas boscosas, misma que creció desde los 10,000 kilómetros cuadrados por año observados en los años 2000 y 2003, hasta los 20,000 kilómetros cuadrados por año ocurridos en 2011-2012. El país que en términos absolutos tiene el mayor índice de pérdida de bosques es Rusia, en cerrado empate con Brasil. Rusia, sin embargo, tiene un mayor índice de ganancia de nuevas áreas boscosas. El tercer y el cuarto lugar en pérdidas lo ocupan los Estados Unidos y Canadá, de manera respectiva, aunque en estos países la generación de nuevos bosques es relativamente alta. La sobre-explotación de los recursos forestales del planeta es de este modo un fenómeno bastante extendido. Por otro lado, como bien lo sabemos, la sobre-explotación no solamente ocurre con los bosques sino también con otros  recursos naturales. Así, por ejemplo, en días pasados nos enteramos por la prensa que la “US Fish and Wildlife Service” de los Estados Unidos destruyó 6 toneladas de cuernos de elefante y de otros objetos de marfil con el objeto de “enviar un claro mensaje que los Estados Unidos no tolerarán el crimen contra la vida salvaje que amenaza con destruir la población de elefantes africanos y otras especies alrededor del mundo”. Como sabemos, los elefantes asiáticos están clasificados como especies en peligro de extinción, mientras que los elefantes africanos lo están como especie vulnerable. Y, por supuesto, la sobre-explotación no se limita a los seres vivos, sean éstos árboles o animales, sino que también se aplica a numerosos recursos naturales y materias primas, con el peligro de que esta situación se mantenga en el futuro. Al menos hasta que el destino nos alcance.",
    "Imaginemos la siguiente situación que bien podría ocurrir en un futuro no muy lejano  –digamos,  una década–. Al regresar a su casa en la noche, un ingeniero –o al menos alguien a quien no lo intimidan los ingenios mecánicos– encuentra que tiene problemas con la cerradura de la puerta principal de su casa y, dado que ya es tarde para llamar al cerrajero y no puede dejar la puerta abierta, decide arreglarla él mismo. Para esto desarma la cerradura y descubre que una de sus piezas está rota. Con esta información identifica la marca de la cerradura y busca en internet los planos digitales de la pieza dañada. Una vez que los encuentra los alimenta a su impresora 3D, la cual reproduce la pieza en poco tiempo. Hecho esto el ingeniero la instala sin dilación en la cerradura, la cual queda funcionando nuevamente. Puede así ir a dormir tranquilo –si bien probablemente un poco más tarde que de costumbre.     Todo esto será posible por medio de las impresoras en tres dimensiones, o impresoras 3D, las cuales prometen revolucionar la manufactura de todo tipo de objetos, incluyendo de manera desafortunada las armas de fuego. Esto último queda de manifiesto por el anuncio hecho esta semana en Austin, Texas por la compañía norteamericana “Solid Concepts” relativo a la reproducción por medio de una impresora 3D de la pistola “Browining 1911”, la cual, según el diario  británico “The Guardian”, estuvo en uso por las fuerzas armadas norteamericanas hasta el año 1985. La pistola reproducida es capaz de realizar cuando menos 50 disparos de balas reales antes de que pudiera quedar inutilizada.El de “Solid Cocnepts” no es el primer caso de fabricación de un arma de fuego con una impresora 3D. En mayo de este año, la compañía “Defense Distributed”, también con base en Texas, anunció la fabricación de una pistola de plástico empleando una impresora 3D, la cual es capaz de disparar balas calibre 0.38.  “Defense Distributed”, además, puso lo planos digitales de la pistola en un sitio de internet a la disposición de cualquiera que quisiera descargarlos. Ante esto, el Departamento de Estado reaccionó rápidamente y en dos días ordenó su remoción. Alegó posibles violaciones a la ley de exportación de armas –razón que de algún modo nos recuerda que Al Capone fue encarcelado sólo por evasión de impuestos–. Para cuando los planos fueron removidos, sin embargo, habían sido ya descargados 100,000 veces según la revista “Forbes”.Las impresoras 3D fabrican objetos capa por capa por medio de una variedad de técnicas. Una de estas, empleada para fabricar objetos de plástico, deposita material en lugres específicos sobre la capa que está en ese momento en formación. Esto se hace por medio de una boquilla a través de la cual se hace pasar por extrusión el plástico a depositar. La boquilla se calienta a una temperatura tal que el plástico se funde y puede fluir a través de la misma. Una vez fuera de la boquilla el plástico se solidifica sobre la superficie del objeto en construcción en el lugar escogido. De este modo, el objeto se construye capa por capa, de abajo hacia arriba, siguiendo las instrucciones del programa en el cual está codificada su forma. La posibilidad de que cualquier persona pueda fabricar en su casa un arma de fuego empleando una impresora 3D con información obtenida de manera libre a través de internet es, por supuesto, preocupante. No obstante, según “The Guardian”, la pistola de plástico de “Defense Distributed” tiene poco valor práctico y es posible que represente tanto peligro para aquel sobre la cual es apuntada como para quien la apunta, pues el material con el que está fabricada puede no ser lo suficientemente fuerte para resistir la explosión de la pólvora en su interior. De este modo, aunque fue fabricada empleando una impresora 3D con un costo de 8,000 dólares –que si bien no es bajo tampoco es excesivamente alto– la pistola de plástico pudiera no constituir un peligro mayor que aquel que representa la libre venta de armas. En contraste, la pistola de “Solid Concepts” está fabricada de metal y se aproxima a un arma real, como puede apreciarse en un video que esta compañía colocó en internet. La impresora 3D empleada en su manufactura, sin embargo, emplea un láser de alta potencia y es considerablemente más cara que aquella empleada por “Defense Distributed”. Al igual que el arma de plástico, una pistola de metal fabricada  por una impresora 3D no está entonces por el momento al alcance de cualquiera. Nada asegura, sin embargo, que no lo esté en el futuro cuando la tecnología de impresión en tres dimensiones se generalice y se reduzcan los precios de las impresoras 3D. Si tal cosa llegara a suceder nos quedaría, no obstante, un consuelo, pues las impresoras 3D podrían emplearse no solamente para fabricar pistolas sino también chalecos antibalas.",
    "En las décadas de los años 60 y 70, el físico estadounidense William Shockley se embarcó en una campaña para convencer a la opinión pública estadounidense, lo mismo que a los políticos y a la comunidad científica, que la “calidad” de la población de los Estados Unidos pudiera estar declinando por el fenómeno que llamó disgenesia. Según Shockley, había una relación inversa entre la calidad –que entendía como inteligencia– de una persona y el número de hijos que procreaba. Así, la población de los “menos aptos” crecería a una tasa mayor que la de los “más aptos” llevando a la baja en la inteligencia promedio de la población estadounidense. En realidad las ideas de Shockley no eran nuevas. Fue Francis Galton –primo en segundo grado de Charles Darwin– quien las expresó por vez primera en la segunda mitad del siglo XIX, dando origen a la disciplina que después fue conocida como eugenesia. En la época en que Shockley lanzó su campaña la eugenesia había sido ya causa de múltiples calamidades en el mundo. Entre éstas se incluyeron programas de esterilización forzada de personas consideradas “defectuosas” y, sobre todo, la campaña del régimen nazi en contra de las “razas inferiores”.  De este modo, en la época en la que Shockley lanzó su campaña de concientización la eugenesia estaba muy desacreditada. No obstante, dada la notoriedad científica de Shockley, dicha campaña  atrajo la atención del público, aunque en buena medida con comentarios adversos, según podemos leer en el libro “Shockley on Eugenics and Race: The Application of Science to the Solution of Human Problems”, editado por Roger Pearson.  Como quiera que haya sido, es posible que la situación que Shockley planteaba no haya sido tan preocupante a fin de cuentas, pues en la década de los años sesenta irrumpieron en la escena los métodos anticonceptivos, con lo que se abatió la tasa de crecimiento de la población, incluyendo, aquella de supuesta baja calidad. Además, lejos de deteriorarse, la sociedad estadounidense se enriqueció con la inmigración de científicos e ingenieros de todo el mundo, notablemente de la India y China. En efecto, según datos de la “National Science Foundation” de los Estados Unidos, en el año 2009 había casi 600,000 estudiantes extranjeros inscritos en universidades de ese país en todas las áreas del conocimiento. En áreas de ciencia e ingeniería este número era de alrededor de un cuarto de millón de estudiantes. China, la India y Corea del Sur son los tres países que más contribuyen, con más de 50% del total. México por su parte se encuentra entre los diez primeros países en este renglón, con casi 3,300 estudiantes inscritos en áreas de ciencia e ingeniería. En términos porcentuales, de acuerdo con cifras citadas por Wikipedia, en el año 2011 el 28% de todos los graduados en universidades estadounidenses en áreas de ciencia, ingeniería y salud eran extranjeros. Igualmente, en el año 2004 el 55% de los estudiantes doctorales en ingeniería en los Estados Unidos eran nacidos en el extranjero.Wikipedia también cita cifras que muestran el impacto que los científicos e ingenieros nacidos en el extranjero tienen en el mercado de trabajo en los Estados Unidos.  En el año 2000 el 37% de todos los científicos e ingenieros con grados doctorales eran nacidos en el extranjero, mientras que también lo era el 45% de los físicos con grado doctoral en el año 2004.Si bien durante décadas los Estados Unidos han representado el mercado de trabajo más atractivo para científicos e ingenieros, el desarrollo que han experimentado los países asiáticos en los últimos tiempos, en particular China y la India –que en conjunto representan un tercio de la población del mundo–, está cambiando el panorama. En el número de esta semana de la revista “Science” se publica el primero de una serie anunciada de artículos sobre las condiciones en la India para el desarrollo de una carrera científica. De acuerdo con este artículo, hace aproximadamente 15 años el gobierno de la India dio inicio a un plan para expandir las instituciones de investigación en el país y crear otras nuevas. Esto ha cambiado la perspectiva para los indios graduados en universidades fuera de su país, que antes buscaban quedarse en el extranjero y que ahora ven a la India como un lugar atractivo para desarrollar su carrera científica.Después de la Segunda Guerra Mundial los Estados Unidos se convirtieron en uno de los dos mayores centros de investigación del mundo –el otro lo era la Unión Soviética que parcialmente se desintegró con la caída del régimen soviético–. Hoy  están surgiendo China y la India, y con esto es posible que el centro de gravedad de la investigación se desplace parcialmente hacia Asia en un futuro no muy lejano. Los Estados Unidos perderían así parte de su inteligencia en la medida en que los científicos e ingenieros asiáticos decidan establecerse en sus países de origen. La perderán, aunque por razones muy diferentes a las anticipadas por William Shockley.",
    "Como sabemos, de manera frecuente al final de un artículo o noticia  publicada en Internet se incluye una sección en la que los lectores pueden expresar opiniones sobre su contenido o sobre las opiniones de otros lectores, que en algunas ocasiones resultan sorprendentemente agresivas. Tan agresivas que difícilmente pensaríamos pudieran expresarse cara a cara. Serían así producto del anonimato desde el cual son emitidas.Un artículo y una editorial publicados en el número de esta semana de la revista británica de divulgación científica “New Scientist” comentan sobre este fenómeno. El que un usuario de Internet esté conectado de manera anónima provoca, según algunas opiniones, que se comporte de manera poco civilizada al no tener que enfrentar las consecuencias de sus palabras. Esto ha sido llamado el “efecto tóxico desinhibidor en línea”.En el artículo de “New Scientist” se plantea la posibilidad de que el anonimato en Internet desaparezca. Para esto existen ya las tecnologías necesarias. Un usuario de Internet, por ejemplo, podría ser identificado por medio de las huellas digitales de sus dedos, con la ayuda de los movimientos característicos de su mano al marcar un número en el teléfono celular, o por la manera de caminar tal como es detectada por el acelerómetro de dicho teléfono. La posibilidad de eliminar el anonimato en Internet le plantea a “New Scientist” varias interrogantes. La primera es si la pérdida del anonimato sería, después de todo, una medida efectiva para reducir los comentarios inciviles en línea, que bien pudieran obedecer en realidad a la ausencia de un contacto visual entre aquellos que interactúan. La segunda interrogante tiene que ver con el papel que ha tenido Internet en la caída en los últimos años de gobiernos que califica “New Scientist” de represores, caída que podría no haber ocurrido sin el anonimato en la red. Un aspecto más se refiere al robo de identidad. En la actualidad, si alguien roba una identidad digital siempre hay la posibilidad de anularla y generar una nueva. Si esta identificación se hiciera mediante nuestras huellas digitales o nuestra manera de caminar no existiría esta posibilidad.  En el marco de la gran difusión que en las últimas semanas se ha dado al espionaje del que fueron objeto varios gobiernos extranjeros por parte de los Estados Unidos, sin embargo, la principal preocupación por la eventual pérdida del anonimato en línea tendría que ver con la mayor facilidad que con ésta habría para que los usuarios de Internet pudieran ser vigilados.  La red Internet tiene una estructura distribuida a lo largo de todo el mundo, la cual está formada por un conjunto de redes interconectadas. Internet se mantiene en crecimiento sostenido por la adición de nuevas redes de manera continua. Si bien Internet se creó en los Estados Unidos por iniciativa oficial, son hoy en día compañías privadas de telecomunicaciones las que la mantienen en operación.Internet es una red de comunicaciones distribuida que no tiene un control centralizado. Buena parte del tráfico de comunicaciones internacionales, sin embargo, pasa por el territorio de los Estados Unidos, como resultado de haber sido este país el iniciador de la red. Esto ocurre aun si implica que un mensaje tenga que viajar una distancia considerablemente mayor que la que existe entre el punto donde se origina dicho mensaje hasta el punto de destino. Así, por ejemplo, un mensaje enviado desde Brasil hacia Uruguay viaja primero a Miami y desde ahí hacia el sur hasta este último país. Esto no resulta impráctico debido a la gran velocidad con la que viajan los mensajes en Internet que elimina las barreras geográficas. Es Brasil precisamente –uno de los países críticos del espionaje revelado en los últimos meses– el que está planteando llevar a cabo las acciones más drásticas para revertir esta situación. Está, por ejemplo, demandando que las compañías de comunicaciones que guardan datos personales de usuarios en Brasil, como Google y Facebook, los mantengan en computadoras que estén físicamente en este país y no en los Estados Unidos, demanda que dichas compañías se niegan a satisfacer. Brasil está igualmente planteando la creación de una red de comunicaciones con países sudamericanos que no dependa de la infraestructura de las compañías de telecomunicaciones basadas en los Estados Unidos. La época que nos tocó vivir ha sido testigo de grandes desarrollos tecnológicos que han tenido un impacto profundo en nuestro estilo de vida. Uno de estos desarrollos sin duda es el Internet, que ha revolucionado las telecomunicaciones de manera que hubiera sido difícil de imaginar apenas hace algunas décadas, y con esto ha cambiado al mundo. Sin duda que lo ha cambiado para bien; aunque es posible que también lo haya hecho para mal, si el Internet y el enorme poder de las computadoras que almacenan el alud de datos que reciben día a día –de los usuarios de la red–ayudan a hacer realidad el mundo concebido por George Orwell hace más de medio siglo.En este contexto, el que la eliminación del anonimato en Internet inhiba o no los comentarios agresivos en línea no tiene, por supuesto, mayor relevancia.",
    "La portada del último número del semanario británico “The Economist” está dedicado a la Ciencia. Podríamos pensar que la Ciencia merece el honor de aparecer en las portadas de todo tipo de revistas, dado lo mucho que le debemos por el impacto tan grande que ha tenido en nuestras vidas en los últimos dos siglos. En esta ocasión, sin embargo, los comentarios de “The Economist” no fueron del todo halagadores, al llenar su portada con la frase “De qué manera se tuerce la ciencia”, en la que hace alusión a algunos problemas que aquejan a la práctica científica, propios de la época en que nos tocó vivir. “The Economist” no es una revista científica. No es tampoco una publicación que se dedique a la divulgación de la ciencia. De acuerdo con Wikipedia, “The Economist” “es una publicación de filosofía liberal en lo económico y en lo político, que trata temas de política, relaciones internacionales, negocios, finanzas, tecnología y ciencia”. Para una revista con estos intereses, no es difícil entender que la ciencia le resulte importante como origen último de la tecnología moderna, la cual a su vez es un elemento esencial de la economía de los países desarrollados. Como sabemos, el método científico es un invento relativamente reciente. Su origen no va más allá de unos cuatro siglos, siendo Galileo Galilei –en los primeros tiempos– una de sus figuras principales. Las ideas centrales del método científico son muy simples, a la vez que poderosas; de hecho, lo son a tal grado que en cierto modo resulta sorprendente que no hayan sido descubiertas sino hasta fechas tan tardías en la historia de la humanidad.  De acuerdo con el método científico, la vía para descubrir las leyes que rigen al mundo es la experimentación. Esto, que hoy nos parece obvio no lo era en tiempos pasados. Así, Aristóteles afirmaba que las hembras –incluidas las de la especie humana– por su supuesta inferioridad tenían menos dientes que los machos. Ciertamente, podía Aristóteles haber salido fácilmente de su error mediante el simple experimento de contarle los dientes a su esposa. Hay que notar, no obstante, que los experimentos están sujetos a errores de diversas clases, lo mismo que a los prejuicios del experimentador. De este modo, para que el resultado de un experimento particular adquiera validez científica debe ser reproducido por otros investigadores de manera independiente. Esto constituye una característica esencial del método científico. De aquí se desprende una segunda característica de la ciencia: sus resultados y verdades nunca son definitivas, sino que están sujetas ser refutadas por nuevos experimentos. Así, cualquier resultado falso, producto de errores cometidos durante la experimentación –o bien producto de los prejuicios del experimentador– sería desechado tarde o temprano de manera inevitable. De este modo, la ciencia avanzaría de manera lineal hacia la perfección y comprensión del mundo que nos rodea.  Sin embargo, este proceso de auto-purificación –por medio del cual la ciencia corrige de manera sistemática sus propios errores– no está libre de obstáculos. En efecto, aunque dicho proceso luce ciertamente sencillo y sólido, como en muchas otras situaciones de la vida encontramos que el diablo está en sus detalles. Esto motivó la portada de “The Economist” y los artículos relacionados que aparecieron en su interior.Una característica de la Ciencia en la actualidad es la enorme expansión que ha tenido, tanto en el número de investigadores practicantes como en el número de artículos de investigación publicados. Al mismo tiempo, se ha consolidado la carrera de investigador científico y se ha generado una intensa competencia por los puestos de trabajo disponibles. El costo de la investigación científica, por otro lado, es muy alto y el investigador tiene que buscar apoyos para llevar a cabo sus proyectos. Esto ha generado igualmente una intensa competencia por los fondos de investigación disponibles.Como resultado de todo esto, el investigador está bajo una gran presión para publicar tantos artículos técnicos como le sea posible, lo que ha llevado, en algunos casos, a la publicación apresurada de resultados. Tan apresurada que un porcentaje alto de los mismos no han podido ser confirmados. “The Economist”, por ejemplo, cita el caso de 63 estudios de gran relevancia en el campo de la investigación sobre el cáncer de los cuales solamente 6 pudieron ser reproducidos.  De la misma manera, el número de artículos publicados y el número de revistas científicas han crecido tanto que se ha dificultado la revisión por pares de los artículos sometidos para publicación –que es uno de los pilares de la investigación científica– y como resultado se ha incrementado la probabilidad de que algunos de los artículos publicados contengan errores.Estos últimos, por supuesto, serán virtualmente eliminados de la literatura científica al no pasar la prueba de la reproducibilidad. En tanto, constituirán un obstáculo para el progreso científico; progreso que, sin embargo, se dará de una forma u otra, de manera lenta o acelerada, como ha ocurrido a lo largo de los dos últimos siglos. Por más que el diablo quiera meter la cola con sus detalles.",
    "Como se comentó en este espacio el pasado domingo, la revista “Science” publicó en su número del pasado 3 de octubre un artículo en el que se describen los resultados de un experimento llevado a cabo con el fin de evaluar las prácticas editoriales de las revistas científicas llamadas de acceso libre. El experimento fue realizado por John Bohannon, quien escribió una serie de artículos falsos en los que reportaban supuestas propiedades anti-cancerígenas de un cierto liquen y lo envió a más de trescientas revistas de acceso libre, siendo aceptado para publicación en más de la mitad. Las revistas de acceso libre, que pueden ser consultadas de manera gratuita a través de internet, se sostienen por medio de las cuotas que cobran a cada autor por la publicación de su artículo. Esto último contrasta con las revistas clásicas que requieren de una suscripción, la cual es típicamente pagada por las bibliotecas que las adquieren. Como revela la investigación de “Science”, el modelo de financiamiento de las revistas de acceso libre ha propiciado la aparición de empresas editoriales inescrupulosas que han hecho negocio publicando artículos científicos de dudosa calidad mediante el pago de una cantidad de dinero, que puede alcanzar los miles de dólares por artículo. Lo anterior, si bien no resulta sorpresivo, no deja de ser preocupante, pues las revistas de acceso libre lucen ventajosas para muchas bibliotecas e investigadores ante los altos costos que han alcanzado algunas revistas científicas, particularmente aquellas publicadas por las grandes empresas editoriales privadas. En algunos casos estos costos son tan altos que las revistas han quedado fuera del alcance de muchas bibliotecas. Se entorpece de este modo la libre circulación de los resultados de investigación científica, condición indispensable para el progreso de las ciencias.La investigación publicada por “Science” ha provocado críticas. Algunos afirman que constituye un ataque en contra de las revistas de acceso libre, que en ciertos casos tienen estándares de publicación similares a las de las revistas clásicas. Se señala, por ejemplo, que el estudio debería haberse extendido a las revistas clásicas, algunas de las cuales con seguridad habrían también caído en la trampa y aceptado el artículo.Se señala también que el artículo de “Science” es tendencioso y va en contra de la ciencia que se hace en los países en desarrollo. Así, los nombres de los autores de los artículos falsos fueron generados permutando nombres propios africanos e incluyendo de manera aleatoria iniciales intermedias. De este modo resultaron nombres ficticios tan sonoros como “Ocorrafoo M.L. Cobange”, quien trabajaría como biólogo en el “Wasse Institute of Medicine” –inexistente– en la ciudad de Asmara –ésta si real–, en Eritrea. Podríamos preguntarnos la razón por la que Bohannon escogió África como lugar de residencia de sus autores ficticios y no colocó a algunos, por ejemplo, en los Estados Unidos, en cuyo caso un posible autor podría haber sido John Smith, empleado de una oscura universidad en ese país.  Según Bohannon esto lo hizo “con el propósito de que no se generasen sospechas en caso de que un editor curioso buscara sin éxito el nombre del supuesto autor en internet”. Esto, que resulta sólo medianamente convincente, revela quizá prejuicios –infundados en muchos casos y válidos posiblemente en otros– hacia la ciencia de los países en desarrollo. Con la misma lógica, a fin de no delatar a un angloparlante nativo como autor del artículo falso, Bohannon tradujo el texto al francés empleando “Google Translate” y de ahí nuevamente al inglés. Bohannon menciona en su artículo que el país con más casas editoras que aceptaron el artículo fraudulento es la India, seguida de los Estados Unidos. Por otro lado, si bien el flujo del dinero por pago de cuotas se origina en buena medida en países en desarrollo, según Bohannon el dinero finalmente termina en un banco en los Estados Unidos o en Europa. Además, en algunos casos son empresas editoriales de gran prestigio las que están al final del proceso. Al desarrollarse la red internet se abrieron nuevas posibilidades de comunicación para la ciencia. Al mismo tiempo, como lo expresa “Science”, se gestó una especie de “salvaje oeste” de la comunicación científica, con casas editoriales poco honestas aprovechando la ocasión y haciendo negocios poco claros. Situación que, sin embargo, con seguridad, se normalizará en el futuro en la medida en que se asiente la polvareda y las revistas de acceso libre alcancen el lugar que merecen. Y el artículo de Bohannon, con todas las críticas que se le puedan hacer, contribuirá sin duda a que esto ocurra.  El experimento nos señala también otro aspecto preocupante: Bohannon pudo identificar más de trescientas revistas a las cuales enviar su artículo y de éstas más de ciento cincuenta lo aceptaron. Esto resulta alarmante no solamente por el hecho en sí, sino por lo que este número expresa en cuanto a la explosión en el número de revistas de investigación. Sólo esperemos que una vez que el salvaje oeste se civilice, el número de revistas científicas se reduzca de manera considerable. De otra manera, ¿qué investigador será capaz de leer todo lo que hoy en día se publica en su campo? Más lo que se acumule en la semana.",
    "Uno de los pilares de la ciencia actual es la llamada “revisión por pares”, mediante la cual un artículo sometido para su publicación a una revista especializada es enviado por el editor de la misma a uno o varios árbitros para evaluar su calidad científica. Sobre la base del reporte o reportes recibidos, el editor decide aceptar o rechazar el artículo en cuestión. No siempre ha sido así, sin embargo. Un ejemplo al respecto son los tres artículos –de una enorme relevancia científica– publicados por Albert Einstein en el año 1905 en la revista “Annalen der Physik” –los llamados artículos del “Annus mirabilis”–. Estos artículos –entre los que incluye aquel en el que Einstein da a conocer su famosa teoría de la relatividad– fueron aceptados para su publicación por Max Planck –el editor de la revista– sin recurrir a revisores externos.No obstante, con la explosión y especialización del conocimiento científico –y en consecuencia del número de artículos publicados– que tuvo lugar a lo largo del siglo XX, los editores de las revistas científicas debieron recurrir a expertos en el tema del artículo en consideración, a fin de decidir su aceptación o rechazo. Este procedimiento, que en principio debería asegurar que dicho artículo representara un avance científico, en el peor de los casos contribuye a evitar la proliferación de artículos de escaso o cuestionable valor. A pesar de sus virtudes, sin embargo, el sistema de evaluación por pares tiene también sus defectos. Así, dado que de manera inevitable involucra elementos de subjetividad por parte de editores y revisores, ha dado lugar al rechazo de artículos en los que se describen resultados de investigaciones que posteriormente han resultado de gran relevancia. En un artículo publicado en el año 2009 en la revista “Scientometrics” por Juan Miguel Campanario de la Universidad de Alcalá, en España, se hace un recuento de los problemas que futuros premios Nobel han tenido para publicar los resultados del trabajo por el cual fueron posteriormente premiados. Estas dificultades han abarcado desde el sortear las opiniones de los revisores contrarias a que el artículo sea publicado, hasta el rechazo del mismo por el editor de la revista. Este problema, no obstante, parece menor en comparación con el que se ha suscitado por la aparición de la red internet con todas las nuevas posibilidades de comunicación que ofrece. En efecto, en el número de esta semana de la revista “Science” se incluye una serie de artículos en los que se discute el problema que representa la proliferación en años recientes de las llamadas revistas de investigación de acceso libre. Estas revistas, que tienen un formato electrónico, pueden ser consultadas de manera gratuita través de internet.Las revistas de investigación tradicionales son editadas en algunos casos por sociedades científicas y en otros por empresas privadas. El costo de operación de estas revistas es comúnmente cubierto mediante la venta de suscripciones a universidades y centros de investigación. Las revistas de acceso libre, en contraste, operan cobrando una cuota al autor o autores del artículo a publicar –típicamente, entre 1,000 y 2,000 dólares por artículo–, situación que, según “Science”, ha llevado a la aparición de revistas científicas más preocupadas en el negocio de cobrar cuotas a sus autores que en asegurar que lo que les aceptan publicar cumpla con un mínimo de calidad. “Science” llega a esta conclusión mediante un experimento realizado por John Bohannon de la Universidad Harvard con un numeroso grupo de revistas de acceso libre, algunas publicadas por casas editoriales privadas de reconocido prestigio. El propósito del experimento fue el de averiguar qué tan rigurosos son sus estándares para aceptar publicar un artículo científico. Para este propósito escribió un artículo en el que se describían las supuestas propiedades anticancerígenas de una cierta planta y lo envió a 306 revistas. Según Bohannon, el artículo tenía tantas fallas que cualquier especialista competente podía fácilmente detectarlas y recomendar que no se publicara. El artículo en cuestión fue, sin embargo, aceptado para publicación en 157 revistas, siendo rechazado solamente en 98. De las 49 restantes no se obtuvo una respuesta.Según Bohannon, de los 255 artículos que fueron aceptados o rechazados, el 60% no mostró signos de que haya experimentado un proceso de revisión por pares. También, de los 106 que parecen haber pasado por esta revisión, el 70% fueron aceptados. Ante la oportunidad que brindan los nuevos medios de comunicación electrónica, no es sorprendente que surgieran y estén surgiendo multitud de empresas editoriales con poca ética profesional que publican, por una cuota, artículos científicos que falsamente afirman pasan por un proceso riguroso de revisión por pares. Esto, ciertamente, es alarmante. Más lo es, sin embargo, que, de acuerdo con los hallazgos de Bohannon, empresas editoriales privadas con prestigio en el campo de las publicaciones científicas hayan igualmente aceptado publicar el artículo con resultados ficticios.   Algunas suscripciones de revistas exitosas editadas por empresas privadas han alcanzado precios estratosféricos –en algunos casos hasta 20,000 dólares por año–. Esto limita la difusión de los resultados de investigación y por consecuencia dificulta el avance científico. En este contexto, la opción que ofrecen las revistas de acceso libre resulta atractiva. Siempre y cuando que, por supuesto, las investigaciones que ahí se publiquen no sean ficticias.",
    "Hace doscientos años Robert Malthus argüía que dado que la población del mundo crecía a una mayor velocidad que la producción de alimentos, el número de habitantes del planeta llegaría a un límite más allá del cual los alimentos disponibles no serían suficientes para toda la población. En este punto harían su aparición diversos mecanismos que actuarían para limitar el crecimiento de la población más allá del límite de sustentabilidad. Estos mecanismos incluirían guerras, hambrunas y epidemias, provocadas estas últimas por condiciones de vida insalubres. Desde los tiempos de Malthus, sin embargo, la población de mundo creció por un factor  de siete –de unos mil millones de personas en 1800 hasta más de siete mil millones en la actualidad–, un crecimiento que Malthus no pudo haber previsto y que ha sido posible gracias al gran incremento en la eficiencia en la producción de alimentos que nos trajo la tecnología agrícola. En los últimos dos siglos, además, se ha elevado de manera considerable el nivel de vida de un porcentaje significativo de la población mundial –que, sin embargo, vive en gran medida en los países desarrollados. El aumento global en población y en niveles de vida, no obstante, ha tenido un costo alto, pues ha llevado a una sobreexplotación de los recursos del planeta, el cual ciertamente acusa el golpe recibido. Al respecto, el pasado viernes el Panel Intergubernamental sobre Cambio Climático (PICC) de la Organización de Naciones Unidas, dio a conocer en Estocolmo, Suecia, un resumen de su quinto reporte sobre calentamiento global y cambio climático, con conclusiones que no por esperadas resultan menos impactantes.Con argumentos científicos sólidos, el grupo de expertos que componen el PICC confirma que la quema de combustibles fósiles genera gases de invernadero que están produciendo un cambio climático global que, de no atajarse, llevará a situaciones sin precedente. Entre éstas se incluyen: un incremento en el nivel de los océanos entre 40 y 80 centímetros en 2100, un incremento en el número de ciclones y periodos de sequía, y un cambio en los patrones de lluvias, que aumentará el contraste entre las regiones secas y la regiones húmedas del planeta.  Como consigna el PICC en su quinto informe, el nivel actual de dióxido de carbono en la atmósfera es un 40% más elevado que el que existía en la época preindustrial. Este nivel, además, no tiene precedente en los últimos 800,000 años –es decir, desde una época muy anterior al surgimiento del hombre moderno–. Si bien este último dato no proviene, por obvias razones, de una medición directa, mediciones realizadas de manera continua desde finales de los años cincuenta muestran claramente que el nivel de dióxido de carbono en la atmósfera se está incrementando de manera paulatina. El crecimiento sostenido del nivel de este gas en la atmósfera es, de este modo y al menos durante los últimos cincuenta años, un hecho incontrovertible. De manera concomitante, durante los últimos cien años, y con la excepción de la última década y de un periodo de tiempo comprendido en las décadas de los años 50 a 70, la temperatura de la superficie del planeta muestra una tendencia a la alza. Esto resulta ser igualmente un hecho incontrovertible, que se ha manifestado, por ejemplo, en la disminución paulatina del volumen de los glaciares y de los hielos polares. Por otro lado, dado que el dióxido de carbono es un gas de invernadero, el incremento de la temperatura terrestre se asocia al crecimiento de su nivel en la atmósfera.Las conclusiones del PICC, sin embargo, son disputadas por algunos. En particular por el Instituto Heartland, con sede en Chicago, Illinois, el cual dio a conocer la semana pasada un documento en el que refuta las conclusiones del PICC y las tacha de alarmistas. De manera específica, arguye que la disminución en la velocidad de crecimiento de la temperatura terrestre observada en la última década prueba que la temperatura del planeta se ha estabilizado y que no hay una conexión entre el dióxido de carbono en la atmósfera y el incremento de la temperatura terrestre.   El PICC, por su parte, señala que la temperatura terrestre registra comúnmente variaciones naturales y que la desaceleración del incremento de temperatura recientemente observado es una de estas variaciones. En efecto, el PICC muestra que el promedio de temperatura por década se ha incrementado de manera paulatina en las últimas tres décadas. Estima también que es probable que el periodo de 1983 a 2012 sea el periodo de 30 años más caliente en los últimos 1400 años.    Las conclusiones del PICC están fundadas en una revisión de publicaciones especializadas y tienen por lo mismo una sólida base científica. Estas conclusiones, además, por ser el resultado de un consenso entre numerosos investigadores, son intrínsecamente conservadoras y en esta perspectiva no pudieran tacharse de alarmistas. Robert Malthus hace dos siglos no previó que los avances tecnológicos futuros en la producción de alimentos, que el ya no vería, permitirían multiplicar por un factor de siete la población del mundo –si bien con un buen número de desnutridos–. En línea con sus conceptos, no obstante, la revolución industrial, con todas sus virtudes, ha generado un factor limitante del crecimiento de la población, ya no tanto por hambrunas, guerras o epidemias, sino por el abuso que hemos hecho de nuestro planeta.",
    "De acuerdo con un artículo publicado este mes en la revista “Astrobiología” por científicos de la Universidad de East Anglia en el Reino Unido, nuestro planeta se está acercando al  límite interior de la zona habitable alrededor del Sol. Una vez fuera de esta zona, el calor del Sol será tan grande que hará imposible la vida sobre la Tierra tal como la conocemos. Los científicos calculan, sin embargo, que esto sucederá dentro de varios miles de millones de años, cuando nuestro Sol envejezca e incremente su tamaño, y como resultado la zona habitable se extienda más allá de la órbita terrestre. Así, si bien la vida sobre la Tierra está irremediablemente condenada a desaparecer algún día abrasada por el Sol –de no hacerlo antes por efecto de alguna catástrofe natural o provocada por nosotros–, ese día está tan lejano que no es motivo de mayores preocupaciones. Como sí lo es el calentamiento global que está experimentando nuestro planeta, que podrá no tener consecuencias tan dramáticas como las que sufrirá la Tierra cuando deje la zona habitable, pero que está a la vuelta de la esquina y no a miles de millones de años de distancia. Además, si bien el calentamiento global implicará en los próximos cien años un incremento máximo en la temperatura de la Tierra de sólo unos pocos grados centígrados, las consecuencias al clima de nuestro planeta podrían ser desastrosas, según arguyen los expertos.  El calentamiento global ha tenido una presencia mayor de la habitual en la prensa en los últimos días. Esto por la anunciada aparición la próxima semana del quinto reporte del Panel Intergubernamental sobre Cambio Climático (PICC) de la Organización de Naciones Unidas. Con este motivo, la revista científica “Nature” publicó en el número de esta semana una sección especial sobre diferentes aspectos del calentamiento global y del cambio climático consecuente. Una versión del próximo informe del PICC filtrada a la prensa en el mes de junio hace prever a los especialistas que el panel destacará en el documento oficial que en los últimos quince años el promedio de temperatura de la superficie de la tierra se ha mantenido estable en gran medida, en contraste con las dos décadas anteriores durante las cuales dicha temperatura se incrementó en más de medio grado centígrado. Este hecho, que está en contra de las predicciones de los expertos, ha sido aprovechado por los activistas que niegan el cambio climático, quienes arguyen que el mismo constituye una prueba de que el calentamiento global se ha estabilizado y que no está asociado al incremento en la concentración de gases de invernadero en la atmósfera, la cual ha crecido de manera paulatina a lo largo de los últimos dos siglos.Así, los escépticos del cambio climático consideran que las variaciones de la temperatura de la Tierra son fundamentalmente debidas a fenómenos naturales y que los gases de invernadero producen sólo efectos pequeños. Esta opinión no es compartida por expertos climatólogos quienes arguyen que la estabilización de la temperatura terrestre en los últimos quince años bien pudiera responder a fenómenos naturales, pero que el calentamiento inducido por los gases de invernadero está subyacente y que el ritmo de crecimiento de la temperatura de la superficie terrestre se reanudará una vez que dichos fenómenos naturales desaparezcan.  También pudiera ser que el calor extra que produce el efecto invernadero –cuya existencia está fuera de toda duda– se hubiese canalizado al fondo de los océanos y no a la superficie de la Tierra. O bien, como una tercera posibilidad, que el calor extra en realidad nunca hubiera llegado a la Tierra, debido a la reflexión de la radiación solar por los contaminantes suspendidos en la atmósfera –producto supuestamente del incremento en la actividad industrial en países asiáticos.     En todo caso, los escépticos del calentamiento global no mencionan que si bien la temperatura terrestre se estabilizó en cierta medida en los últimos 15 años, los 12 primeros años de este siglo están entre los 14 más calientes desde el año 1880.  El clima de la Tierra es algo tan extremadamente complicado que muchos de los mecanismos que lo gobiernan están todavía fuera del entendimiento de los científicos. Esto ha favorecido a los grupos de activistas y organizaciones que niegan el calentamiento global. Una de éstas, el Instituto Heartland, con base en Chicago, Illinois, se refiere a la existencia de dos equipos, los “Verdes” del PICC, que defienden la existencia del calentamiento global y los “Rojos” del Institute Heartland, que asumen la posición contraria. Visto así, el calentamiento global sería en no poca medida un asunto de fe.No lo es, sin embargo, y aunque muchos de los secretos del clima no son aun entendidos, el consenso mayoritario entre los climatólogos es que el calentamiento global es real y que es debido a la creciente emisión de gases de invernadero a la atmósfera. Dada esta situación, los especialistas consideran que es urgente tomar medidas para controlar la emisión de estos gases y evitar una catástrofe ambiental.De no ser así bien pudiera ser que los humanos no podamos aprovechar todos los años de vida que aparentemente el diseño del sistema solar nos tiene reservados.",
    "El término “fracking” está de moda. Lo ha estado ya por algún tiempo en los Estados Unidos y en Europa, y lo está cada vez más en México con motivo de la reforma energética actualmente en discusión. En el contexto actual, la palabra “fracking”, o fracturación hidráulica, se refiere a la tecnología empleada para facilitar la extracción de gas y petróleo de yacimientos no convencionales, que hasta hace poco eran económicamente inviables de explotar. Con la ayuda de la tecnología de “fracking” los Estados Unidos han revertido la tendencia a la baja que su producción de petróleo mantuvo desde 1986, la cual se ha incrementado año con año a partir de 2008. Según el “Annual Energy Outlook 2013” de la Energy Information Administration de los Estados Unidos, la producción de petróleo en ese país, que era de 5 millones de barriles diarios en 2008, aumentó a 6.5 millones de barriles diarios en 2012. Con respecto a la producción de gas natural, mediante técnicas de “fracking” los Estados Unidos incrementaron su producción de este combustible en un 25% entre los años 2005 y 2011.El “fracking” está tan de moda que incluso tiene presencia en la música popular. Así, en la canción “Doom and Gloom” de “The Rolling Stones” –aparecida en el disco editado en 2012 con motivo del 50 aniversario de la banda– Mick Jagger alude a la búsqueda de petróleo empleando el “fracking”, cantando: “Fracking deep for oil but there is nothing in the sump”.   El “fracking” es, no obstante, una tecnología muy controvertida, que sufre un fuerte rechazo por parte de grupos ecologistas preocupados por el daño que, según ellos, ocasiona al medio ambiente. En particular, señalan su potencial para contaminar mantos acuíferos con metano –componente principal del gas natural– y con las sustancias químicas empleadas en la perforación de los pozos. Así, si bien cuenta con defensores —algunos seguramente de buena fe y otros que cuidan los grandes intereses económicos involucrados—, la imagen pública del “fracking” no necesariamente goza de buena salud.En los yacimientos de petróleo o gas de pizarra el combustible está atrapado en un sustrato rocoso poco permeable. Éste impide su libre flujo hacia el pozo de extracción, lo que lo hace económicamente inviable.  Por medio de la tecnología de “fracking” se inyecta al pozo agua a una enorme presión con el objeto de fracturar la roca y permitir así el flujo de combustible hacia el pozo de extracción. Además, con el objeto de optimizar el proceso de recolección de combustible, una vez que se perfora el pozo hasta alcanzar el nivel del yacimiento, la perforación se prolonga horizontalmente a lo largo del mismo. Como se mencionó anteriormente, un punto de preocupación con la tecnología de “fracking” es que las sustancias químicas inyectadas al subsuelo puedan contaminar los mantos acuíferos; con el agravante de que, comúnmente, estas sustancias no son dadas a conocer por las compañías perforadoras, lo que dificulta determinar el posible grado de contaminación de un determinado acuífero. No es este el único problema que los ecologistas asocian al “fracking”. Consideran, por ejemplo, que durante la extracción de gas metano una parte significativa de éste escapa a la atmósfera y contribuye a elevar el calentamiento global. En este respecto, hay que notar que el metano es un gas de invernadero más potente que el dióxido de carbono, principal responsable del cambio climático que aqueja al mundo. Los ecologistas están también preocupados por el agua residual del proceso de perforación de los pozos, la cual regresa a la superficie contaminada con sales y desechos radiactivos. Y por si fuera poco, a la práctica del  “fracking” se le ha responsabilizado de generar temblores de tierra de mediana intensidad.De acuerdo con el Departamento de Energía de los Estados Unidos, nuestro país cuenta con una de las mayores reservas de gas de pizarra. Éstas se localizan en los estados de Coahuila, Nuevo León, Tamaulipas y Veracruz. Dada esta circunstancia hay quien considera que en el gas de pizarra está buena parte del futuro energético del país –ahora que la producción petrolera está menguando– y que este combustible jugará un papel central en la reforma energética.No todo mundo está de acuerdo, sin embargo. Y no solamente por el impacto al medio ambiente que se atribuye al “fracking”, sino por las enormes cantidades de agua que se necesitan para la implementación de un pozo –mínimamente de 8 millones de litros de agua, cantidad suficiente para proveer por una día a 50,000 casas habitación–. Considerando los miles de pozos que se plantea perforar en México, hay quien opina que el “fracking” no es una tecnología adecuada para nuestro país, que no se caracteriza precisamente por su abundancia de recursos hídricos.Por lo demás, habría que esperar a que se tengan datos duros sobre el impacto ecológico de la práctica de “fracking”, a fin de determinar su peligrosidad ambiental. Por lo pronto esta tecnología está bajo fuego intenso en Europa, e incluso ha sido prohibida en Bulgaria y Francia.",
    "Como nos ha informado la prensa en las últimas semanas, la planta nuclear de Fukushima en Japón, que fue severamente dañada por el terremoto y posterior tsunami que golpeó al norte de ese país el día 11 de marzo de 2011, está vertiendo agua contaminada con elementos radioactivos en el Océano Pacífico. Ahora sabemos que esto, en realidad, lo ha estado haciendo desde hace dos años, hecho que había sido ocultado por TEPCO, la compañía dueña de la planta. Como sabemos, al momento de ocurrir el terremoto, los tres reactores de la central de Fukushima –de un total de seis– que en esos momentos estaban en operación se apagaron de manera automática. En seguida, puesto que aun apagados los reactores tienen que seguir siendo enfriados con agua para evitar su colapso, se pusieron en funcionamiento los generadores de electricidad auxiliares a fin de accionar las bombas de agua. El tsunami que siguió al temblor, sin embargo, inundó los edificios de los reactores e inutilizó lo generadores. Con esto se sobrecalentaron y fundieron los núcleos de los tres reactores nucleares, quedando fuera de control y dejando escapar gases radioactivos. Actualmente se bombean unas 400 toneladas de agua diarias en dichos reactores con el objeto de mantener su temperatura controlada. El agua regresa del reactor contaminada con elementos radiactivos y es acumulada en 1,000 tanques gigantescos construidos por TEPCO al lado de la planta. Han ocurrido, no obstante, accidentes y recientemente se reportó una fuga de 300 toneladas de agua altamente radiactiva que habría llegado hasta  el Océano Pacífico. El nivel de radiación medido cerca del tanque fue tal que podría haber matado a una persona con una exposición de cuatro horas. El enorme volumen del agua almacenada después de dos años y la velocidad con la que este crece es otra fuente de problemas para TEPCO. Para intentar solucionarlo, el gobierno japonés –no la compañía TEPCO– invertirá 150 millones de dólares en la construcción de una planta para descontaminarla de radiación y así poder descargarla en el mar.Se ha encontrado también que el agua subterránea en las inmediaciones de la planta se filtra hacia los sótanos de los edificios de los reactores dañados, en donde se mezcla el con agua contaminada filtrándose posteriormente hasta el mar. Se estima que unas 300 toneladas diarias de agua radiactiva terminan descargándose en el Océano Pacífico. Para solucionar este problema, el gobierno japonés –otra vez no la compañía TEPCO– anunció esta semana que invertirá 320 millones de dólares en la construcción de una “pared de hielo” de 30 metros de profundidad alrededor de la planta nuclear y así evitar el flujo de agua, tanto hacia el interior como hacia el exterior de la misma. Dicha pared de hielo se construirá por medio de tubos enterrados a los que se hará circular un refrigerante a una temperatura de menos 40 grados centígrados. Hoy en día, el mundo obtiene el 12% de la energía eléctrica que consume de fuentes nucleares. Hay países, sin embargo, que tienen más vocación nuclear que otros. El ejemplo extremo es Francia, que obtiene el 75% de su electricidad de fuentes nucleares. Antes del desastre de Fukushima el porcentaje correspondiente a Japón era 30%; en la actualidad solamente una planta nuclear japonesa está en operación. A raíz del accidente de Fukushima, Alemania –que en 2011 obtenía el 18% de su electricidad de fuentes nucleares– ha adelantado 11 años su decisión de deshacerse de sus centrales nucleares y ahora esto ocurrirá en 2022. Para sustituirlas dependerá de los combustibles fósiles y de las fuentes renovables de energía. Hoy en día, Alemania genera el 25% de su electricidad por fuentes de energía renovable, entre las que se incluye la energía del Sol. Este porcentaje se está incrementando rápidamente y la meta para el año 2020 es aumentarlo al 35%, y de manera paulatina hasta el 80% en 2050.En interesante hacer notar que, a pesar de ser Alemania un país norteño con relativamente poca radiación solar, está desarrollando fuertemente la energía solar fotovoltaica para la generación de energía eléctrica. Si bien en 2011 Alemania generó por esta vía sólo el 3% de su electricidad, la energía fotovoltaica en este país está creciendo rápidamente, doblándose cada 1.5 años.Hoy en día la mayor fuente de energía del mundo son los combustibles fósiles. Éstos, sin embargo, además de ser finitos, son fuente de contaminación y han llevado a los problemas de cambio climático que aquejan al planeta. Por otro lado, sin bien en algún momento se concibió a la energía nuclear como una fuente confiable de energía, accidentes como los de Fukusihima y Chernobyl la muestran como peligrosa y contaminante, aun sin considerar el problema de la disposición de los desechos radiactivos.  En este contexto, siendo el Sol una fuente prácticamente inagotable de energía y dado que el aprovechamiento de sólo una mínima parte de la energía solar que recibe la Tierra bastaría para satisfacer nuestras necesidades energéticas, el Sol se presenta como la energía del futuro. Sobre todo para un país tropical como el nuestro que, ciertamente, tiene más recursos solares que Alemania.",
    "Los lobos son, ciertamente, animales con una mala reputación, que frecuentemente han sido considerados como la encarnación del mal. Es una loba, por ejemplo, la que en el canto primero de la Divina Comedia impide a Dante acercarse a la cima luminosa en la que reencontraría el camino de la virtud perdido a la mitad de su vida. Siglos más tarde, el cuento infantil de Caperucita Roja de los Hermanos Grimm nos habla de una niña pequeñita a la que un lobo malvado engaña y devora, tal como había hecho antes con su abuelita. Los lobos como encarnación del mal también aparecen en la novela “Drácula” de Bram Stoker. Cuando el protagonista Jonathan Harker, ignorante de los peligros que le acechaban, se acerca al castillo de Drácula en Transilvania en medio de una noche iluminada por la luna llena, lo hace rodeado de aullidos de lobos. Posteriormente, Harker constata que lo lobos obedecen a Drácula, cuando éste les ordena que den muerte a una mujer que reclamaba le devolvieran a su hijo raptado por el vampiro.No obstante, no siempre los lobos son o han sido considerados entes malignos. Baste recordar la leyenda de Rómulo y Remo, quienes al nacer habrían sido abandonados a su suerte en una cesta en el Rio Tíber, logrando sobrevivir amamantados por una loba, episodio que a la postre habría llevado a la fundación de Roma. En otro contexto, el apodo “lobos” es considerado adecuado para equipos de deportes que requieren de rapidez, agresividad o violencia –como es el caso del futbol americano.  De un modo o de otro, los lobos –no como símbolos sino como seres de carne y hueso– no gozan de nuestra simpatía, como animales depredadores y feroces que son. Y las películas de Hollywood, en las que humanos se convierten en lobos asesinos en las noches de luna llena, no contribuyen a conquistarla. De hecho, es precisamente Hollywood el responsable de que la silueta de un lobo aullando, teniendo como trasfondo la luna llena, se haya convertido en una imagen icónica de historias de terror.Pero, ¿cuál es  realmente la razón por la que los lobos aúllan? Según Francesco Mazzini de la Universidad de Parma, Italia, y un grupo de colaboradores de varios centros de investigación en Europa, los lobos aúllan para comunicarse con otros lobos; y lo hacen de una manera consciente y no solamente como una respuesta automática a una cierta situación de estrés. Alcanzaron esta conclusión mediante un estudio realizado en Austria con nueve lobos mantenidos en cautiverio, el cual fue publicado el 22 de agosto pasado en la revista “Current Biology”.Como parte de la investigación, Mazzini y colaboradores separaron y llevaron lejos del grupo a uno de los lobos y registraron los aullidos que los restantes animales emitían en respuesta. Hicieron esto 27 veces, escogiendo en cada ocasión un lobo al azar.  Al mismo tiempo, mediante un análisis de saliva, midieron el nivel de estrés que sufrían los lobos por el hecho de que uno de ellos fuera removido del grupo. Encontraron que el número de aullidos estaba en relación directa con la jerarquía que el lobo segregado tenía en el grupo, siendo más frecuentes cuando se trataba del lobo dominante. Igualmente, la frecuencia de aullidos de un determinado lobo estaba relacionada con la relación particular que dicho lobo tenía con el segregado. Por otro lado, los investigadores no encontraron una relación clara entre el nivel de estrés y la frecuencia de los aullidos. En otra serie de experimentos, se separó a un lobo del grupo y se le colocó en un lugar cercano. Los otros miembros del grupo no podían verlo pero sabían en donde se hallaba. Se encontró que solamente en 2 de 27 experimentos hubo aullidos 20 minutos después de la separación. Esto contrasta con los experimentos en los que se llevó al lobo segregado lejos del grupo, en donde se produjeron aullidos en 26 de 27 pruebas.Mazzini y colaboradores concluyeron que los aullidos son producto del conocimiento consciente que los lobos tienen de su situación y de sus alrededores y no resultan meramente del nivel de estrés al que están sometidos. Es decir, que los aullidos tienen una base social, pues para el grupo de lobos es importante la función del líder. Un lobo responde igualmente a la situación que experimenta cuando es separado de otro lobo con el que mantiene una relación cercana.No obstante, John Teberge de la Universidad de Waterloo, Canadá, opina que es peligroso extender resultados obtenidos con lobos en cautiverio a aquellos que está en libertad. Para éstos, que pueden seguir de manera libre a otros lobos por medio del olor, la función que cumple el aullido puede ser diferente. Señala, además, que los lobos en libertad aúllan después de despertar de un periodo largo de sueño, por causas claramente diferentes a las de los experimentos.Si se pueden extender los resultados obtenidos con lobos en cautiverio a lobos en libertad es algo que tendrá que establecerse con nuevas investigaciones. Resulta, sin embargo, interesante contrastar la imagen de los lobos que nos entrega el artículo de  Mazzini y colaboradores, con aquella idea común de estos animales como bestias asesinas y encarnaciones del mal. Y constatar, además,  que no necesitan de la luna llena para aullar.",
    "Como nos lo enseñan los libros de historia, cuando en el año de 1453 cayó Constantinopla en poder de los turcos otomanos, se obstaculizaron las rutas terrestres de comercio entre Europa y el Lejano Oriente que habían florecido por varios siglos. Esto motivó la búsqueda de rutas marinas alternativas por parte de Portugal y España. En este contexto, entre 1497 y 1499 Vasco da Gama realizó, financiado por Portugal, un viaje ida y vuelta por mar entre Lisboa y la costa occidental de la India, bordeando el continente africano por el Cabo de Buena Esperanza. Vasco da Gama descubrió así, para beneficio de los portugueses, una ruta marina practicable entre Europa y Asia. España, por su parte y como sabemos, envió a Cristóbal Colón a encontrar una ruta a las Indias Orientales viajando hacia el oeste. Cosa que no logró, descubriendo en cambio el Continente Americano, lo que, por supuesto, resultó de más impacto, no solamente para España sino para el resto del mundo. Las especias fueron una de las mercancías asiáticas de más valor para los europeos. De hecho, hay quien considera que fueron la principal motivación para los viajes de exploración de portugueses y españoles al final del siglo XV, que dieron como resultado las nuevas rutas intercontinentales de comercio. Y que llevaron al descubrimiento del Nuevo Mundo, acontecimiento que cambió la faz del planeta y el curso que seguiría en los siguientes siglos –y como bono extra, por si algo faltara, al descubrimiento del chile y de la vainilla, especias que eran desconocidas en Europa y que posteriormente se diseminaron por todo el mundo.¿Por qué hemos sido tan adictos al consumo de condimentos que no tienen valor nutricional por sí solos y solamente modifican el sabor de los alimentos? Se ha aducido que las especies tienen una acción antimicrobiana y que en tiempos pasados se usaron para preservar los alimentos. También que contribuyeron a enmascarar el mal sabor de los mismos por efecto de su putrefacción o fermentación. O bien, que los hemos usado simplemente porque nos gusta su sabor, explicación que, a falta de más evidencia objetiva, resultaría ser la mejor por ser la más simple. Si bien no es claro por qué usamos las especias, lo que sí sabemos es que los hemos empleado por mucho tiempo. Al menos por seis mil años, como concluye un artículo aparecido el pasado miércoles en la revista “PLOS ONE”, publicado por un grupo de investigadores encabezados por Hayley Saul de la Universidad de York en el Reino Unido.     Dicho artículo se refiere a un estudio de restos minerales de comida preservados en peroles prehistóricos encontrados en la región del Báltico occidental. De acuerdo con Saul y colaboradores, dichos restos, con una antigüedad entre 5750 y 6100 años, corresponden a semillas de hierba de ajo (“Alliaria Petiolata”). Estas semillas tienen un fuerte sabor y poco valor nutritivo, lo cual, aunado a que se encontraron en mayor cantidad dentro de los peroles que fuera de ellos y al lado de restos de animales terrestres y marinos, permite concluir a los a investigadores que fueron añadidos de manera deliberada para condimentar la comida.Los autores del artículo de referencia hacen notar que, al menos en uno de los casos estudiados, la costumbre de condimentar los alimentos precedió a la introducción de la agricultura en esa región de Europa. Esto pone a prueba la noción de que los cazadores-recolectores ingerían plantas solamente como fuente de energía.    La vida en la región del Báltico hace 6000 años para la población nómada tiene que haber sido dura. Y, sin embargo, de acuerdo con Saul y colaboradores, los cazadores-recolectores que habitaron esta parte de Europa hace miles de años tuvieron el tiempo y la disposición suficientes para tomarse el trabajo de condimentar su comida, por más que esto no les fuera indispensable para sobrevivir. El gusto por las especies que contribuyó a cambiar al mundo de manera tan radical hace 500 años tendría entonces raíces profundas y nos vendría de mucho tiempo atrás.",
    "El Día de Todos los Santos del año 1755, la ciudad de Lisboa, capital de Portugal, fue sacudida por un sismo de gran magnitud con epicentro en el Océano Atlántico, a unos 200 kilómetros de la costa. Como resultado, Lisboa fue en gran medida destruida, tanto por el temblor mismo, como por el tsunami y el fuego que le siguieron. Uno de los edificios parcialmente destruidos fue la Iglesia del Carmen, que en esos momentos se encontraba repleta de personas asistiendo a la celebración religiosa y sobre las que se desplomó el techo del edificio. Después del sismo la iglesia del Carmen –construida a finales del siglo XIV– no fue reconstruida, sino que ha sido mantenida en estado ruinoso en recuerdo de la tragedia.  La Iglesia del Carmen en Lisboa no ha sido, por supuesto, sino uno de los muchos templos que han sido destruidos por un temblor. Más recientemente, en febrero de 2011, la catedral anglicana de la ciudad de Christchurch en Nueva Zelanda fue destruida parcialmente por un sismo. Aunque afortunadamente en esta ocasión no hubo víctimas mortales en el interior de la catedral, ésta quedó inutilizada. De hecho, está actualmente en vías de ser demolida, si bien los oponentes a que esto ocurra han detenido el proceso que se ha convertido en un asunto político.De un modo u otro, Christichurch se ha quedado de momento sin catedral anglicana. Para resolver este problema en lo inmediato, Victoria Matthews, obispo de Christchurch, recurrió a una solución singular: la construcción de una catedral de cartón, misma que empezó a operar el pasado domingo. La nueva catedral es obra del arquitecto japonés Shigueru Ban que se ha caracterizado por el empleo de materiales de construcción no convencionales; en particular, por el empleo de tubos de cartón de grandes dimensiones como elementos estructurales. La nueva catedral de Christchurch, tiene una capacidad para 700 personas y asemeja una gran tienda de campaña triangular. El cuerpo del edificio lo forman 98 tubos de cartón de 60 centímetros de diámetro y 20 metros de largo. De acuerdo con su diseñador, la construcción tendrá un tiempo de vida de 50 años. Con relación a esto, la pregunta que surge de inmediato es si los tubos de cartón no se mojarán y reblandecerán con las primeras lluvias. La respuesta es que dichos tubos se han recubierto con tres capas impermeables de plástico. Además, el techo del edificio está hecho de policarbonato impermeable para protección de los tubos de cartón.Shigueru Ban ha empleado tubos de cartón para diseñar estructuras y resolver problemas en diversas situaciones de emergencia. Entre otras realizaciones, ha diseñado y construido tiendas de campaña para los refugiados en Ruanda a un costo de 50 dólares por unidad, así como divisiones en los albergues que se implementaron para los damnificados del terremoto que devastó el norte de Japón en 2011; esto último con el fin de proporcionarles un cierto grado de privacidad. Diseñó igualmente una iglesia con tubos de cartón como elementos estructurales después del sismo que afecto la ciudad japonesa de Kobe en 1995. Al cabo de un cierto tiempo de uso, esta iglesia fue desmontada y trasladada a Taiwan en donde se erigió de nueva cuenta, está vez de manera permanente.  Una construcción de cartón no tiene de ninguna manera la durabilidad potencial que tiene una construcción de piedra o concreto. Así, mientras que a la nueva catedral de Chistrichurch se le estima un tiempo de vida de 50 años, la iglesia del Carmen de Lisboa sobrevivió por 350 años y la antigua catedral de Christchurch lo hizo por más de un siglo. Por otro lado, según el arquitecto japonés, la nueva catedral neozelandesa, construida con materiales ligeros y por tanto con poca inercia, es resistente a los terremotos. Esto, por supuesto, es importante en un país como Nueva Zelanda, que está colocado sobre el llamado anillo de fuego del pacífico. Desde este último punto de vista, entonces, un edificio de cartón es superior a uno de piedra o de concreto. Como lo es también desde el punto de vista del impacto que su construcción tiene sobre el medio ambiente y el uso de los recursos naturales, dado que el cartón puede ser reciclado. De manera adicional, es superior por sus menores costos de construcción. Un edificio de cartón, por otro lado, no cumple la función que, según expresa Shigueru Ban en una entrevista concedida al periódico japonés “The Japan Times”, la arquitectura ha tenido de hacer visible la riqueza o el poder político de aquellos que la o lo tienen. En este sentido habrá que reconocer que un edificio de cartón no resulta tan espectacular como un edificio de mármol o una catedral gótica de piedra.  Y, sin embargo, la espectacularidad de un edificio no debe prevalecer sobre aspectos de seguridad, medio ambiente y beneficio a un porcentaje mayoritario de la población.",
    "Como sabemos, el petróleo es un energético no renovable que tarde o temprano se agotará. La fecha para que esto ocurra, no obstante, es motivo de debate entre los especialistas. Según los pesimistas, estamos a punto en que se llegue a un máximo en la producción mundial de petróleo; a partir de este punto, dicha producción comenzará a declinar de manera paulatina, tanto por el agotamiento de los mantos petrolíferos, como por cuestiones geopolíticas al estar las mayores reservas de crudo distribuidas en el mundo de manera inequitativa. Los optimistas, por su lado, consideran que el máximo de producción ocurrirá hasta dentro de tres o cuatro décadas, como consecuencia del descubrimiento de nuevas reservas o de avances tecnológicos que posibiliten la extracción de petróleo ahora inaccesible. Precisamente, uno de estos avances ha permitido a los Estados Unidos incrementar su producción,  tanto de petróleo como de gas natural, empleando la técnica conocida como “fracking”. Esta técnica posibilita la liberación del petróleo o gas atrapado en rocas en el subsuelo. Para esto se perfora un pozo hasta el yacimiento a explotar y se inyecta agua a presión mezclada con arena y algunos productos químicos con el propósito de fracturar la roca y liberar el petróleo o el gas, que emerge a la superficie a través del pozo. Mediante la técnica de “fracking” los Estados Unidos han podido revertir en los últimos años la tendencia en su producción de petróleo que, excepto por un periodo de estabilización en la década de los años ochenta, se había mantenido a la baja desde 1970. El “fracking”, sin embargo, es una tecnología muy controvertida. Sus detractores, por ejemplo, señalan que requiere de grandes cantidades de agua para fracturar las rocas del yacimiento a  explotar, que puede contaminar los mantos acuíferos. Además, parte del agua inyectada regresa a la superficie contaminada con metales pesados y elementos radiactivos y tiene que ser confinada de algún modo. Una manera de hacerlo es reinyectándola en el subsuelo a una profundidad mayor a la de la roca fracturada. Hay evidencia, no obstante, que esta práctica puede producir temblores de tierra de intensidad media.Aplicado a la producción de gas natural, “el fracking” suma puntos a su favor pues el gas generado puede ser empleado como combustible en plantas termoeléctricas en sustitución del carbón, contribuyendo así a reducir las emisiones de gases de invernadero a la atmósfera. Al mismo tiempo, sin embargo, al fracturar las rocas en las que está atrapado el gas se puede propiciar que parte de éste se escape a la atmósfera. Esto contribuiría al calentamiento global, pues el metano, que es el principal componente del gas natural, es un  gas de invernadero más potente que el dióxido de carbono.Un artículo aparecido en el número de esta semana de la revista “New Scientist”, escrito por Michael Brooks, analiza las posibilidades de que la producción de gas natural por la técnica “fracking” pueda servir de puente entre la época actual, caracterizada por el uso de combustibles fósiles convencionales, y el mundo del futuro que necesariamente dependerá de energías renovables. El papel del “fracking” como puente energético hacia el futuro dependerá del grado en el que la experiencia estadounidense con esta tecnología pueda repetirse en otros lugares del planeta, lo cual está por verse. Considera, no obstante, que el “fracking” difícilmente será una solución mágica que resuelva los problemas energéticos del mundo en el futuro inmediato, aunque probablemente contribuirá a paliar el problema del calentamiento global.     Nuestro país en estos momentos está inmerso en una controversia sobre la apertura de PEMEX a la inversión extranjera. La producción petrolera en México a partir de 2004 ha ido a la baja, producto del declive del yacimiento Cantarell en la Sonda de Campeche. Entre las razones esgrimidas en favor de dicha apertura se encuentra la falta de tecnologías en el país para elevar nuestra producción de energéticos, lo cual es difícil de argumentar en contra.No es justificable, sin embargo, que en los años de bonanza petrolera gracias a Cantarell no hayamos hecho lo suficiente para desarrollarlas. La historia misma del descubrimiento del yacimiento es significativa en este respecto: Cantarell fue descubierta en forma accidental por un pescador que fue a reclamar a las oficinas de PEMEX una indemnización porque sus redes de pescar fueron impregnadas por el petróleo que escapaba del yacimiento. Así, de manera accidental, no como resultado de sus exploraciones en busca de petróleo, fue que PEMEX se enteró de la existencia de un campo petrolífero que a la postre resultaría ser el segundo mayor del mundo. La era de los energéticos de fácil extracción en México llegó a su fin y esto nos obliga a desarrollar capacidades tecnológicas para la extracción de combustibles fósiles en condiciones cada vez más complicadas. De acuerdo con datos del Departamento de Energía de los Estados Unidos, México ocupa el sexto lugar mundial en reservas recuperables con técnicas de “fracking”, lo que nos obliga a conocer todo acerca de esta técnica, incluyendo los riesgos ambientales que conlleva.   A menos que nos resignemos a ser simples observadores de compañías extranjeras extrayendo energéticos de nuestro subsuelo sin mayores controles ambientales.",
    "El pasado día 22 de julio la NASA dio a conocer una fotografía de la Tierra tomada tres días antes desde Saturno por la sonda Cassini, que la agencia espacial estadounidense ha mantenido en órbita alrededor de este planeta desde el año 2004. Dicha  fotografía, en la que se ven de manera espectacular en un primer plano los anillos de Saturno, muestra a  la Tierra apenas como un punto luminoso de color azul pálido –si bien en una amplificación de la misma se pueden ver a nuestro planeta y a la Luna como dos objetos separados–. Esto era de esperarse, pues la fotografía fue tomada a una distancia de casi 1,500 millones de kilómetros, que es aproximadamente diez veces la distancia que separa a la Tierra del Sol.Con el objeto de promover el interés en el espacio y lograr un mayor apoyo para sus proyectos, la NASA hizo una invitación al público en general por medio de su página de internet para decir “hola” a la cámara en el momento de tomar la fotografía a la que respondieron 20,000 personas. Como era de esperarse, a pesar de la numerosa participación nadie apareció en la toma saludando con la mano en alto. De hecho, basándose únicamente en dicha fotografía, un extraterrestre no hubiera podido descubrir que nuestro planeta está habitado. Esto, en contraste, sería evidente a partir de una fotografía tomada a menor distancia: digamos desde una altura de cientos de kilómetros sobre la superficie terrestre, que es la altura a la que orbitan la mayor parte de los satélites artificiales. En efecto, una fotografía nocturna tomada desde esta distancia nos revelará que la superficie terrestre está salpicada de luces, que han aparecido en un periodo de apenas cien años, y que en algunas áreas estas luces son más brillantes que en otras que están en casi oscuridad. En el sitio web de la NASA encontramos fotografías de la Tierra con la que podemos comprobar lo anterior. En ellas podemos ver que la intensidad de las luces está asociada con la densidad de población. Así, la brillantez es notablemente alta en los grandes centros urbanos estadounidenses y está casi ausente en lugares como la selva amazónica, el centro de Australia o el desierto del Sahara. Aprendemos, igualmente, que a los habitantes de la tierra les gusta establecerse en las costas, a juzgar por la mayor densidad de luz a lo largo de éstas.    No necesariamente, sin embargo, a una mayor densidad de población corresponde una mayor intensidad de luz, pues ésta también depende del grado de desarrollo económico del área considerada. Así, tenemos que las regiones con mayor profusión de luz corresponden a los territorios de los Estados Unidos, de Europa Occidental y de Japón. China y la India, que en conjunto comprenden un tercio de la población del mundo, aparecen relativamente menos iluminadas.Como sabemos, la luz eléctrica es un invento relativamente reciente, que se desarrolló gradualmente en los Estados Unidos y Europa a lo largo del siglo XIX. En las últimas décadas de ese siglo, Thomas Alva Edison fabricó la primera lámpara eléctrica comercialmente exitosa y con esto se inició la era de la luz, hasta llegar a la época actual en la que existen áreas del mundo profusamente iluminadas y visibles desde el espacio.  La primera lámpara comercial desarrollada por Edison era del tipo incandescente y dependía de un filamento de carbón al cual se le hacía pasar una corriente eléctrica que elevaba considerablemente su temperatura. Como consecuencia del calentamiento, el filamento emitía luz, cuya tonalidad dependía de la temperatura alcanzada. Con el tiempo, el filamento de carbón fue reemplazado por un filamento de tungsteno, y es en esta forma que la lámpara incandescente ha llegado hasta nuestros días, como una de las fuentes más empleadas –si bien no la única–  para producir luz eléctrica.La lámpara incandescente, no obstante, es marcadamente ineficiente, pues solamente alrededor del 10% de la energía que consume es transformada en luz.  Esta lámpara no es, pues, amigable con el medio ambiente y está siendo reemplazada por otras lámparas notablemente más eficientes. Una opción que resulta ser unas cuatro veces más eficiente es la de las lámparas a base de diodos emisores de luz, o LED´s, como comúnmente se les conoce. Estas lámparas son todavía relativamente caras pero, al ser eficientes, resistentes y durables, se presentan como la opción que prevalecerá en un futuro cercano.Sin embargo, aun con las nuevas lámparas más eficientes, algunos consideran que el gasto energético en iluminación es excesivo en algunas regiones del mundo, dada la emergencia climática por la que atraviesa el planeta como consecuencia de la quema de combustibles fósiles. Con respecto a esto, hay que señalar que el sistema de iluminación del mundo consume un 19% de la  energía eléctrica generada a nivel global. Con estos números, no es sorprendente que las luces que artificialmente generamos sean visibles desde el espacio. Aunque afortunadamente no desde Saturno.",
    "Incluso un hombre que es puro de corazón / y reza por las noches / puede convertirse en un lobo cuando la belladona florece / y la luna de otoño brilla.Atribuido por Wikipedia al guionista de la película “El hombre-lobo” (1941).A pesar de lo que diga Hollywood, es muy improbable que la Luna llena tenga el poder para transformar a una persona en un hombre-lobo, que ataca y da muerte a todos aquellos a los que encuentra a su paso con una ferocidad extrema. Más probable es que esto no pase de ser una invención más de la industria del cine, basada en el folclor europeo. Algunos sostienen, no obstante, que la Luna sí influye sobre los humanos, si bien no de un manera tan dramática como para llegar a convertirnos en monstruos. Una revisión rápida en Wikipedia nos da algunos ejemplos: se asume que la Luna llena produce, entre otras cosas, un incremento en la fertilidad humana, en los niveles de violencia entre humanos, y en la posibilidad de morir en una operación quirúrgica. Se sostiene, igualmente, que la Luna llena influye en nuestro estado mental. Prueba de esto último es que incluso existe el término “lunático”, que, según el diccionario de la Real Academia Española, hace referencia a alguien que “padece locura, no  continua, sino por intervalos”. A pesar de su popularidad, sin embargo, las creencias acerca de la influencia que la Luna llena tiene sobre nuestro comportamiento no son más que leyendas sin mayor sustento, por más que hayan  estado a nuestro alrededor por un buen número de años. Aunque, después de todo, la influencia de la Luna llena quizá sí sea real en  algunos casos. Esto, al menos de acuerdo con un artículo aparecido esta semana en la revista “Current Biology”, publicado por un grupo de investigadores médicos encabezado por Christian Cajochen del Hospital Siquiátrico de la Universidad de Basel en Suiza.  En dicho artículo se reportan los resultados de la investigación llevada a cabo con el objeto de averiguar si el ciclo lunar tiene una influencia sobre los hábitos de sueño.El estudio de referencia se realizó con 17 adultos jóvenes, hombres y mujeres, con edades que fluctuaban entre los 20 y los 31 años, y con 16 adultos mayores, también hombres y mujeres, con una edad media de 65 años.  Los investigadores encontraron que en las noches alrededor del día en que la Luna alcanza su máximo brillo, el sueño sufre alteraciones claras. De manera específica, en promedio tardamos cinco minutos más en conciliarlo, dormimos unos 20 minutos menos, y lo hacemos con menos profundidad. Un punto importante a señalar es que los experimentos se llevaron a cabo durante los años 2000-2003, y con un propósito diferente al del estudio que nos ocupa –el objetivo original fue el de determinar cómo afecta la edad a los patrones de sueño–. De este modo, los participantes no tuvieron ideas preconcebidas sobre la investigación en la que participaban, y no pudieron reaccionar de manera subjetiva, falseándola de manera inconsciente.     En contraste con el proceso de transformación de un humano en un hombre-lobo –que  aparentemente, según Hollywood, requiere de la luz de la Luna–, el efecto medido por Cajochen y colaboradores no dependió de la presencia de un mayor o menor brillo nocturno, pues los participantes no pudieron ver la Luna. Así, los investigadores concluyen que la asociación que encontraron entre un sueño más intranquilo y la ocurrencia de la Luna llena debe ser un reflejo, no del hecho de que exista una mayor cantidad de luz nocturna, sino de la existencia de un reloj biológico sincronizado con el ciclo lunar, que persiste aun en la ausencia de luz u oscuridad. Esto,  de manera similar a como el  ciclo biológico de sueño-vigilia se sincroniza con el ciclo día-noche. De confirmarse lo reportado por Cajochen y colaboradores, la Luna llena –o de manera más precisa, el ciclo lunar– sí tendría un efecto sobre nuestro comportamiento. No al grado de convertirnos en hombres-lobo, pero sí de una manera más sutil, alterándonos el sueño.      Por ahora poco más se puede decir científicamente acerca de los lunáticos y del efecto que la Luna llena ejerce sobre los humanos. Si lo consideráramos como argumento para una película resultaría demasiado pobre, pues el que la Luna llena nos haga dormir 20 minutos menos al día no parece ser un tema con el cual se puedan generar ganancias sustanciales para la industria cinematográfica.  Aunque para esta industria los argumentos científicos son lo de menos. Para convencernos basta observar el éxito económico que han tenido un gran número de películas en las que aparecen humanos transformándose en feroces hombres-lobo en las noches de Luna llena.",
    "¿Nos fueron los veneros del petróleo escriturados por el diablo? Si la pregunta se toma en sentido literal difícilmente lo sabremos con certeza, pues para empezar habría que demostrar la existencia del diablo. Si, por el contrario, la tomamos metafóricamente –en lo que respecta al diablo– podemos afirmar que López Velarde tenía razón, aunque la escrituración, como todo pacto demoniaco, habría tenido tanto aspectos negativos como positivos.En efecto, en el tiempo en que López Velarde escribió Suave Patria (1921) la industria petrolera en México –que nació con el siglo– estaba en ascenso, mismo que continuó en las décadas que siguieron hasta convertirse en un sostén fundamental de la economía de País. Habríamos hecho de este modo una suerte de pacto con un diablo metafórico, el cual hubo de escriturarnos una riqueza que nos ha permitido vivir con un cierto desahogo. Todo esto, por supuesto, a un costo y el petróleo hubo de convertirse también en fuente de calamidades de todo tipo como a todos nos consta. Por otro lado, México no ha sido el único país al que el diablo le habría escriturado manantiales de petróleo. De hecho, el petróleo, junto con el carbón y el gas natural, constituyen las principales fuentes de energía del mundo actual, y si bien es cierto que los beneficios de la industrialización no han llegado de la misma manera a todos los pobladores ni a todas las regiones del mundo, no podemos negar que algo se ha progresado en los dos últimos siglos en cuanto a niveles de bienestar. La industrialización, no obstante, también tiene sus aspectos negativos. El ejemplo prototípico en este sentido es el calentamiento global, producto de la quema de combustibles fósiles durante dos centurias que amenaza con cambiar el clima del planeta. Una serie de artículos publicados en el número de esta semana de la revista “Science” trata de otro de los aspectos negativos de la industrialización: la generación de temblores de tierra producto de la inyección a alta presión de fluidos en el subsuelo. Esta inyección se lleva a cabo en diversas circunstancias. En la industria de los energéticos, por ejemplo, se emplean técnicas de fractura de rocas en el subsuelo con el objeto de posibilitar o incrementar la extracción de petróleo o gas. Para este propósito, se inyecta a presión en el subsuelo, agua mezclada con arena y ciertos aditivos químicos, procedimiento que se sabe genera micro-temblores. De acuerdo con un artículo publicado esta semana por William Ellworth del “US Geological Survey” en “Science”, estos micro-temblores son demasiado pequeños para producir daños. Así, de 100,000 pozos que han sido sujetos al proceso de fractura de rocas, el temblor más grande que se ha generado tuvo una magnitud de solamente 3.6. No obstante, durante el proceso se  genera una gran cantidad de agua residual que es reinyectada en el subsuelo a una mayor profundidad, procedimiento que se ha asociado a la generación de temblores de magnitud media en el centro del territorio de los Estados Unidos. Entre éstos se encuentra un temblor en el estado de Oklahoma de magnitud 5.6 que en 2011 destruyó 14 casas.Según otro artículo aparecido también en el último número de “Science”, publicado por un grupo de investigadores de la Universidad Columbia en Nueva York y de la Universidad de Oklahoma, pueden producirse temblores de magnitud media en zonas en las cuales se practica la fractura de rocas, como producto de grandes temblores de origen natural ocurridos en lugares remotos. Este es el caso de los temblores de Chile en febrero de 2010, de Japón en marzo de 2011 y de Sumatra en abril de 2012, los cuales provocaron temblores en varios estados del centro de los Estados Unidos. El uso de la tecnología de fractura de rocas se ha extendido en los últimos años en la extracción del subsuelo del llamado gas de esquisto, lo que ha permitido elevar considerablemente las reservas de gas que son susceptibles de ser recuperadas. Según la Asociación de Proveedores de Gas Natural de los Estados Unidos, esta tecnología permitió incrementar las reservas de gas de ese país en un 39% entre 2006 y 2009. La tecnología de fractura de rocas cuenta, sin embargo, también con críticos, quienes afirman que tiene efectos adversos sobre el medio ambiente. En particular, por su potencial para contaminar los acuíferos con los fluidos inyectados al subsuelo, y a la atmósfera por fugas de gas desde los pozos; y, por supuesto, por su potencial para producir temblores de tierra. Quizá todo esto sea parte del precio que el diablo nos cobra por proveernos de nuevas riquezas energéticas –México, en particular, contaría con una de las mayores reservas mundiales de gas de esquisto. Cuando López Velarde escribía “El Niño Dios te escrituró un establo/y los veneros del petróleo el diablo”, México estaba en proceso de cambio de una sociedad rural a una urbana, en la que las actividades basadas en el petróleo inevitablemente crecerían en importancia a expensas de aquellas basadas en los establos. Poner a estos últimos por delante del primero suena entonces como un despropósito. Aunque, pensándolo bien, el precio que el diablo nos quiere cobrar ahora por los nuevos energéticos quizá sea demasiado grande y posiblemente el poeta zacatecano después de todo algo tenía de razón.",
    "El próximo miércoles 10 de julio habrán transcurrido 162 años desde la muerte de Louis Daguerre, a quien se atribuye haber inventado la fotografía. Fue Daguerre el autor de lo que se considera es la primera imagen fotográfica de un ser humano: una calle parisina en la que en la parte inferior se aprecian dos personas, una de ellas con un pie levantado y la otra aparentemente lustrándole las  botas. Esta fotografía, o daguerrotipo, fue tomada en el año 1838 empleando la técnica desarrollada por Daguerre, la cual hacía uso de una placa de cobre recubierta de yoduro de plata. De acuerdo con la Enciclopedia Británica, un daguerrotipo requería de un tiempo de exposición entre veinte y treinta minutos, de modo que para que una persona u objeto pudiera impresionar a la placa fotográfica de manera nítida requería mantenerse estático durante este tiempo.No fue Daguerre, sin embargo, quién realizó la primera fotografía sobreviviente hasta nuestros días. Este mérito corresponde a Nicéphore Niépce -de nacionalidad francesa al igual que Daguerre-. La fotografía en cuestión, impresa en 1826, está tomada viendo hacia afuera de la ventana en el segundo piso de la casa de Niépce. La técnica empleada requería de horas de exposición y no era apta, por lo tanto, para fotografiar personas u objetos en movimiento.  En 1829 Niépce estableció con Daguerre una sociedad para mejorar su técnica fotográfica. Niépce, no obstante, murió en 1933 y Daguerre quedó sólo al frente de las investigaciones, que el curso de unos pocos años lo llevaron al daguerrotipo. Como resultado, cuando el descubrimiento de la fotografía fue presentada en 1939 a la Academia de Ciencias de Francia, todo el crédito se lo llevó Daguerre.Con el desarrollo del daguerrotipo y de otras técnicas de fotografía se tuvo por primera vez en la historia la posibilidad de registrar imágenes de manera permanente. De un interés particular eran las  imágenes de hechos históricos y pronto la fotografía se llevó por primera vez a un escenario de guerra. Esto ocurrió durante la Guerra de Crimea, ocurrida en los años 1853-1856 y en la que se enfrentaron el Imperio Ruso, por un lado, y una alianza de países, entre los que se encontraba el Imperio Británico, por el otro.       Los registros fotográficos de la Guerra de Crimea fueron llevados a cabo, entre otros fotógrafos, por Roger Fenton, quien estuvo en el escenario bélico en el año 1855 con el apoyo del gobierno británico. De acuerdo con los especialistas, quizá por esto último, así como por limitaciones técnicas que en ese tiempo impedían fotografiar objetos en movimiento, las fotografías de Fenton no incluyeron ninguna acción de Guerra y evitaron mostrar muertos y mutilados.Como sabemos, en la segunda mitad del siglo XIX y principios del siglo XX se superó esta limitación y el tomar una fotografía dejó de requerir de largos tiempos de exposición. Además, las cámaras fotográficas se desarrollaron a tal punto que la fotografía estuvo al alcance de todos. Al menos parcialmente, pues si bien el acto de tomar una fotografía resultó simple, el obtener una impresión en papel de la imagen capturada no lo era tanto.  Para esto, se requería aplicar una serie de procesos químicos a la película fotográfica expuesta a la luz, con el fin de que revelara dicha imagen y la hiciera inmune a una posterior exposición de luz. El llegar, finalmente, a la impresión en papel a partir de la película revelada y estabilizada requería de procesos químicos adicionales.   La introducción de la cámara digital hace dos décadas ha significado un nuevo avance en la fotografía que la ha convertido –ahora sí de manera plena– en algo que está al alcance de todos, con la circunstancia adicional de que es igualmente sencillo tomar imágenes estáticas que grabar el movimiento. La simplificación se dio porque la cámara digital no depende de la química, como dependía su antecesora, sino de la microelectrónica; específicamente de los “chips” que capturan la imagen y que están formados por millones de elementos sensibles a la luz.La facilidad de capturar imágenes –y sonido de manera simultánea– añadida al enorme desarrollo del internet, ha tenido grandes y múltiples impactos en nuestra vida, como a todos nos consta. Ha exhibido por ejemplo, a toda una gama de “ladies” y “gentlemen” filmados en plena acción, que han sido motivo de escarnio público. La filmación de  conversaciones o acciones comprometedoras, lo mismo que de manifestaciones públicas, ha sido igualmente frecuente y ha sido usada para los más diversos fines.       Las cosas han ciertamente cambiando de manera drástica en los últimos 150 años en materia de captura de imágenes. Hace siglo y medio Roger Fenton viajó a Crimea provisto de un voluminoso equipo de fotografía con el fin de llevar a cabo su encargo. Para transportar este equipo en la zona de guerra, Fenton empleó un carromato de cuatro ruedas tirado por caballos. Podemos darnos idea del aspecto y dimensiones del mismo por una fotografía, tomada en Crimea, que lo muestra con la leyenda “Photographic Van” en un costado. Hoy en día, dejando el aspecto artístico de lado, un fotógrafo aficionado podría tomar fotografías de mayor calidad que las que tomó Fenton empleando un teléfono celular. Y, por supuesto, sería más difícil ocultar las calamidades que necesariamente están asociadas a una guerra.",
    "La revista “Science” no es claramente el lugar idóneo para buscar una receta para adelgazar. En su último número, no obstante, publica un comentario sobre una posible técnica para bajar de peso sin necesidad de recurrir al gimnasio. Esta recomendación, que no es propiamente una receta, está basada en los resultados de un artículo publicado esta semana en la revista “Flavour” por Vanessa Harrar y Charles Spence, sicólogos de la Universidad de Oxford en el Reino Unido. De acuerdo con dicho artículo, el sabor de un alimento está influido por las características de los cubiertos que usamos para ingerirlo, incluyendo su peso y su color. Así, Harrar y Spence encuentran que un pedazo de queso puede saber más salado si se utiliza un cuchillo para llevarlo a la boca en lugar de un tenedor. De la misma manera, el yogurt sabe mejor, y da la impresión de ser un producto más caro, si se ingiere por medio de una cuchara ligera de plástico en lugar de emplear una cuchara, también de plástico y con el mismo aspecto, pero más pesada.  De este modo, algo que podríamos intentar aquellos que tengamos el propósito de bajar de peso es buscar los utensilios adecuados para que, por ejemplo, un jugoso pedazo de carne nos sepa a pastura y nos quite de este modo  el apetito. Repitiendo la operación por semanas o meses –según sea necesario– con suerte logaremos nuestro cometido. Como ayuda adicional podríamos recurrir a técnicas complementarias, como la de emplear platos pequeños para servir la comida. Con relación a esto, consultando la página web del “Laboratorio de Alimentos y Marcas” de la Universidad Cornell en los Estados Unidos, nos enteramos que, de acuerdo a investigaciones realizadas por Brian Wansink y Koert van Ittersum, la cantidad de alimentos que ingerimos está influida por el tamaño del plato que usamos. Para mayor precisión, debido a una ilusión óptica, una misma cantidad de alimento colocada en dos platos de diferente tamaño se ve más pequeña en el plato más grande. Como consecuencia, los platos grandes nos impulsan a comer de forma más abundante. La ilusión óptica responsable de este efecto fue documentada por el sicólogo belga Joseph Delboeuf en el siglo XIX. Por efecto de la misma, dos círculos idénticos aparentan tener tamaños diferentes si están rodeados de círculos de diferente diámetro: el círculo rodeado por otro con un diámetro ligeramente mayor aparenta ser más grande que el círculo que es rodeado por un círculo cuyo diámetro es considerablemente mayor. La ilusión de tamaño también tiene que ver con el contraste de colores. En un experimento realizado por Wansink y van Ittersum durante una comida con universitarios, a los comensales se les ofreció un platillo de pasta y dos opciones de salsa, roja y blanca, para complementarlo. La comida era de autoservicio y a cada comensal se le proporcionó un plato, de color rojo o blanco de manera aleatoria. Una vez de regreso los comensales en sus mesas, los investigadores se las ingeniaron para pesar las porciones que cada uno se había servido. El resultado fue que en los casos en que hubo un contraste de colores –salsa roja sobre un plato blanco o salsa blanca sobre un plato rojo– las porciones fueron en promedio un 22% más grandes que en aquellos casos en que no existió dicho contraste.   Regresando a las investigaciones de la Universidad de Oxford, como se mencionó líneas arriba el sabor de los alimentos está influido por las características de los cubiertos empleados. La explicación a esto, según Harrar y Spence, posiblemente tenga que ver con el contraste entre lo que el comensal espera antes de ingerir la comida y lo que experimenta al hacerlo. Así, ante la presencia de una cuchara de plástico espera que ésta sea ligera. Dependiendo de si la experiencia cumple o no cumple con sus expectativas, calificará de una manera u otra a la comida que acaba de ingerir.Al igual que con todos los estudios científicos, la validez de las conclusiones sobre cómo la vajilla y los cubiertos que empleamos afectan nuestra ingesta y gusto por los alimentos, solamente podrá ser establecida en base a estudios adicionales que las confirmen. Al margen de esto, no obstante, podríamos ponerlas a prueba; después de todo existen una gran cantidad de dietas para adelgazar de todo tipo, algunas sin apoyo científico, que se usan de manera indiscriminada. Así, podríamos intentar servirnos el platillo principal del día en un plato para postre, minimizando el contraste de colores. Al sentarnos frente a un apetitoso corte de carne, podríamos igualmente llevarnos los trozos a la boca empleando una cuchara en lugar de un tenedor. Ciertamente podríamos hacer todo esto y algunas otras cosas más. Aunque habría que decidir si valdría la pena.",
    "Con el último vuelo del programa de transbordadores espaciales llevado a cabo en 2011, los Estados Unidos perdieron la capacidad de realizar misiones tripuladas al espacio, en particular a la Estación Espacial Internacional. Se vieron de este modo obligados a recurrir a la agencia espacial rusa que ofrece servicios de transporte, ida y vuelta, a dicha estación. Los vuelos rusos parten del cosmódromo de Baikonur, localizado en la República de  Kazajistán. Este puerto espacial, fundado en el año 1955, ha tenido un papel destacado en la era del espacio. De ahí partieron los cohetes que pusieron en órbita al primer satélite artificial –el Sputnik I– y a la perra Laika, el primer animal en viajar al espacio. Desde Baikonur fueron igualmente puestos en órbita Yuri Gagarin, el primer humano en el espacio, y Valentina Tereshkova, la primera mujer cosmonauta. El puerto de Baikonur ha sido ciertamente clave en la historia de los vuelos espaciales.    Dado que el cosmódromo de Baikonur se inició como una instalación militar, se escogió para establecerlo un lugar remoto en la entonces República Socialista Soviética de Kazajistán, a unos 200 kilómetros al este del Mar de Aral –en una región con tormentas de arena y variaciones extremas de temperatura que alcanzan los noventa grados centígrados a los largo del año. El nombre Baikonur correspondía en realidad al de un pequeño pueblo minero localizado a unos 300 kilómetros del cosmódromo. Los soviéticos, enfrascados en la Guerra Fría con los Estados Unidos, le dieron al cosmódromo el nombre del pueblo minero –lo mismo que sus coordenadas geográficas– con el fin de ocultar su verdadera localización.Al lado del cosmódromo nació un pueblo para albergar a lsus trabajadores. Dicho pueblo fue originalmente llamado Leninski. En 1995, sin embargo, adoptó el nombre del cosmódromo –y, en último término, el del pequeño pueblo minero usado como carnada para engañar a los espías–. El actual Baikonur es entonces un pueblo joven, con poco más de medio siglo de antigüedad. Es además, un pueblo muy peculiar, con grandes contrastes, como lo describe un artículo aparecido está semana en el diario “The New York Times” cuyo autor es Andrew Kramer. En efecto, Baikoiur es hoy en día la puerta principal al espacio para vuelos tripulados –la única otra puerta es el centro espacial chino de Jiu Quan, en Mongolia Interior, que tiene una actividad mucho menor–. De Baikonur depende para sus vuelos no solamente la NASA, sino también la agencia espacial europea, la de Japón y la de Canadá, que tienen participación en la Estación Espacial Internacional. Baikonur y su cosmódromo están de este modo a la vanguardia tecnológica en cuanto a vuelos al espacio se refiere. Al mismo tiempo, el pueblo de Baikonur propiamente dicho muestra atrasos en otros aspectos. Así, según consigna Kramer en su artículo, los primeros teléfonos celulares llegaron a Baikonur apenas en 2005, y fue solamente hasta 2011 cuando el pueblo contó con un aparato de diagnóstico médico por resonancia magnética. Al mismo tiempo Baikonur sufre un proceso de degradación urbana en el que numerosos edificios abandonados –por resultar imprácticos en el clima extremoso de la región– son ocupados por oleadas de nómadas kasajos. Y lo que es peor, el futuro no luce mejor. El cosmódromo de Baikonur fue construido por los rusos cuando Kazajistán formaba pare de la Unión Soviética. Al disolverse ésta en 1991 y convertirse Kazajistán en un país independiente, el cosmódromo quedó en territorio kasajo. A fin de hacer uso de las instalaciones del puerto Rusia paga a Kazajistán una renta de 115 millones de dólares anuales por medio de un contrato que vencerá en 2050. Rusia, sin embargo, ha decidido construir un puerto espacial en su territorio, con el fin  de “tener garantizado el acceso al espacio”, según ha declarado el Presidente Putin. El nuevo centro espacial –cosmódromo de Vostochny– está ya en construcción y estará localizado en el extremo oriente de Siberia. Se planea terminarlo en el año 2018 y transferir todas las operaciones de Baikonur al nuevo puerto espacial en 2020. Con esto desaparecería el cosmódromo de Baikonur con su cauda impresionante de logros espaciales, y seguramente Baikonur se convertiría en un pueblo fantasma al esfumarse el agente que le dio vida. Un fin poco digno de un pasado tan distinguido.Baikonur, no obstante, tiene una ventaja geográfica con respecto al sitio del nuevo puerto espacial ruso: está localizado a una latitud menor, más cerca del ecuador terrestre. Esto es ciertamente una ventaja, pues le permite aprovechar de mejor manera la rotación de la Tierra para reducir la cantidad de combustible necesario para colocar un peso en órbita terrestre. En este sentido, entre más baja sea la latitud mayor será el ahorro de combustible. Rusia, sin embargo, es un país norteño sin acceso a latitudes bajas y, de hecho, esta fue la razón por la que decidió construir el cosmódromo en Kazajistán. Tomando en cuenta solamente aspectos económicos, podría resultar ventajoso para Rusia no transferir por completo sus vuelos espaciales al nuevo cosmódromo.  Si su ventaja geográfica le vale a Baikonur para sobrevivir es algo que veremos en los próximos años.",
    "Cuando en 1863 los clubes y escuelas de Londres practicantes del futbol se reunieron  en una taberna del centro de la capital británica –llamada “Freemason´s Tavern”– para unificar las reglas del juego –que hasta entonces cada quién practicaba a su manera–, con seguridad jamás imaginaron el impacto que su iniciativa tendría con el correr de los años. En efecto, dado el papel preponderante que los británicos jugaban en el mundo en esos momentos, el futbol organizado –el llamado futbol asociación– no solamente cobró fuerza en el Reino Unido, sino que se extendió por todo el mundo. Y lo hizo con tal fuerza y de manera tan exhaustiva que, según la FIFA, el futbol tiene hoy en día 250 millones de practicantes. Además, en el último medio siglo este deporte se ha convertido en un negocio multifacético de enormes dimensiones, cuyo alcance  no es necesario comentar.En comparación con sus parientes cercanos, el rugby y el futbol americano, el futbol asociación luce como un deporte menos peligroso. Ciertamente, según han documentado los expertos, en el futbol hay una probabilidad más baja de salir con un hueso roto o con un hombro dislocado que en el rugby. Una característica del futbol, sin embargo, es el uso de la cabeza para golpear el balón, y dado que éste tiene un peso de casi medio kilogramo y que puede hacer contacto con la cabeza del jugador a una velocidad de 80 kilómetros por hora, cabe preguntarse si esto es peligroso para la salud. Este pudiera ser el caso, de acuerdo con Michael Lipton del Albert Einsten College of Medicine de la Universidad Yeshiva en Nueva York. Lipton y colaboradores llevaron a cabo una investigación con 37 jugadores aficionados de futbol, 28 hombres y nueve mujeres, con una edad promedio de 31 años. Los jugadores habían practicado el futbol 22 años en promedio y todos habían tenido actividad en el último año.El objetivo de la investigación –publicada esta semana en la revista “Radiology” –  fue el de averiguar si el golpeteo continuo de balón contra la cabeza de un jugador de futbol provoca cambios en la materia blanca del cerebro. Los expertos saben que la materia blanca cumple la función de conectar diferentes partes del cerebro en donde se procesa la información, de modo que un daño a la misma afectará las funciones cognitivas. Lipton y colaboradores encontraron que el cerebro de los jugadores que cabecearon el balón un número de veces más allá de un valor umbral –entre 885 y 1,550 veces por año, según la región de cerebro considerada– mostraron alteraciones en la materia blanca que están asociadas a deficiencias cognitivas en pacientes que sufrieron una conmoción cerebral de mediana intensidad. Encontraron, además, que los jugadores con más de 1,800 cabezazos por año –alrededor de cinco por día– tendían a obtener marcas más bajas en pruebas de memoria que aquellos que cabecearon de manera menos frecuente.De este modo, si bien cabecear un balón de futbol por una sola vez probablemente no genere mayores problemas, según Lipton el hacerlo de manera repetida tiene un efecto acumulativo y lleva a alteraciones en la integridad de la materia blanca. Advierte, no obstante, que los resultados de su estudio son preliminares y que fueron tomados con una muestra pequeña de jugadores. Sería entonces necesario que fueran confirmadas por otras investigaciones independientes.De confirmarse los resultados, la solución para niños y jugadores aficionados –que hacen deporte por gusto y por el beneficio de ejercitarse– sería simplemente evitar cabecear el balón. De hecho, Lipton aconseja que, como precaución, se haga esto en los entrenamientos, durante los cuales es más frecuente el cabecear balones. En cuanto a los jugadores profesionales, el peligro de daño cerebral podría simplemente tomarse como un riesgo propio de la profesión. Después de todo hay deportes profesionales más peligrosos para el cerebro, como es el caso del boxeo, en el que –en beneficio del negocio– se pasan por alto detalles relativos a la seguridad de los competidores.Otra solución podría ser un cambio en las reglas del futbol asociación. Así, de la misma manera como se prohibió usar las manos para acarrear el balón en las reglas redactadas en la “Freemason´s Tavern” hace 150 años –y que sí se permite en el rugby y en el futbol americano–, las nuevas reglas podrían prohibir tocar el balón con la cabeza de manera intencional. Hay que admitir, no obstante, que esto último cambiaría el futbol de manera sustancial y por tanto sería improbable que se llevara a cabo.",
    "En octubre de 1945, a pocos meses del final de la Segunda Guerra Mundial, el escritor británico George Orwell hizo una predicción: presagió un mundo dominado por los dos o tres países que tendrían la capacidad necesaria para fabricar una bomba nuclear. Como los bombardeos atómicos de Hiroshima y Nagasaki habían hecho patente, el poder de destrucción de estas nuevas armas era de una magnitud tal que superaba por mucho al de cualquier arma convencional. Para Orwell era claro que, dado el alto costo de las armas nucleares y la infraestructura industrial necesaria para fabricarlas, muy pocos países desarrollarían en las décadas por venir una capacidad nuclear. Al mismo tiempo, razonaba, el enorme poder de destrucción de las armas atómicas haría que ningún país del club nuclear  atacara a otro del mismo club por temor a una respuesta devastadora. En el momento en el que Orwell hizo su predicción, los Estados Unidos era el único país con capacidad nuclear. En pocos años, no obstante, la Unión Soviética detonó su primera bomba atómica convirtiéndose en la segunda potencia nuclear. Esto ocurrió el 29 de agosto de 1949 y más temprano que tarde se hizo realidad el presagio de Orwell: norteamericanos y soviéticos se embarcaron en lo que se conoce como “Guerra Fría”,  que continuó hasta la disolución de la Unión Soviética en 1991.   Como parte de la carrera armamentista, los Estados Unidos y la Unión Soviética realizaron en los años cincuenta y sesenta más de 500 pruebas nucleares en la atmósfera, hasta que en 1963, por un acuerdo que incluyó a estos dos países y al Reino Unido, se decidió suspenderlas de manera indefinida por la contaminación atmosférica que producían. Entre otros efectos, las explosiones atómicas atmosféricas llevaron a una elevación rápida y considerable de la concentración de carbono-14 en la atmósfera, misma que descendió cuando se suspendieron dichas explosiones. Más de medio siglo después, el incremento de la concentración atmosférica de carbono-14 –el mismo que se utiliza para datar objetos antiguos– ha encontrado una aplicación inesperada en el campo de la neurología. De manera específica, ha resuelto la controversia relativa al crecimiento de nuevas neuronas en el cerebro humano.En el año 1998 un grupo de investigadores suecos encontró evidencia del crecimiento de nuevas neuronas en el cerebro humano adulto; de manera específica, en el hipocampo, que es una región situada en lo profundo del cerebro y que se asocia con la memoria y la orientación espacial. Antes de este descubrimiento se pensaba que no se producían nuevas neuronas en el cerebro a lo largo de la vida adulta, por lo que el descubrimiento resultó controvertido. Además, el estudio de los investigadores suecos se llevó a cabo inyectando una sustancia en el cerebro que posteriormente se determinó era tóxica, lo que impidió que fuera reproducido por otros grupos de investigación de manera independiente. De este modo, la controversia continuó. Hasta que las pruebas nucleares vinieron en auxilio de los neurólogos. En efecto, en un artículo publicado esta semana en la revista “Cell” por un grupo internacional de investigadores encabezado por Kristy Spalding del Instituto Karolinska en Suecia, se encuentra que sí se producen nuevas neuronas en el hipocampo en la vida adulta. Esto fue demostrado de una manera que se antoja simple –aunque, por supuesto y como siempre, el diablo estuvo en los detalles–: los investigadores midieron la proporción de carbono-14 en el ADN de las neuronas del hipocampo de personas fallecidas. Obtuvieron así una indicación de la edad de dichas neuronas y por lo tanto si habían o no sido producidas durante la edad adulta. Podemos entenderlo de la siguiente manera.El 99% del carbono en la atmósfera es carbono-12, mientras que, en contraste, el carbono-14 sólo está presente en cantidades diminutas. Por otro lado, la concentración de carbono-14 en una célula de nuestro cuerpo refleja la que existía en la atmósfera en el momento que fue creada, pues los animales dependemos para nuestra alimentación de las plantas que crecen tomando el carbono del aire. Si una neurona creció en 1963, cuando se observó la máxima concentración de carbono-14, ésta debe estar reflejada en su ADN. Si, por otro lado, creció 30 años antes, debe corresponder a la concentración atmosférica de 1933.Empleando este método, Spalding y colaboradores encontraron que un 35% de las neuronas de una cierta región del hipocampo eran neuronas que habían crecido en la edad adulta a un ritmo de 1,400 por día, números que les sugieren que el crecimiento de nuevas neuronas impacta la función cognitiva del cerebro. Los investigadores encontraron, además, que si bien la generación de nuevas neuronas disminuye con la edad, esta disminución es moderada.   Cuando las potencias nucleares llevaron a cabo sus pruebas de bombas atómicas en la atmósfera, no estaban pensando, por supuesto, en crear un laboratorio de investigación que medio siglo después ayudara a desentrañar uno de los misterios del cerebro humano. En la práctica, y para nuestro beneficio –una de cal por todas las que van de arena–, esto fue lo que sucedió de manera inesperada. Tan inesperada que ni George Orwell, que tan acertado fue en su predicción de la Guerra Fría, lo pudo haber anticipado.",
    "“De todo tiene la viña, Sacra y Real Majestad, de todo tiene la viña:uvas, pámpanos y agraz.”Atribuido a fray Hortensio de Paravicino, 1624Sin duda que de todo hay en la viña del Señor y, como ejemplo,la campaña lanzada el pasado 22 de abrilpor la compañía holandesa “MarsOne” para encontrar voluntarios que quieran viajar al planeta Marteha acumulado ya 78,000 interesados. Este número es sorprendente, pues si bien la perspectiva de viajar a un planeta lejano debe resultarfascinante para aquellos con espíritu aventurero, hay que considerarque el viaje sería sin retorno y al ansia de aventura habría que añadir la decisión de abandonar para siempre a la Tierra. En efecto, el proyecto emprendido por “MarsOne” es uno de colonización de Marte y con el objeto de mantener los costos de viaje en un mínimo, no se pretende que los viajeros puedan regresar a nuestro planeta.Tendrían entonces que quedarse en Marte, probablemente para siempre. “MarsOne”, tiene planeado enviar a Marte dos parejas astronautas en el año 2023. Previamente a esto, seplanea enviar varias misiones preparatorias. La primera arribará a Marte en 2016 con una carga de 2,500 kilogramos. Una segunda misión,a lanzar en 2018, hará llegar a la superficie del planeta un carro explorador que será operado a control remoto desde la Tierra. Finalmente, en 2021 serán enviadas a Marteseis misiones con todo lo necesario para desplegar la cápsulas que albergarán a los colonos, al igual que un segundo carro explorador.De acuerdo con los planes, al inicio del año 2022 se tendrán listas en Marte las instalaciones para la producción del agua y del aire que necesitarán los colonos para sobrevivir. En ese mismo año, los componentes de la nave interplanetaria serán puestos en órbita terrestre yuna vez ensambladostodo estará listo para iniciar el viaje. Al llegar las dos primeras parejas a Marte se encontrarán con todo lo necesario para instalarlas cápsulas inflables en las que habitarán. Además, algunas semanas después arribarán cinco misiones con más materiales de construcción y de producción de agua y alimentos, lo mismo que un tercer carro explorador. Ya con la colonia instalada, en junio de 2025 llegarán las siguientes dos parejas de colonos y así sucesivamente cada dos años hasta completar la colonia.Según los directivos de “MarsOne”,poner sobre la superficie de Marte los primeros colonos tendrá un costo de unos 6,000 millones de dólares.Para recaudarlosfondos necesarios, el proyectose concibe como un “reality show” televisivo al estilo de “Big Brother” pero trasmitido desde Marte.Con esto, los fondos para el proyecto serían obtenidos de la venta de derechos televisivos. Los organizadores razonan que si los pasados Juegos Olímpicos de Londres recaudaron 3,700 millones de dólares en regalías en solamente tres semanas, no hay razón para que el proyecto “MarsOne” no recaude 6,000 millones contando con mucho más tiempo.   No todo mundo, sin embargo, está de acuerdo en que “MarsOne” sea factible. Desde el punto de vista económico, por ejemplo, podríamos pensar que el costo de 6,000 millones de dólares resulta más que modesto en comparación con los 2,500 millones de dólares que fueron necesariospara poner al explorador “Curiosity”  en Marte. Ambos proyectos son de una magnitud muy diferente. El “Curiosity” tiene un peso que no llega a una tonelada, muy inferior al peso de los elementos necesarios para construir la colonia en Marte. Además, el que uno de los viajes planeados sea tripulado añade complejidad y costos sustanciales a la misión.Al margen de la controversia sobre los costos del proyecto, sin embargo, hay una dificultad técnica para llevarlo a cabo que se antoja insuperable, y que se discute en un artículo publicado esta semana en la revista “Science”. Esta dificultad tiene que ver con las radiaciones de alta energía a las que estarían expuestos los astronautas tanto durante el viaje interplanetario, como durante su vida en Marte. En la Tierra estamos protegidos de estas radiaciones por el cinturón magnético que rodea a la Tierra. Ni en el espacio ni sobre la superficie de Marte los astronautas gozarán de esta protección y estaríaen riesgo su salud. En el artículo referido se reportan los niveles de radiación interestelar medidos en el interior de la cápsula que transportó al “Curiosity” hasta las inmediaciones de Marte. Estas radiaciones podrían acercarse a los máximos niveles establecidos por la NASA durante toda la vida de un astronauta. Existe, además, el peligro de un aumento repentino de radiación de Solque podría superar este límite.Si bien los aspirantes a colonizar Marte son numerosos en esta etapa del proyecto, es posible que sean muchos menos en el momento de tomar la decisión de  abandonar la Tierra para siempre, pues si bien es cierto que de todo hay en la viña del Señor, también lo es que no hay loco que coma lumbre.  En todo caso, dadas las dificultades previsibles del proyecto, es posible que nunca nadie tenga la necesidad de tomar una decisión en este sentido.",
    "Cuando un irlandés empobrecido del siglo XIX tomaba la decisión de dejar su país para probar fortuna en los Estados Unidos, Australia u otro país lejano, estaba sin duda consciente de que con mucha probabilidad jamás regresaría a su tierra ni volvería a ver a su familia. De la misma manera, sus familiares sabían que la despedida era para siempre y en ese ánimo estaban. Buscando en internet uno aprende que en las inmediaciones del pueblo de Falcarragh en el norte de Irlanda, existe un pequeño puente de piedra llamado el “Puente de lágrimas”. Hasta ahí los familiares del emigrante solían acompañarlo en su camino hasta el puerto de Londonderry, en donde se embarcaría para iniciar su aventura en busca de una mejor vida. La familia no iba más allá del puente, de modo que ahí ocurría la despedida. Así, dadas las circunstancias, ”Puente de lágrimas” resulta sin duda un nombre acertado.La emigración irlandesa, que existía desde el inicio del siglo XIX, se aceleró en la década de los años cuarenta de dicho siglo por la plaga que destruyó los sembradíos de papa, tubérculo del que dependía enteramente un tercio de la población para su alimentación. La hambruna que siguió a la plaga provocó la muerte de un millón de irlandeses y la emigración de otro tanto. Como consecuencia, la población de la isla, que sumaba alrededor 8.5 millones en 1845, disminuyó hasta 6.5 millones en 1852. Se sabe que la papa es originaria de la región del lago Titicaca en la frontera entre Perú y Bolivia. Ahí fue descubierta por los conquistadores españoles quienes la llevaron a Europa en la segunda mitad del siglo XVI, convirtiéndose con el tiempo en una importante fuente de alimentos, particularmente en Irlanda. El causante de la hambruna de la papa fue el hongo “Phytophthora infestans” que los especialistas creen es originario del Valle de Toluca. Europa permaneció aislada de este agente patógeno por cerca de tres siglos; hasta que hizo su aparición en ese continente en 1845. Se pensaba que la hambruna de la papa fue causada por la cepa US-1 de “P. infestans”. Un artículo aparecido en línea esta semana en la revista “eLIFE”, sin embargo, desmiente esta creencia, y encuentra que la hambruna de Irlanda fue en realidad causada por una cepa –denominada HERB-1–  originada en México al inicio del siglo XIX. Esta cepa se propagó a los Estados Unidos y de ahí a Europa, para finalmente desaparecer al despuntar el siglo XX.El artículo de referencia es el resultado de un estudio genético llevado a cabo por un grupo internacional de biólogos moleculares encabezados por Kentaro Yoshida de “The Sainsbury Laboratory” en el Reino Unido, que descifró el genoma del microorganismo causante de la hambruna de la papa.  El estudio fue realizado con hojas secas de plantas de papa atacadas por el hongo, las cuales fueron obtenidas de herbarios europeos.  No es clara la causa por la que desapareció la cepa de “P. infestans”, pero, de acuerdo con Yoshida, es posible que esto haya ocurrido cuando fueron creadas las primeras variedades de papa resistentes al microorganismo al inicio del siglo XX. Si bien en el caso de la hambruna de Irlanda los hechos ocurrieron hace relativamente poco tiempo, la genética ha sido útil también para investigar hechos ocurridos en tiempos más lejanos. Este es el caso, por ejemplo, del estudio llevado a cabo por un grupo de investigadores de instituciones en Europa y los Estados Unidos –publicado recientemente en la revista “Plos Pathogens”– que identificó la bacteria “Yersinia Pestis” como la causante de la llamada plaga de Justiniano, que asoló al Imperio Romano de Oriente en los siglos VI-VIII de nuestra Era. Esta conclusión fue alcanzada después de estudiar esqueletos de víctimas de la epidemia provenientes de un cementerio de la época, los cuales mostraron la presencia de ADN de “Yersinia Pestis”. La  plaga de Justiniano, según algunos, pudo causar hasta 25 millones de muertos y fue causa del debilitamiento del Imperio Romano. Siglos después, la misma bacteria causó en la Europa Medieval la epidemia conocida como la “Muerte negra”, que resultó también en decenas de millones de muertos.   Las investigaciones que desentrañaron el origen de la hambruna de Irlanda y de la plaga de Justiniano son ejemplos de cómo la genética moderna –empleando métodos que hasta hace muy poco tiempo se hubieran antojado mágicos– nos puede ayudar a desentrañar sucesos traumáticos del pasado. Conocer las causas que provocaron estos sucesos es, por supuesto, esencial para prevenirlos en el futuro. Algo que los Irlandeses del siglo XIX o los bizantinos de los siglos VI-VIII hubieran, sin duda, tenido en gran aprecio.",
    "Para aquellos estudiantes que tengan problemas con su curso de matemáticas y estén preocupados por su próximo examen, un grupo de investigadores de la Universidad de Oxford en Inglaterra les ofrece una solución: la estimulación de una región particular de su cerebro por medio de ligeros choques eléctricos. Con este tratamiento lograrán incrementar su capacidad para la realización de cálculos matemáticos y obtendrán una mejor calificación en el examen. Esto al menos de acuerdo con los resultados de un estudio llevado a cabo por dicho grupo de investigación, encabezado por Albert Snowball, y publicado esta semana en la revista “Current Biology”.La relación entre la electricidad y la materia viva fue descubierta a finales de siglo XVIII por el físico y médico Luigi Galvani al tocar accidentalmente con un bisturí una pata de rana que colgaba de un gancho y observar que ésta se contraía. Galvani postuló de este experimento que los músculos de la pata se accionaban por el efecto de la “electricidad animal” que era intrínseca a la materia viva, y que era conducida por los nervios hacia los músculos. Si bien no todo mundo estuvo de acuerdo con esta interpretación  –entre ellos Alessandro Volta, inventor de la pila voltaica que hoy tanto usamos– la electricidad quedó para muchos como la fuerza generadora de la vida.Esto parece ser lo que Mary Shelley tenía en mente cuando publicó en 1818 la novela Frankenstein, la cual versa sobre Víctor Frankenstein y sus desventuras. Este personaje, cuando adolescente, vivió obsesionado con los textos alquimistas y con la búsqueda del “elixir de la vida”. A pesar de su entusiasmo inicial, estos textos pronto empezaron a desilusionarlo y los abandonó de manera definitiva cuando ingresó a la Universidad de Ingolstadt. No sucedió lo mismo, sin embargo, con su obsesión por la búsqueda de la fuente de la vida. Así, con los conocimientos adquiridos y sus propias investigaciones, creó en Ingolstadt un monstruo de dos metros y medio de alto uniendo pedazos de cadáveres. Shelley no detalla el procedimiento que Frankenstein siguió para dar vida a su creación, pero se piensa que involucró el uso de la electricidad, pues en una escena de la novela el protagonista queda muy impresionado cuando fue testigo de un rayo que cayó sobre un roble destruyéndolo por completo. A partir de ese momento se interesó en los fenómenos eléctricos y decidió abandonar los textos alquimistas. Si bien el concepto de la electricidad como la fuerza generadora de vida con el tiempo cayó en descrédito, los estudios sobre los efectos que la electricidad tiene sobre los seres vivos tomaron fuerza. A esto mucho contribuyó Giovanni Aldini, nieto de Luigi Galvani y fogoso defensor de sus ideas. Aldani llevó a cabo estudios sobre la estimulación muscular por medio de corrientes eléctricas. Llevó igualmente a cabo demostraciones públicas que incluyeron el uso de la electricidad para provocar el movimiento de cadáveres de criminales ejecutados que recibieron mucha publicidad. En estas condiciones, no es difícil entender que Mary Shelley hubiera concebido a la electricidad como un medio de animar a su monstruo. Además de sus experimentos con criminales, Aldini se interesó en las aplicaciones terapéuticas de la electricidad y la usó para curar desórdenes mentales. Estos estudios originaron las terapias de choques eléctricos empleadas en la actualidad para el mismo propósito–y que no dejan de ser controvertidas, entre otras cosas, por sus efectos secundarios.La técnica empleada en los estudios reportados por Snowball y colaboradores en “Current Biology” es llamada “Estimulación inter-craneal por ruido aleatorio” y tiene también su origen –en último término– en los estudios de Aldini. Las corrientes eléctricas empleadas son, sin embargo, muy pequeñas, de tal manera que la técnica no es traumática en absoluto.  Los estudios, que duraron cinco días, fueron llevados a cabo con 25 estudiantes de la Universidad de Oxford, los cuales fueron divididos en dos grupos. A un grupo de le aplicó la técnica de estimulación inter-craneal, mientras que al otro solamente se le simuló su aplicación. Al final, se les plantearon pruebas aritméticas de dos clases: pruebas de memorización de operaciones aritméticas tales como 4x8=32 y pruebas que demandan resolver una serie de pasos como 32-17+5=20. Se evaluó el número de respuestas correctas y  velocidad con que se alcanzaron.El estudio encontró que los estudiantes que habían recibido una estimulación cerebral obtuvieron mejores marcas en ambos tipos de pruebas. Además, en pruebas realizadas seis meses después, sin una estimulación adicional, encontraron que aquellos estudiantes que habían recibido el estímulo superaban al otro grupo en la prueba que involucraba operaciones de varios pasos. En la prueba de memorización, en contraste, no se observó una diferencia.De acuerdo con Roi Cohen Kadosh, uno de los autores del artículo de referencia, citado por la revista “Scientific American”, “si se demuestra que la estimulación inter-craneal es segura y resulta efectiva para grupos más grandes de alumnos, la técnica podría modificar las formas tradicionales de estudio. Algunos dirán que aquellos que son malos en matemáticas seguirán igual. Este pudiera no ser el caso”.El tiempo nos dará la respuesta. Lo que sí es seguro es que aun empleando técnicas sofisticadas de aprendizaje, los estudiantes dependerán en gran medida de su esfuerzo personal para obtener mejores calificaciones en matemáticas.",
    "Los niveles de dióxido de carbono (CO2) en la atmósfera alcanzaron el pasado 9 de mayo una cifra récord. En efecto, de acuerdo con la “National Oceanic and Atmospheric Administration” (NOAA) de los Estados Unidos, en esa fecha el observatorio de Mauna Loa, Hawai, midió por primera vez una concentración promedio de CO2 de 400 partes por millón. Esto en principio no es sorprendente, pues la concentración de CO2 en la atmósfera ha cambiado a lo largo del tiempo de manera natural, algunas veces subiendo y otras bajando. La cifra medida el 9 de mayo, no obstante, es de llamar la atención, pues una concentración tan alta no se había producido en los últimos dos millones de años. El observatorio de Mauna Loa ha llevado récord de la concentración de CO2 en la atmósfera de manera continua desde el año 1958. Este récord puede ser consultado en la página web de la NOAA y muestra que la concentración de CO2 medida en Mauna Loa –al igual que en otros lugares del mundo– presenta pequeñas oscilaciones anuales, con mínimos en mayo y máximos en octubre. Los mínimos son debidos a que durante la primavera boreal las plantas remueven el CO2 de la atmósfera para usarlo en la fotosíntesis. Los máximos, a su vez, son el resultado de que las plantas regresan CO2 a la atmósfera en los meses con menos luz solar. Esto fue descubierto por David Keeling en la década de los años cincuenta, cuando trabajaba para la Institución Scripps de Oceanografía de La Jolla, California. La observación por vez primera de la interacción de las plantas con la atmósfera a nivel global hubiera sido suficiente para dar a Keeling notoriedad científica. Descubrió, no obstante, algo más y de mucha mayor transcendencia: encontró que las oscilaciones estacionales de la concentración de CO2 en la atmósfera están montadas en una curva ascendente que indica que dicha concentración está aumentando de manera paulatina. Las mediciones que inició Keeling en 1958 se han continuado de manera ininterrumpida hasta la época actual y confirman claramente esta tendencia, misma que, además, se está acusando con los años. En 1958 Keeling midió una concentración de CO2 de 315 partes por millón. Hoy ha rebasado la barrera de los 400 puntos, lo que representa un incremento de 27 por ciento. Con respecto a los niveles preindustriales en el siglo XIX, el incremento ha sido de 43 por ciento.Podría quizá pensarse que, si bien estos porcentajes son significativos, no tendrían en realidad demasiada importancia pues –aun hoy en día– la concentración de CO2 en la atmósfera es muy pequeña –400 partes por millón representa una concentración de apenas 0.04 por ciento–. Lejos de esto, entre los científicos existe el consenso que aun esta diminutas cantidades de CO2 están generando el calentamiento global que amenaza con un cambio climático global de grandes consecuencias.   En relación a este último punto, un grupo internacional de científicos encabezados por Julia Birgham-Grette de la Universidad de Massachusetts llevó a cabo en el año 2009 una expedición al lago El´gygytgyn, en el noreste de Siberia a 100 kilómetros al norte del círculo polar ártico, con el propósito de investigar el clima ártico desde el tiempo de formación de dicho lago. Esto último ocurrió hace más de tres y medio millones de años por el impacto de un meteorito. Para este propósito, los investigadores extrajeron muestras del fondo del lago a varias profundidades, correspondientes a diferentes épocas geológicas.Los resultados de la investigación fueron publicados esta semana en la revista “Science”. Brigham-Grette y colaboradores encontraron que hace 3.5 millones de años los veranos en la región de lago El´gygytgyn eran unos 8 grados más cálidos que en la actualidad. Encontraron, además, que en esa época el ártico estaba libre de hielos y los bosques boreales se extendían hasta las orillas del Océano Ártico. Todo esto con una concentración estimada de CO2 en la atmósfera de unas 400 partes por millón, similar a la que se midió el pasado 9 de mayo.Aunque con periodos de enfriamiento, las temperaturas en el ártico permanecieron más elevadas que las actuales hasta hace unos 2.2 millones de años. A partir de este punto el mundo se empezó a enfriar y a derivar hacia la edad del hielo, y eventualmente a la época actual más cálida. Según Brigham-Grette y colaboradores, las evidencias muestran que el clima ártico mostró una gran sensibilidad a la concentración de CO2, más grande que la que se asume en los modelos actuales para predecir la evolución del clima por efecto de los gases de invernadero; predicciones que sabemos mantienen preocupados a los especialistas. De estar Brigham-Grette y colaboradores en lo cierto, el mundo enfrentará en el futuro grandes cambios climáticos por la emisión descontrolada de gases de invernadero a la atmósfera. Aunque ciertamente ya no nos tocaría ser testigos de los bosques de coníferas que se extenderían hasta orillas del Océano Ártico. Ni de las ciudades que ahí pudieran construirse. Ni tampoco de los turistas que hasta ahí llegarían en plan de veraneo.",
    "Como lo relató el periódico chileno “La Estrella de Arica” en su edición del 19 de octubre de 2003, cuando Óscar Muñoz –quien tenía como pasatiempo la búsqueda de objetos de valor histórico en la región salitrera del desierto de Atacama– se encontraba excavando en una iglesia en el poblado de La Noria, dio con un cuerpo momificado aparentemente humano, envuelto en una tela blanca y atado con una cinta morada. El hallazgo no hubiera sido especialmente notable si no fuera porque el cuerpo media solamente unos 15 centímetros, tenía la cabeza desproporcionadamente grande, y contaba solamente con 20 costillas en lugar de 24 como es lo normal. Con estas características no debe de sorprendernos que los entusiastas de los OVNIS hayan considerado que se trataba de los restos de un visitante extraterrestre. Un estudio del ADN del cuerpo llevado a cabo por la Universidad Stanford en los Estados Unidos y dado a conocer en días pasados, sin embargo, demostró de manera concluyente –para decepción de los creyentes en los extraterrestres– que se trataba de restos humanos. Se determinó también que posiblemente el sujeto había muerto hace algunas décadas, a una edad entre los seis y los ocho años, y que su madre fue una indígena del norte de Chile. Por otro lado, la razón por la que al morir solamente medía 15 centímetros queda por el momento sin una explicación firme, lo mismo que otros rasgos anatómicos.Sin bien el hallazgo de La Noria ocurrió hace casi una década, en las últimas semanas se le dio difusión en los medios de los Estados Unidos con motivo de la promoción del documental “Sirius”, que trata de la presencia de extraterrestres en nuestro planeta. “Sirius” es parte del llamado “Proyecto revelación” impulsado por Steven Greer, que tiene como objetivo desenmascarar los supuestos esfuerzos del gobierno estadounidense por ocultar la presencia de extraterrestres en la Tierra. Greer es un médico de profesión que después de trabajar por un tiempo como tal se convirtió en experto en temas extraterrestres.      Según Greer, el gobierno estadounidense tiene interés en ocultar la visita de extraterrestres a nuestro planeta por el impacto que tendría el mundo en caso de que se dieran a conocer los secretos de la energía empleada por los extraterrestres para propulsar sus naves a lo largo de distancias interestelares. Con esta energía, supuestamente limpia e infinitamente superior a todo lo que conocemos, prescindiríamos de los combustibles fósiles y cambiaríamos el orden económico mundial. Aseguraríamos el suministro de energía para la industria y el transporte, al mismo tiempo que resolveríamos el problema de contaminación ambiental que amenaza con el colapso del planeta. Resolveríamos, en resumen, los problemas de pobreza, escasez de recursos naturales y desigualdad en el mundo.En la promoción del documental “Sirius”, estrenado el pasado de 22 abril, se manipuló a la momia encontrada en La Noria, presentándola como una prueba de la presencia de extraterrestres en la Tierra. En el documental mismo, no obstante, se incluyeron los resultados de la investigación realizada en la Universidad Stanford que demuestran su origen terrestre.En otra acción relacionada, la organización “Paradigm Research Group”, llevó a cabo esta semana una reunión en Washington, D.C., llamada “Audiencia Ciudadana sobre Revelación”, con el propósito de discutir sobre la presencia de extraterrestres en nuestro planeta y de convencer al Congreso de los Estados Unidos de tomar el tema en serio. Con el objeto de dar credibilidad a la reunión, sus organizadores invitaron a seis ex congresistas, los cuales fueron remunerados con 20,000 dólares cada uno por su participación.   De acuerdo con “The New York Times”, la motivación de los ex congresistas para participar en la reunión –aparte del pago de 20,000 dólares– podría haber sido la curiosidad. Según Carolyn Kilpatrick, ex representante por Michigan, “Nuestro País ha trivializado el tema, lo ha convertido en una broma. Ahora encuentro que es mucho más que eso. Y no es una broma. Y hay datos científicos que muestran que puede haber algo por ahí”.El hecho es que, dado el enorme número de estrellas en nuestra galaxia, se acepta que es muy probable que existan mundos habitados, máxime ahora que sabemos de la existencia de numerosos  planetas orbitando estrellas. Al respecto, en el número de esta semana de la revista “Science” se incluye una sección especial sobre exoplanetas –planetas fuera del sistema solar– en donde se asienta que a la fecha se han detectado 900 de éstos y que varios miles más están bajo investigación.       Cualquier planeta habitado, sin embargo, estará a una distancia fenomenal, de modo que es extremadamente pequeña la probabilidad de que sus habitantes nos hayan, en primer término, escogido para realizar una visita y, en segundo término, que hayan tenido los medios y las ganas para realizar un viaje tan largo con propósitos poco claros.A pesar de esto, podría resultar incomprensible que haya grupos en los Estados Unidos impulsando ideas que se antojan descabelladas. No lo es, sin embargo, cuando tomamos en cuenta que, de acuerdo a una encuesta reciente, un tercio de los norteamericanos cree en la presencia de extraterrestres en nuestro planeta, otro 50 por ciento no está seguro, y sólo el 17 por ciento piensa que es una patraña. Además, 79 por ciento cree que el gobierno mantiene secretos sobre este tema. Con estos números, el asunto tiene un interés político evidente.",
    "El objeto más complejo del Universo conocido es sin duda el cerebro humano. Este órgano contiene cerca de 100,000 millones de neuronas que están en comunicación mutua a través de mensajeros químicos o mediante señales eléctricas. Cada neurona se comunica con cientos o miles de otras neuronas, estableciendo así una red de comunicaciones extraordinariamente compleja. Dados estos números, no es difícil entender que nuestro cerebro haya sido concebido como la estructura más compleja de la que tengamos noticia.  El cerebro humano ha sido caracterizado también como la máquina más compleja del Universo, implicando así que el órgano que nos hace superiores a otras especies en la Tierra constituye una máquina –tal como lo son un motor de automóvil o una computadora–, sólo que de una naturaleza especial e increíblemente más complicada. Las funciones mentales tales como la memoria, el aprendizaje, el razonamiento y la toma de decisiones, lo mismo que la autoconciencia, serían de este modo el resultado de procesos físicos y químicos que ocurren dentro de nuestro cerebro. De este modo, para su operación, el cerebro no dependería de elementos extraños al mundo físico.Si bien no hay un acuerdo generalizado sobre este tema –que ha sido motivo de especulaciones filosóficas a lo largo de más de dos milenios–, hoy se acepta que el cerebro es un objeto susceptible de ser estudiado utilizando métodos científicos y mucho se ha aprendido con este enfoque acerca de su funcionamiento. Por ejemplo, mediante experimentos de resonancia magnética, hoy sabemos en qué parte del cerebro se localiza una determinada función cerebral. Conocemos igualmente la estructura de las neuronas, que son componentes básicas del cerebro, y sabemos que éstas forman una red de comunicaciones increíblemente intrincada. Poco conocemos, sin embargo, acerca de los detalles de la actividad cerebral; que es, por otro lado, motivo de una intensa actividad de investigación hoy en día.Con relación a esto último, el matemático Valentín Afraimovich, investigador del Instituto de Investigación en Comunicación Óptica de la UASLP, está desarrollando, en colaboración del neurofisiólogo Mikhail Ravinovich de la Universidad de California en San Diego, modelos matemáticos para explicar las funciones cognitivas del cerebro humano. El sujeto de estudio en este caso no es el cerebro en sí, sino la actividad cognitiva que sustenta, lo que comúnmente conocemos como “mente”. Como explica Afraimovich, “hace 60 años el matemático francés Jaques Hadmard se preguntaba si alguna vez los matemáticos podrían llegar a saber lo suficiente acerca de la fisiología del cerebro, lo mismo que los neurofisiólogos acerca de los avances en matemáticas, para que pudiera darse una colaboración eficiente entre ellos.” Afraimovich considera que esto es ya una realidad, y que neurofisiólogos y matemáticos están hoy en día colaborando para descubrir los secretos de nuestro cerebro, como lo demuestra el proyecto en el que participa.De acuerdo con Afraimovich, las funciones cognitivas se explican en función de la complejidad del cerebro humano y no en función de sus componentes: “la principal pregunta ya no es, cuáles son los constituyentes del cerebro –neuronas, sinapsis, redes neuronales, etc. –, sino cómo el cerebro hace uso de estos elementos para generar, en sucesión, diferentes funciones cognitivas”. El estudio matemático de un sistema con una complejidad extrema como el cerebro humano, presenta enormes dificultades. No obstante, Afraimovich y colaboradores han logrado desarrollar modelos matemáticos que reproducen funciones cognitivas tales como la memoria y la toma de decisiones, que han atraído la atención de muchos especialistas. Las investigaciones, por otro lado, no han llegado a su fin y según Afraimovich, “están a la espera de nuevos logros en este fascinante campo científico”.Poner en claro como el cerebro realiza sus funciones cognitivas será un avance de gran trascendencia científica. Tendrá con seguridad también enormes impactos tecnológicos y sociales. Ni que decir de la trascendencia que tendría en la filosofía al poner punto final a un debate de dos milenios sobre la naturaleza de la mente humana.",
    "En su autobiografía, el filósofo y matemático británico Bertrand Russell relata que cuando niño, en la segunda mitad del siglo XIX, en su casa tenía terminantemente prohibido comer fruta, pues se consideraba ésta era mala para la salud infantil. Contrario a esto, en la actualidad sabemos que la fruta es indispensable para una buena alimentación. A pesar de la prohibición, Russell fue un niño muy sano, excepto por un ataque benigno de sarampión que sufrió a la edad de once años. Fue sano, quizá debido a las manzanas que robaba y comía a escondidas, según él mismo relata.Hoy en día, si bien hemos superado la percepción de que las frutas son perjudiciales para la salud de los niños, nos encontramos ante un problema de salud pública muy serio asociado al consumo en exceso de otro tipo de alimentos: la epidemia global de obesidad.  En efecto,  según la Organización Mundial de la Salud (OMS), en el año 2008 más de 1400 millones de adultos mayores de 20 años tenían sobrepeso y de éstos, 200 millones de hombres y 300 millones de mujeres –más del 10 % de la población adulta– eran obesos. La OMS define el grado de sobrepeso en función del parámetro llamado Índice de masa corporal, el cual se obtiene dividiendo el peso de una persona entre el cuadrado de su estatura. Para la OMS, una persona tiene un sobrepeso cuando este índice sobrepasa 25, mientras que para valores mayores de 30 la persona se clasifica como obesa.  Los Estados Unidos es uno de los países en el mundo en donde la epidemia de obesidad es más severa. Según estadísticas de la OMS, en el año 2002 un 35% de los estadounidenses mayores de 15 años sufría de obesidad, mientras que un total de 70 % tenía sobrepeso. Estos números se elevaron a 45 % y 80 % en 2010. Las cifras, además de ser alarmantes, están creciendo rápidamente.En este respecto, México no se queda atrás y mientras que en el año 2002 un 25 % de todos los mexicanos mayores de 15 años eran obesos, este número se elevó hasta un 35 % en 2010. En este último año, un 73 % de nuestros compatriotas mayores de 15 años estaban considerados con sobrepeso por la OMS. La epidemia de obesidad global es debida al consumo excesivo de alimentos ricos en grasas y carbohidratos, así como a la falta de ejercicio físico.  Afecta a numerosos países, no solamente industrializados, sino también a países en desarrollo. Afecta igualmente tanto a niños como a  adultos. En un artículo publicado recientemente en la revista “Food Quality and Preference” por un grupo de investigadores de varios países de Europa, se reportan resultados de un trabajo llevado a cabo con el fin de averiguar qué es lo que determina la preferencia de los niños por ciertos sabores y alimentos, y obtener así información que facilite el combate contra la obesidad.La investigación se llevó a cabo con 1,700 niños, entre seis y nueve años, de ocho países europeos –Italia, Estonia, Chipre, Bélgica, Suecia, Alemania, Hungría y España–. Durante el experimento se pidió a los niños que comieran dos clases de galletas: simples en un caso, y con grasa, sal o saborizante añadido –glutamato monosódico– en el otro. Se les proporcionó igualmente jugo sin azúcar y con azúcar añadida. Después de probar los alimentos, a los niños se les preguntó cuál preferían, aquellos simples sin un sabor adicional, o bien aquellos a los que se les añadieron sustancias para modificar su sabor. En lo que se refiere a las galletas una mayoría de niños mostraron preferencia por aquellas a las que se les agregó grasa o sal. Lo mismo sucedió con el jugo al que se le añadió azúcar. En contraste, solamente un 34 % de los niños prefirieron la galleta con saborizante.Hubo, sin embargo, diferencias entre países. Así, mientras que un 85 % de los niños en Estonia les gustaron más las galletas con sal, solamente un 50 % de los niños chipriotas mostraron la misma preferencia. De la misma manera, más del 70 % de los niños alemanes prefirieron las galletas con grasa, en contraste con un correspondiente 35 % de los niños en Chipre. Resultados similares se obtuvieron con relación al azúcar. Se observó igualmente que el gusto por el azúcar y la sal se incrementa con los años, hasta alcanzar un máximo y disminuir al llegar a la adolescencia.Según los autores del estudio, sus resultados muestran que la cultura de cada  país define en cierta medida los gustos de los niños por los sabores. Muestra igualmente que estos gustos no son inalterables sino que cambian a lo largo de la niñez. Dichos resultados tienen valor para el desarrollo de estrategias, particulares para cada país, con el fin de atacar la epidemia de obesidad que amenaza al mundo.   En la segunda mitad del siglo XIX, cuando a Bertrand Russell le toco vivir su infancia, a los niños –al menos a los de la clase acomodada a la que pertenecía– no les era permitido consumir frutas con la consecuente falta de nutrientes necesarios para el funcionamiento del cuerpo. Más de un siglo después, existe, en contraste, una gran permisividad para que los niños coman todos los alimentos que les apetezcan, con el desastroso resultado del que somos testigos. Vale aquí quizá la sabiduría popular: ni tanto que queme al santo, ni tanto que no lo alumbre.",
    "La automedicación, entendida ésta en un sentido amplio, es ciertamente una práctica muy extendida. Así, ante la presencia de una molestia física menor  –de naturaleza estomacal, de las vías respiratorias, o producto de una caída, por mencionar sólo algunas– muchos de nosotros acudiremos sin dudar a la farmacia más cercana para adquirir las medicinas que consideramos nos proporcionarán alivio. Habrá quien, por supuesto, se incline por productos de la medicina tradicional y prefiera productos naturales, también auto-medicados. O bien, quizá decida hacer uso de recetas preparadas con ingredientes que no son necesariamente medicinales, pero cuya combinación, a su criterio, le proporcionarán el alivio buscado.  En algunos casos extremos, la auto-medicación se practica incluso es situaciones más serias, que afortunadamente son las menos. Esto incluye el uso de antibióticos sin prescripción médica, práctica que hasta hace unos pocos años era posible en nuestro país. El que exista una práctica de auto-medicación no es de ninguna manera sorprendente. De hecho, la compartimos con otras especies. Se sabe, por ejemplo, que los chimpancés del África ecuatorial, cuando sufren molestias estomacales por la presencia de parásitos intestinales, consumen hojas de una planta que se sabe es tóxica para dichos parásitos. Estos animales ingieren también hojas enteras las cuales atraviesan sin digerirse por todo el tracto gastrointestinal. La función de estas hojas es arrastrar y expulsar a los parásitos alojados en el intestino.  Es posible incluso que hayamos aprendido de los animales a usar ciertos productos naturales para estimularnos y para curar enfermedades. En este sentido, de acuerdo con Michael Huffman de la Universidad de Kyoto, las raíces de una planta que se emplean en la India como una cura para los parásitos intestinales, son también consumidas por jabalíes salvajes en ese país. Igualmente, se especula que el uso que le dan nativos de Gabón a una planta estimulante del sistema nervioso fue aprendido después de observar los efectos que dicha planta produce en gorilas y puerco espines.Según la leyenda, el café fue descubierto después de que Kaldi, un pastor de cabras en Etiopía en el siglo IX, observó que sus animales se excitaban y adquirían una gran energía después de comer los frutos rojos de un arbusto. Intrigado, Kaldi  probó dichos frutos y se sintió igualmente excitado. Para compartir su hallazgo, se llenó las bolsas de frutos rojos y los llevó a un monasterio cercano. Ahí, los monjes los cocieron, resultando en un líquido amargo y desagradable que desecharon arrojándolo al fuego. Con tan buena suerte, sin embargo, que al tostarse las semillas despidieron el aroma característico del café, que fue así descubierto. Si bien la historia de Kaldi y el café es probablemente sólo una leyenda, es representativa de las muchas historias –esas sí reales– acerca de lo que nos han enseñado los animales con respecto a las propiedades curativas y estimulantes de las plantas, tema acerca del cual parecen tener un amplio conocimiento. En efecto, como se discute en un artículo aparecido esta semana en la revista “Science”, publicado por un grupo de investigadores encabezado por Jacobus C. de Roode de la Universidad Emory en los Estados Unidos, la práctica de la automedicación es común entre los animales. Y no solamente con fines terapéuticos, para curar enfermedades, sino profilácticos, para evitarlas en caso de que exista un peligro de infección. La automedicación se observa, además, no sólo entre los animales superiores con un cerebro suficientemente complejo para aprender por imitación, sino también entre los insectos, lo que, por supuesto, constituye una característica innata. Entre los insectos la práctica de auto-medicación se puede dar de varios modos. Puede ser individual y constituir una medida de autoprotección. En otros casos puede tener un propósito transgeneracional y estar dirigida a proteger a la descendencia. Este es el caso de las moscas de la fruta que depositan sus huevos sobre comida con un gran contenido de alcohol con el objeto de prevenir su infección por avispas parasitoides. De acuerdo con de Roode y colaboradores, el estudio de las formas de automedicación de los animales, resultado de millones de años de evolución, puede ayudar a descubrir sustancias para curar nuestras infecciones, sustancias que tendrán un origen natural. Jacobus de Roode y colaboradores ponen además en perspectiva la importancia de la automedicación entre los animales para la producción de alimentos. En relación a esto, señalan que es importante que no se interfiera con los mecanismos de automedicación con el fin de evitar desequilibrios en las poblaciones de parásitos que pudieran tener efectos negativos en la agricultura. La automedicación pareciera ser entonces una práctica más común de lo que se pudiera haber pensado, y la practica no solamente la especie humana sino un gran número de animales. Incluso algunos con una posición baja en la escala evolutiva y con cerebros primitivos, incapaces de tomar decisiones de manera consciente.Esto no nos libera, no obstante, de la responsabilidad de cuidar nuestras propias prácticas de automedicación –éstas sí llevadas a cabo de manera bien consciente–, a fin de evitar la proliferación de gérmenes  resistentes a los tratamientos médicos a nuestro alcance. En particular, de microbios resistentes a los antibióticos, cuyo desarrollo ha sido fomentado por el abuso de estas sustancias, según afirman los expertos.  .",
    "Si el novelista norteamericano Edgar Allan Poe (1809-1849) se hubiera tardado un siglo en nacer, probablemente habría escrito su cuento corto “Los asesinatos de la calle Morgue” de manera diferente –asumiendo, por supuesto, que efectivamente se hubiera dado a la tarea de escribirlo–. Como sabemos, este cuento gira en torno al  misterioso asesinato de dos mujeres –madre e hija– en una habitación del cuarto piso de una casa de la calle Morgue en Paris. Los asesinatos fueron llevados a cabo con una violencia tal que el cuerpo de la hija fue encontrado metido boca abajo en el tiro de la chimenea.  Todo el asunto constituía un misterio, pues a pesar de los numerosos testigos que se reunieron en la entrada de la casa al escuchar los gritos de las víctimas y subieron rápidamente las escaleras hasta el cuarto piso, una vez que la puerta fue forzada, el asesino desapareció sin ser visto. Esto a pesar de que, en apariencia, el atacante tendría que haber bajado por las escaleras con lo que se habría necesariamente cruzado con aquellos que subían.El misterio fue resuelto por C. Auguste Dupin empleando métodos de investigación similares a los que caracterizaron a Sherlock Holmes años más tarde.  Resultó que los asesinatos fueron cometidos por un orangután llevado a Paris desde Borneo por un marinero y del cual había escapado. Dado que estos animales están adaptados para vivir la mayor parte de tiempo en los árboles, el orangután no tuvo problema en escalar por la cadena de un pararrayos hasta la altura de la ventana de la habitación en el cuarto piso y desde ahí saltar al interior de la misma. El escape después de cometer los asesinatos lo realizó por la misma vía, sin que los vecinos se percataran.“Los asesinatos de la calle Morgue” fue publicada por Poe en 1841, medio siglo antes de que Juan Vucetich en Argentina usara por primera vez las huellas digitales –que son únicas para cada persona– para resolver un caso criminal. Dado que el orangután de Poe estranguló a una de las víctimas, sus huellas digitales debieron haber quedado impresas en el cuello de la mujer. De un examen de dichas huellas habría sido claro que el asesino no era humano. Así, las capacidades de análisis de Dupin hubieran sido menos críticas para resolver el caso.Si en vez de un siglo Poe se hubiera demorado 150 años en nacer, se habría enterado que cada individuo puede ser también identificado por su código genético, que igualmente es único. De este modo, aun si la información que proporcionaron las huellas digitales del orangután no hubiera sido suficientemente precisa, a partir de un análisis del ADN de los mechones de pelo que una de las víctimas le arrancó al agresor en el forcejeo, hubiera quedado fuera de toda duda la naturaleza no humana del mismo. Supongamos ahora un retraso aun mayor en el nacimiento de Poe, que habría ocurrido hace apenas unos pocos años. Asumamos, además, que “Los asesinatos de la calle Morgue” serán publicados en, digamos, el año 2030 y que Poe hará uso de las técnicas forenses disponibles dentro de dos décadas. ¿Cuáles serán estas técnicas? Por supuesto, esto no lo podemos saber con certeza, pero un artículo publicado esta semana en la revista Plos One por un grupo de investigadores encabezado por Renato Zenobi de la Escuela Técnica Federal de Zurich, nos da una pista –sin bien no muy firme.  Según Zenobi y colaboradores, además del genoma y de las huellas digitales, un individuo puede ser identificado por los compuestos químicos contenidos en el aire que exhala. Esta conclusión fue alcanzada mediante un experimento en el que se analizó químicamente el aire exhalado por un grupo de voluntarios a lo largo de nueve días. Los investigadores encontraron que la composición química del aliento era particular a cada individuo. Para ser precisos, la composición del aire exhalado cambia durante el día; para cada persona, no obstante, se encuentra un conjunto de compuestos químicos que es siempre el mismo y que constituye su huella de aliento.El que el aliento constituya una marca que permita identificar a una persona –al igual que el genoma y las huellas digitales– es algo que tendrá que probarse y en esta dirección el artículo de Zenobi y colaboradores nos da sólo una primera indicación. De hecho, estos investigadores no mencionan a la identificación de personas como una posible aplicación de su descubrimiento. Al respecto, su interés se centra en el desarrollo de una técnica de diagnóstico médico, complementaria a los análisis de sangre y orina que se practican en la actualidad. Es decir, si la huella de aliento de una persona sana varía en un determinado momento, este hecho puede ser indicativo de que algo anda mal.Tendrán que venir más investigaciones para demostrar si en verdad el aliento de una persona es único. Esto, no obstante, no nos impide especular sobre cómo un hipotético Edgar Allan Poe habría escrito “Los asesinatos de la calle Morgue” en el año 2030. ¿Habría sido descubierto el orangután asesino por el fétido aliento con el que impregnó el aire de la habitación testigo de sus crímenes?",
    "Si tuviera 50 dólares de sobra y no supiera que hacer con ellos, una posibilidad es que los invierta en la adquisición de un “círculo de hadas” de Namibia. Esta cantidad no es excesiva y habrá quién esté dispuesto a considerar la compra; a reserva, por supuesto, de averiguar más detalles acerca de la misma.  Hay que aclarar en primer lugar que los 50 dólares no le dan derecho a llevarse el círculo a su casa, sino solamente a adoptarlo. De hecho, no sería práctico transportarlo desde Namibia, país situado en la costa occidental de África, pues estos círculos tienen un diámetro de entre 2 y 15 metros –además de un peso considerable.   Si bien esto puede resultar decepcionante, hay que considerar que sus 50 dólares se canalizarán al fondo de conservación de la reserva natural Namibrand en Namibia. La inversión no tendría entonces una motivación materialista, sino que en buena medida constituiría una acción altruista destinada a  la conservación del medio ambiente. Recibiría, eso sí, las coordenadas GPS de su círculo para que le pueda echar un ojo con Google Earth.Los círculos de hadas o círculos encantados pueden ser observados a lo largo de una estrecha franja de 2,000 kilómetros en el desierto de Namibia que se extiende, desde Angola hasta Sudáfrica. El área central de dichos círculos, de forma aproximadamente circular, esta desprovista de vegetación y muy comúnmente está delimitada por una franja de hierba alta. Los círculos de Namibia, -con una densidad entre 11 y 47 círculos por hectárea- se destacan claramente sobre la superficie del desierto que cubierta por hierba más baja que la de la periferia del círculo. Desde el aire la impresión que se obtiene es la de una superficie salpicada de cráteres.El origen de los círculos encantados ha constituido un misterio. Para los habitantes del lugar son obra de los dioses. Para algunos aficionados a las explicaciones esotéricas es irresistible asociarlos a visitantes extraterrestres. De acuerdo a los especialistas científicos, sin embargo, los círculos encantados deben tener una explicación más de este mundo. Las investigaciones científicas sobre el origen de los círculos de hadas de Namibia se iniciaron hace cuatro décadas y al respecto se han aventurado muchas hipótesis. En un caso se han atribuido a emanaciones de gases venenosos de origen natural que impiden el crecimiento de hierba en el centro de los círculos. En otro caso se han achacado a sustancias radiactivas que estarían presentes de manera igualmente natural. En otro caso más, se le ha asociado a colonias de termitas u hormigas que se alimentan de las raíces de las plantas. Ninguna de esta hipótesis, sin embargo, ha podido ser comprobada de manera convincente y el origen de los círculos encantados ha constituido un misterio. Uno de los investigadores que se ha empeñado en develarlo es el biólogo Walter Tschinkel de la Universidad Estatal de Florida en Tallahassee. Comparando fotografías de satélite tomadas a lo largo de cuatro años, Tschinkel descubrió que los círculos de Namibia no permanecen estáticos sino que están “vivos”, en el sentido que nacen, se desarrollan y con el tiempo desaparecen. Tschinkel determinó que el tiempo promedio entre la aparición y la desaparición de un círculo es alrededor de 50 años. No pudo encontrar, sin embargo, una explicación satisfactoria para el origen de los círculos encantados, que siguió envuelto en el misterio.Norbert Juergens de la Universidad de Hamburgo, en un  artículo publicado esta semana en la revista “Science”, asegura haber finalmente develado el misterio. Según Juergens, las culpables son las termitas que devoran las raíces de las plantas en el centro del círculo y les impiden crecer. Como se sabe, las termitas necesitan de humedad para sobrevivir. Los círculos de hadas se localizan en una región con una muy baja precipitación pluvial en el que las plantas absorben la escasa lluvia y le impiden llegar hasta los termiteros. Esto no sucede, sin embargo, en las regiones libres de vegetación de los círculos encantados. Ahí la lluvia permea la tierra y es retenida bajo la superficie proporcionando humedad a las termitas.Por otro lado, por la humedad relativamente alta en el centro del círculo, la hierba en la periferia encuentra condiciones favorables para crecer y alcanza una altura mayor que aquella fuera del círculo. Además, en épocas de sequía, las termitas se alimentan con las raíces de la hierba periférica, lo que provoca el crecimiento de los círculos observado por Tschinkel.  Si bien para los no expertos la explicación ofrecida por Juergens luce coherente, como es lo usual no todos los especialistas están de acuerdo en que el misterio de los círculos encantados hubiera sido finalmente develado. Tschinkel, en particular, en declaraciones recogidas por “Science”, señala que Juergens no ha demostrado que las termitas sean efectivamente las culpables de la ausencia de hierba en el centro de los círculos encantados. De este modo, tal parece que el misterio de los círculos de Namibia sobrevivirá por algún tiempo más.Por otro lado, cuando el misterio se aclare, es posible que los círculos de hadas de Namibia pierdan el encanto que les da precisamente el halo de misterio que los envuelve. Entrarían así a la categoría de fenómenos naturales, con una explicación también natural y por tanto de este mundo.",
    "El pasado miércoles 19 de marzo la NASA anunció que el problema que tuvo el programa de cómputo que controla al explorador marciano “Curiosity” había sido resuelto, y que en pocos días reanudaría su operación normal. Dicho problema ocurrió el 16 de marzo y afectó a una de las dos computadoras de control del explorador. El “Curiosity” cuenta con dos computadoras idénticas y redundantes. El control del explorador lo lleva a cabo una de estas dos computadoras, mientras que la otra actúa como respaldo en caso de necesidad. El 27 de febrero, la computadora que estaba a cargo del control de “Curiosity” –computadora A– sufrió una falla en su memoria “flash”. Esto obligó a la NASA a suspender la operación normal del explorador y a iniciar una transferencia del control a la computadora B. El problema del 16 de marzo ocurrió con esta segunda computadora, antes de que dicha operación se hubiera normalizado. El “Curiosity”, de este modo, ha enfrentado dos problemas en el curso de tres semanas que han afectado a sus dos computadoras de control y que lo han puesto fuera de operación.Como sabemos, el explorador “Curiosity” dejó la Tierra en noviembre de 2011 y arribó a la superficie marciana en agosto de 2012. Con un peso cercano a una tonelada y el tamaño de un automóvil compacto, el “Curiosity” tiene, entre otros objetivos, determinar si en la actualidad existen –o si existieron en el pasado– las condiciones adecuadas para el desarrollo de vida microbiana en Marte. Para esto lleva a bordo sofisticados instrumentos para analizar químicamente muestras del suelo marciano y determinar su composición mineralógica. En medio de los problemas que paralizaron al explorador a lo largo de las últimas cuatro semanas, la NASA anunció el pasado día 12 de marzo que al analizar el polvo obtenido al perforar una roca marciana, el “Curiosity” había encontrado los elementos esenciales para la vida: carbono, hidrógeno, oxígeno, nitrógeno, fósforo y azufre. De manera significativa, los encontró en forma de compuestos químicos aprovechables como alimento por la vida bacteriana. Todo esto, además, fue encontrado en un sitio que parecía ser el lecho de un antiguo lago alimentado por un río que habría descendido de la ladera de un cráter en las inmediaciones. Un análisis mineralógico encontró que la roca estaba compuesta por arcilla en buena medida, lo que implicaba que la roca había permanecido en contacto con el agua por un largo tiempo.El “Curiosity” encontró entonces, por vez primera, que en Marte existieron en el pasado –al menos en el lugar que está explorando– las condiciones adecuadas para el desarrollo de vida bacteriana. Esto, por supuesto, no significa que en verdad se hubiera desarrollado, de lo cual no existe evidencia hasta el momento. El explorador tiene también como objetivos estudiar el clima y la geología de Marte, así como sus condiciones de habitabilidad. En particular, está midiendo el nivel de radiaciones de alta energía que inciden sobre la superficie marciana –rayos cósmicos y partículas energéticas emitidas por el Sol– que son adversas a la vida.  En la Tierra estamos protegidos de estas radiaciones tanto por el campo magnético de nuestro planeta como por su atmósfera. Marte no tiene un campo magnético y su atmósfera es mucho menos densa que la de la Tierra, por lo que no goza del mismo nivel de protección. Conocer los niveles de radiación que llegan a la superficie de Marte es de este modo un tema de gran interés para futuros viajes tripulados a este planeta –por más que se vean lejanos en estos momentos de problemas económicos.  Encontrar si en Marte existen o existieron en el pasado las condiciones adecuadas para el desarrollo de la vida no es ciertamente el único objetivo del “Curiosity”, aunque sí es el más publicitado por la NASA. Esto es entendible, pues la vida extraterrestre es un tema que captura grandemente el interés público y puede proporcionar apoyo popular a las actividades de la agencia espacial.De manera desafortunada, el explorador “Curiosity” experimentó sendos problemas con sus dos sistemas de cómputo antes de cumplir un año sobre la superficie de Marte. Esto contrasta con el explorador “Opportunity” que descendió a la superficie marciana en enero de 2004 y que aún hoy en día, casi diez años después, está activo. Su gemelo, el explorador “Spirit”, que arribó tres semanas antes, tuvo igualmente una larga vida y estuvo activo hasta marzo de 2010 –aunque tuvo igualmente al inicio un problema con la memoria “flash”.  Contrasta igualmente el costo de ambas misiones: 2,500 millones de dólares para el “Curiosity” y un costo combinado de 820 millones de dólares para el “Spirit” y el “Opportunity”. Esta diferencia en costo refleja que el “Curiosity” es un explorador más sofisticado que sus predecesores. De manera consecuente, los especialistas esperan que la información que nos envíe acerca de nuestro planeta vecino sea proporcionalmente más rica y que, en particular, nos esclarezca si Marte fue alguna vez habitable –no por marcianos sino por simples microbios–, y que nos haga saber cuáles son las condiciones de habitabilidad de este planeta –que sería nuestra primera opción para emigrar en caso de necesidad–. Visto esto, esperemos que los problemas que experimentó  el “Curiosity” hayan sido sólo ocasionales.",
    "Algo inusual sucedió en nuestro planeta hace unos 1,200 años; para ser más precisos, en algún momento durante los años 774-775 d.C. A esta conclusión llegó un grupo de científicos japoneses de la Universidad de Nagoya, encabezados por Fusa Miyake, después de estudiar los anillos del tronco de dos cedros existentes en  la isla Yaku en el sur de Japón. Es conocido que los cedros que viven en esta isla son extremadamente longevos. Un caso extremo en este sentido es el llamado Cedro Jomon –con una altura de 25 metros y una circunferencia de 16 metros–, que se estima podría tener una antigüedad de hasta 7,200 años. Los anillos de los árboles se forman debido a que a lo largo del año cambia su velocidad de crecimiento, sobre todo en los climas templados en donde hay mayor variación climática entre estaciones. Así, en la primavera el crecimiento es más rápido y la madera resultante menos densa, en contraste con los meses posteriores en los que el crecimiento es más lento y la madera más densa. De este modo, en condiciones normales cada anillo corresponde a un año y su número nos indica la edad del árbol.   La sucesión de anillos de un árbol, cuyos grosores varían año con año, nos dan adicionalmente información acerca de las condiciones ambientales en las que creció. Así, un mayor espesor nos indica un año favorable con una alta velocidad de crecimiento, mientras que un espesor menor nos habla de las condiciones adversas por las que pasó ese año el árbol.Los anillos de los árboles nos pueden también proporcionar otro tipo de información. En efecto, estudiando los cedros de la isla Yaku, Miyake y colaboradores encontraron que los anillos correspondientes a los años 774-775 d.C. muestran un incremento de 1.2% en su concentración de carbono 14. El carbono 14 es tomado por las plantas de la atmósfera e incorporado a su tejido orgánico –juntamente con el carbono 12, más común– mediante el proceso de fotosíntesis. El incremento de este elemento en anillos correspondientes a años específicos, es entonces una indicación de una elevación de la concentración de carbono 14 en la atmósfera en esos mismos años.¿Qué provocó este incremento? Los especialistas saben que el carbono 14 en la atmósfera tiene su origen en la interacción de dicha atmósfera con los rayos cósmicos y otras radiaciones de alta energía que llegan a nuestro planeta. La elevación en la concentración de carbono 14 en la atmósfera ocurrida en 774-775 d.C. fue entonces debida a un incremento en la intensidad de las radiaciones provenientes del espacio exterior.   A su vez, estas radiaciones se originan durante la explosión de una estrella o bien en una llamarada solar. Miyake y colaboradores descartaron la primera posibilidad, ya que no existen registros históricos de que haya sido observado un evento de este tipo que tendría que haber sido claramente visible. Los investigadores japoneses tampoco consideraron que lo ocurrido en 774-775 d.C. se hubiera originado en una llamarada solar, pues ésta tendría que haber sido demasiado grande y debería haber provocado problemas mayores a la Tierra –incluyendo una reducción sustancial en el nivel de ozono en la atmósfera y la consecuente exposición de las especies vivientes a altos niveles de radiación ultravioleta– lo cual no ocurrió. Así, Miyake y colaboradores no encontraron una explicación que los satisficiera y dejaron el asunto como un misterio en busca de una explicación.En un artículo publicado en línea esta semana en la revista “Geophysical Research Letters” por investigadores de la Universidad Washburn y la Universidad de Kansas en los Estados Unidos, se arguye que los cálculos de Miyake y colaboradores están errados y que no son necesarios niveles de radiación espacial tan grandes como los que habían supuesto estos investigadores para provocar el evento de hace doce siglos. Concluyen que el Sol es el principal sospechoso de haberlo provocado.En una época tan remota, estaba lejana la fecha en que se desarrollaron tecnologías tales como la de las telecomunicaciones y la de generación de energía eléctrica. En este respecto, Carlomagno se evitó preocupaciones, pues de haberlas conocido con seguridad las habría empleado en sus guerras de conquista. Así, habría sufrido grandes contratiempos en el momento en que llegó el golpe de radiación extraterrestre, pues tanto las telecomunicaciones como las redes de distribución de energía eléctrica son grandemente afectadas por las tormentas solares. En relación a esto, habría que recordar que una tormenta solar en marzo de 1989 dejó a Quebec sin energía eléctrica por nueve horas.   El golpe solar de 774-775 d.C. ciertamente no afectó  instalaciones eléctricas, pues estas aun no existían. En contraste, un golpe de intensidad equivalente en la actualidad tendría con seguridad muchas mayores consecuencias. Así, si bien la tormenta solar de 774-775 d.C. no fue catastrófica ni para Carlomagno ni posiblemente para sus contemporáneos, el episodio nos recuerda las amenazas provenientes del espacio exterior a las que estamos expuestos de manera permanente.",
    "El pasado jueves 7 de marzo, la agencia rusa de noticias RIA Novosti publicó declaraciones del científico ruso Serguei Bulat, del Instituto de Física Nuclear de San Petesburgo, según las cuales habrían descubierto en el Lago Vostok en la Antártida un tipo de bacterias hasta ahora desconocido. Dichas bacterias fueron encontradas en muestras de agua extraídas del lago Vostok por la expedición rusa a la Antártida durante el verano austral de 2012.  El Lago Vostok no es un lago cualquiera. Lejos de esto, constituye un enorme cuerpo de agua subterráneo –que en volumen constituye el sexto lago más grande del mundo–, sepultado por una capa de hielo con un espesor alrededor de los 3.5 kilómetros. Se sospechaba la existencia del Lago Vostok desde la década de los años cincuenta, aunque esto no fue confirmado con certeza hasta cuatro décadas después por medio de imágenes de radar tomadas desde el espacio. Después de un buen número de intentos de perforar la capa de hielo y acceder al Lago Vostok para tomar muestras de agua, un equipo ruso finalmente tuvo  éxito en febrero de 2012. Hacer esto no fue una empresa fácil, no solamente por el enorme espesor de hielo que hubo que perforar en la región más fría de planeta –que tiene el récord de temperatura más baja jamás registrada: menos 89 grados centígrados–, sino por el cuidado extremo que fue necesario desplegar a fin de no contaminar el agua prístina del lago con los fluidos empleados en la perforación.    Investigar el agua del Lago Vostok tiene un gran interés científico, pues se cree que se formó hace unos quince millones de años y de existir ahí vida podría haber evolucionado aislada del mundo exterior siguiendo un camino propio –aunque no se sabe con certeza si el agua del lago ha estado en realidad aislada por este número de años–. Según Bulat en sus declaraciones del jueves pasado, el ADN de las bacterias descubiertas en el agua del Lago Vostok las hace apreciablemente diferentes de todo lo que conocemos, lo que implicaría que, efectivamente, evolucionaron de manera independiente. Estaríamos de este modo ante un descubrimiento sensacional.Como es usual, no obstante, no todo mundo está de acuerdo en calificar el hallazgo como tal. David Pearce del “British Antartic Survey” en declaraciones recogidas por la revista de divulgación científica “New Scientist”, considera que lo sorprendente hubiera sido no encontrar vida en el lago Vostok, pues ésta siempre se adapta a condiciones extremas. Pearce afirma, además, que todavía es prematuro hablar del descubrimiento de un nuevo tipo de bacteria y que más estudios son necesarios antes de confirmarlo o refutarlo. El foro para difundir y discutir un hallazgo científico no es, por supuesto, el que ofrecen los medios masivos de comunicación. Para esto existen revistas especializadas que publican artículos de investigación, en los que se discuten en detalle los procedimientos seguidos durante el estudio, los resultados obtenidos y las conclusiones alcanzadas. La decisión para publicar un artículo la toma el editor de la revista basado en la opinión de otros científicos, a los que les pide expresen por escrito una evaluación crítica del artículo en revisión.No obstante lo anterior, difundir un descubrimiento científico en los medios masivos de comunicación cumple el objetivo de hacerlo llegar a un número grande de personas, muchas más que los lectores de una revista científica especializada. Esto, por supuesto, es importante para un proyecto de gran envergadura financiado con fondos públicos –como es el caso de la expedición al Lago Vostok–, que debe rendir cuentas públicas. La difusión masiva de un descubrimiento o logro científico cumple también propósitos de propaganda. Así, cuando los exploradores del Lago Vostok lograron penetrarlo y obtener una muestra de agua en febrero del año pasado, regalaron parte de ésta al Presidente de Rusia en un evento público –lo hicieron, por más que el agua tuviera un color amarillento, que no es claro si fue resultado de una contaminación con el fluido de perforación.El manejo de los descubrimientos científicos y los desarrollo tecnológicos con fines de propaganda es inevitable y de esto hemos tenido ejemplos en los últimos tiempos. Algunas veces este manejo ha sido precipitado. Tal fue el caso del anuncio hecho por la NASA en diciembre de 2010 en relación al descubrimiento –en el Lago Mono, en California–  de bacterias capaces de incorporar arsénico a su material biológico, lo que habría constituido una revolución en el campo de la Biología. Esto, no obstante, no pudo ser posteriormente comprobado de manera independiente por otros investigadores y no pasó de ser un anuncio mediático apresurado.¿Se confirmará, más allá de toda duda, que el Lago Vostok es asiento de vida diferente a la encontramos en otras partes del planeta?  ¿Resultará que el anuncio del pasado jueves fue precipitado? Es posible que tengamos una respuesta a estas preguntas en los próximos meses, en la medida en que se analicen las muestras de agua del Lago Vostok extraídas este año. Por lo pronto, al margen de lo que resulte, el que exista un lago misterioso y enorme, escondido bajo kilómetros de hielo en el continente más frío de la Tierra, es sin duda por sí solo fascinante.",
    "No se trata de una caso de telepatía ni mucho menos, pero los experimentos reportados esta semana en un artículo publicado en la revista “Scientific Reports” por un grupo de de investigadores de los Estados Unidos y de Brasil son, sin duda, de llamar la atención. En efecto, en dicho artículo se reporta que dicho grupo de investigación logró que una rata en los Estados Unidos –en la Universidad Duke–, comunicara sus pensamientos a otra rata en un laboratorio en Natal, Brasil. El experimento se llevó a cabo como sigue.Primeramente, a las dos ratas les fueron implantados electrodos en el cerebro, a través de las cuales fue posible acceder a los impulsos eléctricos que el mismo genera. Los electrodos permiten también estimular al cerebro por medio de impulsos eléctricos externos. Las dos ratas fueron, además, entrenadas para accionar una de dos palancas en respuesta al estímulo de una luz roja. Cuando la rata accionaba la palanca correcta era recompensada con una pequeña cantidad de agua. Después del entrenamiento, una de las ratas, la “trasmisora”, fue capaz de escoger la palanca correcta un 95% de las veces que lo intentó.  La otra rata, la “receptora”, aparte de ser sometida al mismo entrenamiento, fue condicionada para responder a impulsos eléctricos aplicados al cerebro a través de los electrodos. Durante el experimento se grabaron los impulsos eléctricos que generó el cerebro de la rata trasmisora cuando fue puesta en la disyuntiva de escoger una de las dos palancas. Estos impulsos fueron de manera inmediata enviados vía internet a Brasil y allí aplicados al cerebro de la rata receptora, puesta ésta en posición de escoger una de dos palancas, idénticas a las del laboratorio en la Universidad Duke. El resultado fue que la rata receptora en Brasil escogió la misma palanca seleccionada por la rata originadora del estímulo, en siete de diez intentos. Así, la rata en los Estados Unidos trasmitió su pensamiento a la rata en Brasil, sin que hubiera un contacto físico entre ambas.   Si bien, según una nota publicada por el New York Times, algunos especialistas no se mostraron especialmente entusiasmados por este resultado, en otros casos lo consideraron asombroso. Uno de los autores del artículo referido arguye que dichos resultados representan un pequeño paso hacia una futura computadora biológica compuesta por numerosos cerebros actuando en sincronía para realizar una tarea común. En este respecto hay que mencionar que cuando la rata receptora accionaba la misma palanca que la rata trasmisora, ésta última recibía una recompensa adicional de agua y en este sentido tenía un estímulo para trasmitir sus pensamientos de la mejor manera posible.Por supuesto, una cosa es especular sobre una futura computadora biológica y otra hacerla realidad, pues un cerebro con funciones cognitivas superiores es de una complejidad extrema. En este sentido, el cerebro humano, el más complejo de todos, está formado por cerca de 100,000 millones de neuronas, pudiendo tener cada neurona hasta unas 10,000 conexiones. El cerebro en general y no solamente el humano, es ciertamente de una gran complejidad; tan grande, que sabemos muy poco acerca de su funcionamiento. Con el propósito de cerrar este vacío en nuestro conocimiento, el Presidente de los Estados Unidos dio a conocer el pasado 12 de febrero que está preparando un proyecto a diez años con un financiamiento masivo, para descubrir el mapa del funcionamiento del cerebro humano. Este mapa contrastaría con uno que simplemente describiera la posición de las neuronas del cerebro y sus conexiones. En palabras de George Church de la Universidad Harvard, “la diferencia entre estos dos mapas sería equivalente a conocer solamente la posición y extensión de todas las líneas telefónicas, en lugar de saber dónde, cuándo y cómo, estas líneas trasmiten información”.     Según algunos investigadores, el proyecto para desarrollar el mapa del cerebro humano tiene paralelismos con el proyecto que en 2003 desentrañó el genoma humano. No todos los expertos, sin embargo, están igualmente entusiasmados en un proyecto que supondría un costo que rondaría los 3,000 millones de dólares en diez años. Entre otras cosas, consideran que desarrollar un mapa del funcionamiento del cerebro es considerablemente más complicado que lo que fue secuenciar el genoma humano y que los objetivos planteados no son factibles al corto plazo.Por lo pronto, y en espera de que la controversia se aclare y sepamos la suerte que corra el proyecto anunciado, nada nos impide especular sobre lo que significará y los beneficios que nos traerá conocer en detalle cómo opera un órgano que ha sido calificado como el objeto más complejo del Universo. Nada nos impide tampoco especular sobre la posibilidad de construir una computadora biológica, integrada por una conjunción de los objetos más complejos del Universo, que potencie hasta niveles difíciles de concebir la inteligencia humana. Aunque, para que esto suceda, mucho tendrá que avanzar la ciencia para convencer a un número suficiente de voluntarios de participar como unidades de dicha computadora. En particular, para que accedan a que se les implanten electrodos en el cerebro.",
    "De ser usted un fanático coleccionista de meteoritos, con seguridad está de plácemes por el asteroide o meteoroide que –de manera inesperada– penetró a la atmósfera de nuestro planeta el pasado viernes 15 de febrero, explotando a unos 20 kilómetros de altura sobre la región de Cheliabinsk en los Montes Urales. En efecto, según la NASA, dicho asteroide tenía un peso entre 7,000 y 10,000 toneladas, y al desintegrarse habría dispersado una enorme cantidad de fragmentos para regocijo de los coleccionistas.Viendo la oportunidad de hacer negocio, poco tardaron los traficantes de meteoritos en hacer su aparición en la región de Cheliabinsk. Según un reportaje publicado por el New York Times el pasado día 18, el lunes siguiente a la explosión del asteroide hicieron su aparición en el poblado de Deputatskoye, en donde cayeron una gran cantidad de fragmentos, fuereños que ofrecían diversas cantidades de dinero por los mismos. De la misma manera, pronto aparecieron en internet ofertas de meteoritos, supuestamente provenientes del meteoro de Cheliabinsk. Consultando, por ejemplo, la página de eBay, uno puede encontrar ofertas de meteoritos que “provienen de la región de Cheliabinsk”, o que de manera directa se anuncian como fragmentos del meteoro del pasado día 15. Los precios van desde decenas de dólares hasta cientos de dólares, por meteoritos que pesan unos pocos gramos.El problema, por supuesto, es saber si son auténticos. De acuerdo con el reportaje del New York Times, al ser el comercio con meteoritos ilegal en Rusia, es difícil asegurarlo. A pesar de esto, como la página de eBay lo atestigua, los coleccionistas no han dejado de adquirir los supuestos  meteoritos de Cheliabinsk que ahí se han puesto a la venta. En correspondencia con el interés de los coleccionistas, el meteoro de Cheliabinsk no ha sido un acontecimiento menor. Lejos de esto, ha sido el mayor de que se tenga noticia desde aquel que derribó millones de árboles en Tunguska en 1908. En ambos casos, los asteroides explotaron en el aire y no se sabe que hayan producido un cráter, arribando a tierra sólo fragmentos pequeños.En contraste, en el año 1492 cayó a tierra cerca del pueblo de Ensisheim, Alsacia, Francia, un meteorito de 127 kilogramos. Este meteorito constituye el más antiguo que haya sido directamente observado, y del cual se tenga suficiente material disponible para investigación, según afirma Ingrid Rowland de la Universidad de California en Irvine, en un artículo publicado en 1990 en la revista “Meteoritics”. El meteorito de Ensisheim se partió en varios pedazos al caer. De acuerdo con las historias de la época, fue encontrado por un muchacho en el fondo de un agujero de un metro de profundidad, en un campo a cercano a la ciudad amurallada. Dio aviso a los habitantes del pueblo, quienes cortaron pedazos del meteorito para llevárselos como recuerdo. Como resultado de este vandalismo, y de que posteriormente se regalaron fragmentos del meteorito a personajes importantes de Europa, actualmente se conserva solamente una piedra de poco más de 50 kilogramos. El meteorito de Ensisheim constituyó todo un acontecimiento en su momento y fue tomado como un presagio de calamidades por venir, guerras y enfermedades incluidas. Rowland reproduce en su artículo la descripción que en su momento dio de dicho acontecimiento –desde Siena, hasta donde habían llegado las noticias–, el historiador Sigismondo Tizio, “En este punto es menester mencionar el inmenso portento que fue visto este año en Alemania: el séptimo día de noviembre, cerca de la ciudad de Ensisheim, una gran piedra cayó de cielo, de forma triangular, carbonizada y acompañada de truenos y relámpagos”. En una ilustración de la época, se muestra al meteoro lanzado rayos sobre el pueblo, ante la mirada asombrada de sus habitantes. Hoy en día, a más de cinco siglos de su arribo a la Tierra, el meteorito de Ensisheim sigue captando la atención, particularmente en su pueblo natal, que celebra anualmente una reunión para recordar la llegada de su meteorito, y en donde reside colocado dentro un nicho de cristal. Además, con el objeto de proteger su integridad, existe la “Hermandad de Guardianes del Meteorito de Ensisheim” que, cuando se reúnen, se visten con capas rojas y sombreros blancos.Cheliabinsk, Tunguska y  Ensisheim, son solamente tres acontecimientos que involucran la caída de cuerpos celestes a nuestro planeta de los innumerables que se han dado a lo largo de la historia de la Tierra. Afortunadamente, a pesar de su espectacularidad, ninguno de los tres provocó víctimas fatales. En otros casos no ha sido así. El más famoso es posiblemente el que se dio hace 65 millones de años, cuando un aerolito de dimensiones considerables habría provocado la extinción masiva de especies, incluyendo a los dinosaurios. Se especula que las extinciones masivas anteriores también pudieron haber sido producidas por la caída de asteroides a la Tierra. Así, los asteroides que rondan a nuestro planeta, sí pueden ser causantes de calamidades mayores como se creía en el pasado. Aunque, como dice el dicho, no hay mal que por bien no venga, y si bien es cierto que un asteroide pudo haber matado a los dinosaurios, los mamíferos nos sobrepusimos al desastre y todavía por aquí andamos. Y con fuerzas renovadas.",
    "En la primavera del año 1927, el mineralogista Leonid Kulik logró llegar, después de muchos esfuerzos, al lugar en donde estimaba había caído 19 años antes un asteroide de gran tamaño. Esto, en un área cerca al rio Tunguska, en el corazón de Siberia. Kulik sabía por rumores de pobladores de la región alrededor de Tunguska, que en 1908 había sido avistada en el cielo una bola de fuego que explotó con gran estruendo, generando una onda expansiva que se sintió a decenas de kilómetros de distancia. Los rumores no habían sido confirmados, pero Kulik consideraba que correspondían a hechos reales; de manera específica, al ingreso de un cuerpo celeste a la atmósfera de nuestro planeta.Con el objeto de despejar dudas, Kulik logró que la Academia Rusa de Ciencias financiara una expedición encabezada por él mismo a la región de impacto del hipotético asteroide. Una vez allí, pudo comprobar que existían amplias evidencias que apoyaban sus supuestos. En efecto, se encontró con una amplia zona en donde los árboles habían sido derribados por una poderosa explosión. De manera peculiar, además, dichos árboles yacían dispuestos radialmente hacia un centro común, que Kulik dedujo constituía el epicentro de dicha explosión.Es de notar también que en dicho epicentro, si bien los árboles estaban quemados y sin ramas, se mantenían de pie, dando la impresión de un campo de postes telegráficos. Esto último era una indicación de lo violento de la explosión, que arrancó las ramas de los árboles sin derribar los troncos.  A pesar de todas las evidencias que le daban la razón, sin embargo, Kulik no encontró el cráter que él esperaba como producto del impacto del meteorito con el suelo.Si bien se han dado varias hipótesis para explicar la devastación de Tunguska –algunas exóticas, como aquella que la atribuye a una explosión de antimateria–, la más aceptada en la actualidad es la que originalmente defendió Kulik; es decir, que fue producto del impacto de un asteroide.   De acuerdo con el sitio de internet de la NASA, se estima que el asteroide de Tunguska tuvo un tamaño de unos 40 metros, un peso de 100,000 toneladas y habría ingresado a la atmósfera a una velocidad aproximada de 50,000 kilómetros por hora. A esta velocidad, el asteroide habría elevado la temperatura del aire a su alrededor hasta unos 25,000 grados centígrados. La combinación de temperatura y presión del aire por la enorme velocidad con que ingresó a la atmósfera, habría provocado la explosión del asteroide a una altura de unos 8,500 metros sobre Tunguska. Con estos números, no nos sorprende lo que hoy sabemos: que 80 millones de árboles fueron derribados en Tunguska y que la devastación se extendió a lo largo de 2000 kilómetros cuadrados. No es sorprendente tampoco que los efectos de la explosión se hayan sentido en sitios tan alejados como Londres, en donde por varios días las noches estuvieron iluminadas por la luz del Sol dispersada por los restos del asteroide suspendidos en la atmósfera. Por otro lado y de manera afortunada, dado que el fenómeno ocurrió en un área con muy poca población, no se sabe de víctimas fatales.Aunque el evento de Tunguska sucedió hace más de un siglo, se ha convertido nuevamente noticia  por causa del asteroide que sorpresivamente apareció el viernes pasado en la región de los Montes Urales, cerca de la ciudad de Chelyabinsk, explotando en el aire, destrozando vidrios de ventanas e hiriendo a más de mil personas a causa de la onda de choque que siguió a la explosión.Según la NASA, el asteroide de Chelyabinsk tenía un tamaño entre 15 y 17 metros, con un correspondiente peso entre 7,000 y 10,000 toneladas. Ingresó a la atmósfera a una velocidad de unos 65,000 por hora y por efecto, tanto del calentamiento por el rozamiento con la atmósfera, como por la presión a la que fue sometido a causa de su velocidad, explotó a una altura de 20 kilómetros, liberando una energía de 300 kilotones. Esta energía es equivalente a 18 bombas atómicas como la arrojada sobre la ciudad de Hiroshima al término de la Segunda Guerra Mundial. El asteroide del pasado viernes fue sensiblemente menor al que explotó sobre Tunguska en 1908. Este último, no obstante, lo hizo sobre un área virtualmente despoblada y no produjo víctimas fatales, en contraste con el primero que se desintegró cerca de una ciudad con una población de más de un millón de habitantes. Ciertamente, la probabilidad de que un asteroide del tamaño del de Tunguska impacte directamente a un centro mayor de población es pequeña. El evento de Chelyabinsk, no obstante, nos recuerda que estamos permanente sujetos a un bombardeo desde el cielo. En este respecto, la NASA tiene establecido un programa para vigilar asteroides cerca de nuestro planeta y determinar el peligro potencial que representan, aunque de momento no podamos hacer nada para evitarlo.Como quiera que sea, el asteroide de Chelyabinsk nunca fue avistado y nos tomó a todos por sorpresa, cuando ese mismo día estábamos a la espera de otro asteroide, el cual paso muy cerca de la Tierra –en términos astronómicos– sin mayor peligro, como se había anticipado.",
    "Escudriñar el pasado remoto es ciertamente fascinante. Atrae, por ejemplo, saber que el faraón Ramsés III, que reinó en Egipto hace más de 3000 años, muy probablemente murió asesinado como resultado de una conjura de personas cercanas a él; o bien, enterarnos que nuestros antecesores convivieron en Europa hace decenas de miles de años con los neandertales y que incluso pudieron haberse cruzado. Los especialistas investigan el pasado desde múltiples enfoques, aprovechando circunstancias y empleando las más diversas técnicas. Así, tenemos que por medio de la Piedra Rosetta, descubierta en 1799 durante la expedición de Napoleón a Egipto, Jean François Champollion pudo descifrar los jeroglíficos egipcios, lo que permitió profundizar en la historia del Antiguo Egipto. De la misma manera, empleando técnicas genéticas modernas, fue posible determinar que el individuo cuya momia fue encontrada junto a la de Ramsés III, estaba emparentado con él y que, por otras evidencias, posiblemente tomó parte de la conjura para asesinarlo.  Conforme se desarrollan técnicas analíticas de todo tipo y cada vez más sofisticadas, se multiplican y se hacen más poderosos los medios que tenemos para acceder al pasado remoto. En este sentido, podemos traer a colación el episodio relativo a la extinción masiva de especies –incluyendo a los dinosaurios– que se dio hace aproximadamente 65 millones de años.        En un artículo publicado en 1980 en la revista “Science”, que llevó como primer autor a Luis Álvarez, premio Nobel de Física en 1968, se aventuró una hipótesis en torno al origen de esta extinción de especies. De acuerdo con Álvarez y colaboradores, la misma fue producto de la caída a nuestro planeta de un meteorito de unos 10 kilómetros de diámetro. El impacto de un objeto de este tamaño con la Tierra habría levantado una nube de polvo que se dispersó por toda la atmósfera, permaneciendo suspendida por años. La nube de polvo habría bloqueado la luz solar, dificultando la fotosíntesis. En estas condiciones, según la hipótesis, se interrumpió la cadena de alimentación, lo que a su vez provocó la extinción masiva de especies, en particular de los dinosaurios.La hipótesis anterior se apoya en el descubrimiento de concentraciones elevadas de iridio en un estrato geológico que se extiende por todo el mundo cuya antigüedad corresponde a la de la extinción masiva de especies. Dicho descubrimiento fue llevado a cabo por Walter Álvarez –hijo de Luis Álvarez– en estratos geológicos expuestos de las montañas del norte de Italia, así como de otros lugares del mundo. El iridio es muy escaso en la superficie terrestre y una concentración tan elevada como la que encontraron los Álvarez sugiere un origen extraterrestre. De manera específica, como sugiere el artículo de referencia, fue resultado del impacto de un meteorito que habría dispersado iridio por toda la superficie de la Tierra.Algo que dio sustento adicional a la hipótesis de meteorito fue el descubrimiento –posterior a la muerte de Luis Álvarez– del cráter de Chicxulub, cuyo centro se localiza en el mar, muy cerca de la costa noroeste de Yucatán. Dicho cráter, que tiene un diámetro de 180 kilómetros, se formó hace 65 millones de años y, según la opinión de un gran número de expertos, es el origen de la extinción masiva de especies que marcó el fin de los dinosaurios.No todos los especialistas, sin embargo, están de acuerdo con esta opinión y algunos han alegado que la caída del meteorito de Yucatán y la extinción de especies fueron eventos separados por cientos de miles de años y en consecuencia uno no pudo haber sido resultado del otro. Contrario a esta opinión, un artículo aparecido esta semana en la revista “Science”, publicado por investigadores de universidades en los Estados Unidos, Holanda y el Reino Unido, ofrece nuevos datos que indican que ambos acontecimientos, meteorito y extinción de especies, se dieron de manera simultánea. Esta conclusión se basa en un estudio –empleando técnicas analíticas de gran precisión–, tanto de los materiales despedidos por el impacto de Chicxulub, como de los sedimentos ricos en iridio asociados a la extinción de especies. Los investigadores encontraron que ambos acontecimientos ocurrieron hace un poco más de 66 millones de años, separados apenas por 32,000 años. Este último periodo de tiempo está dentro del margen de error de las mediciones, y  concluyen que muy probablemente el meteorito de Chicxulub sí fue causa de la extinción de especies, si bien no la única.Así, las técnicas de investigación modernas nos proporcionan una visión de los que podría haber acontecido hace 66 millones de años. Por supuesto, dado el tiempo transcurrido, no esperaríamos una certeza absoluta. La incertidumbre sobre sucesos en tiempos muy remotos, no obstante, con seguridad se reducirá en la medida que avancen nuestras técnicas analíticas de investigación.",
    "Al despuntar el Siglo XIX, la ciudad de Tombuctú –situada en el país africano de Malí, en la frontera sur del desierto del Sahara– era para los europeos un lugar exótico, misterioso y de fábula, a la vez que uno de los lugares más lejanos e inaccesibles del mundo. Tal era la fascinación de los europeos por Tombuctú, que la Sociedad Geográfica en París ofreció un premio 10,000 francos a aquel europeo que llegara hasta esa ciudad y regresara sano y salvo para contarlo. Este honor correspondió al francés René Caillié, quién realizó la hazaña en 1828 disfrazado de musulmán y reclamó con éxito el premio. Para decepción de los europeos, sin embargo, Tombuctú resultó ser según Caillié  “una masa de casas de apariencia lastimosa, hechas de tierra”, muy lejos del lugar esplendoroso que se esperaba. A juzgar por las imágenes y documentales en video que uno puede encontrar en internet, si bien Tombuctú es ciertamente un lugar exótico, su apariencia –con construcciones de barro y calles polvosas sin pavimentar– difícilmente contradice las apreciaciones de  Caillié. Para evaluar la importancia que en algún momento tuvo Tombuctú, no obstante, no hay que dejarse llevar por su imagen hoy en día. En efecto, además  de tener una antigüedad de casi un milenio, Tombuctú tuvo épocas de gran esplendor, ocupando un lugar privilegiado en la ruta de las caravanas que atravesaban el desierto del Sahara transportando sal, marfil, oro y esclavos. En correspondencia con su esplendor comercial, Tombuctú se constituyó en un centro académico de gran importancia, con una universidad que llegó a tener 25,000 estudiantes.Como legado de su pasado académico, existen actualmente en Tombuctú numerosas bibliotecas particulares, mantenidas de generación en generación, y que se dice en total podrían representar cientos de miles de manuscritos de gran antigüedad. Para preservar esta herencia cultural, que habla del pasado intelectual de África, el gobierno de Malí estableció en 1973 el Instituto Ahmed Baba de Altos Estudios e Investigaciones Islámicas”, el cual incluye un centro de documentación que actualmente reúne 30,000 manuscritos antiguos. Esos manuscritos, en algunos casos con más de siete siglos de antigüedad, fueron  recolectados de bibliotecas privadas de Tombuctú, al igual que de otros lugares del África Occidental. Los textos tratan de arte, medicina, ciencia y caligrafía, entre otros temas, incluyendo copias antiguas del Corán. Dada la gran importancia de la biblioteca del Instituto Ahmed Baba, causaron una gran preocupación entre los expertos las noticias aparecidas en la prensa al inicio de la presente semana en el sentido de que dicha biblioteca había sido quemada por los islamistas radicales que tenían tomada Tombuctú desde abril del año pasado. Como se sabe, en 2012 el Movimiento Nacional para la Liberación del Azawad se rebeló contra el gobernó de Malí y declaró de manera unilateral la independencia del Azawad, región dentro del cual se localiza Tombuctú. Esta ciudad estuvo en poder de los rebeldes hasta el pasado lunes cuando fue liberada por tropas francesas y malíes.  Afortunadamente, tal parece  que las noticias sobre la destrucción de manuscritos en Tombuctú, atribuidas al alcalde de esta ciudad, son exageradas y si bien algunos manuscritos habrían sido efectivamente destruidos, la mayor parte de los mismos se encontraría a salvo.  Dadas las acciones de intolerancia religiosa de los ocupantes islamistas de Tombuctú, que destruyeron monumentos en la ciudad, el grado de destrucción de los manuscritos queda, no obstante, en la incertidumbre.  Por la importancia cultural de la biblioteca de Tombuctú, hay que cruzar los dedos porque la destrucción haya sido mínima.El episodio, por otro lado, pone en perspectiva dos hechos notables. Primeramente, por supuesto, hay que mencionar la rica tradición intelectual de la ciudad de Tombuctú, que cambia radicalmente nuestra visión sobre el pasado cultural de África. Por otro lado, no deja de llamar la atención que colecciones particulares de libros pudieran haber sido mantenidos, generación tras generación, por cientos de años y guerras de por medio. Lo anterior, además, en una ciudad que, si bien tuvo épocas de esplendor, no las ha tenido todas consigo en los últimos años, por no decir siglos. Esto último es evidente, al menos desde los tiempos de René Caillié.",
    "Como sabemos, la concentración de bióxido de carbono en la atmósfera terrestre ha experimentado un incremento continuo desde el inicio de la revolución industrial hace dos siglos. Actualmente, dicha concentración es de 390 partes por millón, que es 37% más elevada que la que se tenía en el año 1880. De manera concurrente, la temperatura promedio de la superficie de la Tierra se ha elevado en casi un grado centígrado en los últimos cien años, provocando, entre otros fenómenos, la fusión de los hielos árticos y antárticos.Aunque hay quien niega que ambos incrementos estén correlacionados, una mayoría de expertos considera que la temperatura terrestre se ha elevado causa del crecimiento del nivel de bióxido de carbono en la atmósfera. Este crecimiento, a su vez, es producto de la quema de combustibles fósiles –petróleo, gas natural y carbón– que aportan más de 80% del total de la energía consumida por el mundo. Si bien estabilizar –ya  no digamos eliminar– la emisión de gases de invernadero a la atmósfera luce problemático, los especialistas urgen a tomar medidas para aminorar su crecimiento, dada la emergencia climática por la que atraviesa el planeta. En particular es necesario sustituir paulatinamente los combustibles fósiles por otras fuentes de energía no contaminantes. Entre las energías alternativas disponibles para sustituir a los combustibles fósiles se encuentran la hidroeléctrica, la eólica, la nuclear, la geotérmica, la biomasa y la solar. No todas estas fuentes de energía, sin embargo, tienen el mismo potencial. Lejos de esto, la energía solar, por su volumen, es con mucho la dominante. Por ejemplo, Richard Pérez de la Universidad Estatal de Nueva York en Albany, estima que el Sol tiene el potencial para proporcionar una cantidad de energía 1500 veces más grande que toda la energía que actualmente consume el mundo. En contraste, con la excepción de la energía eólica, ninguna de las otras fuentes de energía disponibles podría por sí misma satisfacer nuestras necesidades energéticas a nivel global. Además de lo anterior, y al margen del problema de la contaminación atmosférica, los combustibles fósiles constituyen una fuente de energía no renovable que eventualmente llegará a su fin y que tarde o temprano tendremos que sustituir por fuentes renovables. A este respecto, Richard Pérez estima que las reservas existentes de petróleo, si se consumieran en ausencia de cualquier otro energético, son equivalentes a 15 años de energía al ritmo actual de consumo global. Los números correspondientes para el gas natural y el carbón son 13 y 54 años, de manera respectiva. En lo que se refiere a la energía nuclear –que si bien no produce gases de invernadero, sí es altamente contaminante del medio ambiente– las reservas de uranio existentes equivalen a un máximo de 19 años de consumo global de energía.De acuerdo a lo anterior, el aprovechamiento de una parte minúscula de la energía del Sol podría satisfacer por completo nuestras necesidades energéticas presentes y muy probablemente futuras. Así, en la medida en que se desarrollen tecnologías más y más sofisticadas para atrapar y manipular la radiación del Sol, cabe esperar que el mundo del futuro dependa cada vez más de la energía solar. ¿Cuál es la tecnología solar del futuro? Con las tecnologías del presente podemos aprovechar la energía del Sol de diferentes maneras. Podemos, por ejemplo, usarla para calentar agua para consumo doméstico –una aplicación que está cada vez más extendida–, o bien para generar energía eléctrica en centrales termo-solares. Otra opción es la que nos ofrecen los paneles solares o fotovoltaicos, que transforman directamente la energía del Sol en energía eléctrica. Esta opción es muy atractiva pues, una vez instalados, dichos paneles nos entregan energía gratis sin generar contaminación. Trabajan, además, de manera silenciosa y sin partes móviles. Una opción más nos la ejemplifican las plantas verdes, que son capaces absorber y hacer uso de la radiación solar para reproducirse y sintetizar material orgánico, empleando para ello el proceso de fotosíntesis. Los paneles fotovoltaicos y la fotosíntesis constituyen dos opciones solares con características propias en cierto modo opuestas. Por un lado, los primeros no son por sí mismos capaces de almacenar la energía que producen, la cual tiene que ser consumida al momento. Las plantas, por el contrario, almacenan la energía que extraen del Sol en forma de energía química en el material orgánico que sintetizan.Es interesante comparar la eficiencia para aprovechar la energía del Sol de estas dos opciones, producto una de la Naturaleza y la otra de un desarrollo tecnológico reciente. Podría uno quizá pensar que las plantas, resultado de miles de millones de años de evolución, serían más eficientes que los módulos solares. Lejos de esto, un estudio publicado en la revista “Science” por un grupo interdisciplinario de científicos concluye que los módulos solares son de hecho diez veces más eficientes que las plantas en cuanto a aprovechar la energía del Sol. Así, por esta vez habremos superado a la Naturaleza.Aunque habría que reconocer que posiblemente ésta nunca pretendió participar en una competencia para fabricar el dispositivo solar más eficiente y que la eficiencia energética no era una de sus prioridades. Como si lo de debe ser para nosotros, si hemos de superar nuestros apuros climáticos.",
    "De acuerdo con la “National Oceanic and Atmospheric Adminstration” (NOAA) de los Estados Unidos, el año 2012 fue a nivel global el décimo año más caliente desde 1880, año este último a partir del cual se tienen registros anuales de temperatura que abarcan todo el planeta. 2012 fue en promedio, según la NOAA, 0.57 grados centígrados más caliente con respecto al promedio de temperatura global a lo largo del Siglo XX. Esto, consideran los expertos, es indicativo del calentamiento global que actualmente sufre la Tierra y que ocasiona eventos climáticos extremos, incluyendo sequías e inundaciones, lo mismo que episodios de frío y calor intensos.  Tomado de manera aislada, el que un año cualquiera muestre una anomalía de temperaturas no es ciertamente una prueba de un cambio climático global, pues el clima sufre continuamente variaciones –en uno u otro sentido– de manera natural. No obstante, el cambio observado en 2012 se alinea con la tendencia registrada desde 1976, cuando de manera sistemática –por 36 años en sucesión– la temperatura media anual ha sido superior al promedio.En efecto, según datos publicados por la NOAA en su página de internet, mientras que anteriormente a 1976 las temperaturas anuales promedio a nivel global variaban al azar con respecto al promedio del siglo XX, algunas veces siendo mayores y otras veces menores, las temperaturas de todos los años a partir de 1976 han sido, en contraste, siempre mayores. Aun más, los doce años del presente siglo están entre los 14 años más calientes desde 1880, y en este respecto el año que tiene el récord es 2010 –seguido de 2005–, con una temperatura 0.66 grados centígrados superior al promedio.De acuerdo con los expertos, el cambio climático global es producto del incremento en la concentración de gases de invernadero en la atmósfera. Mediciones de la NASA indican que actualmente la concentración atmosférica de bióxido de carbono –el principal gas de invernadero– alcanza las 390 partes por millón, mientras que en 1960 era de 315 partes por millón y en 1880 de solamente 285 partes por millón. De este modo, de 1880 a la fecha dicha concentración se incrementó en un 37%, con el consecuente calentamiento del planeta.Actualmente los niveles de emisión de bióxido de carbono a la atmósfera están a la alza y de no tomarse medidas para estabilizarla se calcula que podrían duplicarse en 50 años. La pregunta es si es posible lograr dicha estabilización. En un artículo publicado en 2004 en la revista “Science” por  R. Pacala y R. Socolow de la Universidad de Princeton, en los Estados Unidos, se arguye que una nivelación del crecimiento de las emisiones de bióxido de carbono puede darse con la tecnología existente. Esto estabilizaría la concentración de este gas a un valor inferior al doble de su nivel pre-industrial. Para esto, Pacala y Socolow proponen atacar el problema en siete frentes, cada uno de los cuales reducirá la emisión de bióxido de carbono por una misma cantidad hasta estabilizarla. Estas acciones incluyen: medidas de ahorro de energía, el desarrollo de plantas generadoras de electricidad a base de carbón –plantas carbo-eléctricas– dos veces más eficientes que las actuales, el reemplazo de 1400 plantas carbo-eléctricas por plantas que funcionen a base de gas natural, el desarrollo de automóviles con motores dos veces más eficientes, y la captura y almacenamiento del bióxido de carbono producido en las plantas carbo-eléctricas. Como podemos ver, muchas de estas medidas están dirigidas hacia las plantas generadoras de electricidad a base de carbón, que se encuentran entre los principales culpables de los apuros por los que está atravesando nuestro planeta.Las medidas propuestas para estabilizar la emisión de gases de invernadero incluyen igualmente: doblar la capacidad de generación de energía eléctrica de origen nuclear, incrementar por un factor de 10 la generación de electricidad eólica construyendo 2 millones de molinos de viento, aumentar 100 veces la generación de energía eléctrica por medios solares, y multiplicar por 12 la producción de alcohol combustible a partir del maíz o la caña de azúcar. Esto último requeriría de un dieciseisavo de toda la tierra cultivable disponible en el mundo.No todos los expertos, sin embargo, están de acuerdo en que la solución sea tan “simple” como lo plantean Pacala y Socolow. En una discusión organizada esta semana por la revista “Science” para abordar el problema, en la que participó Pacala, queda claro que el planteamiento hecho hace ocho años es ya obsoleto –puesto que no se tomaron en esos momentos medidas correctivas para disminuir la emisión de bióxido de carbono– y que en la actualidad serían necesarias más acciones que las originalmente enunciadas. Aun así, hay expertos que dudan que las medidas propuestas puedan llevar a una solución para el calentamiento global. En todo caso y por lo que se ve, no parece que en los próximos años vayamos a ser testigos de una estabilización en las emisiones de bióxido de carbono, con lo que el cambio climático seguirá probablemente con su curso actual. Dado el caso, no nos quedaría sino acostumbrarnos a sus inconveniencias.",
    "Hubo un tiempo en que las instalaciones físicas de la Universidad Autónoma de SanLuis Potosí, la más antigua en el Estado, se reducían en buena medida al Edificio Central. Hoy en día, si bien dichas instalaciones han crecido substancialmente y se extienden por toda la ciudad capital e incluso fuera de ella, el Edificio Central sigue siendo el símbolo de la Universidad. Es, además, un símbolo muy pertinente, acorde con la trascendencia que la Universidadha tenido y tiene en nuestro medio.En efecto, el Edificio Central es uno de los inmuebles más antiguosde la ciudad de San Luis Potosí y en su origen fue precisamente un centro educativo. Fue construido en el Siglo XVII como sede del Colegio de la Compañía de Jesús. En la actualidad, aunque su aspecto original cambió sustancialmente después de la remodelación que experimentó en 1874 –que modificó sufachaday piso superior–, en su interior aún podemos apreciarla construcción original de la planta baja, incluyendo su arcada alrededor del patio central.Después de la expulsión de los jesuitas en 1767, el Edificio Central tuvo una suerte azarosa que lo convirtió en cuartel en más de una ocasión. Retornó, no obstante, a su vocación original –aunque no necesariamente siempre con un mismo enfoque–, como sede sucesiva de:el Colegio Guadalupano Josefino en 1826, el Seminario Conciliar en 1855, el Instituto Científico y Literario en 1861 y, finalmente,la Universidad de San Luis Potosíel 10 de enero de 1923.Esta última conversión tiene relevancia nacional, pues significó el nacimiento de la primera universidad autónoma en MéxicoEl antecedente directo de la UASLP fue el Instituto Científico y Literario. En 1923, el Gobernador Rafael Nieto tuvo la iniciativa de reunir a este instituto con la Escuela Normal y el Hospital Civil para formar una sola institución autónoma que estuviera alejada de los vaivenes de la política estatal, con patrimonio y gobierno propios. Las consecuencias de esta iniciativa y las condiciones del entorno que la propiciaron están descritas en los libros: “Los primeros pasos de la autonomía universitaria en San Luis Potosí 1922-1924”,cuya autora es María Gabriela Torres Montero, y “El Instituto Científico y Literario de San Luis Potosí”,escrito porMaría Gabriela Torres Montero, Enrique Delgado López y Alejandro Gutiérrez Hernández. Ambos libros fueron publicados por la Editorial Universitaria Potosina.El entusiasmo quela educación superior despertaba en algunos sectores en nuestro país en la década de los años veinte no esciertamentealgo de lo que podamos sentirnos orgullosos los mexicanos. En San Luis Potosí en particular, en un discurso pronunciado en febrero de 1921, en el que hacía saber de sus planes de crear una universidad autónoma, el Gobernador Nieto afirmaba: “El Instituto Científico y Literario de San Luis Potosí, se ha conquistado en los últimos tiempos un gran número de enemigos. Muchos  de estos enemigos argumentan que la enseñanza superior y profesional es un lujo en la mezquindad de nuestro medio económico y social; que si no estamos en condiciones de atender ni siquiera medianamente la enseñanza elemental y primaria del Estado, es un absurdo gastar una buena porción de nuestro presupuesto en la formación de una aristocracia intelectual.” Añadía que los pobres recursos dedicados a la educación superior sólo pueden resultar en “profesionales mediocres que van a engrosar las filas del proletariado intelectual”.El Gobernador Nieto argumentaba como respuesta: “Es cierto que es una anomalía reprobable que tengamos un establecimiento profesional más o menos atendido, y que descuidemos en cambio, la enseñanza elemental y primaria. El remedio no está, sin embargo, en suprimir lo bueno sino en corregir lo malo”.Es en el contexto anterior, de ninguna manera favorable para el desarrollo de la educación superior, que debemosapreciar el valor de la autonomía alcanzada por la UASLP hace 90 años. Hoy en día el Edificio Central de la UASLP –testigo del desarrollo educativo del Estado a lo largo de su historia, incluyendo el nacimiento de la autonomía universitaria en México–ya no alberga actividades docentes después de haberlo hecho –si bien con interrupciones– por más de tres siglos. En un pasado reciente, el Edificio Centralfue poco a poco abandonado porsus estudiantes. Primeramente yde manera paulatina por los estudiantes de las escuelas profesionales; en seguida,por los de secundaria y preparatoria. La última escuela en dejar el Edificio Central fue la Escuela de Física, al final de los años setenta. El que no albergue ya estudiantes no significa, por supuesto, que el Edificio Central haya perdido relevancia. Por el contrario, constituye el emblema más distintivo de la Universidad. Así, los eventos organizados el pasado 10 de enero por la UASLP y su Rector, arquitecto Manuel Fermín Villar Rubio, para conmemorarlos 90 años de autonomía, fueron culminados con un espléndido espectáculo de música y fuegos artificiales en sincronía. Lanzados estos últimos  –por supuesto– desde el techo del Edificio Central.",
    "Para aquellos interesados en viajar al espacio, la compañía Virgin Galactic ofrece lugares en su nave SpaceShip2 de seis asientos por el módico precio de 200,000 dólares por persona en viaje redondo. Si la intención es rentar el SpaceShip2 para un vuelo “Charter”, el costo resulta ser aún más bajo: un millón de dólares en total, es decir, seis boletos por el precio de cinco. Los vuelos de Virgin Galactic –a iniciar en una fecha todavía no determinada– tendrán una duración de 2 horas y alcanzarán una altura de 110 kilómetros. Serán, además, vuelos sub-orbitales, de modo que subirán y bajarán sin dar una vuelta completa a la Tierra. A pesar de esto, los afortunados pasajeros del  SpaceShip2 tendrán la oportunidad de experimentar la ingravidez durante cinco minutos y de comprobar con sus propios ojos que nuestro planeta es, efectivamente, redondo,  tal como nos lo enseñan los libros.Por otro lado, si bien una altura de 110 kilómetros es considerable –la altitud de crucero de un avión jet comercial es de 10 kilómetros–, el viaje de Virgin Galactic no nos lleva en realidad fuera de nuestro planeta, aunque ciertamente sí a un lugar de difícil acceso. Otro sería el caso de un viaje a la Luna, a Marte o a algún otro planeta o asteroide de nuestro sistema solar que están a distancias considerablemente mayores. En efecto, tenemos que la Luna se encuentra a una distancia media de 384,000 kilómetros de la Tierra y que un viaje de ida y vuelta hasta allá requiere de una semana, en contraste con las dos horas del vuelo de Virgin Galactic. La Luna, por su lado, está relativamente cerca en comparación con Marte, que se encuentra a una distancia media de 225 millones de kilómetros y se requieren más de seis meses para alcanzarlo.Por lo demás, las complicaciones para un viaje espacial se incrementan desproporcionalmente con el camino a recorrer. Esto se traduce en costos. Por ejemplo, cuando se ofrecieron viajes turísticos a la Estación Espacial Internacional  –en  órbita a unos 400 kilómetros de altura– tuvieron precios alrededor de los veinte millones de dólares –pasaje de ida y vuelta, y gastos de alojamiento y alimentos incluidos. Aunque no ha habido hasta ahora ningún viaje tripulado a Marte, la NASA tiene planes para realizar uno en algo así como 20 años. Un empresario holandés tiene igualmente planes para enviar una misión al planeta rojo con financiamiento privado. Las dificultades técnicas que hay que superar para llevar a cabo estos planes, no obstante, son mayúsculas. Uno de los problemas mayores a sortear son las radiaciones cósmicas que permean el espacio profundo. Estas radiaciones están constituidas por átomos que viajan a grandes velocidades impulsados durante la explosión de estrellas, las cuales se sabe pueden provocar cáncer, entre otras enfermedades. Así, si los astronautas han de permanecer por largo tiempo en el espacio profundo, la nave espacial tiene que ser blindada de las radiaciones cósmicas. Para complicar las cosas, en un artículo publicado esta semana en la revista PLOS ONE por investigadores médicos de la Universidad de Rochester en los Estados Unidos, se afirma que la radiación cósmica a la que estarían expuestos los astronautas durante los  tres años que duraría la misión a Marte podría acelerar el inicio del mal de Alzheimer. Esta conclusión fue obtenida mediante el estudio de ratones que fueron expuestos a una dosis de radiación de átomos de fierro equivalente a la sufrirían los astronautas en su viaje rumbo al planeta vecino. Encontraron que los ratones irradiados mostraban alteraciones neurológicas indicativas del mal de Alzheimer. Aún más, según Kerry O´Banion, autor principal del artículo de referencia, los átomos de fierro de la radiación cósmica son tan energéticos que sería necesario un blindaje de dos metros de espesor de plomo o concreto para bloquearlos de manera efectiva, lo que se antoja de difícil realización.  Si bien la radiación cósmica llena todo el espacio profundo, aquí en nuestro hábitat y en su vecindad inmediata estamos protegidos de su influencia por el campo magnético de la Tierra. De este modo, por más que los turistas de Virgin Galatic estén expuestos en su viaje al espacio a un medio peligroso, dicho medio no es a fin de cuentas demasiado diferente al que tenemos en la superficie de nuestro planeta en cuanto a las condiciones para la vida. Así, no sería sorprendente que, tal como algunos auguran, la era del turismo espacial  a lugares no demasiado alejados de nuestro planeta–para los adinerados, por supuesto– ya está aquí para quedarse.No es el caso de los viajeros del espacio profundo que sí encontrarán un medio ambiente tremendamente hostil hacia la vida tal como la conocemos. No esperaríamos entonces que por un buen tiempo se den viajes tripulados a Marte, ya no digamos grupos de vacacionistas en plan de veraneo en el planeta rojo. Esto aun sin tomar en cuenta los problemas económicos por los que atraviesa el mundo que nos indican que, por más planes que se hagan, en cuanto a viajes por el espacio profundo, el horno no está para bollos.",
    "¿Cuál es el mejor remedio para una cruda? Para algunos, ésta debe ser, sin duda, una pregunta de gran relevancia en estos días de fiestas decembrinas. Esta pregunta, por otro lado, no está huérfana de respuestas. Por el contrario, al igual que en el caso de las dietas para bajar de peso, existen numerosas recetas para aliviar las consecuencias de la fiesta de la noche anterior. Entre los remedios más socorridos en nuestro país se encuentran: un plato de menudo, unos chilaquiles bien picosos, y una sopa de camarón igualmente picosa. Una búsqueda rápida en internet nos arroja otras opciones: el jugo de naranja o de tomate, la miel con limón, una comida ligera –pan tostado–, o grasosa –sándwich de tocino–, la aspirina, el Alka Seltzer, el jengibre y el café, por mencionar sólo algunas pocas recomendaciones. Aunque de poco valor práctico, en internet se encuentran también recetas exóticas: ciruelas en escabeche, te de excremento de conejo o de cuerno de rinoceronte, ojos de borrego en conserva mezclados con jugo de tomate, y canarios fritos. Otro grupo de recetas están basadas en la premisa según la cual, para no sufrir una cruda o resaca no hay que dejarla llegar, combatiéndola con más alcohol. Los especialistas, sin embargo, desaconsejan esta práctica. Por lo demás, a menos que se tenga la intención de permanecer borracho de manera indefinida, la resaca llegaría más tarde o más temprano.   Las recetas anteriores son para cuando la cruda ya ha llegado. Para prevenirla, como primera opción se recomienda abstenerse de beber alcohol –lo que indudablemente resulta inoperante en un buen número de casos–. Otras recomendaciones menos drásticas son: beber con el estomago lleno, hacerlo lentamente –un trago por hora– a fin de que el hígado alcance a procesar el alcohol ingerido, comer un plátano antes de empezar a beber, o tomar un vaso de agua entre trago y trago. A pesar de la gran cantidad de remedios para aliviar la resaca que podemos encontrar a nuestro alrededor, en un artículo publicado en 2005 en la revista “British Medical Journal” por un grupo de investigadores de la Universidades de Exeter y Plymouth en Gran Bretaña, encabezados por Max Pittler, se concluye que no existen evidencias convincentes de que alguna de ellas sea realmente efectiva. En dicho artículo se hace un estudio de la literatura científica existente sobre las propiedades anti-cruda de diferentes sustancias y se encuentra que si bien hay indicaciones que algunos productos pudieran ser efectivos –como el aceite gama linoléico extraído de la borraja–, es necesario llevar a cabo más investigaciones para confirmarlo.En relación a este último punto, un estudio publicado por científicos coreanos en el año  2009 en la revista “Journal of Food Science”, encuentra que los espárragos son un medio efectivo para las crudas. Los investigadores encontraron que los aminoácidos y minerales que contienen estas plantas ayudan a proteger al hígado de los efectos del alcohol, al mismo tiempo que aceleran su metabolización. Además, encontraron que la concentración de estas sustancias es mayor en las hojas que en el tallo de los espárragos, que normalmente son desechadas.De este modo, a la comida tradicional de fin de año el próximo 31 de diciembre habría que añadir los espárragos, con la esperanza de que trabajen a favor de aquellos que beben a razón de más de un trago por hora, y puedan así disfrutar, en condiciones razonables, del primer día del año.    Por otro lado, si bien una cruda de principio de año afecta a la economía de manera solamente relativa por ser el 1 de enero un día feriado, las múltiples resacas a lo largo del año tienen efectos apreciables sobre la misma. En este respecto, Pittler y colaboradores escriben que el abuso del alcohol le cuesta anualmente a la economía estadounidense entre 12,000 y 30,000 millones de dólares. Encontrar una cura efectiva para las crudas –como alegan haberlo hecho los investigadores coreanos– tiene entonces una importancia económica considerable.Pittler y colaboradores se refieren también a los aspectos éticos asociados al desarrollo de remedios para la resaca, por la posibilidad de que esto pueda incentivar el consumo de alcohol. Consideran, no obstante, que existe poca evidencia de que los efectos de las crudas, por más devastadores que sean, inhiban el consumo de alcohol.Esto último es sin duda cierto, al menos en una mayoría de casos. Tanto así que al muy conocido dicho “Dios mío, si en la borrachera te ofendo, con la cruda me sales debiendo”, lo tomamos de una manera más que festiva.",
    "No es infrecuente que la causa de la muerte de personajes famosos esté envuelta en controversias o especulaciones; particularmente por la sospecha de que pudieran haber sido víctimas de un crimen. Este, por ejemplo, es el caso de Alejandro Magno, que murió en Babilonia a un mes de cumplir 33 años y de quien se ha especulado fue envenenado. Es igualmente el caso de Napoleón Bonaparte, muerto en el exilio en la isla de Santa Elena a la edad de 51 años. En relación a este último personaje, si bien el reporte de la autopsia que se le practicó poco después de morir indica que falleció por cáncer gástrico, hubo quien afirmara que igualmente fue envenenado. Esta aseveración se apoyó en los altos niveles de arsénico que fueron encontrados en cabellos que supuestamente pertenecían a Napoleón.La muerte del faraón Tutankamon, que reinó en Egipto hace más de tres mil años, ha sido también motivo de especulación. Tutankamon es el faraón más famoso a pesar de haber muerto muy joven –antes de cumplir los 20 años–. Esto debido a los tesoros encontrados en su tumba que fue descubierta intacta en 1922. Así, en el caso de Tutankamon crece la fascinación que comúnmente despiertan los estudios forenses de personajes famosos, por el gran interés que produce todo lo referente al Antiguo Egipto.  La investigación de la causa de la muerte de un personaje famoso ocurrida cientos o miles de años es ciertamente un ejercicio fascinante. Este ejercicio, haciendo acopio de las técnicas de análisis modernas, ha producido en algunos casos resultados más o menos concluyentes. Este es el caso, por ejemplo, de Napoleón quien hoy sabemos muy probablemente  –y después de todo– sí murió de cáncer gástrico como se consigna en su autopsia. Igualmente, es muy probable que Tutankamon, con todo y su corta edad, haya muerto de causas naturales y no asesinado como se especuló. Un trabajo más de investigación forense exitosa, a miles de años de distancia del suceso criminal –que fue comentado ampliamente en días pasados por los medios masivos de comunicación–, apareció publicado esta semana en la revista “British Medical Journal”. Dicho trabajo, llevado a cabo por investigadores egipcios y europeos encabezados por Zahi Hawass del Consejo Supremo de Antigüedades  de Egipto, se refiere al asesinato del faraón Ramsés III como resultado de la llamada “conjura del harén”.   Existe información histórica que Ramsés III, quién reinó en Egipto durante los años 1186-1155 a. C., fue víctima de una conspiración para asesinarlo. Dicha conspiración fue llevada a cabo por personas cercanas a faraón. Entre los principales conjurados de encontraba Tye, una esposa secundaria del faraón, y Pentaware, el hijo de ésta. La razón para el complot habría sido el descontento de Tye por la intención de Ramsés III de desconocer a Pentaware como su sucesor. La existencia de la conjura del harén se desprende del llamado Papiro Judicial de Turín.    De acuerdo con Hawass y colaboradores, no obstante, de dicho documento no es claro si la conjura tuvo éxito y terminó con la muerte de Ramsés III o bien si el faraón sobrevivió al atentado y murió poco después. Así, con el objeto de aclarar este punto, los investigadores llevaron a cabo una serie de estudios antropológicos, forenses, radiológicos y genéticos con la momia del faraón, así como con otra momia no identificada, pero que se presume es la de Pentaware.  Por medio de imágenes tomográficas se encontró que la momia de Ramsés III presenta una profunda cortada en la garganta, la cual no se había descubierto hasta ahora por la gruesa capa de vendas que le cubren el cuello. Según Hawass y colaboradores, esta lesión, que seccionó la tráquea y el esófago y llegó hasta la columna vertebral, es lo suficientemente severa para haber causado la muerte inmediata del faraón. Si bien no descartan que la lesión hubiera sido producida durante del proceso de embalsamamiento del  cadáver, consideran que esto es improbable pues algo así no ha sido encontrado en otra momia egipcia. Se confirmaría así la conjura del harén relatada en el papiro de Turín, aportando, además, evidencias de que Ramsés III murió en el atentado.Se encontró, por otro lado, que la supuesta momia de Pentaware corresponde a un sujeto que murió a la edad de 18-20 años, lo cual es consistente con la hipótesis de que se trata del hijo de Ramsés III. Presenta además signos de estrangulamiento, lo que sugiere que tuvo una muerte violenta. Así las cosas, con el objeto de determinar la filiación genética de ambas momias, se les practicaron pruebas de ADN, encontrando que ambos están efectivamente emparentados y que probablemente se trate de padre e hijo. El misterio que rodea a los últimos días de Ramsés III queda así resuelto con un buen margen de certidumbre, más de tres mil años después de su muerte. Al mismo tiempo, no obstante, se abre otra interrogante, relativa a las circunstancias que rodearon la muerte de Pentaware, otro de los actores principales de la conjura del harén.En todo caso, la solución dada al caso de la muerte de Ramsés III ejemplifica las capacidades que ha alcanzado la ciencia forense. Así, y ya que “quien puede lo más puede lo menos”, podríamos esperar que pocos crímenes cometidos en la actualidad queden sin aclararse.",
    "Hace cien años el Imperio Británico disfrutaba de su máximo esplendor y se extendía por los cinco continentes, desde Canadá hasta Australia, pasando por Sudáfrica y la India, entre muchos otros territorios. Inglaterra había sido, además, la cuna de la Revolución Industrial y uno de los centros en donde se desarrolló la revolución científica en el Siglo XVII, con Isaac Newton como figura emblemática. Dos siglos después, en pleno auge del Imperio Británico, Charles Darwin provocó una nueva revolución intelectual con la publicación en 1859 de su obra El Origen de las Especies. Con estos y numerosos otros antecedentes de por medio, es quizá justificable, o al menos entendible, que los ingleses se hayan sentido hace cien años como el centro del mundo. En este contexto, ocurrió hace un siglo en Inglaterra un curioso acontecimiento que se relata en lo que sigue. El 18 de octubre de 1912, el reconocido paleontólogo británico Arthur Smith Woodward, juntamente con Charles Dawson, también paleontólogo pero aficionado, dieron a conocer a la Sociedad Geológica de Londres el descubrimiento de restos fósiles de un supuesto lejano antecesor nuestro, mitad humano y mitad simio. Dichos fósiles, que fueron posteriormente conocidos como El Hombre de Piltdown, por haber sido excavados en una cantera del pueblo de Piltdown, en el sur de Inglaterra, consistían de algunos fragmentos de cráneo y una mandíbula con dos dientes.Los fósiles de Plitdown poseían características peculiares. En efecto, mientras que el cráneo parecía tener un volumen similar al del hombre moderno, la mandíbula y los dientes eran más cercanos a los de un simio. Esto se ajustaba a las creencias –prejuicios–  en boga, según las cuales durante la evolución humana el cerebro creció primeramente en tamaño, lo que a su vez habría llevado a una evolución de las otras características que nos distinguen de los simios –ahora se sabe que la evolución procedió justamente al contrario.El anuncio de Woodward y Dawson fue aceptado por la comunidad científica británica sin demasiada reticencia, parcialmente porque estaba de acuerdo con los prejuicios de los expertos, pero también por otros factores. En referencia a esto último, un artículo publicado en febrero del presente año en el periódico británico “The Guardian” hace notar que en la época en la que surgieron los fósiles de Piltdown, los franceses habían ya descubierto los fósiles Cro-Magnon, mientras que los alemanes habían hecho lo propio con los Neandertal y Heidelberg. Con el descubrimiento de Dawson, prosigue “The Guardian”, los ingleses tenían ahora su propio fósil, lo que, podríamos añadir, reivindicaba su posición como primer imperio del mundo. Además, como la tierra que vio nacer a Charles Darwin, Inglaterra debía haber jugado un papel relevante en la evolución de la especie humana.       El caso es que el Hombre de Piltdown resultó ser un fraude y los pedazos de cráneo en realidad pertenecían a un hombre moderno, mientras que la mandíbula y los dientes eran de un orangután. Para consumar el engaño, cráneo y mandíbula fueron teñidos de color café, el mismo color de la cantera en donde supuestamente fueron encontrados. El fraude fue cuidadosamente planeado y más cuidadosamente ejecutado; tanto, que se requirieron 40 años para ponerlo en claro y no hay duda que fue perpetrado por expertos. Es todavía un misterio, sin embargo, quien o quienes fueron los autores. Un artículo publicado esta semana en la revista británica  “Nature” por Chris Stringel del Museo de Historia Natural de Londres da algunas pistas al respecto y lista a cuatro sospechosos. Entre éstos se encuentra en primer lugar Dawson, quien tendría como motivo para realizar el fraude el deseo de ser reconocido científicamente. Stringer considera igualmente como sospechoso a Woodward y a Martín Hinton, quien fuera ayudante de Woodward. Hinton es sospechoso pues, ya muerto, entre sus pertenencias se encontraron huesos y dientes teñidos de manera similar a los huesos de Piltdown. Un cuarto sospechoso es Teilhard de Chardin, quién en su juventud ayudó a Dawson en sus excavaciones y aportó un tercer diente al Hombre de Piltdown.   Stringel está actualmente participando en un esfuerzo por aclarar el misterio del Piltdown y para este propósito están sometiendo los restos fósiles a una serie de pruebas con instrumentos analíticos cuyas capacidades ampliamente superan las de los instrumentos existentes un siglo atrás. Es interesante notar, sin embargo, que de acuerdo a expertos, aun con las técnicas analíticas disponibles hace cien años el fraude podría haber sido fácilmente detectado. En particular, podría haber quedado claro que los fósiles habían sido artificialmente teñidos. Si no se les practicaron las pruebas necesarias fue por razones de prejuicio científico o bien de nacionalismo extremo.En todo caso, aun a destiempo, esperemos que el misterio de Piltdown se aclare en los próximos meses.",
    "Los rinocerontes son animales fascinantes por varias razones. Primeramente por su tamaño, ya que después del elefante los rinocerontes son los animales terrestres más grandes que conocemos. También por su cuerpo voluminoso, igual que por su cuerno o cuernos arriba de la nariz y por su supuesto mal humor, que los haría embestir a ciegas en caso de ser provocados. No podríamos tampoco dejar de mencionar su piel áspera y arrugada, con grandes pliegues en los rinocerontes asiáticos que les dan un aspecto acorazado.Estas dos últimas características –mal humor y piel arrugada– motivaron el cuento infantil de Rudyard Kipling que lleva por título “Cómo fue que los rinocerontes se hicieron de su piel”. En este cuento, el rinoceronte protagonista roba un pastel ante la mirada del dueño, quien poco puede hacer dado lo formidable del ladrón. Poco después, no obstante, la víctima tuvo oportunidad de desquitarse: cuando el rinoceronte se despojó de la piel a fin de tomar un baño en el mar, le vació una gran cantidad de migajas en el interior de la misma. Desde entonces el rinoceronte  sufre de una comezón crónica que lo hace rascarse de manera permanente. Para esto se frota violentamente contra árboles o contra el piso. Esta costumbre ha hecho que su piel se le haya arrugado y adquirido los pliegues característicos. Y como ni así ha logrado aliviar la comezón, el rinoceronte siempre está de mal humor. Lo anterior es, por supuesto, una fantasía. Hay, no obstante, un número de historias reales en las que están involucrados los rinocerontes. Una de éstas se refiere al rinoceronte asiático –de unos tres años de edad– que fue donado al Zoológico de Dublín por el ministro de finanzas de la India en 1864 y el cual tuvo un final trágico. En efecto, según se consigna en un artículo publicado el pasado mes de octubre en la revista “Proceedings of the Royal Irish Academy”, el rinoceronte nunca se adaptó a su nueva casa y siempre estaba “malhumorado” y comía poco. Así las cosas, murió en menos de un año.El esqueleto y la piel del rinoceronte fueron preservados y actualmente se encuentran montados en el Museo Zoológico del Trinity College en Dublín. El tratamiento que se le dio a la piel del animal, no obstante, fue tan equivocado que recientemente surgieron dudas sobre si se trata de un rinoceronte de la India –como se había supuesto– o bien de un rinoceronte de Java. Ambas especies difieren morfológicamente, en particular por los dobleces de la piel. Ésta, sin embargo, fue estirada a tal grado cuando fue montada sobre el esqueleto que perdió los dobleces. Afortunadamente un análisis forense empleando técnicas de ADN identificó sin ambigüedad que sí se trata de un rinoceronte de la India.  Otra historia de rinocerontes –ocurrida hace millones de años– es referida en un artículo publicado este 21 de noviembre en la revista en línea PLOS ONE por investigadores de Francia, Bélgica y Turquía. De acuerdo con dicho artículo, hace 9.2 millones de años un rinoceronte adulto de unos 10-15 años fue atrapado por la lava de la erupción del volcán Cardak en Turquía, muriendo de manera instantánea. Por la alta temperatura –400-450 grados centígrados– a la que estuvo expuesto, el cadáver del rinoceronte se desecó rápidamente. Posteriormente, el cuerpo fue desmembrado por el flujo de lava, separando la cabeza del cuerpo, la cual fue horneada a unos 400 grados centígrados. Los investigadores encontraron el cráneo del rinoceronte articulado con la mandíbula a unos 30 kilómetros del cráter. Lo notable del caso es que con un ejercicio forense, proyectado millones de años hacia el pasado, pudieron determinar las circunstancias de su muerte.  Las historias más impactantes sobre rinocerontes, sin embargo, son las que actualmente están ocurriendo y que tienen que ver con la caza ilegal de estos animales. En efecto, desde hace unos cinco años se ha incrementado de manera significativa el número de animales muertos por cazadores furtivos. Esto debido a que existe una gran demanda de cuernos de rinoceronte en Asia por sus supuestas propiedades curativas, en particular contra el cáncer.La demanda, impulsada por la bonanza económica asiática, ha incrementado el precio de los cuernos de rinoceronte, que por peso valen más que el oro. Como consecuencia, mientras que en el periodo de seis años entre 2000 y 2005 se cazaron en Sudáfrica –en donde vive la mayor parte de los rinocerontes del mundo– un total de 252 rinocerontes, en lo que va del presente año el número correspondiente es 455.   De continuar esta tendencia los expertos consideran que pudiéramos ser testigos de la extinción de los rinocerontes en algunos años. Hay que hacer notar que la especie más abundante es el llamado rinoceronte blanco, del cual se estima viven alrededor de 20,000 individuos. En el otro extremo, del rinoceronte de Java sólo quedan algunas pocas decenas de ejemplares.  Tal parece que los rinocerontes están indefensos, pagando culpas que no deben. Ciertamente, el rinoceronte de Kipling merece por sus acciones el castigo de la comezón eterna. El grueso de los rinocerontes, en contraste, no se ha ganado de ninguna manera el trato que les damos. Además, consideraciones morales aparte, el mundo perdería mucho con su extinción.",
    "Hacia el final del primer milenio de nuestra Era, las ciudades del periodo clásico  Maya sufrieron un colapso que algunos expertos han asociado a la ocurrencia de episodios prolongados de sequía severa. Esta hipótesis ha sido confirmada por un artículo aparecido esta semana en la revista “Science”, publicado por un grupo internacional de investigadores encabezado por Douglas Kennett de la Universidad Estatal de Pensilvania. En dicho artículo se presenta un recuento detallado de los periodos de sequía y humedad que ocurrieron a lo largo de 2000 años en la región de los mayas y se muestra como dichos periodos tienen una correspondencia con acontecimientos del mundo maya, incluyendo el colapso de sus instituciones y ciudades hace unos mil años.  Determinar si un cierto periodo miles de años en el pasado fue seco o húmedo no parece ser una empresa sencilla. Y no lo es, efectivamente, de modo que cabe preguntarse cómo fue que lo lograron Kennett y colaboradores. La respuesta es que lo hicieron por medio de una estalagmita de 56 centímetros de largo encontrada en la cueva Yok Balum en Belice, no lejos de la ciudad maya de Tikal, en la región del Petén en Guatemala.  Se sabe que una estalagmita se forma sobre el piso de una cueva al gotear agua con minerales disueltos desde la bóveda de dicha cueva. Su proceso de formación es, por otro lado, muy lento. Así, los últimos 45 centímetros de estalagmita estudiada por Kennet y colaboradores tardaron 2000 años en depositarse.   Esto último fue determinado midiendo las concentraciones de isótopos radiactivos de Uranio y Torio a lo largo de la estalagmita que se sabe proporcionan una medida del tiempo que ha transcurrido desde su depósito. De este modo, fue posible datar la estalagmita de Yok Balum a lo largo de su longitud midiendo las correspondientes concentraciones de isótopos. Una vez que se contó con esta información, los periodos de humedad y sequía a través del tiempo fueron determinados midiendo en la estalagmita –a lo largo de su longitud– la concentración de un cierto isótopo del oxígeno. Dicho isótopo está relacionado con las condiciones climáticas que prevalecieron en la cueva Yok Balum en el momento de la formación de la estalagmita. Como resultado, los investigadores pudieron determinar con gran precisión cuando ocurrieron periodos de humedad y sequía extrema en un periodo de 2000 años, periodo que comprendió el ascenso y el colapso de la civilización maya clásica. El conocimiento preciso de los cambios climáticos a través del tiempo permitió a Kennett y colaboradores correlacionar dichos cambios con acontecimientos –de diversa índole– consignados en las estelas mayas. Así, encontraron que un periodo de humedad excepcional entre los años 440 y 660 de nuestra Era, trajo como consecuencia un florecimiento de la civilización maya y un crecimiento de la población de sus ciudades. Al periodo húmedo, no obstante, le siguió uno de sequía gradual entre los años 660 y 1000. Este periodo –y dado el crecimiento de población de los años favorables, que no habría resultado sustentable en las nuevas condiciones– trajo guerras entre ciudades, menor productividad agrícola y turbulencias políticas, que habrían llevado a la eventual desintegración de las ciudades mayas del periodo clásico. Un posterior episodio de sequía severa entre los años 1000 y 1100 –ya en el periodo denominado posclásico–habría dado la puntilla a la época de mayor esplendor de la civilización maya.  La severidad de las consecuencias de las sequías en México es ilustrada por aquella ocurrida entre 1535 y 1575, que afectó la península de Yucatán. Esta sequía, que aparece en el registro de la estalagmita de Yok Balum, produjo cerca de medio millón de muertes en 1535. Esto, según cifras mencionadas por Kennet y colaboradores. De acuerdo con estos autores, el colapso del la civilización maya clásica apuntaría a un crecimiento poblacional que resultó no sustentable en la medida en que cambiaron las condiciones que inicialmente lo propiciaron. Las ciudades mayas clásicas habrían sido así víctimas de su propio éxito inicial.",
    "Como se comentó en este espacio hace una semana, el pasado mes de octubre, seis científicos y un funcionario del Departamento de Protección Civil de Italia, miembros de la comisión de prevención de riesgos de ese país, fueron condenados a seis años de prisión. Esto, por su actuación previa al sismo de la madrugada del 6 de abril de 2009 que destruyó la ciudad de L´Aquila en el centro de la península italiana. La causa de la condena fue no haber alertado de manera adecuada a la población sobre los riesgos que enfrentaba por un posible sismo de gran magnitud en los días por venir. En los meses previos al sismo del 6 de abril, L´Aquila fue sacudida por numerosos temblores de baja intensidad que muchos creyeron anticipaban un sismo mayor. En estas circunstancias, el gobierno italiano reunió a la comisión de riesgos para que evaluara la situación y emitiera una declaración. En la reunión de dicha comisión,  celebrada seis días antes de la ocurrencia del temblor, los científicos afirmaron que, pese a los numerosos movimientos de tierra que se habían presentado en los meses previos, no existía una probabilidad particularmente alta de que fuera a ocurrir un sismo importante, pero que esto de ninguna manera podía descartarse.La opinión de los expertos es que la ocurrencia de un sismo no puede predecirse científicamente, y en estas circunstancias a la comisión de riesgos sólo le quedó evaluar la probabilidad de que pudiera ocurrir uno de gran magnitud. En este respecto, L´Aquila está situada en la región de más alta actividad tectónica en Italia y, según la Wikipedia, ha experimentado unos diez sismos mayores –incluyendo el del 6 de abril de 2009– en los últimos 700 años. Los sismos de magnitud equivalente al del 6 de abril son entonces relativamente infrecuentes en esa región de Italia.Por otro lado, la serie de temblores que se presentaron en los  meses previos al 6 de abril  de 2009 no era indicativa de la inminencia de un sismo importante. Esto, según Giuseppe Grandori del Politécnico de Milán –citado por la revista “Nature” en un artículo publicado en septiembre de 2011– quien señala que una serie de temblores de mediana magnitud solamente ha anticipado un gran sismo en un 2% de los casos. Así, no habría efectivamente existido motivo especial de alarma previo al terremoto del 6 de abril.El caso es que, como lo atestiguaron sus habitantes, pocos días después de la reunión de la comisión de riesgos ocurrió un sismo en L´Aquila que destruyó la ciudad. Así, dicho sismo se sumó al 2% de casos en los que un gran terremoto sigue a una serie de temblores menores. La comisión de riesgos, por su parte, tuvo su terremoto propio y fue acusada de no haber comunicado de manera correcta a los habitantes del L´Aquila del riesgo que corrían. Los científicos condenados aducen que ellos hicieron lo correcto en función de la información que poseían en esos momentos. Palabras más, palabras menos, el abogado de uno de los acusados arguye lo siguiente. Asumiendo que un amigo le pidiera que le recomendara el medio más seguro para viajar, él le contestaría que es el avión, basando su respuesta en las estadísticas de los accidentes aéreos, que son pocos en comparación con los que sufren otros medios de transporte. Si, no obstante, el avión tuviera un accidente y murieran todos sus  pasajeros, su amigo habría corrido con mala suerte, pero esto no significaría que su consejo fue incorrecto.En contraste, algunos habitantes de L´Aquila que perdieron familiares en el sismo aducen que la comisión de riesgos los mal informó sobre el peligro que corrían, emitiendo una declaración en el sentido que la serie de temblores menores que se había presentado eran benéficos, pues ayudaban a reducir la probabilidad de un sismo mayor –esta declaración en realidad fue hecha por el funcionario del Departamento de Protección Civil también condenado, sin la presencia de los científicos expertos–. Por esta declaración, muchos habitantes de L´Aquila habrían permanecido en sus casas la noche del 5 de abril en lugar de mudarse a un lugar más seguro.En una declaración publicada por la revista “Nature”, un ciudadano de L´Aquila, que perdió a su esposa y a su hija de nueve años en el sismo, reprocha a la comisión de riesgos no haberle proporcionado toda la información necesaria para que hubiera tomado una decisión sobre permanecer o no en su casa la noche del sismo. En lugar de esto, recibió una información “adormecedora” que lo hizo tomar una decisión que resultó fatal.Estudios sobre la percepción que tenemos de los riesgos que afrontamos de manera cotidiana, muestran que la misma no está necesariamente basada en hechos objetivos, tales como estadísticas de accidentes o de desastres naturales. Así, para alguien que perdió a su familia en el sismo de L´Aquila, es posiblemente difícil entender que fue víctima de un fenómeno impredecible, que tenía una probabilidad muy pequeña de ocurrir.    El problema que enfrentan los científicos condenados por el sismo de L´Aquila, se originó porque no supieron comunicar adecuadamente los riesgos que representa un fenómeno con una probabilidad muy pequeña de ocurrir, pero de un gran poder destructor. Fueron, sin embargo, condenados de manera injusta, pues esto de ninguna manera resulta una empresa sencilla.",
    "La madrugada del 6 de abril de 2009, un terremoto de magnitud 6.3 en la escala de Richter devastó el pueblo medieval de L´Aquila en la región de Abruzzo en el centro de Italia, matando a 309 de sus habitantes y dejando sin casa a otros 65,000. El sismo de L´Aquila alcanzó notoriedad sobre todo porque, como parte de sus secuelas, seis distinguidos geofísicos italianos, además de Bernardo De Bernardinis, Subdirector del Departamento de Protección Civil, fueron sujetos a juicio acusados de homicidio imprudencial por no haber alertado adecuadamente a la población de L´Aquila sobre los riesgos sísmicos que enfrentaba. Después de un año de juicio, esta semana los acusados fueron condenados a seis años de cárcel. A muchos esta sentencia les pareció excesiva, incluyendo al fiscal que había solicitado solamente cuatro años de cárcel como castigo. Los sentenciados formaron parte de la comisión de protección civil que evaluó los riesgos sísmicos para la población de L´Aquila en los días anteriores al 6 de abril de 2009. En los meses previos a esta fecha, la región de Abruzzo fue sacudida por un gran número de temblores de pequeña magnitud que mantuvieron alarmada a la población. De acuerdo con un artículo aparecido en la revista “Nature” en septiembre de 2011, en enero de 2009 se sintieron en L´Aquila 69 temblores, en febrero fueron 70, 100 en marzo y 57 durante los primeros días de abril. En ese mismo artículo se hace un recuento de los acontecimientos anteriores y posteriores al terremoto del 6 de abril. Un actor clave para los mismos fue un geofísico aficionado de nombre Giampaolo Guliani, quién contribuyó grandemente a  aumentar la alarma entre la población de L´Aquila, difundiendo predicciones sobre la inminente ocurrencia de un terremoto de gran magnitud. Las predicciones de Guliani, quien había trabajado por 20 años como técnico científico en un laboratorio nacional cercano a L´Aquila, se basaron en la medición de los niveles de gas radón, que presumiblemente aumentan previo a la ocurrencia de un terremoto, lo que, sin embargo, no es aceptado por la comunidad científica. Guliani provocó un grado tal de alarma entre la población, que el 30 de marzo fue impedido legalmente para seguir manifestándose. El Departamento de Protección Civil, además, convocó a una reunión urgente de la comisión de riesgos en L´Aquila, con el fin de proporcionar a los ciudadanos “toda la información de que dispone la comunidad científica sobre la actividad sísmica de la últimas semanas”.   En la reunión de dicha comisión se mencionó que aunque la probabilidad de que hubiera un sismo de gran magnitud en L´Aquila –como el de 1703, que destruyó a la ciudad– era pequeña, no podía descartarse totalmente. No obstante, en declaraciones a la prensa, De Bernardinis manifestó que la situación en L´Aquila era ciertamente normal y que no representaba un peligro. Además, refiriéndose al gran número de temblores que se habían presentado por meses, expresó que “la comunidad científica me asegura que, por el contrario, se tiene una situación favorable debido a la descarga continua de energía”. Esta última afirmación ha sido muy cuestionada por los expertos, incluyendo a los miembros de la comisión de riesgos, como carente de bases científicas. Ha sido también un punto central para aquellos que acusan a la comisión de riesgos de no haber alertado a los ciudadanos sobre la posibilidad de que ocurriera un gran temblor en el corto plazo y que, por el contrario, hubiera emitido una declaración –a través de De Bernardinis– que los hizo bajar la guardia con resultados funestos. Por su parte y de manera sorprendente, Guliani predijo el sismo del 6 de abril un día antes y alertó como pudo a sus conciudadanos de los riegos que corrían. Tal parece, no obstante, que Guliani no tenía una seguridad absoluta de que el terremoto fuera a ocurrir. Así, aunque el 5 de abril afirmaba que un sismo se presentaría en un plazo de 24 horas, la noche de ese día durmió en su casa con su familia, si bien –según un artículo publicado en 2010 por el periódico británico “The Guardian”– lo hizo vestido y con las puertas y las ventanas abiertas para salir corriendo en caso de que empezara a temblar.    El caso es que, como coinciden la mayor parte de los expertos, los terremotos son impredecibles y en estas circunstancias resulta sorprendente que alguien pueda ser condenado a una pena de cárcel por no alertar sobre los peligros de algo que no se tiene la seguridad que va a ocurrir. Ciertamente los expertos italianos no son acusados de no haber predicho un terremoto –lo que de ninguna manera podían hacer–. En lugar  de eso son acusados de no haber sabido comunicar los riesgos que representa de un terremoto improbable, pero que podría ocurrir.Aun así, resulta difícil aceptar que alguien pueda ser culpado por las muertes resultantes de un fenómeno natural. En todo caso, si se han de señalar culpables, éstos posiblemente fueran, en primer lugar, De Bernardinis, y en segundo, Guliani. Este último por su activismo desaforado sin contar con un respaldo científico sólido, y el primero por hacer una declaración irresponsable que desorientó a  los habitantes de L´Aquila, lo que hizo en respuesta a la situación de alarma creada por Guliani.Aunque es difícil aceptar que por esto pudieran merecer a una condena de varios años de cárcel.",
    "La madrugada del 6 de abril de 2009, un terremoto de magnitud 6.3 en la escala de Richter devastó el pueblo medieval de L´Aquila en la región de Abruzzo en el centro de Italia, matando a 309 de sus habitantes y dejando sin casa a otros 65,000. El sismo de L´Aquila alcanzó notoriedad sobre todo porque, como parte de sus secuelas, seis distinguidos geofísicos italianos, además de Bernardo De Bernardinis, Subdirector del Departamento de Protección Civil, fueron sujetos a juicio acusados de homicidio imprudencial por no haber alertado adecuadamente a la población de L´Aquila sobre los riesgos sísmicos que enfrentaba. Después de un año de juicio, esta semana los acusados fueron condenados a seis años de cárcel. A muchos esta sentencia les pareció excesiva, incluyendo al fiscal que había solicitado solamente cuatro años de cárcel como castigo. Los sentenciados formaron parte de la comisión de protección civil que evaluó los riesgos sísmicos para la población de L´Aquila en los días anteriores al 6 de abril de 2009. En los meses previos a esta fecha, la región de Abruzzo fue sacudida por un gran número de temblores de pequeña magnitud que mantuvieron alarmada a la población. De acuerdo con un artículo aparecido en la revista “Nature” en septiembre de 2011, en enero de 2009 se sintieron en L´Aquila 69 temblores, en febrero fueron 70, 100 en marzo y 57 durante los primeros días de abril. En ese mismo artículo se hace un recuento de los acontecimientos anteriores y posteriores al terremoto del 6 de abril. Un actor clave para los mismos fue un geofísico aficionado de nombre Giampaolo Guliani, quién contribuyó grandemente a  aumentar la alarma entre la población de L´Aquila, difundiendo predicciones sobre la inminente ocurrencia de un terremoto de gran magnitud. Las predicciones de Guliani, quien había trabajado por 20 años como técnico científico en un laboratorio nacional cercano a L´Aquila, se basaron en la medición de los niveles de gas radón, que presumiblemente aumentan previo a la ocurrencia de un terremoto, lo que, sin embargo, no es aceptado por la comunidad científica. Guliani provocó un grado tal de alarma entre la población, que el 30 de marzo fue impedido legalmente para seguir manifestándose. El Departamento de Protección Civil, además, convocó a una reunión urgente de la comisión de riesgos en L´Aquila, con el fin de proporcionar a los ciudadanos “toda la información de que dispone la comunidad científica sobre la actividad sísmica de la últimas semanas”.   En la reunión de dicha comisión se mencionó que aunque la probabilidad de que hubiera un sismo de gran magnitud en L´Aquila –como el de 1703, que destruyó a la ciudad– era pequeña, no podía descartarse totalmente. No obstante, en declaraciones a la prensa, De Bernardinis manifestó que la situación en L´Aquila era ciertamente normal y que no representaba un peligro. Además, refiriéndose al gran número de temblores que se habían presentado por meses, expresó que “la comunidad científica me asegura que, por el contrario, se tiene una situación favorable debido a la descarga continua de energía”. Esta última afirmación ha sido muy cuestionada por los expertos, incluyendo a los miembros de la comisión de riesgos, como carente de bases científicas. Ha sido también un punto central para aquellos que acusan a la comisión de riesgos de no haber alertado a los ciudadanos sobre la posibilidad de que ocurriera un gran temblor en el corto plazo y que, por el contrario, hubiera emitido una declaración –a través de De Bernardinis– que los hizo bajar la guardia con resultados funestos. Por su parte y de manera sorprendente, Guliani predijo el sismo del 6 de abril un día antes y alertó como pudo a sus conciudadanos de los riegos que corrían. Tal parece, no obstante, que Guliani no tenía una seguridad absoluta de que el terremoto fuera a ocurrir. Así, aunque el 5 de abril afirmaba que un sismo se presentaría en un plazo de 24 horas, la noche de ese día durmió en su casa con su familia, si bien –según un artículo publicado en 2010 por el periódico británico “The Guardian”– lo hizo vestido y con las puertas y las ventanas abiertas para salir corriendo en caso de que empezara a temblar.    El caso es que, como coinciden la mayor parte de los expertos, los terremotos son impredecibles y en estas circunstancias resulta sorprendente que alguien pueda ser condenado a una pena de cárcel por no alertar sobre los peligros de algo que no se tiene la seguridad que va a ocurrir. Ciertamente los expertos italianos no son acusados de no haber predicho un terremoto –lo que de ninguna manera podían hacer–. En lugar  de eso son acusados de no haber sabido comunicar los riesgos que representa de un terremoto improbable, pero que podría ocurrir.Aun así, resulta difícil aceptar que alguien pueda ser culpado por las muertes resultantes de un fenómeno natural. En todo caso, si se han de señalar culpables, éstos posiblemente fueran, en primer lugar, De Bernardinis, y en segundo, Guliani. Este último por su activismo desaforado sin contar con un respaldo científico sólido, y el primero por hacer una declaración irresponsable que desorientó a  los habitantes de L´Aquila, lo que hizo en respuesta a la situación de alarma creada por Guliani.Aunque es difícil aceptar que por esto pudieran merecer a una condena de varios años de cárcel.",
    "Tal parece que la energía solar es contagiosa. Al menos esto es lo que se concluye en un estudio publicado en la revista “Marketing Science” el pasado 20 de septiembre, por Bryan Bollinger de la Escuela Stern de Negocios de la Universidad de Nueva York y  Kenneth Gillingham de la Universidad Yale. En dicho estudio, Bollinger y Gillinham encontraron que la decisión de un propietario de instalar paneles solares en su casa está influenciada por los paneles que sus vecinos puedan haber instalado en sus respectivas viviendas. La conclusión de Bollinger y Gillinham se deriva de un estudio de las instalaciones solares domésticas en el área de la bahía de San Francisco, California, que se extiende hasta el Valle del Silicio al sur de la bahía. Se encuentra que dichas instalaciones solares se hacen más densas en ciertos puntos geográficos. Las mayores concentraciones de paneles solares, además, no  necesariamente corresponden a las áreas con una mayor densidad de población. Esto podría ser indicativo de brotes de “epidemias” de paneles solares que han surgido en áreas específicas. Al igual que en una epidemia real, los paneles solares se multiplicarían por contagio.El contagio podría ser por una vía visual: un propietario decide instalar un sistema solar en su casa después de ver uno instalado en algún lugar en los alrededores. Podría igualmente haberlo decidido por otra vía: por ejemplo, después de que su vecino le hablara de las virtudes de los paneles solares que tiene instalados en su casa.  La visión anterior se refuerza si se comparan las distribuciones geográficas de los paneles solares de los años 2003 y 2006. En efecto, se encuentra que si bien hubo un crecimiento en el número total de sistemas solares instalados en el área, la concentración desigual de paneles observada en 2003 se mantuvo en 2006 con la misma distribución geográfica. Las concentraciones encontradas en 2006 fueron, sin embargo, más acusadas que las de 2003, lo que indica un avance en la epidemia. Así, en cuanto mayor es el número de sistemas solares instalados, mayor es la velocidad con la que se instalan sistemas nuevos y con esto, la epidemia sigue su curso ascendente.El estado de California tiene instalada una capacidad de generación de energía eléctrica fotovoltaica –o sea, por medio de paneles solares– de alrededor de 1500 megavatios, la mayor parte en los techos de casas particulares. Existen planes para incrementar esta capacidad –que es aproximadamente igual a la de la planta nuclear de Laguna Verde en el estado de Veracruz– hasta 3000 megavatios en 2016. El rápido crecimiento que está experimentando la capacidad solar de California es debido, por un lado, a la abundancia de recursos solares por su clima seco y, por el otro, a los incentivos gubernamentales para que los californianos instalen paneles solares en el techo de sus casas y autogeneren parte de la energía eléctrica que consumen. Con un sistema solar doméstico, la energía eléctrica en exceso generada durante el día es inyectada a la red eléctrica externa, mientras en las horas sin sol, la electricidad requerida es provista por la compañía eléctrica. La energía entregada a la red externa cuenta de manera positiva para el propietario de la vivienda, que de esta manera ve reducida su factura por consumo de electricidad. La proliferación de instalaciones solares domésticas ha sido limitada por la inversión necesaria para instalar el sistema solar. El precio de los paneles solares, sin embargo, se ha mantenido en descenso en los últimos años, reduciendo el monto de dicha inversión. Aunado a esto, en el caso de California los incentivos gubernamentales han proporcionado vías de infección para la propagación de la epidemia de paneles solares documentada por Bollinger y Gillinham. Por otro lado, esta epidemia resulta positiva, pues ayuda a disminuir la generación de gases de invernadero que han provocando el cambio climático que afecta actualmente a nuestro planeta. No es, sin embargo, la única epidemia tecnológica de la que somos testigos. Lejos de esto tenemos numerosos ejemplos. Uno de los más notorios es la epidemia de teléfonos celulares que se ha incrementado sin freno en los últimos años. Una de las vías de infección para esta epidemia, que debía ser la única, es la necesidad de contar con un medio de comunicación más conveniente que el teléfono fijo. Existen, no obstante, otras vías de propagación, como es la imitación de otras personas que ya cuentan con un teléfono celular.Otro ejemplo, de  consecuencias más graves para la vida citadina, es la epidemia de automóviles que amenaza con ahogar nuestras calles y que tiene igualmente en la imitación a una de sus vías de infección.El artículo de Bollinger y Gillinham señala que nuestra decisión para instalar un sistema generador de electricidad solar en nuestra casa puede estar influenciada por el hecho que nuestro vecino cuente ya con uno es su casa. La imitación sería pues también una de las vías de propagación de la epidemia de paneles solares. Al contrario de otras epidemias, no obstante, ésta es bienvenida. .",
    "Angkor Wat es un templo de gran relevancia para Camboya. Tanto que incluso su imagen está impresa en la bandera de ese país del sudeste asiático. Al igual que el Taj Mahal indio o las pirámides egipcias, la imagen de Angkor Wat es inconfundible. En su núcleo, el templo consta de tres patios concéntricos. El patio más interior es un cuadrado de 60 metros de largo, con una torre en cada una de sus cuatro esquinas. En el centro del patio se sitúa una quinta torre de mayor altura. Son estas torres, la central de 65 metros, las que le dan al templo su inconfundible perfil desde el exterior. El conjunto está rodeado por una barda de 4.5 metros de altura y por un foso de agua de 3.6 kilómetros de largo. El templo de Angkor Wat fue designado en 1992 por la UNESCO “Patrimonio de la Humanidad”.El templo de Angkor Wat formaba parte del asentamiento urbano de Angkor que abarcaba una extensión considerable. Un estudio de imágenes de radar tomadas de forma remota en un área de casi 3,000 kilómetros, publicado en 2007 por un grupo  internacional de investigadores encabezados por Damian Evans de la Universidad de Sydney, reveló que el complejo urbano de Angkor se extendía a lo largo y ancho de una superficie de más de 1,000 kilómetros cuadrados. Esto lo convierte en el mayor asiento urbano de baja densidad del mundo preindustrial. En Angkor se construyó una compleja infraestructura para la captación, almacenamiento y distribución de agua, la cual habría posibilitado el desarrollo de grandes extensiones de cultivos de arroz. Por la complejidad y extensión de esta infraestructura, Angkor ha sido concebida como una “ciudad hidráulica”. Por otro lado, la infraestructura de Angkor a la larga constituyó su punto débil y la llevó al colapso en el curso de dos siglos. Esto, por los grandes impactos al medio ambiente que significó. De acuerdo con Evans y colaboradores “La modificación del medio ambiente en Angkor fue lo suficientemente extensa y substancial para haber producido  numerosos y muy serios problemas ecológicos, incluyendo deforestación, sobrepoblación, degradación del suelo y erosión”. Todo esto habría sido demasiado para una infraestructura tan compleja e interdependiente como la que existió en Angkor.En tanto duró, no obstante, la civilización jemer medieval produjo obras tan notables como el templo Angkor Wat. Éste fue construido entre los siglos X y XIII de nuestra Era, empleando de cinco a diez millones de bloques de piedra con pesos de hasta 1500 kilogramos. Los bloques de piedra fueron extraídos de canteras cercanas al pie de la montaña Kulen, en un punto distante unos 30 kilómetros del templo. En cuanto al medio de transportación desde la cantera hasta el sitio de construcción del templo, un artículo publicado en línea esta semana en la revista “Proceedings of the National Academy of Sciences” por dos investigadores japoneses arroja luz al respecto. En dicho artículo los investigadores reportan que, observado mapas de “Google Map”, descubrieron una estructura lineal de 2.4 kilómetros de largo al pie de la montaña Kulen. Una investigación de campo puso de manifiesto que dicha estructura, a lo largo de la cual se situaban numerosas canteras, formaba parte de un sistema de canales para el transporte de los bloques de piedra hasta Angkor Wat, al igual que hacia otros templos en el área de Angkor. La ruta de transporte, además, seguía casi una línea recta de desde las canteras a los sitios de construcción.Por otro lado, si bien los materiales de las diferentes canteras al pie de la montaña Kulen son indistinguibles por su color, por medio de mediciones magnéticas sí fue posible distinguirlos y establecer el orden en que fueron explotadas las canteras localizadas a lo largo de la estructura lineal. No es infrecuente que realizaciones tan notables como las que ocurrieron en Angkor hace casi un milenio nos causen asombro. Los jemeres de Angkor Wat, no obstante, tuvieron –obviamente– suficiente tiempo, recursos, tecnología y expresión artística para llevar a cabo su empresa, lo cual no es particularmente sorprendente, dado que lo mismo ocurrió en otras partes del mundo en condiciones similares.Tampoco nos sorprende que no hayan previsto las consecuencias de perturbar al medio ambiente al grado en que lo hicieron, lo que eventualmente los llevó al colapso. Después de todo, nosotros en el Siglo XXI no actuamos muy diferente en este sentido.",
    "En un sonado caso ocurrido hace algunos años, el médico británico Andrew Wakefield fue acusado de conducta científica inadecuada en relación a un artículo publicado en 1998 por él y otros 12 autores en la revista médica “The Lancet”.  Una investigación llevada a cabo por el Consejo Médico General británico encontró que Wakefield había llevado a cabo prácticas profesionales deshonestas y lo despojó de la posibilidad de ejercer la medicina en Gran Bretaña. “The Lancet”, por su parte,  retiró en febrero de 2010 dicho artículo de sus récords. El  artículo de Wakefield y colaboradores se sumó así al creciente número de reportes científicos que han sido retirados de circulación después de haberse comprobado que sus autores habían incurrido en prácticas inadecuadas que invalidaban sus conclusiones. El affaire Wakefield, sin embargo, no constituyó simplemente un caso más de malas prácticas científicas comprobadas; esto, por las repercusiones sociales que tuvo. En efecto, entre las conclusiones del artículo se establecía la posibilidad de que la vacuna triple contra el sarampión, las paperas y la rubeola pudieran provocar en los niños autismo y problemas gastrointestinales severos. Estas conclusiones de manera natural causaron alarma entre la población y reluctancia al uso de la vacuna. Incluso se temió que por esto último se desatara una epidemia de sarampión en la Gran Bretaña. El caso Wakefield fue tan notorio que fue incluido por la revista Time en su lista de grandes fraudes científicos.Para decidir sobre aceptar o rechazar la publicación de un artículo de investigación que le ha sido enviado, el editor de una revista científica busca la opinión de expertos en el tema. Esta práctica en principio permite filtrar artículos con fallas metodológicas que no apoyen sólidamente sus resultados y conclusiones. El mecanismo no es, sin embargo, perfecto y en ocasiones se publican artículos con errores –intencionales o no– e incluso con datos fabricados para apoyar una cierta conclusión deseada. Hay un buen número de incentivos para que esto ocurra. El investigador en la actualidad está bajo una gran presión para conseguir fondos para sostener su actividad, los cuales se obtienen sobre la base de sus resultados de investigación previos reflejados en su producción científica. Esto lo pone en competencia con otros colegas e incluso puede amenazar la estabilidad de su puesto de trabajo. No es sorprendente, entonces, que se den casos de prácticas científicas impropias y que se publiquen artículos de investigación con datos alterados o de plano fabricados. Estos artículos, si son detectados, pueden ser retirados de circulación por la revista que los publicó. En un artículo aparecido esta semana en la revista norteamericana “Proceedings of the National Academy of Sciences”, publicada por un grupo de investigadores encabezados por Arturo Casadevall del Albert Einstein Medical College de la ciudad de Nueva York, se analiza el fenómeno del retiro de artículos científicos. Casadevall y colaboradores analizan las causas del retiro de 2047 artículos  del área  biomédica indexados en PubMed, que incluye más de 25 millones de registros a partir de 1940. Encuentran que de los 2047 artículos estudiados, el 21 % fue retirado debido a que contenían errores. En contraste, el 47 %  lo fue por fraude científico, el 14 % porque constituía trabajo duplicado y el 10 % por plagio. Los investigadores encontraron, además, que el número de artículos retirados por todas las causas está en aumento y que el fraude científico, que es la causa mayoritaria, se ha incrementado por un factor 10 desde 1975. Casadevall y colaboradores analizan, además, la distribución geográfica de los artículos retirados. Encuentran que la lista de países con más artículos caídos en desgracia es encabezada por los Estados Unidos, lo que resulta natural, pues es con mucho el país con mayor número de publicaciones. A los Estados Unidos le siguen en número de artículos retirados  Alemania, Japón y China, países que descuellan igualmente en el número de artículos que publican. Es interesante hacer notar, sin embargo, que si bien los Estados Unidos encabezan la lista de artículos retirados por todas las causas, el peso específico que este país tiene en el número de retiros por causa de fraude científico es notablemente grande. Esto podría significar que las presiones que sufren los científicos norteamericanos son correspondientemente mayores a las de sus contrapartes, incluso en países desarrollados.El fraude científico es posiblemente inevitable, pues después de todo la ciencia es una actividad practicada por los humanos con todas sus limitaciones. Sin embargo, aun con este aspecto negativo, la práctica científica con toda su cauda de aplicaciones ha demostrado ser enormemente redituable para nosotros. Después de todo, un resultado científico fraudulento con el tiempo será de manera inevitable detectado y corregido. Aunque en tanto esto último suceda, el fraude científico pueda tener consecuencias negativas, como el caso Wakefield lo ha demostrado. Por lo demás, nada es perfecto, ni aun la ciencia.",
    "Hace ocho siglos Gengis Kan se embarcó en campañas militares que, en que el curso de apenas dos décadas, extendieron los límites del Imperio Mongol hasta alcanzar hacia el este la costa del Océano Pacífico y hacia el oeste la orilla del Mar Caspio. Gengis Kan dirigió campañas en contra de sus vecinos en el norte de la actual China. Se enfiló también hacia el poniente hasta casi alcanzar a Europa y a su paso destruyó ciudades que le opusieron resistencia como Samarcanda y Bujara en el actual Uzbequistán, sacrificando en masa o haciendo esclavos a sus pobladores. Con el tiempo, los sucesores de Gengis Kan extendieron los límites del  Imperio Mongol aun más, al grado que en superficie es el segundo imperio más grande que jamás ha existido, superado solamente por el Imperio Británico.  Los expertos explican que la razón del éxito de los mongoles, y en particular de Gengis Kan, residió en las tácticas militares que emplearon y que resultaron superiores a las de sus enemigos –a los que pudieron superar incluso en condiciones de inferioridad numérica–. Los mongoles, no obstante, eran guerreros a caballo y para ganar batallas, además de tácticas militares, hubieron de disponer de suficiente alimento para sus monturas –además de alimento para ellos mismos, por supuesto. En este respecto, Amy Hessl de la Universidad del Este de Virginia y Neil Pederson de la Universidad Columbia en los Estados Unidos sugieren que la expansión de los mongoles pudo haber sido producto de un periodo de abundantes lluvias que habría experimentado Mongolia en tiempos de  Gengis Kan. El exceso de humedad habría hecho crecer más cantidad de hierba de lo usual y permitido sostener, tanto a los caballos empleados en las empresas guerreras, como al ganado que a su vez servía de alimento a los guerreros. Una fluctuación climática habría de este modo generado condiciones favorables para las campañas militares mongolas. Hessl y Pederson arriban a esta conclusión después de estudiar restos de árboles recogidos en un campo de lava en las inmediaciones del volcán Khorgo en Mongolia, el cual está situado en la región de origen de Gengis Kan. Como nos lo explican los especialistas, los anillos concéntricos de crecimiento que se observan al cortar el tronco de un árbol nos cuentan la historia del clima durante el periodo de vida de dicho árbol. En efecto, se sabe que estos anillos son debidos a que a lo largo del año la velocidad de crecimiento de los árboles varía de acuerdo a la estación, siendo más grande durante la primavera que durante el invierno. Así, la madera crecida durante esta última estación  es más densa que aquella que crece en la primavera, lo que se hace visible en los anillos concéntricos. De acuerdo con esto, cada anillo corresponde a un año y para saber la edad a la que murió un árbol basta contar el número de anillos de su tronco. Además, la fecha de la muerte de un árbol puede ser determinada comparando su patrón de anillos con el correspondiente patrón de un árbol que haya crecido en la misma época y en las mismas condiciones climáticas, pero que aun esté vivo.Por otro lado, las condiciones climáticas que experimentó un árbol en un determinado año determinan el espesor del anillo correspondiente. Es decir, si las condiciones son favorables al crecimiento, el espesor del anillo será mayor. Una variación en el espesor de los anillos a lo largo de un periodo nos indica entonces una correspondiente variación climática. Estudiando los árboles encontrados en Khorgo, Hessl y Pederson encuentran que, efectivamente, en las primeras décadas del Siglo XIII, cuando se dieron las conquistas de Gengis Kan, el clima en Mongolia fue más húmedo y favoreció el crecimiento de los árboles y en consecuencia de la hierba que necesitaron los mongoles para lanzar sus incursiones militares.Si  bien los mongoles asolaron todo a su paso, el establecimiento de un imperio a todo lo largo de Asia estimuló el intercambio comercial entre Europa y el Lejano Oriente y en este sentido se podría decir que no hay mal que por bien no venga y que las conquistas de los mongoles tuvieron al menos una consecuencia positiva.Es posible, no obstante, que los habitantes de Samarcanada, Bujara y otras ciudades   –que fueron exterminados o tomados como esclavos por las tropas de Gengis Kan– expresarían su desacuerdo con lo anterior, de tener la oportunidad de hacerlo. Quizá les resultara contradictorio que –siguiendo a Hessl y Pederson– el mejoramiento en las condiciones climáticas en Mongolia, que habría resultado en una mayor producción de alimentos y que aparentemente sólo podría tener consecuencias positivas para la raza humana –o, al menos, no negativas para aquellos que se encontraban a miles de kilómetros de distancia–  les hubiera resultado en un desastre de tal magnitud. Para las víctimas de Gengis Kan, quizá fuera más apropiado decir que en su caso, hubo un bien –aunque no para ellos– que por mal les vino.",
    "El cuento corto “Los crímenes de la calle Morgue”, publicado en 1841 por el escritor norteamericano Edgar Allan Poe, es considerado el primer relato del género policiaco, género que después se popularizó con escritores como Arthur Conan Doyle creador de Sherlock Holmes. En dicho cuento, Poe relata el brutal asesinato de una anciana señora y su hija ocurrido en un departamento en el cuarto piso de un edificio situado en la calle Morgue de París. La anciana fue degollada mientras que a su hija la encontraron estrangulada y boca abajo dentro del tiro de la chimenea. Los crímenes se cometieron con un enorme despliegue de fuerza. Tal que fueron necesarios cinco hombres para sacar el cadáver atrapado en la chimenea.  Lo más extraño del caso fue que los vecinos, que llegaron al lugar de los hechos instantes después de cometidos los crímenes, e incluso escucharon los gritos de las víctimas, no encontraron a nadie dentro del departamento. Éste, además, no tenía aparentemente otra salida que la puerta por la que entraron los vecinos y por la cual no pudo escapar el culpable.El misterio fue resuelto por Auguste Dupin –antecesor directo de Sherlock Holmes– empleando una combinación de observación, razonamiento inductivo y conocimiento enciclopédico. El culpable resultó ser un orangután de Borneo llevado a París por un marinero y del cual había logrado escapar. Dado que los orangutanes en su hábitat natural viven en las ramas de los árboles, el simio del relato no tuvo demasiada dificultad para trepar por el cable de un pararrayos hasta la atura de una ventana del departamento de sus víctimas y de ahí saltar al interior del mismo.De haber vivido en la época actual, es posible que Poe no hubiera recurrido a Dupin y a sus poderes inductivos para resolver el misterio de la calle Morgue. En su lugar, podría quizá haber recurrido a un análisis de ADN del manojo de cabellos que la policía encontró  en las manos de la anciana muerta y que en su desesperación arrancó a su atacante. Dado que los orangutanes divergieron de la línea evolutiva que dio origen a nuestra especie hace más de 10 millones de años, el análisis de ADN hubiera indicado sin ambigüedad que no se trataba de cabellos humanos y que muy probablemente pertenecían a un orangután. Esto hubiera reducido considerablemente el grupo de sospechosos  y facilitado encontrar al culpable.El uso del análisis de ADN en la investigación de delitos se ha desarrollado rápidamente en los últimos años. Una aplicación extendida consiste en comparar el ADN recogido en el lugar del crimen con el ADN de delincuentes fichados con anterioridad. De encontrarse una coincidencia, y dado que el ADN constituye una “huella digital” para cada persona, el culpable habría sido identificado con una gran probabilidad. Igualmente, la coincidencia del ADN encontrado en el lugar del crimen con el de un sospechoso será una prueba de su culpabilidad.Recientemente, un grupo de investigadores encabezado por Manfred Kayser del “Erasmus Medical  Center” en Róterdam, Holanda, desarrolló un método para predecir simultáneamente el color de los ojos y del cabello de una persona a partir de un análisis de su ADN. Dicho método fue probado con 1500 personas de tres diferentes partes de Europa y tuvo una precisión de 70-90% en la predicción del color del cabello –rubio, café, rojo o negro–. La técnica tiene aplicaciones para la identificación del culpable de un crimen en caso de que no exista un sospechoso, pues reduce el grupo dentro del cual hay que buscarlo. Los resultados fueron publicados el pasado 22 de agosto en la revista “Forensic Science International: Genetics”. Conocer el color de ojos y de cabello del culpable reduce sin duda el número de sospechosos entre los cuales hay que buscarlo, pero esta información no es aun tan específica como pudiera desearse. En este sentido, en un artículo aparecido esta semana en la revista “PLOS Genetics”, encabezado por el mismo grupo de investigación referido líneas arriba, se describen los resultados de un estudio llevado a cabo para determinar que parte o partes del genoma humano determinan tal o cual rasgo facial. La investigación se llevó a cabo con más de 5000 personas de origen europeo. Sus características faciales fueron extraídas por medio de imágenes de resonancia magnética, mismas que se correlacionaron con sus respectivos genomas. Como resultado, se pudieron identificar cinco genes que determinan características faciales tales como el ancho de la cara, la separación entre los ojos y lo pronunciado de la nariz. La identificación de todos los genes que determinan la cara de una persona está todavía muy lejos de alcanzarse y los resultados presentes son apenas los primeros pasos en esta dirección. Una vez que se haya alcanzado éxito, sin embargo, la ciencia forense tendrá una herramienta poderosa a su disposición. Podría preverse, por ejemplo, que mediante el análisis del ADN recogido en el lugar del crimen una computadora sea capaz de crear un retrato del culpable que pueda llevar a su captura. Esto sería equivalente a la actual práctica de elaborar retratos hablados mediante la información proporcionada por testigos, pero sin los inherentes elementos subjetivos.La ciencia forense ciertamente ha avanzado enormidades desde los tiempos de Edgar Allan Poe. Con seguridad, no obstante, avanzará mucho más en el futuro.",
    "El continente antártico es el lugar más frío de la Tierra. Fue ahí –de manera precisa, en la estación rusa Vostok– en donde se observó en 1983 la temperatura más baja jamás registrada: menos 89 grados centígrados. La Antártida, con una temperatura media a lo largo del año que no llega a los cero grados centígrados en el mejor de los casos, no es ciertamente un lugar atractivo para vivir. Esto, aun sin considerar que está cubierta permanentemente por una capa de hielo cuyo espesor se mide en kilómetros,  y que es producto de millones de años de mal tiempo. Esta capa de hielo, por otro lado, tiene una gran importancia científica, pues en la misma se encuentra grabada la historia climática de la Tierra. En efecto, el hielo antártico se formó por la acumulación y compactación gradual de nieve a lo largo de decenas de millones de años. Así, si perforamos la capa de hielo, a medida que penetramos por debajo de su superficie nos encontramos con estratos que pertenecen a épocas geológicas cada vez más antiguas. Un estudio de las características del hielo a diferentes profundidades nos dará entonces información sobre la evolución del clima de la Tierra. La capa de hielo antártica es además interesante porque bajo su superficie se localiza una gran cantidad de lagos  subterráneos. El más grande de ellos, el lago Vostok, es de dimensiones equivalentes al lago Ontario, lo que lo coloca como el séptimo lago más grande del planeta. El agua del lago Vostok está sujeta a una enorme presión por la capa de hielo que soporta, la cual  tiene un espesor de cuatro kilómetros. Se mantiene líquida por el aislamiento térmico que le proporciona dicha capa y por el calor que recibe del interior de la Tierra.    El lago Vostok fue sepultado por el hielo hace decenas de millones de años y desde entonces presuntamente se encuentra aislado sin contacto con el mundo exterior.  Constituiría así un verdadero mundo perdido en el que posiblemente se encuentren organismos vivos que hayan seguido una evolución propia, atendiendo a las condiciones de su medio ambiente. De existir, obstante, la vida tendría que limitarse a formas microbianas, pues la ausencia total de luz solar y la escasez de nutrientes en el lago habrían eliminado otra forma de vida más avanzada.Para saber si en el lago Vostok existen organismos vivos es, por supuesto, necesario contar con una muestra de agua del mismo. Para este propósito, un equipo de científicos rusos ha llevado a cabo, ya por una buena cantidad de años, un proyecto de perforación de la capa de hielo del lago Vostok a fin de acceder a su superficie. En febrero de este año, dicho grupo anunció que finalmente había tenido éxito y que había logrado extraer una muestra de agua del lago. Perforar cuatro kilómetros de hielo antártico en condiciones infrahumanas y poniendo todo el empeño en evitar una posible contaminación del agua prístina del lago Vostok, no fue de ninguna manera una empresa sencilla, y a la que de manera entendible se le dio gran publicidad. El Ministro de Recursos Naturales y Medio Ambiente ruso incluso regaló a Vladimir Putin parte del agua extraída, presentándosela como “agua con más de un millón de años de antigüedad”. Putin, por su lado, se refirió a la misma como “agua que bebieron los dinosaurios” –en una evidente exageración, pues los dinosaurios se habrían extinguido unos cincuenta millones de años antes de que se sellara el lago “Vostok”. El próximo mes de diciembre –con el inicio del verano austral– el equipo ruso reanudará las exploraciones en el lago Vostok. En esta ocasión, sin embargo, no estará solo en su empeño, pues un equipo de científicos británicos en días pasados anunció que intentarán una empresa similar, aunque en un lago diferente, el lago Ellsworth, en el otro extremo de continente Antártico.Perforar kilómetros de hielo antártico, si bien es complicado, no es en realidad el principal problema que enfrentan los equipos  ruso y británico. Lo más difícil es evitar que la prístina agua del lago Vostok se contamine durante la operación, lo que sería catastrófico. Los rusos utilizan una técnica de perforación que emplea fluidos contaminantes y que por lo mismo ha sido motivo de críticas –en este respecto, no es claro si el color amarillento del agua regalada a Putin es el que tiene de origen o bien es debido a contaminación con los fluidos de perforación–. Para evitar este problema, los británicos utilizarán para perforar el hielo solamente agua caliente.Hay quién piensa que, si bien el lago Vostok puede tener de diez a veinte millones de años de antigüedad, el agua que contiene no es en realidad tan antigua, pues continuamente está siendo reemplazada por corrientes subterráneas. De ser este el caso, el agua del lago habría estado aislada solamente por 10,000-20,000 años. Así, Putin habría doblemente exagerado con su comentario sobre el agua que bebían los dinosaurios. En contraste, desde el punto de vista científico los lagos subterráneos de la Antártida mantendrán su importancia y probablemente justificarán de manera amplia los recursos invertidos en su exploración. Habremos, no obstante, de esperar hasta febrero del siguiente año para conocer los resultados de las investigaciones rusas y británicas.",
    "En su portada del número de enero del año 2000, la revista “Scientific American” mostraba la imagen de un humano primitivo vestido con pieles, junto a la de otro igualmente vestido pero con rasgos anatómicos claramente diferentes. Junto a las imágenes se podía leer el texto “No estuvimos solos”. La portada hace alusión a uno de los artículos en el interior de la revista en la que se apunta que a lo largo de los últimos dos millones de años posiblemente hayamos compartido nuestro hábitat con otras especies cercanas parientes nuestras. Esto con la excepción de los últimos 30,000 años, durante los cuales habríamos estado solos en el planeta después de la desaparición del Neandertal.Es sabido que los humanos modernos convivieron en Europa  por diez mil años con los Neandertales. Estudios genéticos de fósiles Neandertal incluso indican que ambas especies se cruzaron y mezclaron sus genes. Hace unos 30,000, sin embargo, mientras nuestra especie florecía, los Neandertales se extinguieron por razones inciertas. En este sentido se especula que no pudieron sobrevivir al encuentro con los humanos modernos mejor equipados intelectualmenteLos Neandertales no han sido, por otro lado, los únicos parientes cercanos con los que hayamos convivido –en algunos casos de manera estrecha, según la genética–. En efecto, en 2010 un grupo internacional de científicos encabezado por investigadores del Instituto Max Planck de Antropología Evolutiva en Alemania, anunció el descubrimiento de restos fósiles de una muchacha de más de 50,000 años de antigüedad, en una cueva en el sur de Siberia. Los restos son de una especie distinta a la especie Neandertal pero cercanamente emparentada, que ha llegado a ser conocida como Denisovana por el nombre de la cueva en la que fueron encontrados dichos restos.Los restos de la muchacha Denisovana en si no son espectaculares, pues apenas consisten de un pequeño fragmento del dedo meñique. No obstante han resultado de gran importancia, pues su ADN ha sido relativamente bien conservado por el frio imperante en la cueva en la que permanecieron por decenas de miles de años y que promedia cero grados centígrados a lo largo del año. Esto, juntamente con una nueva técnica para descifrar el código genético desarrollada en el Instituto Max Planck, ha hecho posible que obtener la información genética de la muchacha Denisovana con una precisión tal que es posible compararla con la de una especie viva. Así, a pesar de solo contar con un fragmento de hueso, podemos saber que tenía piel morena, y pelo y ojos café. Los resultados del estudio han sido reportados esta semana en la revista “Science” por el mismo grupo de investigadores que originalmente anunció el descubrimiento en la cueva siberiana. Al igual que con los Neandertales, el genoma de los Denisovanos indica que en algún momento se cruzaron con los humanos modernos. También, del mismo modo que los primeros, los Denisovanos se extinguieron por razones aún por determinar, pero que pudieron tener que ver con una desigual competencia con nuestra especie. Una comparación del genoma de la cueva siberiana con el nuestro puede arrojar luz al respecto.  Svante Päävo, uno de los autores del artículo de referencia, hace notar que los Denisovanos muestran cambios en genes asociados al “alambrado” del cerebro y del sistema nervioso, y que es tentador especular que lo que llevó a los humanos modernos fue un cambio en las conexiones cerebrales.La técnica empleada para descifrar el genoma Denisovano será aplicada en el futuro a otras muestras fósiles, en particular a los fósiles Neandertales lo que sin duda permitirá hacer comparaciones precisas entre el genoma humano moderno con el de especies extinguidas. Otro candidato en este respecto es el fósil de una misteriosa especie de enanos –“hobbits”– de un metro de altura descubierta en 2003 en una isla de Indonesia y que habría sobrevivido aislada hasta hace unos 12,000 años. El descifrar el código genético de especie extintas nos ayudará a entender  que es lo que nos ha hecho tan exitosos, al punto de terminar como la especie dominante –solitaria– en la Tierra después de millones de años de evolución. El periodo de tiempo durante el cual hemos estado solos con seguridad será actualizado en el futuro en la medida en que avancen las investigaciones. Que lo hemos estado por miles de años es, no obstante, un hecho que difícilmente será puesto en duda.",
    "Una caricatura publicada por la revista “Life” en febrero de 1896 muestra a un granjero que, sonriente y portando una guadaña, posa para un fotógrafo que le pide que por favor sonría. El granjero, sin embargo, no posa para una fotografía ordinaria, sino para una que emplea los entonces recién descubiertos rayos x. Esto nos queda claro porque en un recuadro de la caricatura se incluye otra que muestra el esqueleto de granjero en la misma posición sosteniendo la guadaña.La caricatura posiblemente no tendría nada de extraordinario de no ser porque fue publicada muy poco tiempo después del descubrimiento de los rayos x. En efecto, fue el 28 de diciembre de 1895 el día en que Wilhelm Roentgen anunció el descubrimiento de rayos invisibles que podían atravesar materiales opacos y que denominó rayos x, para indicar que tenían una naturaleza desconocida. Para probar la existencia de estos rayos, Roentgen mostró con una imagen de rayos x de su esposa  obtenida unos días antes, en la que podían verse claramente los huesos de su mano.  Con esto, Roentgen ponía al descubierto la existencia de unos rayos misteriosos capaces de atravesar paredes, al mismo tiempo que evidenciaba el enorme potencial de los mismos para el diagnostico médico. Con estos dos ingredientes, no es de extrañar que los rayos x hayan capturado de manera instantánea la atención del público, que hacía toda clase de especulaciones acerca de los mismos y acudía a presenciar demostraciones del asombroso fenómeno.Desde el punto de vista médico, los rayos x proporcionaron una herramienta poderosa para observar el interior del cuerpo humano, herramienta que los médicos adoptaron en cuestión de meses.  Desde una perspectiva científica, el descubrimiento de los rayos x es el punto de inicio de los grandes descubrimientos que se dieron en el ámbito de la física en las primeras décadas del Siglo XX, y que en el curso de cinco lustros condujeron al desarrollo de la bomba nuclear primero, y a las centrales nucleares para la generación de electricidad después. Como sabemos, la operación de los reactores nucleares genera desechos radiactivos que emiten rayos invisibles –uno de ellos de naturaleza similar a los rayos x pero más energético– que pueden provocar diversos problemas de salud. Entre éstos se cuentan el cáncer y las  mutaciones genéticas. La seguridad de la centrales nucleares es entonces un aspecto preocupante, como nos lo demuestran los accidentes nucleares que se han producido en la últimas décadas, el último en Fukushima en marzo de 2011.  En un artículo publicado día 9 de presente mes en la revista “Scientific Reports” por científicos japoneses, se describe un estudio llevado a cabo con mariposas recogidas en diversos lugares alrededor de la planta de Fukushima. El objetivo del estudio fue determinar si la liberación de radiación por el accidente en la planta había producido anormalidades físicas en las mariposas y si éstas eran producto de cambios en el DNA que condujeran a mutaciones heredables entre generaciones.Para este propósito, primeramente se recogieron 144 mariposas dos meses después del accidente. Se observó que un 12% de este primer grupo de mariposas presentaba anormalidades, ya sea en las patas, las antenas, los ojos, o las alas. Con el objeto de determinar si la radiación había producido mutaciones genéticas, se aparearon las mariposas del primer grupo con otras que no habían sufrido exposición a la radiación. El experimento se llevó a cabo en Okinawa, que está a casi 2,000 kilómetros de Fukushima y hasta donde no pudo haber llegado la radiación del accidente nuclear. Se encontró que un 18% de las mariposas de la nueva generación presentaron  anormalidades. Además, este porcentaje se incrementó hasta el 33% en una siguiente generación. Así, el resultado de los experimentos indica que la exposición a la radiación produjo cambios en el DNA de las mariposas que resultaron mutaciones heredables de una generación a otra.En otro experimento, se formó un segundo grupo de mariposas recogidas en los alrededores de Fukushima seis meses después del accidente y se determinó su porcentaje de anormalidades. Éste resultó ser más del doble del que mostraron las mariposas del primer grupo, recogidas cuatro meses antes. Aun más, cuando se aparearon las mariposas del segundo grupo, la descendencia resultó con escalofriante porcentaje de anormalidades de casi un 60%. Todo esto habría sido consecuencia de una mayor exposición a las radiaciones.A poco más de un siglo del descubrimiento de los rayos x, las radiaciones invisibles de alta energía han mostrado su utilidad en un gran número de áreas, incluyendo la medicina, la industria y la investigación científica. Representan, al mismo tiempo, un gran peligro para la salud. En el caso de las centrales nucleares, las radiaciones invisibles constituyen un subproducto extremadamente peligroso. A tal grado que han convertido a la industria núcleo-electricidad en una industria altamente controvertida.  Para mayor información, favor de consultar con las mariposas de Fukushima.",
    "Tal como se tenía planeado, el pasado 6 de agosto el explorador “Curiosity” ingresó de manera controlada a la atmósfera del planeta Marte y se posó suavemente y sin novedad sobre su superficie mediante una complicada maniobra. Una vez en tierra firme, el explorador inició el procedimiento previsto para ponerse en forma e iniciar su misión de exploración de dos años. Ha comenzado por enviar fotografías de la superficie marciana, primero  en blanco y negro, y luego en color y alta definición.No es “Curiosity” el primer explorador en arribar a Marte. Lo precedieron los gemelos “Spirit” y “Opportunity” en  2003. De hecho, este último aun está activo recorriendo la superficie marciana y enviando datos e imágenes a la Tierra. El “Curiosity” es, no obstante,  un explorador más grande y complejo que los exploradores gemelos que tienen el tamaño de un carrito de golf y un peso de unos 180 kilogramos. En comparación, el “Curiosity” pesa casi una tonelada  y su tamaño se acerca al de un automóvil pequeño.Marte ha recibido naves terrestres desde los años setenta. Las primeras no eran vehículos exploradores y solo pretendían descender suavemente a la superficie marciana. Este fue el caso de la nave soviética Mars 2, que se convirtió en 1971 en el primer objeto enviado desde la Tierra en alcanzar la superficie de Marte. Cabe señalar, no obstante, que no lo hizo de manera suave por una falla en su computadora y terminó estrellándose en el  suelo marciano. A la Mars 2 siguió la Mars 3 que sí logró aterrizar de manera suave, aunque falló al poco tiempo y no pudo enviar datos científicos a la Tierra. La primera nave espacial que, además de posarse suavemente en la superficie marciana pudo establecer comunicación con nuestro planeta, fue la norteamericana ”Viking 1”, la cual fue seguida por la “Viking 2”, igualmente exitosa.  De estas dos misiones provienen las primeras imágenes de la superficie de Marte. A las naves “Viking” siguieron otras misiones con el mismo destino. Algunas tuvieron éxito y otras fracasaron, pero con el descenso de “Curiosity” suman ya doce naves terrestres que de una u otra manera  han alcanzado la superficie de nuestro planeta vecino.En estas circunstancias, de existir los marcianos estarían sin duda alarmados por lo que deben interpretar como indicios de una invasión interplanetaria. Podría haber incluso marcianos que aseguren haber avistado en el cielo un objeto que asemejaba a un platillo volador, y que no sería otra cosa que la cápsula dentro de la cual ingresó el “Curiosity” a la atmósfera marciana.      De haber marcianos cabría la posibilidad de que existiera un equivalente de H.G. Wells que relate en su novela La Guerra de los Mundos la invasión de su planeta por habitantes del planeta azul vecino, quienes llegarían equipados con máquinas infernales capaces de destruir todo a su paso mediante rayos de fuego.  Nuestros vecinos planetarios sin duda verían en los objetos caídos del cielo a lo largo de los últimos 40 años un indicio de la inminencia de la invasión terrestre anticipada por dicha novela. Desde que fue observado a través de un telescopio, se fantaseó con la posibilidad de que Marte albergara vida inteligente.  Uno de los máximos entusiastas de esta posibilidad fue el astrónomo norteamericano Percival Lowell, quien se tomó en serio las observaciones del también astrónomo Giovanni Schiaparelli que indicaban la existencia de canales sobre la superficie de Marte. Lowell especuló que dichos canales fueron construidos por una civilización marciana para transportar agua desde los polos hasta el ecuador del planeta. Las ideas de Schiaparelli y Lowell tuvieron vigencia durante la última década del Siglo XIX y la primera del Siglo XX –y posiblemente influenciaron a H.G. Wells que publicó la Guerra de los Mundos en 1898–. Dichas ideas, sin embargo, no sobrevivieron por mucho tiempo e incluso el mismo Lowell tuvo que admitir que posiblemente fueran erróneas. Hoy en día tenemos una razonable seguridad de que lo son y de que no hay vida inteligente en Marte. Las naves que han logrado arribar a su superficie lo muestran como un lugar árido y seco, sin el menor rastro de vida, aunque la podría haber a nivel microbiano.Es fascinante, sin embargo, ver la similitud del paisaje marciano mostrado en la panorámica enviada hace unos días por el “Curiosity” con un desierto en nuestro planeta. Dicha fascinación persiste aun después de razonar que la similitud es en realidad sólo  aparente, y que la superficie de Marte es un lugar inhóspito, incapaz de albergar vida superior tal como la conocemos en la Tierra. En estas circunstancias, es poco probable que exista un H.G. Wells en Marte y que alguien allá se haya alarmado por el arribo del “Curiosity”. Aun si éste hubiera sido el caso, la preocupación habría sido en vano, pues en la Tierra no estamos en estos momentos en posibilidades, ya no digamos organizar una invasión de Marte, sino incluso de enviar una misión tripulada.Así, tendremos en las décadas por venir que conformarnos con la información y las tarjetas postales que desde Marte nos lleguen a control remoto. Lo que, por otro lado, no es poca cosa.",
    "Una construcción icónica de la ciudad de Nueva York, el antiguo estadio de beisbol de los Yanquis, fue demolido en el año 2009 para dar lugar a un parque. Esto, a pesar de las protestas de aquellos que demandaban fuera conservado por su significado histórico. Dicho estadio fue inaugurado en 1923 y sirvió de casa por 85 años a los Yanquis de Babe Ruth, Joe Dimaggio y Mickey Mantle. Fue testigo de 39 de los 40 títulos de liga y de 26 de las 27 Series Mundiales ganadas por los Yanquis, que convierten a este equipo, con mucho, en el máximo ganador en la historia de las Grandes Ligas. El papel histórico que jugó el estadio de los Yanquis en el beisbol norteamericano no fue,  desgraciadamente, suficiente para salvarlo de la demolición que se inició apenas los Yanquis contaron con un nuevo escenario. El nuevo estadio de los Yanquis fue construido junto al antiguo y  en su diseño se mantuvieron algunas características del original como reconocimiento a su significado histórico, incluyendo la característica cenefa en su parte superior.  Como quiera que haya sido, sin embargo, el hecho es que el inmueble histórico acabó convertido en un montón de escombros.La demolición de antiguo estadio de los Yanquis puede ser también visto en el contexto de los desechos que produjo. La percepción más común sobre la basura se centra en aquella generada por las casas-habitación, las oficinas y los comercios, y que es recogida por los servicios públicos municipales. De hecho, sin embargo, esta clase de desechos es  solamente un porcentaje pequeño del total. En efecto, se encuentra, por ejemplo, que el sector de la construcción y demolición, juntamente con la minería y la manufactura, generan el 75% de todos los desechos que producen los 27 países de la Unión Europea. Por otro lado, el consenso entre los expertos es que la generación de desechos en el mundo no es auto sostenible y que se deben tomar medidas para mitigar sus efectos. En línea con esto, en el número especial de la revista “Science” aparecido esta semana, se incluye una sección con varios artículos en la que se trata el problema de los desechos y de las maneras de atacarlo. Como se asienta en la introducción a dichos artículos, en ellos se resalta cómo los desechos pueden a menudo llegar a ser valiosos convirtiéndose en insumos para otras industrias y contribuyendo así a disminuir el consumo de materias primas. Además, a partir de los desechos es posible fabricar combustibles, medicinas y cosméticos, al mismo tiempo que los metales y los plásticos pueden ser reciclados varias veces. De la misma manera, algunos desechos pueden ser usados para la generación de energía eléctrica.La facilidad para reciclar un producto debe ser contemplada en su diseño. Hay quien piensa que el problema de los desechos es debido precisamente a una falla de diseño. Es decir, si un material o producto no puede ser reciclado es porque necesita ser rediseñado. La concepción de un producto no debe entonces contemplar solamente las fases de fabricación y comercialización, sino también una fase posterior de reciclado.Según los expertos, para un uso y disposición eficiente de los desechos se deben superar barreras de naturaleza no solamente científica y tecnológica, sino también política, económica e incluso sicológica. Como ejemplo de esto último, se cita la resistencia natural a considerar como agua potable aquella obtenida de la purificación de aguas negras, lo que técnicamente es, sin embargo, posible.El no haber demolido el estadio de los Yanquis no hubiera, por supuesto, resuelto el problema de la basura en el mundo. Podemos tomarla, no obstante, como una indicación de que tenemos que modificar nuestra percepción acerca de la generación de desechos y tomar en cuenta el impacto ambiental que éstos producen. Después de todo, construir un nuevo estadio no era absolutamente necesario.Todo lo anterior al margen de la poca consideración que se tuvo por un recinto de enorme significado histórico –y posiblemente sentimental– para numerosos norteamericanos.",
    "En un lapso de algunas horas entre hoy mañana, tendrán lugar dos acontecimientos emblemáticos del desarrollo científico y tecnológico del último siglo. Por un lado, en los primeros minutos del lunes se espera el arribo del explorador “Curiosity” a la superficie del planeta Marte. Por otro lado, el día de hoy se cumplen 67 años del bombardeo atómico de Hiroshima al final de la Segunda Guerra Mundial. Ambos, el explorador “Curiosity” y la bomba nuclear, son producto de los avances que se dieron en el campo de la física durante la primera mitad del Siglo XX. Entre estos avances destaca el descubrimiento de la estructura del átomo y el desarrollo de las teorías que la explican. Con estos conocimientos los físicos predijeron y comprobaron que el rompimiento –fisión– de ciertos átomos pesados –uranio y plutonio– libera una gran cantidad de energía y aventuraron la posibilidad de construir una bomba con una potencia sin precedentes. Esto, mediante la liberación incontrolada de la energía almacenada en una cierta masa de uranio o plutonio. En esta perspectiva, los científicos trabajando en el proyecto Manhattan se dieron a la tarea de idear métodos para fabricar una bomba nuclear. Desafortunadamente tuvieron éxito y construyeron en primera instancia, no una, sino tres bombas. Una de ellas fue arrojada por los norteamericanos sobre la ciudad de Hiroshima a las 8:15 horas del día 6 de agosto de 1945. Otra había sido detonada a manera de prueba en el desierto de Nuevo México tres semanas antes. La última explotó sobre la ciudad de Nagasaki el 9 de agosto de 1945.       Además de la destrucción de Hiroshima y Nagasaki, se estima que hasta diciembre de 1945  los ataques nucleares a ambas ciudades produjeron en conjunto entre 150,000 y 250,000 muertes. Estos números resultan abrumadores, no se diga después de leer los escalofriantes testimonios de los sobrevivientes al ataque nuclear, que Dante bien podría haber suscrito. Adicionalmente, unas 280,000 personas fueron expuestas a la radiación, sufriendo de sus efectos a lo largo de su vida en mayor o menor medida.     El proyecto Manhattan que llevó a la fabricación de la bomba nuclear fue establecido por el presidente Roosevelt como un medio para defender a los Estados Unidos de un desarrollo similar por parte de Alemania. Este país, no obstante, nunca desarrolló armas nucleares y de hecho se rindió a los aliados meses antes de que los Estados Unidos hubieran fabricado su primera bomba nuclear. En estas condiciones el objetivo del gobierno norteamericano para el desarrollo final y uso de la bomba fue el de obligar a Japón a aceptar una rendición inmediata y evitar así la muerte de más soldados norteamericanos. Esto último, sin embargo, ha sido muy controvertido y hay quienes aseguran que el uso de las armas atómicas en Hiroshima y Nagasaki fue injustificado, pues Japón en esos momentos estaba prácticamente derrotado y a punto de rendirse. En contraste con el bombardeo de Hiroshima y Nagasaki, el próximo arribo del explorador “Curiosity” de casi una tonelada de peso a la superficie de Marte, constituirá –de resultar exitoso– una realización tecnológica con pocos puntos de controversia. El “Curiosity” descenderá mediante una complicada maniobra que requerirá de un control preciso de cada una de sus etapas, control que será llevado a cabo por la computadora a bordo del explorador. Así, sin la existencia de las computadoras, el “Curiosity” no podría haberse concebido. Las computadoras, no obstante, son solamente un eslabón de la cadena de desarrollos científicos y tecnológicos que hacen posible a un ingenio como el “Curiosity”, pues éstas no habrían existido sin la electrónica. A su vez, al igual que la tecnología nuclear, la electrónica es en último término también producto de la física de la primera mitad el Siglo XX. De este modo, podemos considerar que la bomba atómica y el “Curiosity” tienen antecesores comunes, lo que es en realidad el caso de un buen número de las tecnologías presentes que tanto han impactado nuestra vida.  Este impacto, por otro lado, algunas veces ha sido para bien y otras para mal. Una tecnología, por supuesto, no es intrínsecamente buena o mala y sólo será una u otra cosa en función de la aplicación que le demos. Ciertamente, mal empleada puede resultar negativa por más virtudes que pueda tener. Desde esta perspectiva todas las tecnologías son iguales, aunque, haciendo de uso de la frase de George Orwell, podríamos afirmar que si bien todas las tecnologías son iguales, algunas son más iguales que otras.    Este quizá sea el caso de la tecnología nuclear a la que el presidente Truman, al anunciar la explosión nuclear sobre Hiroshima, calificó como ”fuerza tremenda para el avance de la civilización lo mismo que para su destrucción”. Al cabo de los años, tal parece que la energía nuclear, si bien ciertamente constituye una fuerza tremenda, presenta demasiados problemas para basar sobre la misma el avance de la civilización, al mismo tiempo que representa un peligro para su destrucción. Tanto los bombardeos de Hiroshima y Nagasaki, como los accidentes de Chernobyl y Fukushima, así nos lo indican.",
    "Según el sitio de internet de la NASA, en las primeras horas del próximo lunes 6 de agosto arribará a la superficie de Marte el explorador “Curiosity”, después de un viaje de poco más de 8 meses desde nuestro planeta. En contraste con sus predecesores, los exploradores gemelos “Spirit” y “Opportunity”, que descendieron en 2004 a la superficie marciana de manera más o menos violenta, el “Curiosity” pretende hacerlo suavemente. En efecto, mientras que tanto el “Spirit” como el “Opportunity” hicieron su último acercamiento velozmente al suelo firme protegidos por globos inflados, rebotando como pelotas un buen número veces antes de detenerse, el “Curiosity” intentará descender suavemente suspendido por cables desde un módulo levitando a 20 metros de altura. Cabe hacer notar que el tamaño del “Curiosity”, considerablemente más pesado que el “Spirit” y el “Opportunity”, hizo imposible un descenso como el practicado con estos dos exploradores.La maniobra de descenso se iniciará con el explorador “Curiosity” ingresando a la atmósfera marciana a una altura de 125 kilómetros en el interior de una cápsula con protección térmica. Esta protección es indispensable, pues con una velocidad de ingreso de 21,000 kilómetros por hora, el roce con la atmósfera marciana elevará de manera considerable la temperatura exterior de la cápsula. En poco más de cuatro minutos, la fricción atmosférica habrá frenado la velocidad de caída de la cápsula hasta unos 1,500 kilómetros por hora, momento en el cual se abrirá un paracaídas para reducirla aún más. A una altitud de 1.6 kilómetros, cuando  la velocidad de caída se haya reducido hasta unos 300 kilómetros por hora, el “Curiosity”, acoplado a un módulo propulsor, se desprenderá de la cápsula protectora y del paracaídas e iniciará un descenso frenado por cohetes. Finalmente, a una distancia de 20 metros de la superficie, con el modulo propulsor levitando, el “Curiosity” descenderá pendiente de cables y se posará suavemente sobre el suelo marciano. En total, desde el ingreso a la atmósfera marciana hasta el aterrizaje del explorador transcurrirán siete minutos, durante los cuales las maniobras a realizar deberán ser ejecutadas con gran precisión. Todo lo anterior se realizará de manera autónoma sin la intervención de la NASA, pues por la distancia a la que se encontrará Marte en esos momentos, las órdenes que pudieran enviarse desde la Tierra tardarían cerca de 14 minutos en llegar. La maniobra que intentará realizar el “Curiosity” es entonces considerablemente más compleja que las que realizaron sus  predecesores y habrá que cruzar los dedos para que tenga éxito. La última etapa de esta maniobra, no obstante, es necesaria para evitar que un descenso con cohetes hasta la superficie de Marte levante una nube de polvo que dañe los instrumentos del explorador.En caso de que el “Curiosity” logre posarse suavemente sobre la superficie de Marte, la NASA estará en posibilidades de extender de manera amplia el conocimiento que se tiene sobre este planeta. En efecto,  el “Curiosity”, con un peso de una tonelada y un costo de 2,500 millones de dólares, es considerablemente más complejo que las sondas que lo precedieron, contando con mejores cámaras de video, diez  instrumentos científicos de gran sofisticación, y una mayor movilidad sobre el terreno marciano. Adicionalmente, a diferencia del “Spirit” y el “Opportunity” que dependen para operar de la energía del sol,  el nuevo explorador cuenta con una fuente de energía nuclear que le da una mayor autonomía.  El “Curiosity” pretende llevar a cabo estudios para averiguar la composición química de las rocas marcianas, así como de la insolación y del clima de Marte. Sin embargo, el objetivo más publicitado –por el interés público que despierta– es el relativo a averiguar si Marte tiene, o tuvo en el pasado, condiciones adecuadas para albergar vida. Aunque el explorador no pretende determinar si existe vida en Marte, sí hará una búsqueda de los elementos indispensables para su desarrollo.De todos los planetas del sistema solar, el que más ha capturado la atención de los países capaces de lanzar naves interplanetarias ha sido Marte. Las razones para esto son obvias. Es decir, los planetas más cercanos son más fáciles de explorar y en este respecto Marte y Venus son nuestros vecinos inmediatos. Venus, sin embargo, es un planeta extremadamente caliente, con una temperatura de 400 grados centígrados en su superficie, que en poco se parece al nuestro. Por el contrario, Marte, a pesar de ser un planeta frío, árido y aparentemente muerto, guarda una cierta semejanza con la Tierra. De este modo, podemos concebir que exista vida en Marte –así sea microbiana– pero difícilmente pensamos que la habrá en Venus.El explorador “Curiosity” es el proyecto más sofisticado de cuantos se han dedicado a la exploración de Marte. Parte de esta sofisticación reside en el método de aterrizaje, suspendido de cables desde una plataforma en levitación. Mientras que algunos expertos aseguran que esta maniobra es riesgosa, otros afirman que no lo es tanto y que, en todo caso, es la técnicamente adecuada. En poco más de una semana tendremos noticias desde Marte y sabremos quién tiene razón. Esperemos que sean buenas.",
    "Imaginemos que nos trasladamos algunas décadas hacia el futuro en una máquina del tiempo. Es posible que una vez allá encontremos que el problema del calentamiento global, aunque no se haya resuelto por completo, sí se haya paliado en cierta medida a través de la sustitución parcial de los combustibles fósiles por fuentes renovables de energía, entre ellas –de manera notable– la energía solar. Así, quizá podríamos ser testigos de la siguiente escena. El propietario de una casa decide que el exterior de la misma necesita de una mano de pintura y acude al supermercado a comprar los materiales necesarios para aplicársela. Hasta aquí no habría habido nada que nos sorprendiera, de no ser porque la pintura que adquiere es bastante más sofisticada que las que conocemos, pues no solamente cumple con el propósito de decorar la casa y de protegerla contra la humedad o  aislarla térmicamente, sino que adicionalmente la provee de energía eléctrica a partir de la energía del Sol. De hecho, el propietario había decidido repintar su casa no solamente para mejorar su aspecto, sino porque la cantidad de energía eléctrica que producía había decaído significativamente. Una cubierta de pintura que cumpla con la función de convertir la energía del Sol en energía eléctrica –es decir, que constituya una celda solar– debe consistir de al menos tres capas distintas, aplicadas una tras la otra. La capa intermedia tiene la función de atrapar a la radiación solar, misma que convierte en una corriente eléctrica con la ayuda de las dos capas externas. Las conexiones eléctricas que llevarían la electricidad a los distintos puntos de la casa para su consumo se fijan en dichas capas. El diseño de la celda solar podría ciertamente ser más complicado e incluir más de tres capas. Desde el punto de vista del usuario, no obstante, la aplicación de la pintura no representaría mayor problema y sólo necesitará de más de trabajo de su parte.    Regresando al presente de nuestro viaje al futuro, no encontramos aventurado afirmar que en algún momento no muy lejano se desarrollen pinturas como las descritas que nos permitan aprovechar eficientemente la radiación solar. De hecho, a manera de ejemplo, la compañía química alemana BASF afirma haber desarrollado una pintura para automóviles la cual, además de su función decorativa, podría generar parte de la electricidad que consume el vehículo. Una tecnología como la que encontramos en nuestro imaginario viaje al futuro tiene, entre otras, una característica distintiva: hace uso de de las paredes y del techo de la casa sin requerir de soportes adicionales, con la reducción de costos consecuente que podría alcanzar un 50%. Siguiendo esta idea, un número de laboratorios en el mundo están desarrollando celdas solares transparentes que podrían instalarse sobre los vidrios de las ventanas de casas y edificios. De este modo, las paredes de cristal de un rascacielos, además de permitir su iluminación interior, lo proveerán de parte de la energía eléctrica que consuma.En primera instancia podría parecer imposible que una celda transparente pueda generar electricidad solar, pues para que esto suceda la celda primero tiene que atrapar a la radiación del Sol, cosa que un objeto transparente no puede hacer. No obstante, hay que considerar que dicha radiación tiene componentes infrarroja y ultravioleta que no podemos ver, además de la componente visible a la que son sensibles nuestros ojos. Son precisamente las componentes infrarroja y ultravioleta de las que hacen uso las celdas transparentes. En relación a esto último, un grupo de investigadores de la Universidad de California, Los Ángeles (UCLA) reportó esta semana en la revista “ACS Nano”  el desarrollo de una celda solar de gran transparencia que hace uso fundamentalmente de la radiación infrarroja del Sol. Dado que sólo aprovecha una parte de la radiación solar, la eficiencia de dicha celda es de solamente 4%, que es pobre en comparación con las eficiencias de las celdas comerciales que pueden superar el 15%. Tiene en cambio una trasparencia mayor al 60% y puede ser usada como ventana de iluminación.   Aun con una baja eficiencia de conversión, las celdas solares transparentes generarán cantidades sustanciales de energía si se usan de manera masiva en lugar de ventanas y fachadas de vidrio –que de otra manera solamente tendrían la función de iluminación–. Después de todo, la fotosíntesis de las plantas, que genera varias veces más energía a partir del Sol que toda la que consume el mundo, tiene una eficiencia de conversión que apenas ronda al 5%. Esta baja eficiencia es, sin embargo, compensada por las grandes áreas de la superficie terrestre que cubre la vegetación. Dados los avances tecnológicos actuales es posible que no tuviéramos que viajar muchas décadas hacia el futuro para encontrar que casas y edificios producen por sí mismos al menos parte de la energía que consumen. Y hay la posibilidad de que esto pudiera darse por procedimientos tan simples –para el usuario– como pintar las paredes y el techo de la casa con la pintura adecuada, o cubrir ventanas y fachadas de vidrio con la película correcta. Pero esto sólo nos lo dirá el tiempo.",
    "Suponga que por alguna razón usted quisiera enviar un mensaje al futuro, dirigido a generaciones que vivan dentro de diez mil años o incluso dentro de un millón de años. ¿Qué método escogería? Pensar en un millón de años luce bastante complicado, pues además de que tendríamos que escoger un vehículo trasmisor que sobreviva todo este tiempo, nos enfrentamos al problema de cómo hacer inteligible un mensaje a aquellos que serán sin duda intelectualmente diferentes de nosotros. Enviar un mensaje diez mil años hacia el futuro resulta también una empresa difícil, aunque quizá más manejable. De hecho tenemos ejemplos cercanos al respecto. Uno de éstos, las pirámides egipcias, nos hacen saber, a una distancia temporal de más de cinco mil años, del inmenso poder del que gozaban los faraones en cuyo honor fueron construidas. La mayor de todas, la pirámide de Keops, ha sobrevivido todo este tiempo gracias a sus grandes dimensiones, aunque ahora luce algo maltrecha por el embate de terremotos y la destrucción deliberada a que ha sido sometida, además de que los profanadores de tumbas la han hecho su víctima desde épocas tempranas.El envío de mensajes inteligibles hacia tiempos distantes en el futuro ha sido en las últimas décadas motivo de estudios detallados. Esto, por el interés de alertar a las generaciones futuras de los sitios en los que se hayan depositado desechos radiactivos, provenientes tanto de las centrales nucleares generadoras de energía, como de los procesos de fabricación de armas nucleares. Como sabemos, uno de los grandes problemas de la industria nucleoeléctrica es el del confinamiento de los desechos altamente radiactivos que generan, y que en su mayoría se guardan en albercas en los mismos reactores nucleares que los producen, o bien en sus cercanías. El peligro que esta práctica representa se hizo evidente durante el accidente de la central nuclear de Fukushima provocado por el Tsunami que golpeó a Japón en marzo de año pasado, y que expuso al medio ambiente algunos de los desechos nucleares almacenados en el interior de los edificios que albergan a los reactores.Una de las soluciones que se han propuesto para el manejo seguro de los desechos radiactivos –y que se han implementado en algunos pocos casos– es su confinamiento en depósitos situados a varios cientos de metros bajo tierra. Esto hace necesario poner signos de advertencia alrededor del sitio del confinamiento sobre su peligrosidad. Algunos, además, consideran que los signos tendrían que sobrevivir por todo el tiempo en que dura la radioactividad de los desechos, lo que significa en algunos casos cientos de miles de años.¿Por qué medio podría grabarse información que sobreviva tanto como el tiempo de vida de los desechos radiactivos? En el congreso  “Euroscience Open Forum”, celebrado esta semana en Dublín, Irlanda, Patrick Charton, investigador de la agencia francesa para el manejo de los desechos radiactivos, presentó una posible solución. Ésta consiste de dos discos de  zafiro de 20 centímetros de diámetro. A uno de estos discos se le graba con platino la información deseada, la cual se protege uniendo herméticamente los dos discos. El creador del dispositivo piensa que podría guardar la información grabada –hasta 40,000 páginas– por 10 millones de años.Esto, por supuesto, no resuelve completamente el problema, pues, como el mismo Charton se cuestiona, ¿en qué idioma tendría que hacerse la advertencia del peligro radiactivo para que la entiendan los habitantes del planeta tierra dentro de cien mil años? Los jeroglíficos egipcios, por ejemplo, fueron descifrados gracias a la Piedra Rosetta, que contiene un mismo texto grabado en tres escrituras distintas, incluyendo la jeroglífica. Estas escrituras corresponden a tres periodos de la historia de Egipto, el último cercano al inicio de nuestra Era. Si no tomamos las providencias necesarias, nuestros lejanos descendientes podrían tener que recurrir a una Piedra Rosetta para entender los mensajes.El Departamento de Energía de los Estados Unidos tomó al toro por los cuernos y se dio a la tarea de desarrollar un sistema de signos de alerta para el confinamiento localizado cerca de Carlsbad, Nuevo México, dedicado al almacenamiento de desechos radiactivos resultantes de la fabricación de armas nucleares. Se pretende que dicho sistema tenga una vigencia de 10,000 años.Es posible, no obstante, que nos estemos preocupando demasiado y que diez mil años en el futuro, con una tecnología infinitamente superior, nuestros descendientes no necesitaren de rudimentarias advertencias para protegerse del material radiactivo que hayamos sembrado aquí y allá. En todo caso, posiblemente estemos siendo candil de la calle y oscuridad en la casa, más preocupados por un futuro lejano e impredecible, que por un presente lleno de desechos radiactivos mantenidos en condiciones vulnerables, y que no parece vayan a ser confinados de manera segura con la celeridad requerida.",
    "En los últimos días se ha reavivado la controversia en torno a la muerte del líder palestino Yasser Arafat, ocurrida en París en noviembre de 2004. Esto debido a que un análisis de laboratorio recientemente dado a conocer halló dosis elevadas de una sustancia altamente radiactiva en la ropa de Arafat, lo que sugiere que murió envenenado.Si bien el interés público sobre los detalles de muerte del líder palestino es avivado por las  implicaciones políticas que tiene su posible envenenamiento, la causa de la muerte de un personaje célebre es por sí misma motivo de curiosidad. Esta curiosidad, además, no se reserva para personas recientemente fallecidas, y por el contrario aumenta en la medida que nos trasladamos hacia el pasado. Un ejemplo de esto son las sesiones históricas clínico-patológicas organizadas por la Escuela de Medicina de la Universidad de Maryland, en los Estados Unidos, en las que se busca diagnosticar los problemas de salud de personajes célebres sobre la base de la información clínica disponible. Entre los personajes estudiados se cuentan Pericles, Alejandro Magno, Herodes, Colón, Mozart y Darwin, entre  otros.   En la sesión del pasado mes de mayo, comentada en la revista “Science” del día 5 de julio, se estudió  el caso de Lenin, quien murió con convulsiones a los 53 años de edad en enero de 1924. Antes de morir Lenin estaba en cama discapacitado sin poder hablar de no ser monosílabos. Sobre la base de los síntomas que presentó antes y al momento de su muerte, así como de la autopsia que se le practicó, uno de los presentadores de la sesión clínico-patológica llegó a la conclusión que Lenin sufrió de múltiples infartos cerebrales. Otro participante, no obstante, consideró la posibilidad de que Lenin hubiera sido envenenado. El probable sospechoso en este caso habría sido Stalin, quien se benefició con su muerte. Por supuesto, tal como ocurre con Arafat, la muerte de un personaje célebre, más allá de la curiosidad que despierta, tiene interés por las consecuencias que puede acarrear ¿Qué habría sucedido si Lenin, por enfermedad o por asesinato, no hubiera muerto en esos momentos? Probablemente la historia de la Unión Soviética habría caminado por rumbos distintos a los que siguió en las décadas subsecuentes.Otro caso que ha sido objeto de estudio clínico-patológico es el de Beethoven, quien murió en marzo de 1827 a la edad de 56 años. Beethoven padeció a lo largo de su vida de todo un catálogo de enfermedades, incluyendo viruela, tifus, gota, artritis, así como episodios de dolor abdominal, vómito, diarrea y constipación. Sufrió, igualmente de dolor de ojos, de dolor de cabeza, de hemorragias nasales, de hinchazón de piernas y de asma. Y, por supuesto, irónicamente a los 28 años empezó a perder el sentido del oído, quedando sordo a la edad de 45 años. Al morir Beethoven se le practicó una autopsia que reveló severos daños en hígado, riñones y páncreas. Además, sorprendentemente, pues Beethoven estuvo componiendo música hasta muy poco tiempo antes de su muerte, su cerebro también estaba dañado. La causa de la muerte de Beethoven ha sido motivo de controversia. Algunos la han atribuido a envenenamiento con plomo, basados en el análisis químico del cabello y de los huesos de cráneo del compositor que arroja un contenido de plomo por arriba de lo normal. El plomo habría provenido del vino barato al que Beethoven era aficionado. Se ha señalado también que la cirrosis hepática que fue evidente al momento de su autopsia –debida a un alto consumo de alcohol o a otra causa–, pudiera haber contribuido a su muerte.El caso Beethoven está recogido en el libro “Post mortem: resolviendo los más grandes misterios médicos de la historia”, cuyo autor es Philip Mackowiak, organizador de las sesiones clínico-patológicas de la Universidad de Maryland. Mackowiak escribe que sí se asume que los padecimientos de Beethoven no fueron debidos a una desafortunada conjunción de múltiples enfermedades, y que por el contrario dichos padecimientos –incluyendo la sordera– tuvieron una sola causa, ésta probablemente fue la sífilis.  Así, la muerte de uno de los compositores más grandes que han existido, que –según testimonios de la época– conmovió hasta a la misma naturaleza que desencadenó una tormenta eléctrica en el mismo momento en que el músico exhaló el último suspiro, habría sido más que ordinaria.  Las consecuencias de sordera que padeció Beethoven, por el contrario, están muy lejos de ser ordinarias. En primera instancia podríamos quizá pensar que fueron negativas, pues impidieron al compositor escuchar la música que creaba, además de que lo pusieron en un estado de tensión impropio para desarrollar su arte. Hay quien piensa, no obstante, que la sordera fue positiva y que el aislamiento sonoro en realidad le ayudó a crear sonidos que de otra manera no hubiera concebido. Se cumpliría así el dicho según el cual, no hay mal que por bien no venga.",
    "La compañía estadounidense Abound Solar fabricante de paneles solares, anunció esta semana que presentará una solicitud de declaración de quiebra. Dicha compañía había recibido del Departamento de Energía de los Estados Unidos una garantía de crédito por 400 millones de dólares para la construcción de dos plantas de fabricación de paneles solares en Colorado e Indiana. Contra esta garantía, Abound Solar obtuvo un préstamo de 70 millones de dólares, de los cuales se considera que sólo se recuperarán de 10 a 20 millones de dólares después de la quiebra. La pérdida será absorbida por los impuestos que pagan los trabajadores norteamericanos.Abound Solar no es la primera compañía fabricante de módulos solares que es apoyada financieramente por el gobierno de los Estados Unidos y que se va a la quiebra. Anteriormente, en septiembre de 2011, la compañía Solyndra con base en California corrió la misma suerte. Al igual que Abound Solar, Solyndra había obtenido una garantía de crédito del Departamento de Energìa, en su caso por 535 millones de dólares. Contra esta garantía Solyndra recibió un préstamo de 528 millones de dólares de fondos públicos. Ambas compañías Abound Solar y Solyndra fabrican módulos solares basados en celdas de película delgada. Estas celdas compiten en costos con las celdas más comunes que son fabricadas a partir de silicio. Este material es el mismo que se emplea para la fabricación de los “chips” que usan las computadoras, aunque no con la misma pureza. Las celdas de película delgada tienen ventajas potenciales sobre aquellas basadas en silicio. En efecto, como su nombre lo indica, este tipo de celdas se fabrican depositando una serie de películas con espesores muy pequeños –aproximadamente un centésimo del grosor de un cabello– sobre un soporte adecuado –una placa de vidrio, por ejemplo–. Requieren así de muy poco material base y por tanto potencialmente podrían alcanzar un bajo costo.En contraste, las celdas de silicio tienen espesores varios cientos de veces más grandes que las de película delgada, de modo que para su fabricación se requiere de mucho más material base lo que eleva su costo.En la práctica, no obstante –y por diversas razones–, los módulos solares basados en celdas de película delgada no difieren mucho en precio de los módulos con celdas de silicio. De hecho, tradicionalmente estos últimos han dominado el mercado mundial de módulos solares. Actualmente, entre el 80% y el 90% del total de módulos vendidos son de silicio. En años recientes, como resultado tanto del surgimiento de China como fabricante de módulos solares, como de una reducción en los precios del silicio, el costo de los paneles solares basados en este material se ha reducido de manera considerable –30-50% en 2011–. Esto ha hecho que sus contrapartes de película delgada pierdan competitividad. En estas circunstancias –agravadas, además, por la reducción de la demanda de módulos solares debido a los problemas económicos por los que atraviesa Europa–, es fácil entender que Solyndra y Abound Solar no hubieran podido resistir el embate de las compañías chinas y se hayan ido a la quiebra. Por otro lado, la reducción de precios de los módulos solares ocurrida en los últimos años hace que, bajo ciertas circunstancias, la generación de energía eléctrica por métodos solares se haya vuelto competitiva con otros medios tradicionales de generación de electricidad –con la  ventaja de que el Sol es una fuente no contaminante del medio ambiente. Esto ocurre en nuestro país en el caso de las casas-habitación que están clasificadas por la Comisión Federal de Electricidad como de alto consumo de energía eléctrica. En tal caso, resulta ventajoso económicamente instalar paneles solares para la generación, no de toda la energía eléctrica consumida, sino solamente de aquella cantidad suficiente para que la casa habitación sea reclasificada en una tarifa inferior. Con los costos actuales de módulos solares –aproximadamente 1 dólar por Watt– una instalación solar con estas características se pagaría aproximadamente en cinco años.Lo anterior está basado en un esquema mediante el cual se interconectan los módulos solares a la red eléctrica pública, de tal modo que toda la energía producida en exceso durante el día sea comprada por la compañía proveedora de electricidad al mismo precio al que ésta se la vende al usuario. De este modo, se reduce la cantidad a pagar por consumo de energía eléctrica.Así, si bien la energía eléctrica solar solo es competitiva en ciertas circunstancias especiales, tendríamos la esperanza que en los próximos años su costo se reduzca aun más, hasta el punto en que su uso sea generalizado. Aprovecharíamos de este modo una energía prácticamente inextinguible, limpia y gratis –una vez que hayamos pagado los módulos solares, por supuesto. Para esto posiblemente los chinos tendrán que esforzarse en reducir aún más el costo de los módulos solares. Aunque en el camino pudieran hacer quebrar a empresas menos competitivas, en perjuicio, en último término, de los dineros públicos.",
    "La década de los años sesentas fue testigo de uno de los acontecimientos más notables en la historia de la música popular: la llamada “Invasión Británica” a los Estados Unidos que, como sabemos, se inició en 1964 con el arribo de los Beatles a ese país. A la llegada de éstos pronto siguió la de un gran número de grupos ingleses con un sonido nuevo que dominó el  panorama musical de los Estados Unidos por un buen número de años.Lo sorprendente del caso es que, según los conocedores, los orígenes musicales de las bandas de la Invasión Británica se sitúan precisamente en los Estados Unidos. De manera precisa, y no en poca medida, en el Rock and Roll, estilo musical que apareció en ese país en los años cincuenta y que era practicado por artistas tanto negros como blancos. El Rock and Roll, a su vez, tuvo sus raíces en música negra del sur de los Estados Unidos que se expandió hacia el norte industrializado en la primera mitad del Siglo XX.En estas circunstancias cabe preguntarse ¿por qué no ocurrió en los Estados Unidos la revolución musical que sí se llevó a cabo en Inglaterra? Es decir, ¿Por qué fueron precisamente los jóvenes músicos ingleses los que crearon una revolución de tan grandes proporciones a partir de raíces musicales no propias? ¿No habría sido más natural que lo hubieran hecho sus contrapartes norteamericanas? Dar una respuesta precisa a las preguntas anteriores es complicado, por decir lo menos. En términos generales, la evolución musical depende de manera compleja de factores múltiples de índole social, cultural, económica, tecnológica e incluso comercial, sin olvidar el concurso de los compositores e intérpretes que son los que a fin de cuentas crean y diseminan la música.En un interesante artículo publicado esta semana en la revista “Proceedings of the National Academy of Sciences” de los Estados Unidos por investigadores británicos se arroja luz en este respecto. En dicho artículo se identifica a los oyentes, consumidores de la música, como una de las fuerzas que determinan la dirección de su evolución. Aun más, se especula que la creación musical puede darse incluso sin la participación de los compositores y descansar enteramente en los consumidores. En este caso, la fuerza creativa se derivaría de la opción que tienen estos últimos para escoger la música que desean escuchar.Dicha conclusión está basada en experimentos realizados con un programa de computadora denominado “Darwin Tunes”. El experimento se inicia con la computadora generando al azar 100 secuencias de sonidos con “genes digitales” característicos. Si bien los sonidos en cada secuencia inicial se repiten a intervalos fijos, en todos los casos constituyen una cacofonía que poco tiene que ver con cualquier concepción que tengamos de la música.Las secuencias, no obstante, tienen la oportunidad de “aparearse” y generar descendencia que hereda características combinadas de los padres, cual si fueran organismos vivos. Además, durante el proceso de reproducción se generan mutaciones al azar que introducen cambios adicionales en la descendencia. De esta manera, después de reproducirse por muchas generaciones, las secuencias primitivas evolucionan hacia formas de música con un cierto grado de complejidad. La fuerza que dirige esta evolución es el gusto de los “consumidores”, quienes determinan las secuencias de sonidos a las que les es permitido reproducirse.Para este propósito, 7,000 voluntarios reciben vía Internet 20 secuencias escogidas al azar entre las 100 posibilidades. A estas secuencias los participantes otorgan una calificación con cinco niveles, desde “no la puedo soportar” hasta “me gusta”. Después de esto, “Darwin Tunes” escoge las diez secuencias que en promedio resultan mejor calificadas y las aparea para producir 20 descendientes que sustituyen a las 20 secuencias que fueron evaluadas. De esta manera se mantiene una reserva de cien secuencias de antigüedad variada, sujetas a un proceso de selección natural. Como resultado, después de 2,500 generaciones dicho proceso transformó las secuencias de ruido originales en algo estructurado con rasgos musicales de cierta complejidad. Se habría producido así música mediante un proceso de selección natural, a partir de sonidos caóticos y sin el auxilio de un compositor.Por otro lado, como los autores del artículo referido lo reconocen, el factor que han identificado en su trabajo es solamente uno de varios que determinan la evolución de la música. Esta evolución estaría, además, determinada por los compositores, que introducen sonidos nuevos que no necesariamente resultan agradables en una primera impresión. Los gustos musicales individuales, además, están influidos por los gustos colectivos, un factor que fue claramente visible en la revolución de los años sesenta.Otro factor evidente en esa época fue el aspecto comercial que manejó de manera exitosa la imagen de los grupos británicos y la difusión de su música –factor  que en México funcionó desafortunadamente en sentido inverso, bloqueando a los grupos ingleses con pocas excepciones, en beneficio de algunos grupos nacionales no necesariamente más creativos.El artículo de los investigadores británicos es iluminador y revela uno de los mecanismos que impulsan la evolución de la música. Estamos, no obstante, lejos de entender cabalmente qué fue lo que ocasionó la revolución musical de los sesentas, un fenómeno más que fascinante.",
    "Jonathan Swift describe en “Los viajes de Gulliver” las peripecias del protagonista en el país de los liliputenses, semejantes a los humanos pero con sólo 15 centímetros de altura. Gulliver igualmente visita Brobdingnag, país de gigantes doce veces más altos que nosotros. Según Swift, las proporciones del cuerpo de los enanos y de los gigantes habrían sido las mismas que las de nuestra especie, de modo que los habitantes de Liliput y Brobdingnag habrían lucido igual que nosotros, excepto por la escala.Los Viajes de Gulliver fueron publicados en 1726 y quizá no sea razonable esperar que Swift hubiera sido muy puntilloso en cuanto a cuidar la consistencia de su novela con los principios de la mecánica. Después de todo, estos principios fueron descubiertos y publicados por Isaac Newton en 1686, apenas 40 años antes de la publicación de los Viajes de Gulliver. Hoy en día, no obstante, el curso de física elemental de la escuela preparatoria nos enseña –o nos debería enseñar– que los enanos y gigantes tal como los concibió Swift no pueden existir; esto al menos en nuestro planeta. En realidad, aun sin el beneficio de un curso elemental de física, podemos fácilmente  convencernos de esta imposibilidad. Pensemos, por ejemplo, en un caballo que mediante una técnica –mágica– hacemos crecer hasta diez veces su tamaño normal, manteniendo todas sus proporciones. De este modo, si inicialmente el caballo tenía un peso de 400 kilogramos, después del crecimiento alcanza un valor 1000 veces mayor, es decir ¡400 toneladas! –esta cifra se obtiene multiplicando el peso inicial por 10x10x10–. En contraste, la resistencia de las patas del caballo sólo crece 100 veces, debido a que dicha resistencia varía de acuerdo con el área transversal de las de las mismas; es decir, por un factor igual a 10x10=100.Tendríamos así un caballo con patas demasiado delgadas y diez veces menos resistentes en relación con su peso que las de un caballo de tamaño normal. Dicho caballo difícilmente podría sostenerse en pie, ya no digamos sobreponerse a una caída al correr a galope.  Todo lo anterior adquiere relevancia con relación a un artículo publicado el pasado 6 de junio en la revista electrónica PLoS ONE por un grupo de investigadores de la Universidad de California-Berkeley. En dicho artículo, cuyo primer autor es Jean-Michel Mongeau, se reporta un estudio de los movimientos que cucarachas y lagartijas llevan a cabo al encontrar un obstáculo. En particular, se investigaron las acrobacias que éstas realizan cuando, al correr a gran velocidad, se encuentran de manera repentina con el final de la pista. Para este propósito, colocaron a una cucaracha sobre una regla suspendida en el aire y la pusieron a correr a lo largo de la misma. Para no perder detalle, la filmaron con una cámara de alta velocidad en la medida en que aproximada al borde de la regla. Los resultados fueron sorprendentes. Encontraron que, lejos de detener su carrera al aproximarse al vacío, la cucaracha se siguió de frente. No cayó, sin embargo, pues en el último momento se agarró con las patas del filo de la regla y con esto giró a gran velocidad, aterrizando sobre la cara inferior de la regla en donde siguió corriendo en dirección contraria. Cuando realizaron el mismo experimento con las lagartijas encontraron un resultado similar: estos animales se precipitan hacia el vacío a gran velocidad y aprovechan el impulso que llevan para girar rápidamente agarradas con las patas traseras del borde de la regla.  Es de notar la gran velocidad con que, tanto cucarachas como lagartijas, llevan a cabo el giro hacia la cara inferior de la regla, operación que tan sólo les toma  0.15 y 0.2 segundos, de manera respectiva. Es de notar, igualmente, que esto es posible por un efecto de escala. Es decir, sólo un animal pequeño es capaz de tales acrobacias. De este modo, por ejemplo, no podríamos esperar ver a un elefante –sin importar que tan bueno sea el circo que lo presente– que corra desaforadamente sobre una tarima y al llegar a la orilla se agarre con las patas traseras y gire para terminar boca arriba con las patas apoyadas por abajo del templete.Podría quizás parecer superfluo investigar cómo lagartijas y cucarachas se las arreglan para sortear obstáculos. No lo es, sin embargo, pues de esta manera encontramos la solución que la naturaleza dio a un problema que es relevante para el diseño de robots, específicamente aquellos que requieran desplazarse en terrenos accidentados. Como es el caso, según Mongeau y colaboradores, de robots destinados a la búsqueda de personas sepultadas por un terremoto. En línea con lo anterior, en el artículo de referencia se describe el desarrollo de un mini-robot inspirado en las acrobacias filmadas de cucarachas y lagartijas. Este mini-robot, que puede reproducir dichas acrobacias en buena medida, es un ejemplo de un diseño inspirado en la naturaleza. El voltear hacia la naturaleza para encontrar soluciones a problemas técnicos ha resultado en general una estrategia productiva. Esto no es sorprendente, pues después de todo, la naturaleza llega a sus diseños a través de cientos de millones de años de evolución. Y como dice el refrán, más sabe el diablo por viejo que por diablo.",
    "Así como una golondrina no hace verano, un solo evento climático extremo no constituye una prueba de que nuestro planeta esté experimentando un cambio climático a nivel global. Muchas golondrinas, no obstante, sí anuncian la llegada de la primavera. De la misma manera, según algunos expertos del clima, numerosos eventos extremos de frío o calor, lluvia o sequía, serían una indicación de que nuestro planeta está experimentando un cambio climático paulatino.Es una tesis aceptada por numerosos investigadores que el clima de la Tierra está efectivamente cambiando; esto debido al aumento en la concentración de gases de invernadero en la atmósfera que ha ocurrido desde el inicio de la revolución industrial hace dos siglos. Dicho aumento ha llevado a un incremento de la temperatura promedio de la superficie del planeta que, aunque pequeño e inferior a un grado centígrado, sería suficiente para generar episodios climáticos extremos de una mayor severidad y frecuencia.Como sabemos, nuestro país está sufriendo actualmente un episodio de sequía que asola a varios estados del centro-norte –incluido San Luis Potosí–, y que se reporta como el más grave en los últimos 50 años.  No es claro si esta sequía es causada por el cambio climático, pues un evento extremo aislado como el que estamos padeciendo pudiera obedecer a otras causas. Estaría, no obstante, de acuerdo con aquellos que predicen eventos climáticos extremos con una mayor frecuencia y severidad por efecto del calentamiento global.  Por su lado, de acuerdo con la “Nacional Oceanic and Atmospheric Administration”, los Estados Unidos han sufrido el inicio de año –enero a mayo– más caliente desde que se iniciaron los registros del clima en 1895. Así, el promedio de temperatura registrado en los meses de primavera –marzo-mayo– fue casi 3 grados centígrados más alto que el promedio de temperatura en el periodo 1900-2000. Fue también superior en más de un grado centígrado al récord anterior, vigente desde 1910. Al igual que México, los Estados Unidos están sufriendo de un episodio de sequía y los primeros cinco meses del año han tenido en promedio menos precipitaciones que lo normal. Hay, además, contrastes a lo largo del país y hay estados como Óregon y Washington en la costa oeste que han tenido más precipitaciones de lo usual. El calentamiento global, aparte de los esperados efectos climáticos, pudiera tener también impactos menos obvios. En este respecto, en un artículo publicado el pasado 3 de junio en la revista “Nature Climate Change”, un grupo de investigadores europeos y norteamericanos encabezados por Michelle van Vilet de la Universidad Wageningen en Holanda, analizaron el impacto que el calentamiento global pudiera tener en la industria de generación de energía eléctrica de Europa y los Estados Unidos. Aproximadamente un 80% de la energía eléctrica que consume el mundo se genera en centrales termo-eléctricas y nucleo-eléctricas. Si bien ambos tipos de centrales operan a partir de fuentes de energía primaria muy diferentes, comparten la última etapa del proceso de producción del fluido eléctrico. Es decir, tanto las plantas termo-eléctricas como las núcleo-eléctricas producen su energía empleando generadores accionados por turbinas, que son a su vez movidas por vapor a alta temperatura. En el caso de las plantas termo-eléctricas, el vapor se produce quemando combustibles fósiles, mientras que las plantas nucleo-eléctricas emplean la fisión del átomo para el mismo fin.   Por otro lado, una vez que el vapor deja la turbina tiene que ser enfriado para ser reutilizado y para este propósito es necesario emplear agua de enfriamiento, que es también indispensable para evitar el sobrecalentamiento de las plantas. El agua es de este modo un elemento indispensable para la industria de generación de electricidad, la cual de hecho consume una buena parte del agua dulce disponible –alrededor del 40% en Europa y los Estados Unidos–. No es de este modo difícil entender que dicha industria pueda ser afectada de manera adversa por la escasez de agua derivada de sequías u otros fenómenos climáticos.   Aun más, van Vilet y colaboradores encontraron que la generación de electricidad puede ser afectada no solamente por la escasez de agua de enfriamiento sino también por el incremento en la temperatura de la misma en ríos y lagos debido al calentamiento global. Es decir, a mayor temperatura del agua de enfriamiento por un evento climático extremo, mayor será el volumen de la misma necesario para lograr el mismo efecto.  La conclusión del estudio de van Vilet y colaboradores es que, como resultado del cambio climático, en el periodo 2031-2060, los Estados Unidos y Europa probablemente enfrentarán durante los meses de verano reducciones en su producción de energía eléctrica de 4-16% y  6-19%, en forma respectiva.El calentamiento global del planeta muestra así una nueva faceta. No solamente será responsable de eventos climáticos extremos más frecuentes y con mayor intensidad, o bien de la fusión del hielo en los polos lo que conducirá a un incremento en el nivel de los océanos, lo mismo que de cambios climáticos permanentes en el mundo. Además de todo esto, el monstruo que representa el calentamiento global bien pudiera volverse en contra de las plantas termoeléctricas generadoras de gases de invernadero. A las que, no obstante, en buena medida debe su existencia.",
    "En el año 1824, el médico inglés John Ayrton Paris demostró ante miembros del Colegio Real de Médicos en Londres el fenómeno de persistencia retiniana. Para este propósito, empleó un dispositivo muy simple conocido como taumátropo, que consiste de un disco de cartón con una imagen dibujada en cada una de sus caras y dos cuerdas insertadas en dos perforaciones en puntos opuestos a lo largo de su periferia. El disco se pone a oscilar sobre un eje en su superficie girando las cuerdas con los dedos índice y pulgar, mostrando de manera alternada la imagen en cada una de sus caras. De este modo, ya que el ojo retiene por un corto tiempo las imágenes en la retina, las dos figuras en el disco se funden en una sola y se produce un efecto de movimiento.  En realidad, el fenómeno que demostró Paris se explica hoy en día en términos diferentes a los de la persistencia de imágenes en la retina. Dicho fenómeno, no obstante, es real y constituye el fundamento del cinematógrafo –inventado por los hermanos Lumiere al despuntar el Siglo XX–, que es a su vez la base de toda la industria cinematográfica y la parafernalia que existe alrededor de la misma.En este contexto, se considera que el taumátropo del Siglo XIX –que se convirtió en un juguete muy popular en la época– fue el primer dispositivo exitoso para representar objetos en movimiento, y que como tal constituye el primer y humilde antecedente del cinematógrafo. Al menos esto es lo que se creía hasta que los investigadores franceses Marc Azema y Floret Riviere descubrieron pinturas rupestres realizadas en el Paleolítico, algunas hace 30,000 años, en las que se representan animales en movimiento. En un artículo publicado recientemente en la revista “Antiquity”, en el que se resumen 20 años de investigaciones en cuevas del sur de Francia, Azema y Riviere refieren que en el Paleolítico el movimiento se representó empleando dos técnicas: una por medio de la superposición de varias imágenes sucesivas, y otra colocando en línea, yuxtapuestas, imágenes sucesivas.Los investigadores encontraron numerosos casos de empleo de la primera técnica. En un caso, por ejemplo, se muestra un bisonte con ocho patas, representando dos estados del movimiento del animal. En otro caso se observan cinco imágenes superpuestas de un caballo, cada una en una posición diferente, tanto de las patas como de la cabeza y la cola. El efecto más acusado de movimiento se observa cuando se iluminan las pinturas con una antorcha moviéndose a lo largo de la pared. Estas pinturas y la cueva que las aloja constituirían de este modo un cinematógrafo de la Edad de Piedra. Como un ejemplo de la segunda técnica para representar movimiento, Azema y Riviere citan el descubrimiento de una costilla de un animal bovino del final del Paleolítico, en la cual fueron grabadas en línea, de derecha a izquierda, las imágenes sucesivas de un león en carrera. Sin embargo, lo que posiblemente resulta más impactante es haber descubierto que el traumátropo, cuya invención algunos atribuyen a John Ayrton y otros al astrónomo John Heschel,  no fue producto de ninguno de los dos, ni de alguno de sus contemporáneos, sino de un remoto y desconocido antecesor nuestro de la Edad de Piedra. En efecto, Azema y Riviere, hacen referencia a un disco de hueso descubierto en el siglo XIX en el yacimiento arqueológico de Laugerie-Bassie en el sur de Francia. A este disco, de unos 3.1 cm de diámetro, le fue grabada la imagen de un antílope en cada una de sus caras. En una de éstas, el antílope se muestra parado y en la otra yaciendo sobre sus patas dobladas, como estuviera muerto. Al igual que con los modernos traumátropos, si se hace girar el disco rápidamente y de manera repetida por 180 grados, se tiene una ilusión de movimiento en la que el antílope cae y se vuelve a erguir de forma repetitiva. Al disco le fue también practicado un orificio en su centro el cual, según los investigadores, cumplía la misma función que cumplen el par de orificios en los dispositivos modernos; es decir, servía para insertar un tendón de animal y hacer girar al disco rápidamente.       Resulta sorprendente que los hombres de la Edad de Piedra hayan concebido y logrado introducir el sentido del movimiento en sus obras artísticas. Por otro lado, resulta igualmente sorprendente y lamentable que esta habilidad de alguna manera se haya perdido para el arte en los milenios subsecuentes. Como quiera que sea, si bien ahora es claro que tendremos que ir cuando menos 10,000 años hacia el pasado para encontrar los primeros antecedentes del cinematógrafo, también habremos de reconocer que no por mucho madrugar amanece más temprano. Es decir, aun si nuestra civilización hubiera mantenido desde la prehistoria el secreto del cinematógrafo, el desarrollo del mismo tal como lo conocemos hubiera tenido de cualquier manera que esperar hasta el Siglo XX a que dieran las condiciones tecnológicas adecuadas.",
    "Es posible que en medio de un congestionamiento de tráfico y ante el evidente aumento año con año del número de automóviles circulando en nuestra ciudad, más de uno haya puesto en duda la racionalidad de usar al automóvil como medio de transporte. Con más calma, no obstante, no podemos dejar de reconocer que el automóvil es un invento de una gran utilidad; invento que, entre otras cosas, proporciona independencia de movimiento y rapidez de traslado –de no encontrarnos con un embotellamiento, por supuesto–. Permite, igualmente, escoger un lugar para vivir sin que la distancia al lugar de trabajo sea un factor determinante.Hoy en día el automóvil ha alcanzado en muchos lugares del mundo un lugar preponderante y se ha convertido en algo indispensable y altamente apreciado. En este contexto, resulta extraño pensar que eso no siempre fue así y que, por el contrario, en sus primeros años de existencia los automóviles eran a menudo catalogados en los Estados Unidos como vehículos peligrosos, que ponían en riesgo la seguridad de los peatones en las calles.Como sabemos, los primeros vehículos autopropulsados con motores de gasolina fueron desarrollados de manera independiente por Kart Benz y  Gottllieb Daimler  en Alemania, en la década de los años 80 del Siglo XIX. Fue, no obstante, en gran medida en los Estados Unidos en donde a lo largo del Siglo XX el automóvil se desarrolló como un medio de transporte de uso masivo.En un inicio, a finales del Siglo XIX, los automóviles fueron concebidos esencialmente como carruajes de caballos, pero sin caballos. En los albores del Siglo XX esta concepción cambió y los automóviles en lo subsequente fueron diseñados totalmente como vehículos autopropulsados, con características propias diferentes de aquellas de los coches de caballos. Entre las características que diferencian a un vehículo autopropulsado de un carruaje de tracción animal, se encuentra la velocidad que el primero puede alcanzar y que es considerablemente más alta en comparación con la del segundo. Una mayor velocidad implica, por supuesto, un mayor peligro para los peatones, con lo que las calles antes amigables se volvieron azarosas. Así, no son sorprendentes las opiniones en contra que muchas personas expresaban al inicio del Siglo XX con respecto a los automóviles. Opiniones que, no obstante, se hicieron favorables a partir del la década de los años 30.Peter Norton, autor del libro “Combatiendo el tráfico: el nacimiento de la edad del motor en la ciudad norteamericana”, analiza las causas que llevaron al cambio de actitud de los estadounidenses hacia los automóviles. Como señala el autor, en la década de los años 20 del siglo pasado era preocupante para muchas personas la circulación por las calles de  automotores que provocaban accidentes y muertes entre los peatones. En el contexto de las primeras décadas del Siglo XX, en el que la calle era concebida como un espacio para el peatón –en la que incluso podían jugar los niños– los automóviles constituían intrusos peligrosos.Pronto, no obstante, la presión de los fabricantes de automóviles, la ampliación de las calles para el tráfico de los mismos, así como la evidente ventaja que representa poseer un automóvil, entre otros factores, llevaron a un cambio gradual en la percepción de la calle –percepción que se habría consolidado en los años sesenta–,  conceptualizada ahora como un espacio para los automóviles en donde el peatón se transformó en intruso. Éste adquirió así una responsabilidad compartida con el automovilista en evitar ser atropellado.En las argumentaciones esgrimidas para lograr el cambio de opinión con respecto a los automóviles se habrían incluido incluso aspectos ideológicos. Se argüía, por ejemplo, que la independencia de movimientos que trae el automóvil está de acuerdo con el espíritu independiente de los norteamericanos. Así, por múltiples causas, según Norton, las ciudades norteamericanas entraron de lleno hace ya medio siglo a la edad del motor y con ello el peatón cambió su estatus de propietario de las calles a intruso en las mismas.Al margen de las causas que en México nos llevaron, al igual que en los Estados Unidos, a adoptar al automóvil como medio de transporte urbano, es claro que en nuestro país las calles pertenecen enteramente a los automotores y ni de lejos a los peatones. Como tales, somos intrusos en nuestras propias calles y compartimos con los automovilistas la responsabilidad de cuidarnos de sufrir un accidente. Vistas así las cosas, y dado que los automotores llegaron para quedarse, como peatones deberíamos pensar en pactar un trato justo con éstos y recobrar la propiedad de algunas de nuestras calles a las que los automóviles llegaron en calidad de “paracaidistas”. En este respecto, debemos recuperar para los peatones la totalidad del centro histórico de nuestra ciudad, el cual tiene una traza colonial que, por supuesto, nunca fue pensada para que la transitasen automóviles, que más que propietarios legítimos de las antiguas y estrechas calles lucen como extraños intrusos.",
    "La portada de la revista “Science” de la presente semana muestra la imagen de un edificio de Beirut destruido en 2006 por un bombardeo de la fuerzas de defensa de Israel. La portada anticipa una sección especial en la que se analizan diferentes aspectos de los conflictos entre humanos y la violencia que de éstos deriva. En dicha sección se incluyen artículos, tanto de investigación como de divulgación, que analizan, entre otros muchos temas, los conflictos de origen racial y religioso, los sacrificios humanos, el uso militar de aviones dirigidos a control remoto, el terrorismo y la posibilidad de vivir en paz.Un artículo que nos resulta particularmente impactante –por la similitud con acontecimientos actuales en nuestro país– es el que describe el hallazgo de una serie de tumbas masivas cerca del sitio conocido como Tel Brak en el noreste de Siria, cerca de la frontera con Irak El descubrimiento fue realizado con arqueólogos británicos en el año 2006. Una de estas tumbas, de unos 3 por 20 metros, contiene los restos de 150 personas jóvenes, con edades que fluctuaban al morir entre los  20 y los 35 años. Lo sorprendente del caso es que esto ocurrió en la Edad del Cobre, hace casi 6000 años.Los investigadores especulan que las tumbas masivas de Tel Brak pudieron haber tenido su origen en enfrentamientos militares entre el centro urbano de Tel Brak y las ciudades del sur de Irak. Esto se apoya en que las tumbas no contenían restos ni de mujeres ni de niños o personas mayores, y sólo de jóvenes aptos para la guerra. Aunque no hay indicios de quienes fueron los vencedores, lo que sí queda claro es que quienesquiera que hayan sido festejaron la victoria con una gran comida, que incluyó la matanza de 75 vacas y 300 borregos y chivos. Esto se sabe porque junto a los restos humanos fueron encontrados aquellos del festín, incluyendo los platos empleados en el mismo.    Un asunto que se pone en perspectiva en otro de los artículos de la edición especial de “Science” es la relación que hay entre la violencia y la vida en los centros urbanos –de los cuales Tel Brak es un ejemplo temprano–, cuya aparición implicó estructuras sociales más complejas que aquellas de los grupos nómadas. Habría dos puntos de vista en este respecto. Aquel según el cual la violencia disminuye con la urbanización, de modo que hoy en día la probabilidad de morir asesinado es considerablemente menor que antes de la aparición de las ciudades. En contraposición, hay quién señala que existen en la actualidad sociedades pequeñas en las que la violencia es muy reducida, como ocurre con los indios Pueblo de Nuevo México, con apenas dos asesinatos en un año por cada 100,000 habitantes; esto en contraste con las ciudades de Baltimore y Detroit en donde el número correspondiente es de 34 homicidios anuales.     Al margen de esta controversia, sin embargo, no podemos negar que la violencia va y viene independientemente del desarrollo urbano. En México la padecimos durante buena parte del Siglo XIX e inicio del XX. Posteriormente gozamos de un periodo de paz relativa que duró el resto del siglo, para finalmente ser testigos de un resurgimiento violento en los últimos años. Todo esto al mismo tiempo que México se convertía en una país cada vez más urbano.Por otro lado, independientemente de cualquier otra característica que se le quiera adjudicar, el conocimiento científico tiene un valor utilitario. Es decir, si conociéramos las leyes que gobiernan a un determinado fenómeno natural, podríamos, en principio, desarrollar una tecnología para controlarlo, e incluso ponerlo a trabajar en nuestro beneficio. Esto último es evidente con la tecnología electrónica que nos ha proporcionado desde las computadoras hasta los sistemas de telecomunicación que han cambiado nuestra vida. Si bien entre las tecnologías de más impacto están aquellas relacionadas con ciencias como la Física la Química o la Biología, se esperaría que un mayor conocimiento de los fenómenos sociales y sus causas ayudaran a prevenir la violencia entre humanos. Al respecto, en una editorial que forma parte del número especial sobre conflictos humanos “Science” escribe: “Nuestra especie tiene una larga historia de desconfianza hacia los extranjeros, de desprecio para aquellos que no son de nuestro grupo y de pelear unos contra otros de muchas maneras y en muchos lugares, usando la tecnología más mortífera disponible al momento. Este número especial de “Science” sobre conflictos entre humanos ilustra el papel que tiene la comunidad científica en la adquisición de un entendimiento profundo sobre la historia evolutiva de estos conflictos y, lo más importante, demuestra como la ciencia puede identificar los mayores factores de riesgo que llevan a la violencia masiva”.      En tanto la ciencia toma un papel más activo en la prevención de conflictos humanos, una reflexión sobre Tel Brak nos lleva a concluir que en cuestión de violencia no hay nada nuevo bajo el sol. Al menos no desde hace unos seis mil años.",
    "En una entrevista aparecida al principio de esta semana en el diario cubano Juventud Rebelde, una investigadora miembro del Instituto Cubano de Literatura y Lingüística, comenta que en un intento por ser originales los padres cubanos están dando a sus hijos nombres inventados que en no pocas ocasiones resultan extravagantes, por decir lo menos. En algunos casos éstos se obtienen combinando, de manera más o menos creativa, nombres propios comunes. Así, encontramos nombres como Mayren, que es un híbrido de Mayra y René, y  Adaris, que lo es de Ada y Darío.Otra manera –menos creativa– en que los padres cubanos encuentran nombres para sus hijos es simplemente invirtiendo un nombre propio común. De este modo, Airam, se obtiene de  María, y Ostenre de  Ernesto. Aunque no da un argumento sólido, la investigadora cubana se muestra preocupada por la proliferación de nombres propios tan poco convencionales y considera que el fenómeno debe ser estudiado a fondo. Por otro lado, quienes sí pudieran tener preocupaciones bien fundamentadas son los recién nacidos. En efecto, supongamos, por ejemplo, que a unos padres cubanos les gusta el nombre combinado Pedro Alberto para uno de sus hijos y así deciden registrarlo. En aras de la originalidad, sin embargo, invierten las palabras resultando Ordep Otrebla. Este nombre impronunciable, a todas luces sería una inconveniencia futura para el directamente afectado. Al margen de preocupaciones, no obstante, es un hecho que los lenguajes están en continua evolución y que todos los nombres propios –al igual que todas las palabras– tuvieron que haber nacido en algún momento. En este respecto, un grupo de investigadores de Italia, los Estados Unidos e Israel, encabezados por Alexander Petersen, llevó a cabo un estudio acerca del nacimiento y muerte de palabras en el periodo 1880-2008. Dicho estudio, publicado en la revista “Nature” el pasado mes de marzo, fue llevado a cabo con 10 millones de palabras pertenecientes a tres idiomas, Inglés, Español y Hebreo. Para esto, lo investigadores aprovecharon el proyecto de digitalización de libros de Google, que cubre el 4% de todo el material bibliográfico publicado hasta a la fecha. De acuerdo con el artículo de referencia, las palabras están sujetas a un proceso darwiniano de evolución similar al que experimentan los seres vivos. Como sabemos, las especies compiten unas con otras por los recursos limitados disponibles y solamente  sobreviven aquellas que mejor se adaptan al medio ambiente. De manera similar, las palabras, que nacen por diferentes circunstancias –por ejemplo, como resultado de cambios tecnológicos como es el caso de la palabra internet–  tienen que competir para sobrevivir por recursos limitados, que en este caso son los medios impresos disponibles, los escritores de textos y los lectores de los mismos. Si una palabra no alcanza los medios escritos y no es leída lo suficiente tenderá a desaparecer. Petersen y colaboradores ilustran lo anterior por medio de tres palabras del idioma inglés:  “Roentgenogram”, “Radiogram” y “X-ray”, que son con las que se conoce a la técnica para la obtención de imágenes del interior del cuerpo humano empleando rayos X. Las tres palabras nacieron después de que Wilhelm Roentgen descubrió estos rayos en 1895, y la propiedad que éstos tienen de atravesar el cuerpo humano e imprimir su huella en una película fotográfica.Estudiando la base de datos de Google, Petersen y colaboradores encontraron que a partir de 1920 y hasta 1980 –aunque con variaciones notables–, de las tres apalabras anteriores la más frecuentemente empleada fue “Roentgenogram”. Lo anterior exceptuando un corto periodo alrededor de 1920 en donde fue superada por “Radiogram”. En cambio, a partir de 1980 el uso de estas dos palabras decayó hasta casi desaparecer en la actualidad al ser sustituidas por la palabra X-ray. Los investigadores especulan que esto fue debido a  X-ray es una palabra más corta que sus competidoras, lo que la hace estar mejor adaptada para la lucha darwiniana.Un aspecto muy interesante del artículo referido es haber encontrado que en los últimos 20 años ha disminuido la frecuencia con que nacen nuevas palabras. Al mismo tiempo, se encontró que ha aumentado la frecuencia con que mueren las existentes. Esto último, de acuerdo con Petersen y colaboradores, es atribuible a la aparición de los procesadores digitales de texto equipados con correctores de palabras. En efecto, durante la edición digital de un texto las palabras escritas con errores ortográficos son subrayadas como incorrectas –o de plano cambiadas a criterio del corrector– y de esta manera tienden a desaparecer. Así, la edición digital de textos ha modificado las fuerzas que moldean la evolución del lenguaje, de modo que las palabras tienden a permanecer inalteradas.Vistas de este modo las cosas, podríamos pensar que las preocupaciones que han generado la aparición de nuevos nombres propios en la isla de Cuba es posiblemente exagerada, pues la supervivencia de un nuevo nombre depende de lo bien adaptado que esté para la competencia evolutiva. Nombres impronunciables –resultado de invertir palabras pronunciables– gozarían entonces una efímera existencia. En cambio, nombres propios como Mayren y Adaris, cortos y de fácil pronunciación, tendrán más posibilidades de sobrevivir para beneficio de nuestra lengua. A esto, mucho ayudaría que los correctores de texto los incorporaran a su base de datos.",
    "Si al caminar por al calle nos encontrásemos de frente con un perro que nos gruñe y nos enseña los dientes, no tendríamos ninguna duda de que el animal no tiene intenciones amistosas. En tal caso, lo prudente es alejarnos de la escena lo más rápido posible, sobre todo si el perro es de gran tamaño. De la misma manera, cuando un perro recibe a su dueño meneando la cola, podemos estar seguros de que le da un gran gusto verlo.    Las actitudes de los perros nos demuestran, sin duda, su estado de ánimo. Pero ¿cómo realmente nos percibe un perro furioso que amenaza con mordernos? ¿Por medio del sentido del olfato, quizá como un olor molesto? ¿O mediante el sentido de la vista como una visión desagradable? Y en general podíamos preguntarnos ¿cómo funciona la mente de los perros que, como sabemos, muestran una gran inteligencia?Como una vía para contestar a este tipo de preguntas, un grupo de investigadores encabezados por Gregory Berns de la Universidad Emory, en los Estados Unidos, se embarcó en un proyecto para desarrollar una técnica que permitiera estudiar de manera directa el funcionamiento del cerebro de los perros. Para esto, los investigadores se propusieron usar la técnica de imagen por resonancia magnética funcional (IRMf). Esta técnica ha sido empleada en humanos para obtener imágenes del cerebro, en las cuales es posible identificar las regiones del mismo que son activadas –neurológicamente hablando– como respuesta a un estímulo específico. Berns y colaboradores demostraron que la técnica IRMf puede también ser empleada para investigar el funcionamiento del cerebro de los perros. Los resultados del estudio están descritos en un artículo que aparecerá próximamente en la revista electrónica PLoS ONE.  Para obtener imágenes nítidas de resonancia magnética es necesario que el sujeto bajo estudio –en este caso, un perro– permanezca sin moverse por un tiempo mínimo. Para lograr esto, los investigadores aprovecharon la habilidad que tiene esta especie para entender y obedecer órdenes humanas; habilidad que han desarrollado a lo largo de las decenas de miles de años en que han vivido domesticados. De este modo, en una primera parte del estudio a los perros participantes –un pastor escocés y un mestizo norteamericano cazador de ardillas– se les entrenó para que permanecieran quietos en el interior del tubo de resonancia magnética por el tiempo necesario, aún con los ruidos que emitía el aparato. Como ayuda para lograr esto último, se les cubrieron los oídos con orejeras. Cabe hacer notar que en el equipo que realizó el estudio se encontraba un entrenador de perros.De acuerdo con los investigadores, los animales se mostraron felices y accedieron de muy buena manera a participar en el estudio, entrando por su propio pié al tubo de resonancia, incluso cuando no les tocaba turno. Una vez que los perros aprendieron a permanecer quietos en posición de esfinge en el interior del aparato, con el hocico fijo en un apoyo y viendo hacia la salida del tubo, se les mostraron de manera aleatoria dos tipos de señales manuales que fácilmente podían reconocer. Estas señales eran, o bien una mano izquierda extendida apuntando hacia arriba, o bien dos manos extendidas apuntando horizontalmente una contra la otra. Los perros fueron entrenados para saber que a la primera señal pronto seguiría un porción de comida –¡un “hot dog”!–, mientras que en el segundo caso no recibirían nada.Como resultado, cada vez que a los perros se les mostró la señal asociada al alimento mostraron actividad neurológica en una región específica en el interior del cerebro. En contraste, cuando les fue mostrada la señal que no implicaba recibir comida, los animales no mostraron esta actividad. Esto es, los perros responden a estímulos visuales generados por los humanos.La conclusión principal del artículo Berns y colaboradores es que su estudio demostró que la técnica IRMf puede ser aplicada a los perros para investigar el funcionamiento de su cerebro. A partir de aquí avizoran la posibilidad de realizar estudios para contestar un buen número de preguntas, que resultan todas fascinantes. Por ejemplo ¿hasta que punto perciben los perros nuestro estado de ánimo a partir de nuestras expresiones faciales?  ¿Son capaces de entender el lenguaje que hablamos o solamente lo perciben como una serie de sonidos inconexos? De la misma manera, dada la íntima asociación que los humanos hemos sostenido con los perros durante decenas de miles de años ¿nos perciben de manera diferente a como perciben a otras especies animales?En la literatura encontramos numerosas obras de ficción con personajes animales que comparten con los humanos muchas habilidades intelectuales. ¿Hasta qué punto podrían esas obras corresponder a la realidad? Por supuesto, ningún animal puede igualar el potencial intelectual humano. Pero, ¿hasta donde llega este potencial en los animales? Los perros y la IRMf nos podrían dar la respuesta en un futuro cercano.         .",
    "Como sabemos, las discusiones sobre religión –al igual que aquellas sobre  futbol– son un cuento de nunca acabar. La razón de esto es que las creencias religiosas –o antirreligiosas– son algo propio de cada persona, y el pretender influir o imponer puntos de vista en este respecto siempre lleva a dificultades. Comúnmente, los sicólogos asocian las creencias religiosas al pensamiento intuitivo, que contrasta con el pensamiento analítico. El pensamiento intuitivo es inmediato, como cuando reconocemos a una persona por su cara. El pensamiento analítico, en cambio, es retardado e implica un cierto proceso de razonamiento. Ambos tipos de pensamiento conviven en nuestra mente, aunque el analítico puede imponerse al intuitivo en ciertas circunstancias. Por otro lado, si bien por lo general asumimos que las creencias religiosas se mantienen estables por largo tiempo, en el número del pasado jueves de la revista “Science”, dos investigadores de la Universidad de la Columbia Británica, en Canadá, publicaron un artículo en el que concluyen que la fortaleza de las creencias religiosas puede ser disminuida mediante manipulaciones que induzcan el pensamiento analítico.Los investigadores basaron sus conclusiones en los resultados de cinco experimentos. El primero de ellos, destinado a encontrar una correlación entre el pensamiento analítico y la religiosidad, fue llevado a cabo con 179 estudiantes canadienses de licenciatura. A estos se les presentaron tres problemas para medir su razonamiento analítico. La redacción de estos problemas sugería una respuesta rápida intuitiva, la cual, no obstante, era incorrecta. Para encontrar la respuesta correcta, el estudiante tenía que razonarla. Como un ejemplo de lo anterior, a los participantes se les planteó el siguiente problema: si los lirios creciendo sobre la superficie de un lago lo cubren completamente en 48 días, a un ritmo de crecimiento tal que la superficie cubierta se dobla cada día ¿en cuantos días se cubre la mitad del lago? La respuesta intuitiva inmediata –e incorrecta– es 24 días–. Un razonamiento analítico, en contraste, nos lleva a concluir que los lirios cubren la superficie del lago en 47 días, pues un día antes de ser éste cubierto por completo, lo estuvo sólo a la mitad.   Con el objeto de correlacionar los resultados del experimento anterior con sus creencias religiosas, a los estudiantes se les hicieron igualmente una serie de preguntas para evaluar su religiosidad. Se les preguntó, por ejemplo, si creían en Dios, si en su vida sentían la presencia divina o si recurrían a Dios cuando estaban en problemas. El resultado fue que a mayor calificación en las pruebas analíticas, menor calificación obtuvieron en las pruebas de religiosidad.La parte más novedosa del estudio, sin embargo, fue la que se obtuvo con los experimentos diseñados para determinar si era posible modificar la religiosidad de una persona induciéndola a pensar analíticamente. Para este propósito, se hicieron preguntas a dos grupos de estudiantes a fin de evaluar sus creencias religiosas. Previo a esto, a un primer grupo se le presentó una imagen de “El pensador” de Rodin, que tiene una fuerte connotación analítica, mientras que al segundo grupo le fue mostrada una imagen del Discóbolo de Mirón, que tiene un impacto visual equivalente al de la escultura de Rodin pero sin su significado analítico.El resultado fue que las calificaciones del grado de religiosidad del primer grupo de estudiantes fueron inferiores que las del segundo. Resultados similares fueron obtenidos con otros experimentos. En marcado contraste, pruebas que se habían hecho meses antes a los mismos estudiantes, sin buscar inducirles un comportamiento analítico, no mostraron diferencias entre los dos grupos.Tenemos así que, de manera sorprendente, es posible cambiar las creencias religiosas mediante una inducción hacia el pensamiento analítico. Ciertamente, durante los experimentos del artículo de referencia no se produjeron cambios espectaculares –un creyente no se convirtió en ateo, por ejemplo–, pero sí disminuyeron de manera significativa las actitudes religiosas del primer grupo de estudiantes. .    El estudio reportado en “Science” es, por supuesto y como todo estudio científico, neutral y busca establecer bases objetivas para explicar la incredulidad religiosa. No busca, de ninguna manera hacer juicios de valor sobre la misma, ni sobre los méritos  de los pensamientos intuitivo y analítico en el momento de tomar decisiones. En esto los autores son muy claros y lo expresan de manera explicita al final de su escrito.El tema, no obstante, fácilmente genera controversia. En relación a esto, un artículo de opinión publicado en la revista “Nature” el pasado jueves se muestra escéptico sobre las conclusiones del artículo de “Science”. En particular, considera que la religiosidad es un fenómeno tan complejo que incluso pudiera no ser susceptible de estudiarse científicamente.Sin embargo, aun si en la actualidad esto último fuera cierto, probablemente no lo será en un futuro cercano. Así, cuando dispongamos de bases objetivas para explicar la religiosidad –o la ausencia de la misma–, la consideraremos como un fenómeno natural sin ninguna connotación, positiva o negativa. Por lo pronto, podremos quizá decir que el ser o no ser creyente es un reflejo de la manera en que manejamos nuestros procesos de pensamiento intuitivo y analítico. Es decir, es una cuestión de enfoque.",
    "Quizá pueda sonar inverosímil la afirmación según la cual hay una conexión directa entre una presa de grandes proporciones y la ocurrencia de temblores, también de grandes proporciones, en sus inmediaciones. Esto, no obstante, es lo que sostienen algunos geofísicos en relación al temblor de Wenchuan, China, que en mayo de 2008 –poco antes de las olimpiadas de Beijing– mató a 80,000 personas, y que estaría relacionado con la presa Zipingpu cercana al epicentro del temblor. Un hecho que apoya esta asociación es que el llenado de la presa se inició en septiembre de 2005 y dos años y medio después se produjo el terremoto. En una región, además, en donde no había ocurrido un sismo de esta magnitud en cuando menos 1,000 años, según los expertos.En anteriores ocasiones se ha establecido una relación clara entre el movimiento de grandes masas de agua, como ocurre durante el llenado o vaciado de una presa, y la ocurrencia de temblores. Estos, no obstante, han tenido una magnitud máxima de 6.3, que contrasta fuertemente con la magnitud 8 del terremoto de Wenchuan.  Como lo explican los geofísicos, los temblores tienen su origen en el movimiento continuo que experimentan las diferentes placas que forman la corteza terrestre. Por efecto de este movimiento, las fuerzas de rozamiento a lo largo de la falla geológica entre dos placas pueden dar lugar a una gran acumulación de energía. Al alcanzar dichas fuerzas un cierto valor crítico, la energía acumulada se libera de forma súbita generando un temblor.En el caso del embalse de Wenchuan, se especula que el enorme peso de la masa de agua –la cortina de la presa tiene una altitud de 156 metros– pudo disparar el temblor por la presión que ejerció sobre la falla geológica, que está justamente debajo del embalse a una profundidad entre 5 y 19 kilómetros. Otra posibilidad es que el agua de la presa se haya filtrado hasta la falla, lubricándola y acelerando su fractura. Los escépticos, no obstante, argumentan que el agua no podría haber penetrado hasta la profundidad máxima de 19 kilómetros estimada para el epicentro del sismo, aunque otros argumentan que en realidad dicho epicentro ocurrió a una profundidad de entre 6 y 9 kilómetros, hasta la cual sí podría haber ocurrido la filtración. En este contexto, la revista “Science” en el número correspondiente a la presente semana, hace eco de la preocupación de algunos geofísicos por los planes chinos de crear una serie de embalses a lo largo de río Yangtsé en una zona con actividad sísmica. Dichos embalses tienen como propósito la generación de energía eléctrica, que China necesita para sostener su acelerado crecimiento económico.Como sabemos, China cuenta con la central hidroeléctrica de Tres Gargantas, la más grande del mundo, con una potencia instalada de 21,000 KW. Esta se localiza en la presa Tres Gargantas, que retiene las aguas del Yangtsé y forma a lo largo de su cauce un delgado lago de 600 kilómetros de longitud. Podemos apreciar lo enorme de la instalación china si la comparamos con la central hidroeléctrica de Chicoasén, la mayor de nuestro país, la cual tiene una potencia de “sólo” 2,400 MW. Entre otros, China tiene actualmente en construcción cuatro embalses en el Río Yangtsé que incrementarán su capacidad de generación de energía eléctrica en 40,000 MW, lo que es casi la capacidad total de nuestro país en este rubro. Los nuevos embalses se localizarán río arriba desde la presa de Tres Gargantas, y según los críticos tendrán un impacto ecológico negativo de consideración, además de que darán lugar a posibles problemas sísmicos.   El desarrollo económico del mundo, no obstante, necesita de cantidades crecientes de energía eléctrica. En actualidad esta energía en su mayor proporción se obtiene de la quema de combustibles fósiles, que sabemos han provocado un cambio climático en nuestro planeta. China y otros países están buscando sustituir a dichos combustibles fósiles por fuentes de energía renovables no contaminantes, una de las cuales es precisamente la hidroelectricidad. La energía hidroeléctrica requiere de crear lagos artificiales de gran extensión, que por lo mismo tienen un importante impacto sobre el medio ambiente. En esto estribaría su desventaja más grande, que la alejaría de ser una fuente ideal de energía, la cual, por supuesto, no existe.  Por otro lado, la hidroelectricidad se presenta como una fuente muy atractiva pues aprovecha la energía de las caídas de agua y no libera productos de desecho que contaminen el medio ambiente. La energía en la caídas de agua, además,  depende de la fuerza de gravedad –de la que, como sabemos, no podemos librarnos– y terminará disipándose en el medio ambiente, independientemente que hagamos uso de ella o no.  La hidroelectricidad se presenta así como una fuente de energía altamente atractiva. Esto se mantendrá posiblemente en el futuro. Aunque no en el caso de que resulten ciertas las especulaciones y que efectivamente un embalse gigante cerca de un zona sísmica sea capaz de provocar un terremoto de gran magnitud. En tal circunstancia, resultaría que la hidroelectricidad tiene después de todo sus bemoles. Podríamos así aplicarle el dicho según el cual, “no todo lo que brilla es oro”.",
    "Según informes aparecidos en días pasados en la prensa, un cantante inglés de apellido Gallagher habría llamado “pequeño idiota” al “Chicharito” Hernández, que, como sabemos, juega futbol en Inglaterra. Esto aconteció durante en una entrevista concedida a una cadena televisiva en México. Aparentemente, para Gallagher la traducción de “chicharito”  es “pequeño idiota” y de ahí su comentario. No es claro si el cantante de marras de plano desconoce nuestro idioma o si sus declaraciones fueron debidas a que es fanático del equipo de futbol  Manchester City, que es acérrimo rival del Manchester United –para el que juega el Chicharito–. Lo que sí es claro es que están en línea con el estereotipo que se tiene de un jugador de futbol, el cual no tendría como característica distintiva a su inteligencia.  A pesar del estereotipo, no obstante, hay jugadores de futbol a los que difícilmente podríamos tachar de poco inteligentes. Un ejemplo es Pelé, considerado por muchos el mejor jugador del mundo en toda la historia, que ha tenido una exitosa carrera después de retirarse como futbolista, que lo llevó incluso a ser ministro de deportes de Brasil. Tenemos, por supuesto, también ejemplos en sentido contrario. Este es el caso de Garrincha, el mejor extremo derecho que ha existido jamás, campeón del mundo con Pelé en 1958 y 1962 –y que fue, además, extraordinario por muchas otras razones–, que murió en la miseria, alcoholizado y a la edad temprana de 49 años.   En todo caso, individualidades aparte, los comentarios de Gallagher no coinciden con un estudio publicado el pasado 4 de abril en la revista electrónica PLoS por investigadores de universidades suecas. En dicho estudio se concluye que el éxito de un jugador de futbol depende en buena medida de su capacidad cerebral –además, por supuesto, de sus habilidades físicas para este deporte–. De manera específica, depende de lo que se conoce como funciones ejecutivas, localizadas en el lóbulo frontal del cerebro, y las cuales se piensa están relacionadas con algunos aspectos de la inteligencia.Según los expertos, las funciones ejecutivas se encargan de regular pensamiento y acción en situaciones no rutinarias. En el caso de un futbolista, le permiten evaluar la situación en el campo de juego en un determinado momento, y en función de dicha evaluación tomar una decisión sobre correr, pasar la pelota o disparar a gol, entre otras opciones. Esto, además, tiene que hacerse rápidamente.El estudio de referencia se llevó a cabo con 57 futbolistas profesionales de ambos sexos, de equipos suecos tanto de la primera división como de divisiones inferiores. Se cuidó que no hubiera diferencias significativas ni en edad ni en nivel de educación entre los diferentes grupos de participantes.  En la primera parte del estudio, a los futbolistas se les aplicaron pruebas para cuantificar sus funciones ejecutivas, Dichas pruebas consistían en unir con una línea, bajo presión de tiempo –60 segundos–, todos los puntos dentro de un cuadrado. El ejercicio debía repetirse un cierto número de veces y la solución en cada caso tenía que ser diferente a las anteriores. De este modo, el participante estaba forzado a recordar cada vez todas las soluciones que ya había ideado y a discurrir una nueva. El ejercicio simulaba así las condiciones que el futbolista afronta en el campo de juego.El resultado del estudio fue que los jugadores de la primera división obtuvieron calificaciones promedio significativamente más altas que aquellas de los jugadores de divisiones inferiores. Además, se encontró que las calificaciones de ambos grupos de futbolistas profesionales eran más altas que las de la población en general.    En una segunda parte del estudio se hizo un seguimiento por dos años del desempeño de los jugadores en sus respectivos equipos; específicamente, se llevó un conteo del número de goles que cada uno anotó, así como de las asistencias para gol que realizó. Se encontró que los puntos obtenidos por cada jugador estaban en relación directa con sus calificaciones en las pruebas. La conclusión fue que es posible predecir el éxito futuro de un futbolista por medio de una prueba que cuantifique sus funciones ejecutivas.     En base a esta conclusión, los autores del artículo comentan que sus resultados tienen relevancia para el mercado de futbolistas, pues, en base a una medición será posible determinar en qué medida tendrá éxito en el futuro un determinado jugador. Pelé y Garrincha, como muchos de sus contemporáneos, desarrollaron su carrera futbolística básicamente en Brasil. En contraste, hoy en día el futbol como negocio ha avanzado considerablemente y el traspaso de futbolistas sudamericanos –e incluso mexicanos– hacia equipos europeos es algo común. El mercado del futbol ha crecido tanto que frecuentemente nos enteramos de fichajes de futbolistas por cantidades exorbitantes, que se acercan, e incluso superan, los cien millones de dólares. En este sentido, los resultados del estudio de referencia pueden ciertamente llegar a ser significativos.En tanto eso sucede, podemos señalar que muy probablemente los juicios expresados sobre el Chicharito son equivocados, y que el desafortunado fin de Garrincha no debe ser indicativo de su desarrollo cerebral.",
    "Imagine que por una semana no pone orden en las cosas que usa todos los días. No tiende su cama, ni regresa al cajón del tocador el cepillo y el peine que usa para peinarse. Tampoco lava los platos después de comer, ni mucho menos los guarda en el lugar destinado para tal propósito. Ni ordena el periódico al terminar de leerlo y lejos de eso lo arroja deshojado por aquí y por allá. En suma, usa las cosas y al final las deja en cualquier lugar, sin preocuparse en donde. Con estas prácticas, al acabar la semana –posiblemente antes– su casa alcanzará un grado de desorden tal que hará difícil vivir en ella. En este punto tendrá que hacer un esfuerzo para ordenarla nuevamente si es que quiere rescatarla como un lugar para vivir.La tendencia de las cosas a desordenarse a menos que exista una fuerza en sentido contrario es algo común en el Universo y no se circunscribe a nuestro entorno inmediato, ni a lo que nosotros hagamos o dejemos de hacer para mantenerlo ordenado. En particular, ocurre en el mundo de los átomos. En relación a esto, recordemos que los materiales están compuestos de átomos. Estos, además, son tan pequeños que para verlos tenemos que hacer uso de los microscopios más potentes que existen. Al mismo tiempo, en un material sólido los átomos están tan juntos que el mismo nos parece homogéneo e impenetrable, y no un material compuesto de partes minúsculas estrechamente entrelazadas.Sabiendo de su existencia cabe preguntarnos ¿cómo están colocados los átomos dentro de un material sólido? ¿En completo desorden como los asistentes a un concierto de rock, o, por el contrario, ordenados como los soldados de un regimiento en marcha? La respuesta es que existen las dos posibilidades: hay tanto materiales ordenados como materiales desordenados. A los primeros los llamamos cristales y a los segundos amorfos. Además, como podíamos haber sospechado, hay también materiales con un orden atómico intermedio ente estos dos extremos.Al contrario de los humanos, que difícilmente cambiamos de hábitos, los átomos son más flexibles y el orden dentro de un material está determinado por la manera cómo fue tratado en el pasado. Así, por ejemplo, podemos obtener un material sólido altamente ordenado si lo calentamos más allá del punto en el que se hace líquido y en seguida lo enfriamos muy lentamente. Por el contrario, si lo enfriamos muy rápidamente no daremos tiempo a sus átomos a que se ordenen –cosa que, por otro lado, buscan de manera natural– y terminarán ocupando posiciones azarosas dentro del material sólido. Aunque los cristales no son los materiales más comunes en la naturaleza, sí encontramos ejemplos de ellos sin dificultad –los podemos reconocer por sus superficies planas que forman ángulos precisos entre ellas–. Como ejemplos de cristales naturales podemos incluir a la sal en grano y a la azúcar cristalizada, lo mismo que a los diamantes y a otras piedras preciosas. El ejemplo más impactante de cristal natural, por otro lado, es orgullosamente mexicano –aunque desgraciadamente no fabricado por nosotros– y se encuentra en una cueva de la mina Naica en el estado de Chihuahua. Dicha cueva, descubierta en el año 2000, contiene cristales naturales de yeso –sulfato de calcio– que son con mucho los cristales más grandes que existen en el mundo y que tienen dimensiones de hasta diez metros de largo y un metro de diámetro. Dichos cristales han sido estudiados extensamente por un grupo de investigadores de la Universidad de Granada, en España, en conjunto con investigadores de la Universidad de Tohoku en Japón. Concluyen que el extraordinario tamaño de los cristales de Naica se debe a su lenta formación, la cual habría tomado cerca de un millón de años en años en completarse. A lo largo de este tiempo, los cristales se habrían formado en un medio acuoso, mantenido a una temperatura superior a los 50 grados centígrados, en el que se hallaban disueltos los materiales que poco a poco se depositaron sobre los cristales haciéndolos crecer. Algo así como una olla en la estufa, con agua líquida e ingredientes cuidadosamente escogidos, y una temperatura controlada de manara precisa a lo largo de un millón de años.  Hay que notar que hasta fechas recientes la cueva de Naica, que está a una profundidad de 300 metros, estuvo inundada, pues los mantos freáticos están a una profundidad de  120 metros. Sólo después de que comenzó la explotación de la mina, fue que mediante el bombeo de agua los cristales de yeso quedaron descubiertos por primera vez en un millón de años.El pasado miércoles el grupo referido de la Universidad de Granada ganó la portada de la revista “Science” con un artículo en el que pusieron en claro el mecanismo por el cual se forman los cristales de yeso. Esto arroja más luz sobre qué fue lo que posibilitó la formación de cristales naturales de dimensiones tan fuera de lo común en la cueva de Naica.Al margen de las explicaciones, sin embargo, lo que queda claro es que, dadas las condiciones adecuadas, los átomos son capaces de organizarse y ordenarse en gran escala. Sin duda un ejemplo a seguir.",
    "En la obra “La Crónica de Nuremberg”, publicado en Alemania en 1493, encontramos ilustraciones de seres monstruosos, supuestamente humanos, que habrían vivido en los confines del mundo entonces conocido. Las ilustraciones nos muestran individuos con cuerpo humano pero con cuello y pico de avestruz, con cabeza humana y cuerpo de caballo, o bien con cuerpo humano y cabeza de perro. Están incluidos también humanos con cuatro ojos, con un sólo ojo, con seis brazos, sin cabeza pero con los ojos, nariz y boca en el pecho, lo mismo que enanos con solamente una pierna rematada por un enorme pié, tan grande que a su dueño le servía como parasol en caso de ser necesario. Esto último, era aparentemente muy frecuente, pues los enanos mono-pié habrían vivido en un país cálido y de hecho se les dibuja acostados de espaldas con el pié levantado para hacerse sombra.    Los monstruos con características humanas no fueron, por supuesto, inventados por la Crónica de Nuremberg y por el contrario han sido una constante en nuestra historia. En particular, como sabemos, la mitología griega incluye cíclopes, arpías, centauros y sirenas, entre muchos otros seres fantásticos. El escritor romano Plinio el Viejo, por su lado, describió al principio de nuestra Era monstruos humanos –incluidos los enanos mono-pié– en su obra “Historia Natural”.    No obstante, en la medida en que se exploraba el planeta y no se encontraban rastros de los esperados monstruos, las historias sobre los mismos perdieron credibilidad. Aunque no por completo y aun en la actualidad de cuando en vez sabemos de supuestos seres mitad humanos mitad bestias que habitan en regiones de difícil acceso. Un ejemplo de esto es el Yeti, también conocido como el abominable hombre de las nieves, que supuestamente vive en la región de los montes Himalaya y que sería una especie de gorila inteligente. Como quiera que sea, puesto que ya quedan muy pocos lugares sin explorar, es cada vez más difícil sostener la existencia de monstruos humanos en nuestro entorno inmediato.De este modo, dada nuestra fascinación por los seres fantásticos, hemos tenido que buscarlos fuera de nuestro planeta. Así, han surgido un buen número de historias sobre visitas de extraterrestres a la Tierra –con imágenes incluidas– que, sin embargo, han tenido una credibilidad limitada. Por otro lado, la especulación sobre la existencia de vida extraterrestre inteligente se ha abordado también de manera seria. En este sentido, se piensa que con toda probabilidad existe vida inteligente extraterrestre dado el inmenso número de estrellas en el Universo a cuyo alrededor orbitan planetas capaces de mantenerla. Con el objeto de detectarla, desde la década de los años setenta en el siglo pasado se ha mantenido operando el proyecto SETI –búsqueda de inteligencia extraterrestre, por sus siglas en inglés– que ha escudriñado el cielo por décadas con el fin de descubir señales de radio provenientes de civilizaciones avanzadas fuera de nuestro sistema solar.      Hay también proyectos para detectar la existencia de planetas extra-solares capaces de sostener vida. Para esto es necesario que el planeta orbite alrededor de su estrella a una distancia tal que su temperatura permita la existencia de agua líquida, que sabemos es esencial para la vida tal como la conocemos en la Tierra. En relación a esto, astrónomos europeos trabajando en el Observatorio La Silla en Chile, concluyeron recientemente que sólo en la Vía Láctea puede haber miles de millones de “Súper-Tierras” –planetas rocosos con una masa hasta 10 meses la masa de la Tierra– gravitando alrededor de estrellas conocidas como enanas rojas, a la distancia adecuada para permitir la vida. Habría de este modo alrededor de 100 Súper-Tierras habitables a una distancia de 30 años luz de nuestro planeta. En una entrevista publicada por la BBC, sin embargo, uno de los astrónomos responsables del estudio hace notar que el que una Súper-Tierra esté a la distancia correcta de una enana roja no necesariamente asegura que sea adecuada para la vida. Esto debido a que en las enanas rojas son frecuentes las erupciones solares que pueden bombardear el planea con rayos X y radiación ultravioleta que no son precisamente benéficos para la vida.Aun con este pero, no obstante, el enorme número de planetas potencialmente habitables en regiones por ahora inaccesibles para nosotros –tal como lo fueron los confines de mundo conocido para los europeos en el medioevo– nos asegura la existencia en el Universo de mundos habitados aparte del nuestro. Somos así libres de fantasear sobre el aspecto que tendrán sus moradores al igual que nuestros antecesores. Aunque posiblemente ahora con un poco más de bases.",
    "A mediados del Siglo XIX, las dos clínicas de maternidad del Hospital General de Viena arrojaban índices de mortalidad por fiebre puerperal entre las parturientas que resultaban inexplicables a primera vista. En efecto, las estadísticas indicaban que la mortalidad en la Clínica 1 alcanzaba valores dos veces más altos que los de la Clínica 2, a pesar de que los procedimientos médicos seguidos en ambos lugares eran aparentemente los mismos. El misterio fue resuelto por Ignaz Semmelweis, médico asociado a dichas clínicas, que ofreció una explicación sorprendente en su momento.Según Semmelweis, si bien las prácticas médicas eran las mismas en ambos establecimientos, las parturientas eran atendidas en cada uno de ellos por profesionales con diferente perfil: estudiantes de medicina en la Clínica 1 y estudiantes de obstetricia en la Clínica 2. Este hecho, aparentemente banal, hacía la gran diferencia. Aun hoy nos parece sorprendente que el perfil profesional de la persona que atendía a una parturienta haya constituido un factor importante para la salud de la misma. Deja de sorprendernos, sin embargo, cuando nos enteramos que era práctica frecuente que los estudiantes de medicina acudieran a atender partos después de haber trabajado con cadáveres y que –vistos los resultados– no tenían por costumbre lavarse las manos a conciencia.Así, faltos de esta costumbre, los estudiantes transportaban gérmenes patógenos desde los cadáveres hasta las parturientas, con resultados frecuentemente funestos para estas últimas. En la Clínica 2, por el contrario, las estudiantes de obstetricia no trabajaban con cadáveres y la mortalidad por fiebre puerperal era consecuentemente más baja. La solución al problema de la Clínica 1 fue muy simple y consistió en obligar a los estudiantes a lavarse las manos antes de entrar a la sala de partos –empleando para ello un líquido similar al que  hoy se usa para limpiar pisos–. Con esta medida, el índice de mortalidad por fiebre puerperal cayó drásticamente de manera inmediata. Aunque tuvo consecuencias prácticas trascendentales, la explicación ofrecida por Semmelweis no fue aceptada de manera inmediata por todos, pues no concordaba con las ideas dominantes en la época sobre el origen de las enfermedades. Hay que recordar que en esos momentos no se habían identificado a los gérmenes, de manera incontrovertible, como los agentes causantes de las enfermedades infecciosas, de modo que Semmelweis no pudo explicar satisfactoriamente qué en específico era lo que transportaban los estudiantes desde el anfiteatro que producía la fiebre puerperal. Pronto, sin embargo, estudios llevados a cabo por diferentes investigadores –Luis Pasteur, entre ellos– pusieron en claro el papel que juegan los gérmenes en las enfermedades y con esto obtuvimos la receta para prevenirlas: había que eliminar o, en todo caso, evitar y combatir, a los microbios. En este sentido, buscar la limpieza y la esterilización en todos los órdenes era una práctica que debería adoptarse.Así, a lo largo del último siglo –al menos en los países desarrollados–, la vida se hizo cada vez más libre de gérmenes patógenos. Esto condujo a un impresionante incremento en la esperanza de vida de la población –como resultado en buena medida de una disminución en la mortalidad infantil–, la cual se dobló a nivel global en el curso de una centuria. Es posible, sin embargo, que hayamos exagerado un poco la nota y que, a fin de cuentas, una limpieza extrema no sea tan positiva como se había pensado. Algo así como, “ni tanto que queme al santo, ni tanto que no lo alumbre”. Al menos esto es lo que sugieren los resultados de un artículo publicado esta semana en la revista “Science” por un grupo de investigadores de los Estados Unidos y Alemania, según los cuales la exposición a ciertos gérmenes a una edad temprana ayuda a desarrollar el sistema inmunológico y a evitar enfermedades de origen auto-inmune como el asma y la colitis ulcerosa.En dicho artículo se reportan los resultados de un estudio llevado a cabo con dos grupos de ratones. Un primer grupo fue mantenido en un ambiente estéril, mientras que un segundo grupo creció en el ambiente normal del laboratorio. A ambos grupos de ratones le fue inducida o bien asma o bien colitis ulcerosa, y se estudió su respuesta inmunológica. Encontraron una respuesta más severa en aquellos ratones que crecieron en un ambiente estéril, indicando que la exposición temprana a los gérmenes cumple la función de entrenar al sistema inmunológico para responder de manera más moderada a la presencia de cuerpos extraños.Estos resultados están de acuerdo con la llamada “Hipótesis de la higiene”, según la cual el ambiente cada vez más estéril que prevalece en los países desarrollados es el causante del incremento observado en la incidencia de enfermedades auto-inmunes. Si bien no es claro en qué medida el estudio referido –realizado con ratones– puede extenderse a la especie humana, sí nos da una indicación de que es posible que una mayor limpieza no sea necesariamente sinónimo de una mejor salud. En otras palabras: “veneno que no mata…..fortalece”.",
    "Las máquinas de movimiento perpetuo de primera especie, caracterizadas por funcionar de manera continua sin un suministro de energía, son, por supuesto, una opción muy atractiva. Dichas máquinas operarían a un costo muy bajo, además de que no contribuirían a la contaminación ambiental por no usar combustible. Las máquinas de movimiento perpetuo resolverían  muchos de los  problemas que aquejan a nuestro planeta, tales como el del suministro oportuno de petróleo a países que lo necesitan y el del calentamiento global producto de la quema de combustibles fósiles. A lo largo de la historia se han propuesto un buen número de máquinas de movimiento perpetuo. Éstas han consistido desde ingenios giratorios para elevar agua a una cierta altura empleando a la gravedad como única fuerza impulsora, hasta motores eléctricos para automóviles que toman la energía para moverse de una batería, la cual es recargada por un generador de electricidad impulsado por el giro del motor.  Ciertamente las máquinas de movimiento perpetuo lucen atractivas y sin duda darían solución a numerosos problemas. Ninguna, sin embargo, ha sido construida que funcione como tal –aunque se han presentando casos de presuntas máquinas de este tipo que posteriormente resultaron fraudulentos. No es sorprendente que así sea. En efecto, no podríamos esperar construir una bomba para subir agua accionada enteramente por la fuerza de gravedad, pues esta fuerza –como bien sabemos– más que subir hace bajar a los objetos. De la misma manera, un automóvil que se mueva empleando la corriente eléctrica de una batería, cuya carga es a su vez proporcionada por el mismo motor, sería en cierto modo equivalente a una víbora que para subsistir se devora la cola.La razón última que imposibilita la construcción de una  máquina de movimiento perpetuo de primera especie es que ésta violaría la ley de conservación de energía –que constituye un principio de la Física firmemente establecido–, según el cual no es posible extraer energía de la nada.Y no obstante, si bien es imposible construir una máquina que opere eternamente sin suministro de energía, sí podemos construir ingenios que desde un punto de vista práctico se asemejen a una máquina de movimiento perpetuo. Un ejemplo de esto es un motor eléctrico accionado por la energía eléctrica proporcionada por paneles de celdas solares. En efecto, tenemos que para todo propósito práctico el Sol es una fuente inagotable de energía que se mantendrá brillando por miles de millones de años. Podría, de este modo, mantener operando nuestro motor de manera indefinida. La energía del Sol es, además, no contaminante. Un ingenio que opere con energía solar es entonces, y en muchos respectos, equivalente a una máquina de movimiento perpetuo.    Sí, aunado a todo lo anterior, tomamos en cuenta que la energía del Sol es muy abundante –sobre la superficie de la Tierra incide una cantidad de energía solar que es 7000 veces más grande que la toda la energía que consume el mundo–  podemos esperar que esta energía se convierta en el futuro en la panacea para superar muchos de los problemas que aquejan a nuestro planeta.  Habría, no obstante, que superar antes algunos obstáculos. En efecto, si bien el combustible solar es gratis, abundante y prácticamente eterno, para explotarlo es necesario construir primero las máquinas que lo transformen en otras formas de energía que directamente podamos aprovechar. Un tipo de máquina especialmente importante en este respecto es la celda solar, que transforma la energía del Sol directamente en energía eléctrica con eficiencias que van del 10 al 20%.  Construir celdas solares, sin embargo, tiene un costo y en la medida en que éste fue tradicionalmente alto no existieron los incentivos económicos suficientes para fabricarlas de manera masiva. Esta situación está empezando a cambiar y hay signos de que en muchos países –entre ellos el nuestro– el costo de producir electricidad por medio de celdas solares –cuyo precio se redujo un 30% en el último año– se equipara ya con el costo de la electricidad generada por otras fuentes.   Una manera de reducir el monto de las facturas de electricidad en nuestro país –empleado ya en otros lugares– es mediante la instalación de un sistema solar propio para la producción de electricidad. Este sistema se  interconectaría con la red de CFE y permitiría tanto el flujo de energía eléctrica desde dicha red hacia la vivienda –cuando la producción propia fuera insuficiente–, como el flujo en la dirección contraria –cuando hubiera un exceso en la producción–. De esta manera, el propietario de la vivienda pagaría a CFE por concepto de consumo de energía eléctrica solamente la diferencia entre el flujo de energía entrante y el flujo saliente.Dada la evolución de los precios de las celdas solares en los últimos años, es posible que en un futuro cercano el uso de la energía eléctrica de origen solar sea cada vez más frecuente y sustituya en porcentajes cada vez mayores a la electricidad generada por métodos tradicionales.Se harán de este modo realidad los sueños de aquellos que durante mucho tiempo quisieron fabricar máquinas de movimiento perpetuo de primera especie para hacer más fácil nuestra existencia. Aunque de una manera quizá inesperada para ellos, y sin violar las leyes de la Física.",
    "Como bien sabemos, el espíritu de aventura de los navegantes y exploradores europeos de la llamada “Era de los descubrimientos” –siglos XV al XVII– cambió drásticamente la faz del mundo. En particular, convirtió a algunos países europeos en potencias coloniales que subsistieron por siglos –por lo general, para el perjuicio de los países conquistados y la prosperidad de los conquistadores. Los viajes de Colón, Vasco da Gama y Magallanes, entre otros, que buscaban abrir nuevas rutas comerciales entre Europa y el Lejano Oriente, fueron ciertamente impulsados por intereses económicos. Estos viajes no hubieran sido posibles, sin embargo, sin el concurso de marineros intrépidos, dispuestos a arriesgar la vida en la exploración de mundos desconocidos. El espíritu aventurero no es, por supuesto, exclusivo de los europeos ni se ha dado sólo en una época particular; por el contrario ha sido una constante a lo largo de la historia –y prehistoria– humana. Así, sabemos que los pobladores originales del continente americano cruzaron hace más de 10,000 años desde Asia a través del estrecho de Bering –convertido a la sazón en un puente terrestre por efectos de la glaciación–.  La avanzada de estos primeros pobladores tuvo que haber estado compuesta por aventureros dispuestos a enfrentar los peligros –reales o imaginarios– que podrían encontrar en  tierras ignotas.  No obstante su prevalencia, el espíritu de aventura no es generalizado y por el contrario es atributo de sólo una minoría. En relación a esto, y a manera de ejemplo, con seguridad no fueron muchos aquellos a lo que les pasó por la mente intentar llegar al Polo Sur de la Tierra hace poco más de un siglo, cuando este paraje no había sido todavía conquistado –aun en la actualidad, posiblemente no abunden aquellos dispuestos a correr la aventura–.  Hoy en día muy probablemente tampoco encontremos muchas personas dispuestas a viajar al espacio en un cohete –a pesar de que ya se cumplió medio siglo desde el primer viaje espacial llevado a cabo por Yuri Gagarin–. Pudiera ser que cuando los vuelos orbitales sean cosa de rutina – si es que alguna vez ocurre– una mayoría esté dispuesta a experimentarlos. Ciertamente, la mayor parte de nosotros preferimos la seguridad, a aventurarnos por caminos que no han sido ya exhaustivamente explorados por otros. Vistas así las cosas, una posible clasificación de los humanos –entre muchas otras que se pueden hacer– es entre aventureros natos dispuestos a correr grandes peligros –una pequeña minoría– y aquellos –la  gran mayoría– que prefieren la seguridad. Esta clasificación es, por supuesto, una sobre simplificación de la situación y muchas personas –circunstancias de por medio– caerían en un punto intermedio entre los dos extremos. Se tiende a pensar que las tendencias hacia la aventura o al sedentarismo, que reflejan diferencias de personalidad, son exclusivas de los humanos y de otros vertebrados. Sin embargo, en un artículo aparecido en el último número de la revista “Science”, publicado por investigadores de universidades norteamericanas, se encuentra que entre las abejas también existen diferencias que podríamos catalogar como de personalidad. Estas diferencias hacen que una minoría sea más intrépida que el resto cuando de emprender nuevas aventuras se trata.  Para llegar a esta conclusión, los investigadores referidos seleccionaron un grupo de abejas dadas a la aventura. Se sabe que dentro del grupo de abejas recolectoras, algunas –menos del 5 %– son más osadas que el resto y que, cuando la colmena tiene que dividirse, son las que buscan sitios adecuados para levantar un nuevo panal. Una vez que este sitio es localizado, lo comunican al resto del enjambre y lo dirigen a su nuevo hábitat.  Ya seleccionado el grupo de abejas aventureras, los autores del artículo investigaron si éstas actuaban igualmente como buscadoras de nuevas fuentes de comida. Encontraron que es 3.4 veces más probable que una abeja buscadora de nuevos sitios para anidar se dedique al mismo tiempo a buscar nuevas fuentes de comida, a que esto último lo haga una abeja pasiva que espera le indiquen cual será su nuevo hogar. Esto demuestra que las abejas exploradoras llevan el gusto por la aventura en la sangre.En realidad, más que en la sangre, el gusto lo llevan en los genes, al igual que los humanos. Esto según los autores citados, quienes encontraron grandes diferencias en la expresión de dichos genes en el cerebro de las abejas exploradoras en comparación con aquellas que no lo son. Así, de manera sorprendente, los humanos compartimos características –osadía o pasividad– con las abejas, a pesar de la gran distancia evolutiva que existe entre ambas especies. Según los investigadores referidos, esto no es debido a que exista un ancestro común. Por el contrario, especulan que ambas adaptaciones evolucionaron de manera independiente.",
    "En el otoño de 1991 dos alpinistas alemanes descubrieron en un barranco de los Alpes de Ötztal, en la frontera entre Austria e Italia, el cadáver de un hombre semienterrado en el hielo. Dicho descubrimiento no habría constituido un acontecimiento especialmente notable, y hubiera pasado en buena medida desapercibido, de no haber sido porque el hombre en cuestión –bautizado posteriormente como ”Ötzi, el Hombre del Hielo” – vivió y murió en la Edad del Cobre, hace más de 5,000 años.  Al morir Ötzi una serie de circunstancias afortunadas ayudaron a preservar sus restos por miles de años. Primeramente, el cadáver se desecó y momificó por las condiciones climáticas imperantes, al mismo tiempo que, por alguna razón, se mantuvo a salvo de ser devorado por los animales carroñeros. Posteriormente, con el transcurrir de los años, fue cubierto paulatinamente por el hielo por el avance de los glaciares. Sólo recientemente, al retroceder estos últimos  por efecto del actual calentamiento global, fue que Ötzi emergió a la superficie para ser descubierto por casualidad. Se tiene así el cadáver de un hombre prehistórico en excelente estado de conservación, acompañado, además, de restos de la ropa que vestía y de los utensilios que portaba en el momento de su muerte. Entre estos últimos se cuentan un hacha de cobre, un cuchillo, un arco de madera en fase de elaboración y un carcaj con flechas, algunas de ellas también sin terminar. El Hombre del Hielo constituye una ventana hacia un pasado remoto que nos permite echar un vistazo a la vida de nuestros antepasados en tiempos idos para siempre. Una vez descubierto, Ötzi fue motivo de numerosos estudios científicos y hoy sabemos –o tenemos al menos una certeza razonable– muchas cosas acerca de la vida que llevaba y de las circunstancias que rodearon a su muerte.  En relación a esto último, aparentemente Ötzi tuvo un fin violento y murió poco después de sufrir una herida de flecha en la espalda cerca del cuello. Sabemos también que como última comida consumió carne de cabra. Falleció, además, a una edad relativamente temprana alrededor de los 45 años. Por otro lado, tomografías practicadas a la momia revelan que Ötzi sufría de arterioesclerosis –endurecimiento de las arterias– y de caries dental.En un artículo publicado esta semana en la revista “Nature” por un grupo internacional de investigadores se desvelan más secretos del Hombre del Hielo; esto a partir de haber logrado secuenciar su genoma. Así, ahora sabemos que Ötzi posiblemente sufría de la enfermedad de Lyme, que es trasmitida por la picadura de las garrapatas y que, entre otros síntomas, produce inflamación de las articulaciones. De la misma manera, es probable que tuviera ojos café, sangre tipo O y que padeciera de intolerancia a la lactosa. Tenía, así mismo, una predisposición genética a las enfermedades cardiovasculares.El conocimiento de su genoma también permitió averiguar que Ötzi está emparentado con los habitantes de la isla italiana de Cerdeña en el Mar Tirreno, lo que abre interrogantes sobre los medios que empleó para llegar hasta los Alpes de Ötztal. Además de lo anterior y de acuerdo con los expertos, el hacha de cobre que llevaba Ötzi al morir es signo de un estatus social elevado, pues en su tiempo las hachas más comunes estaban fabricadas de piedra.En la actualidad, entre los factores de riesgo para las enfermedades cardiovasculares se cuentan una mala dieta, la inactividad física y el tabaquismo, mismos que no podríamos esperar tuvieran la misma vigencia hace 5,000 años. De acuerdo a los expertos, los problemas de arterioesclerosis que sufría Ötzi serían entonces explicados en base a su predisposición genética a la enfermedad.Podemos quizá concluir que Ötzi, a pesar de su elevado estatus social, no fue una persona particularmente afortunada pues –aparte de que murió asesinado– padecía de un buen número de enfermedades, en particular de arterioesclerosis. Sufría de esta última enfermedad a pesar de que muy posiblemente no incurría en ninguno de los actuales factores de riesgo para la mismaÖtzi fue así doblemente desafortunado, pues por un lado padecía de una enfermedad potencialmente mortal –aunque al final no murió de la misma– que consideramos es en buena medida producto de nuestro tiempo, mientras que por el otro no contaba con los medios de que disponemos hoy en día para combatirla.",
    "Con más de 1200 millones de habitantes –el 17% de la población mundial–, la India es después de China el segundo país más poblado de la Tierra. Además, puesto que el crecimiento demográfico chino es menos acelerado que el de la India, se espera que este último país se convierta en el curso de unas pocas décadas en el más poblado del planeta. En términos demográficos la India es un país gigantesco que, a manera de comparación, tiene una población diez veces más grande que la de México. La India está, además, en camino de convertirse en una potencia científica y tecnológica; de hecho, aun ahora destaca en algunos campos. Es, por ejemplo, miembro del selecto grupo de países que poseen armas nucleares, membresía que alcanzó en el año 1975 cuando detonó su primera bomba nuclear. En otros logros tecnológicos, en el año 1980 la India puso en órbita un satélite empleando un cohete propio, mientras que en 2008 logró colocar en una órbita lunar una sonda para el estudio de nuestro satélite, la cual pudo identificar por primera vez moléculas de agua en su superficie. La India cuenta también con un programa espacial vigoroso que incluye planes para poner astronautas en órbita terrestre –e incluso más allá–, lo mismo que planes para colocar un vehículo explorador en la superficie lunar y enviar una sonda a Marte. En actualidad, buscando expandirse más allá de lo nuclear y lo espacial, la India está haciendo esfuerzos para desarrollarse de manera amplia en todas las áreas científicas y tecnológicas. En el número de la revista “Science” publicado esta semana aparecieron varios artículos, que incluyen una entrevista hecha al Primer Ministro indio Manmoahan Singh, en donde se da cuenta de estos esfuerzos. Entre otros indicadores del progreso científico de la India mencionados por “Science”, tenemos que entre los años 2000 y 2010 este país duplicó el número de artículos científicos publicados hasta alcanzar 40,000 por año. El impacto que éstos tuvieron –medido por el número de veces que fueron citados por otros autores– también creció de manera significativa.  Un factor que limita el desarrollo científico de la India es la falta de investigadores. Para paliar este déficit, entre otros esfuerzos se está buscando repatriar a científicos indios trabajando en el extranjero. Para esto, les están ofreciendo condiciones de trabajo atractivas que incluyen abundantes recursos para investigación y salarios competitivos. Al respecto, de acuerdo con el director del Instituto Indio de Ciencia, Educación e Investigación en Pune –citado por “Science”–, “Un profesor que empieza su carrera puede ser más rico en la India que en los Estados Unidos”. Sin embargo, no todo es miel sobre hojuelas para los investigadores en la India y “Science” registra quejas en relación a esto. Algunas de estas quejas –que para nada resultan sorprendentes en nuestro país–  tienen que ver con una excesiva burocracia que lo mismo entorpece la importación de insumos para la investigación que impone al investigador frecuentes reportes del progreso de su trabajo. En este respecto, un investigador comenta: “Me evalúan como si estuviera construyendo una carretera. Quieren un reporte cada 3 kilómetros”.La India planea incrementar substancialmente en el futuro inmediato los recursos dedicados a la ciencia. De acuerdo con “Science”, durante el Congreso Científico Indio celebrado el mes pasado, el Primer Ministro Singh dio a conocer un plan a cinco años para doblar el gasto indio en investigación y desarrollo, hasta alcanzar el 2% del PIB en el año 2017. Esto representaría 8,000 millones de dólares anuales.La ciencia, tal como la conocemos, se originó en Europa en los siglos XVI y XVII. Posteriormente, a lo largo del los siglos XIX y XX –una vez que se hubieron acumulado suficientes conocimientos cientìficos– nació la tecnología moderna que se apoya en la ciencia. Esta tecnología hace uso de conocimientos científicos para crear ingenios altamente sofisticados –cada vez en mayor grado–, que de otro modo hubieran sido imposibles, no solamente de fabricar, sino incluso de concebir. De estos ingenios científicos tenemos hoy en día numerosos ejemplos, que han cambiado drásticamente nuestras condiciones de vida en todos los órdenes. A lo largo de la segunda mitad del siglo pasado, cuando el impacto de la tecnología científica se hizo más intenso, el centro de gravedad de la ciencia –o al menos uno de sus centros– se movió de su cuna en Europa hacia el oeste –los Estados Unidos–. Ahora, en el Siglo XXI, tal parece que dicho centro proseguirá su viaje hacia el oeste y se estacionará en las décadas por venir en los países asiáticos. Entre éstos destacan China y la India, tanto por constituir entre ambos más de un tercio de la población del mundo, como por su vigorosa política de desarrollo científico.En nuestro continente, con la excepción de Brasil que, aunque tarde, “se puso las pilas”, en América Latina estamos viendo pasar la ola científica en su viaje hacia el oeste sin que nos hayamos montado en la misma. Suponiendo que lo hiciera ¿Tendremos que esperar en México a que pase por segunda vez?",
    "Como consecuencia de los avances médicos y de salud pública a lo largo del siglo pasado se ha producido un incremento continuo en el promedio de vida de la población.  Al respecto, según datos de la Organización Mundial de la Salud, la esperanza de vida al nacer a nivel global se incrementó de 31 años en 1900 a casi 66 años en 2005, mientras que en países como Japón, Suiza y España, supera los 80 años. A lo largo del último siglo, entre otros muchos avances médicos, se han erradicado enfermedades infecciosas que antes eran devastadoras y se han desarrollado técnicas que no hace mucho tiempo nos hubieran parecido propias de la ciencia ficción.La generación de nuevas terapias y dispositivos médicos que se antojan de fantasía científica no es, por supuesto, cosa del pasado y están por el contrario continuamente surgiendo como producto de las grandes inversiones pública y privada que se dedican a la investigación biomédica. Un dispositivo que casi entra en esta categoría fue dado a conocer la semana que hoy termina en el congreso de la Asociación Americana para el Avance de la Ciencias, celebrado en la ciudad de Vancouver, Canadá. Dicho dispositivo, que fue también motivo de un artículo publicado el pasado jueves en la revista “Science Translational Medicine”, fue realizado por investigadores de varias universidades y centros de investigación de los Estados Unidos, incluyendo la Universidad Harvard y el Instituto Tecnológico de Massachusetts, en conjunto con la compañía privada “Microchips” fabricante de los dispositivos. El dispositivo reportado consiste básicamente en una cápsula para liberar de manera controlada un fármaco para el tratamiento de la osteoporosis. La cápsula se implanta dentro del cuerpo y la liberación del fármaco se programa de manera inalámbrica por medio de una computadora externa. Dicha liberación también puede hacerse en un determinado momento desde el exterior. La comunicación es, además, bidireccional, de modo el dispositivo implantado puede enviar información hacia el exterior. La cápsula tiene un tamaño equivalente al de una memoria USB para computadora y encierra a dos microchips y a la electrónica de control y de comunicaciones necesaria para la operación de dispositivo. El fármaco a liberar –en dosis de tan sólo 40 millonésimas de gramo– es colocado en pequeños compartimentos alojados en los microchips –diez por cada microchip, lo que da un total de 20 dosis–, los cuales son sellados por medio de una membrana muy delgada de una aleación de titanio y platino. Para liberar una dosis del fármaco se funde una de las membranas empleando una diminuta corriente eléctrica. Esto se hace siguiendo una rutina previamente establecida, según el tratamiento médico a seguir. Los dispositivos fueron puestos a prueba con siete pacientes mujeres con edades entre los 65 y los 70 años, a las que se les administró una dosis diaria del fármaco a lo largo de 20 días. De acuerdo con el artículo de referencia, los resultados del estudio fueron equivalentes a aquellos obtenidos por el procedimiento tradicional administrando el fármaco mediante inyecciones diarias. Esto demuestra las grandes ventajas del nuevo dispositivo, pues elimina la incomodidad que muchos pacientes experimentan ante las inyecciones. Las inyecciones se eliminan, no obstante, al costo de someterse a la operación necesaria para implantar el dispositivo –en el abdomen–, aunque ésta se lleve a cabo en el consultorio del médico con anestesia local. Para aliviar la incomodidad de esta operación –ciertamente mayor que la de una simple inyección subcutánea–, Microchips está desarrollando un dispositivo para 400 dosis, lo que alargaría a un año el tiempo entre implantes.De acuerdo con Robert Farra de la compañía Microchips y uno de los autores del estudio, el dispositivo desarrollado podría ser empleado para administrar fármacos para otras enfermedades, incluyendo el cáncer. El volumen de los contenedores en el microchip, no obstante, es pequeño por necesidad, de modo que la tecnología no podría ser usada para el tratamiento de otras enfermedades, como la diabetes, que requieren de volúmenes mayores del fármaco a administrar.    Se podría especular que a futuro se desarrollen dispositivos que se implanten dentro del cuerpo y que no solamente administren fármacos de manera controlada, sino que además incluyan sondas que proporcionen a la computadora de control la información necesaria para que pueda ajustar la dosis a liberar. Con esto, dicha computadora tomaría el control total de la lucha contra la enfermedad. Esto último suena ciertamente a ciencia ficción, aunque dados los vertiginosos avance biomédicos,  no podemos dudar que en algún momento se haga realidad. Antes de que esto ocurra, hay que esperar a que el dispositivo anunciado esta semana esté disponible comercialmente, lo que tardaría todavía algunos años. El costo anunciado del dispositivo –de 10,000 a 12,000 dólares–, sin embargo, posiblemente haga que muchos, de cualquier manera, se decidan por la opción de las inyecciones diarias.",
    "El pasado mes de diciembre se cumplieron 100 años desde que se alcanzó por primera vez el Polo Sur de nuestro planeta. Esto fue llevado a cabo por la expedición encabezada por el noruego Roald Amundsen, quien de este modo superó al británico Robert Falcon Scott, que también pretendía ser el primero en pisar el polo austral. Scott también llegó a su destino, sólo que un mes después de que lo hiciera Amundsen. Es de notar que la expedición de Scott resultó trágica, pues ninguno de sus integrantes sobrevivió a las duras condiciones climáticas de la Antártica –en donde se localiza el Polo Sur– durante el viaje de regreso a la civilización. El continente antártico tiene un área de 14 millones de kilómetros cuadrados, lo que lo coloca como el cuarto continente de mayor extensión, después de Asia, América y África. Es, además, el continente más frío e inhóspito, con una temperatura media a lo largo del año de 17 grados centígrados bajo cero. De hecho, es en la Antártica en donde se ha observado la temperatura más baja jamás registrada en la superficie de nuestro planeta: 89.3 grados centígrados bajo cero, medida el 21 de julio de 1983 en la estación rusa Vostok.   Debido a las bajas temperaturas que imperan en la Antártica, la mayor parte de su superficie está permanentemente cubierta de una gruesa capa de hielo, que llega a alcanzar en algunos lugares un espesor de casi cinco kilómetros. En la actualidad, un siglo después de las expediciones de Amundsen y Scott, la Antártica sigue siendo motivo de un gran interés. En este respecto, sabemos de una carrera entre equipos internacionales con objetivos centrados en este continente. En efecto, bajo los hielos  antárticos, a una profundidad de varios kilómetros, existen enormes lagos que no han sido explorados hasta ahora. Entre éstos el más grande es el lago Vostok, que tiene una extensión de similar a la del lago Ontario, en la frontera entre los Estados Unidos y  Canadá, pero con un volumen de agua tres veces mayor. Según los expertos, es posible que el lago Vostok haya estado aislado por un periodo que iría de los 15 millones a los 25 millones de años y esto le da una enorme importancia científica. En particular, es prometedora la perspectiva de encontrar formas de vida que hubieran evolucionado de manera independiente a lo largo del periodo de aislamiento y que pudieran resultar desconocidas.   Se ha establecido también un paralelismo entre las condiciones que imperan en los lagos antárticos con aquellas que prevalecerían en mundos fuera de nuestro planeta, entre ellos Europa –satélite de Júpiter– en el que se sabe existen lagos subterráneos.   Con el objeto de acceder al lago Vostok, el Instituto de Investigación del Ártico y la Antártica de San Petesburgo, Rusia, ha estado perforando el hielo arriba de dicho lago a lo largo de las últimas dos décadas. El extremo frío austral y lo corto del verano en esas latitudes ha hecho extremadamente difícil la tarea. Ésta pareció haber alcanzado éxito hace un año, hacia el final del verano austral. No fue, sin embargo, hasta el pasado miércoles 8 de febrero que el equipo ruso confirmó haber finalmente llegado a la superficie del lago después de perforar casi cuatro kilómetros de hielo, noticia que fue ampliamente difundida por la prensa.      El fin del verano austral en la Antártica, sin embargo, obligó a suspender los trabajos, de modo que la investigación de los secretos que pudiera guardar el lago Vostok tendrá que esperar hasta el próximo mes de diciembre.No han faltado las críticas al proyecto de exploración del lago Vostok. Estas se han centrado en la posible contaminación del agua prístina del lago por los líquidos anticongelantes empleados en la perforación del pozo en el hielo. Los rusos, no obstante, consideran que esto no ocurrirá pues la alta presión a la que se encuentra el agua del lago –por el enorme peso del hielo que soporta– hará que la misma suba a lo largo del pozo empujando a  los anticongelantes hacia arriba e impidiéndoles penetrar al lago. En particular, según los expertos rusos, durante la perforación reportada el pasado 8 de febrero el agua habría subido decenas de metros, congelándose en el pozo y aislando nuevamente al lago del exterior.  En la medida que en tengan un interés puramente científico, las investigaciones antárticas son plausibles, aun con el peligro de contaminar medios ambientes que han permanecido aislados por tiempos que se miden en escala geológicas. Hay que tomar en cuenta, no obstante, que no siempre dichas investigaciones y exploraciones de la Antártica –y por supuesto de muchos otros lugares en nuestro planeta– han estado impulsadas por propósitos enteramente científicos.  Para convencernos de esto último, basta con echar una ojeada al mapa “político” de la Antártica, que muestra la forma casi circular característica de este continente dividida en sectores –cual rebanadas de pastel–, cada uno de ellos reclamado por un país distinto.",
    "En un comunicado de prensa del pasado jueves, la NASA anunció el descubrimiento de 11 sistemas planetarios en nuestra galaxia que incluyen un total de 26 nuevos planetas, algunos con un tamaño similar al de la Tierra y otros de dimensiones gigantes comparables a las de Júpiter. Cada uno de los nuevos sistemas solares  incluye de dos a cinco planetas, con periodos de traslación alrededor de sus respectivas estrellas de entre 6 y 143 días. Este descubrimiento fue realizado por la nave Kepler de la NASA, lanzada al espacio en marzo de 2009 en una órbita alrededor del Sol. Según el sitio de Internet de la agencia espacial norteamericana, el propósito de la sonda Kepler fue el de encontrar planetas fuera de nuestro sistema solar con condiciones similares a las de la Tierra y que pudieran albergar vida tal como la conocemos. Para que esto suceda, un planeta debe estar dentro de la “zona de habitabilidad”, es decir, a una distancia tal de su estrella de modo que la temperatura en su superficie no resulte ni tan alta ni tan baja para que pueda existir agua en forma líquida, que sabemos es esencial para la vida.   La técnica de detección de planetas empleada por la sonda Kepler es en principio muy simple y se basa en la disminución del brillo aparente de una estrella cuando el planeta se interpone entre ésta y la Tierra. De este modo, la fluctuación periódica del brillo de una estrella será indicativo de la existencia de un planeta girando a su alrededor.  Así mismo, el tiempo entre dos fluctuaciones de luminosidad corresponderá al año solar del planeta, mientras que la magnitud de dichas fluctuaciones será indicativa de su tamaño. Descubrir un planeta empleando la técnica anterior podría quizá parecer simple. No es, por el contrario, de ninguna manera una tarea fácil por los cambios extremadamente pequeños en el brillo estelar que tienen que ser medidos. Éstos, además, dependen del tamaño del planeta en cuestión, de modo que un planeta pequeño es más difícil de detectar que uno más grande. Encima de todo esto, es necesario observar simultáneamente un gran número de estrellas para lograr descubrir una posible fluctuación de la luminosidad de solamente algunas pocas de ellas. Para este propósito, la sonda Kepler ha medido de manera repetida el brillo de cerca de 150,000 estrellas en una estrecha porción de la Vía Láctea. Hasta la fecha, ha confirmado la existencia de más de 60 planetas extrasolares, incluyendo los 26 anunciados esta semana.Si descubrir un planeta orbitando una estrella lejana es una empresa difícil de llevar a cabo, averiguar si tiene condiciones similares a las de la Tierra resulta todavía más complicado. En este sentido, el pasado mes de diciembre la NASA anunció que, por primera vez, la sonda Kepler confirmó la existencia de un planeta de tamaño similar a la Tierra –2.4 veces más grande– dentro de la zona de habitabilidad de una estrella similar a nuestro sol. No es claro, sin embargo, si dicho planeta es en realidad similar a la Tierra, pues no se sabe si es rocoso como nuestro planeta o gaseoso como los planetas gigantes de nuestro sistema solar. Adicionalmente, tenemos que aun dentro de la zona de habitabilidad un planeta puede tener condiciones climáticas radicalmente diferentes a las de la Tierra. Así, Venus y Marte, a pesar de ser planetas vecinos nuestros, sufren de un clima inhóspito en nada parecido al nuestro –con todo y el calentamiento global que sufrimos.En efecto, Venus tiene una atmósfera extremadamente densa de dióxido de carbono y el efecto invernadero que ésta genera mantiene de manera permanente la temperatura en la superficie del planeta por arriba de los 400 grados centígrados. Marte, en contraste, es frío y tiene una atmósfera muy tenue –también de dióxido de carbono– que no logra amortiguar las variaciones de temperatura entre el día y la noche que pueden alcanzar los 100 grados centígrados.Con seguridad en los meses y años por venir sabremos de la existencia de un número cada vez más grande de planetas extrasolares dentro de la zona de habitabilidad de sus respectivas estrellas –a través de la sonda Kepler y por otros medios–. Tendremos, así mismo, evidencia de que algunos de estos planetas se parecen al nuestro en cuanto a condiciones climáticas y bien podrían ser asiento de vida avanzada similar a la terrestre. Por el contrario, el que lleguemos a tener evidencias directas de esto último posiblemente sea mucho más improbable. Así, por lo pronto y al igual que en los tiempos de Giordano Bruno –que en el año 1600 fue condenado por la inquisición romana y ejecutado en la hoguera por creer en la existencia de otros mundos similares a la Tierra– no nos queda sino especular sobre la existencia de vida inteligente en otras partes del Universo.Con la ventaja, no obstante, de no resultar tan peligroso como hace 400 años.",
    "Según un informe publicado por el Departamento de Comercio de los Estados Unidos el pasado mes de noviembre, las operaciones de las compañías multinacionales de este país en el extranjero están aumentando de manera substancialmente más rápida que las domésticas –aunque aquellas todavía sobrepasan a estas últimas en proporción de dos a uno–. Como reflejo de esto, en la última década dichas compañías suprimieron más de 800,000 puestos de trabajo en los Estados Unidos, al mismo tiempo que crearon casi tres millones de empleos en el extranjero, principalmente en países emergentes como China, la India y Brasil. Todo esto, por supuesto, con el objeto de reducir costos de operación y maximizar ganancias.   En relación a esto último, aunque el movimiento hacia el exterior de las multinacionales norteamericanas haya sido originalmente motivado por los bajos salarios en el extranjero, o bien alguna otra ventaja competitiva, de acuerdo con el informe referido, en la actualidad tiene la motivación adicional de aprovechar el creciente mercado de los países emergentes; esto, dados los problemas económicos por los que atraviesan los Estados Unidos desde hace algunos años. Es decir, los productos fabricados en el extranjero por las empresas norteamericanas están, de acuerdo al reporte de referencia, encontrando mercados alternativos al decaído mercado estadounidense. Por otro lado, según un reporte dado a conocer esta semana por la “National Science Foundation” (NSF) de los Estados Unidos, el fenómeno de transferencia de puestos de trabajo desde los Estados Unidos hacia el extranjero abarca también a aquellos empleos clasificados como de alta tecnología. En efecto, los puestos de investigación creados por las compañías multinacionales en el extranjero crecieron rápidamente en la última década hasta alcanzar en 2009 el 27% del total de investigadores contratados –sumados aquellos en los Estados Unidos y en el extranjero–. Este crecimiento fue especialmente notable entre los años 2005 y 2009 cuando las nuevas contrataciones de especialistas en el extranjero constituyeron el 85% del total. Con los números anteriores, resulta que más de un cuarto de los investigadores empleados por las compañías multinacionales norteamericanas residen fuera de los Estados Unidos.    Como publica la revista “Science” en su número de esta semana, si bien los Estados Unidos alcanzaron en el año 2000 un máximo de 2.5 millones en el número de empleos en el campo de alta tecnología –que comprende las industrias farmacéutica, aeroespacial, de cómputo y de maquinaria de alta precisión–, a partir de ese año este número disminuyó rápidamente, perdiendo el país en el curso de una década casi 700,000 puestos de trabajo que representaron el 28 % de su fuerza laboral. La contratación de investigadores mexicanos en México por parte de compañías multinacionales constituye una oportunidad para desarrollar campos de alta tecnología en nuestro país. Para que esto se diera, sin embargo, sería necesario que en México hubiera una oferta suficientemente grande de investigadores en campos de relevancia tecnológica como es el caso de países asiáticos como China o la India que están aprovechando las nuevas condiciones en el mundo. Esto, desgraciadamente, no ocurre en nuestro país sino por excepción. En efecto, si bien es cierto que se han hecho grandes esfuerzos para la formación de investigadores mexicanos mediante el generoso programa de becas del CONACyT –para realizar estudios de posgrado tanto en México como en el extranjero–, el apoyo federal a la investigación no ha sido igualmente generoso. Así, México dedica escasamente el 0.4% del PIB en investigación y desarrollo. En comparación, los Estados Unidos dedica a este rubro el 2.7%, Japón y el 3.3%, Corea del Sur el 3%, China el 1.4% y Brasil el 0.9%, por mencionar solamente algunos países.         De este modo, aunque México está formando investigadores, no existen suficientes puestos de trabajo en nuestras universidades o en la industria para absorberlos. Aun en el caso de que un recién formado investigador tenga la suficiente suerte de encontrar un trabajo, el apoyo monetario para que pueda desarrollar su campo de investigación no está de ninguna manera garantizado. Estamos de este modo en peligro de limitarnos a ver pasar las oportunidades de desarrollo que otros países sí están aprovechando.Si ese fuera el caso, a pesar de que nos convertiríamos en meros espectadores del progreso de China y de otras naciones, no podríamos de ninguna manera decir que nos quedamos como el chinito nomas….",
    "De acuerdo con información aparecida la pasada semana en el periódico Wall Street Journal, la compañía Eastman Kodak se estaría declarando en bancarrota a finales del presente mes de enero o a principios de febrero. Esto marcaría la caída de una compañía que mucho contribuyó al desarrollo de la fotografía y de la industria del cine tal como las conocemos en la actualidad.Como sabemos, Kodak fue por una centuria la compañía dominante en la industria de la fotografía y a la cual le dio forma en buena medida. En efecto, podemos decir que George Eastman –el fundador de Kodak– inventó al fotógrafo amateur que casi todos llevamos dentro, cuando comercializó hace más de cien años la primera cámara fotográfica Kodak, misma que publicitó con el eslogan “Usted presiona el botón, nosotros hacemos el resto”.  Esta cámara, puesta en el mercado en 1888, se vendía por 25 dólares cargada con suficiente película fotográfica para 100 tomas. Una vez agotadas las 100 fotografías, la cámara debía ser enviada a las instalaciones de Kodak en Rochester, N.Y., para que la película fuera revelada y las fotografías impresas. Hecho esto, la cámara era nuevamente cargada con película fotográfica y enviada de regreso al cliente juntamente con sus impresos. Para los estándares actuales este proceso luce ciertamente complicado. Hay que tomar en cuenta, sin embargo, que antes de la aparición de la primera cámara Kodak la fotografía era cosa de especialistas, pues a una sesión de fotografía –además de su cámara– el fotógrafo tenía que llevar consigo todo un laboratorio de revelado, ya que una vez expuestas las películas tenían que ser procesadas en cuestión de minutos. No es de sorprender, entonces, el éxito que Eastman tuvo con su primera cámara fotográfica. Además de lo anterior, el fotógrafo amateur recibió de Kodak un nuevo impulso con la aparición en 1900 de la cámara “Brownie”, de fácil manejo y con un accesible costo de solamente un dólar.  El declive actual de Eastman Kodak está asociado a la aparición de la cámara digital que ha sustituido casi por completo a las cámaras analógicas o de película fotográfica. Como bien sabemos, la  cámara digital nos permite tomar fotografías con una facilidad pasmosa y con resultados inmediatos, lo que posibilita hacer correcciones y repetir la toma en caso necesario. Esto contrasta con la fotografía analógica que implica un cierto tiempo de espera antes de poder ver el resultado; es decir, hasta que la película fotográfica sea procesada en una instalación especializada.La cámara digital es producto de la misma tecnología electrónica que ha dado lugar a los procesadores y a las memorias de computadora, entre otros “chips” o circuitos integrados.  Esta tecnología tuvo sus inicios hace más de medio siglo y desde entonces las dimensiones de estos circuitos se han hecho cada vez más pequeñas, siguiendo lo que se conoce como la “ley de Moore”.   Según esta ley, el número de transistores –uno de los elementos electrónicos básicos– contenidos  en un “chip” se dobla aproximadamente cada dos años. Como resultado, un “chip” de dos centímetros por dos centímetros de área puede contener actualmente miles de millones de transistores. En una cámara digital la película fotográfica es sustituida por un dispositivo denominado CCD, que consiste de un gran número de detectores –“pixeles” – arreglados en una cuadrícula. La ley de miniaturización también se cumple para los dispositivos CCD, de tal manera que el número de “pixeles” de una cámara digital–como bien nos consta– crece rápidamente con los años. Como resultado, las cámaras digitales pueden tomar fotografías cada vez más nítidas. De este modo, no es sorprendente que la fotografía analógica, una tecnología propia de la primera mitad del Siglo XX, haya perdido la carrera ante la pujante industria electrónica de nuestros días. En relación a lo anterior, podemos quizá decir que la fotografía digital ha dado origen a una revolución en la manera como concebimos a los dispositivos para capturar imágenes –lo que queda evidenciado por el gran número de fotografías y videos que aparecen continuamente en situaciones inesperadas– y que esta revolución tiene un alcance similar a la generada por Eastman Kodak hace más de un siglo.   Así las cosas, no nos queda sino añorar –a los que tenemos la edad suficiente– tiempos idos, de cámaras analógicas y de películas fotográficas Kodak.",
    "En el mes de junio del pasado año fue subastado el violín “Lady Blunt ” –fabricado por Antonio Stradivari en 1721–  en la asombrosa cantidad de 16 millones de dólares. El violín fue vendido por una fundación musical japonesa y lo obtenido fue dedicado a un fondo de ayuda a las víctimas del maremoto que asoló Japón en marzo de 2011.Tan asombroso como el valor que ha alcanzado el Lady Blunt lo ha sido el incremento que este valor ha tenido con los años. En efecto, en 2008 dicho violín fue vendido en 10 millones de dólares en una transacción privada, mientras que en 1971 alcanzó en subasta pública un valor de “apenas”  200,000 dólares. Por otro lado, aunque el violín Lady Blunt ha sido el que más alto precio ha alcanzado en subasta, otros violines fabricados por Antonio Stradivari, así como también por su contemporáneo Giuseppe Guarneri –que vivieron en Cremona, Italia, en los siglos XVII y XVIII– han alcanzado precios de subasta de millones de dólares.   Dado el alto prestigio que tienen los violines Stradivari y Guarneri se han llevado a cabo un buen número de investigaciones buscando averiguar el secreto de su sonido a fin de reproducirlo. Dicho sonido ha sido atribuido a diversos factores; entre éstos se encuentran el barniz aplicado al instrumento y el tratamiento químico que se le dio a la madera con que dicho instrumento fue fabricado. Se ha propuesto también que la época fría que padeció Europa entre los siglos XVI y XIX –conocida como Pequeña Edad del Hielo– fue determinante para la producción de violines de calidad superior; esto debido a que el frío hizo más lento el crecimiento de los árboles que de esta manera produjeron madera más densa, lo que habría influido positivamente en el sonido de los violines fabricados con la misma.    En contra de la creencia tradicional, no obstante, pudiera ser que no haya en realidad nada especial en los violines de Cremona. Al menos esto es lo que afirma un artículo publicado esta semana en las Memorias de la Academia Nacional de Ciencias de los Estados Unidos por investigadores de instituciones de Francia y los Estados Unidos. En dicho artículo se reportan los resultados de un estudio en el que participaron 21 violinistas profesionales. A los mismos se les proporcionaron seis violines antiguos –dos Stradivari de los años 1700 y 1715, y un Guarneri de 1740– y tres violines modernos, y se les pidió que los evaluaran. Los tres violines antiguos tienen un valor combinado de aproximadamente 10 millones de dólares, que es unas 100 veces mayor que el valor combinado de los violines modernos.Con el objeto de evitar prejuicios, a los participantes no se les dio información sobre la procedencia y antigüedad de los instrumentos y solamente se les hizo saber que entre los mismos había cuando menos un Stradivari. Las pruebas se llevaron a cabo en una habitación con poca luz y los violinistas usaron lentes de soldador; todo esto con el fin de que no pudieran identificar  la edad de los instrumentos por su apariencia. Asimismo, se aplicó en el soporte de la barbilla un perfume especial de modo que tampoco hubiera manera de identificar esta edad por medio del olor que despide el instrumento.Se hicieron dos tipos de pruebas. En la primera se le proporcionaron a cada uno de los participantes todos los instrumentos por pares, formados éstos por un violín antiguo y otro moderno –en total nueve combinaciones–, y se les pidió que compararan su calidad sonora. Aproximadamente la mitad de los participantes se pronunciaron por un violín antiguo, mientras que la otra mitad lo hizo por uno moderno. La excepción ocurrió cuando se evaluó el Stradivari del año 1700, que la mayor parte de los participantes –de  manera sorprendente– calificó como inferior en comparación a los violines modernos. En una segunda prueba se proporcionaron por turnos los seis violines a los participantes y se les pidió que escogieran al mejor y al peor. Como resultado, uno de los violines modernos fue escogido por 8 participantes como el mejor y por ninguno como el peor. En contraste, el Stradivari 1700 –  que no había salido bien librado de la primera prueba– fue escogido por 6 participantes como el peor y solamente por uno como el mejor.De acuerdo a lo anterior, no existe una correlación entre el valor que alcanza un violín en el mercado con la calidad del sonido que emite, tal como este último es percibido por violinistas profesionales que lo mismo prefieren un violín antiguo que uno moderno. La percepción que los violines de Cremona construidos hace tres siglos emiten un sonido de calidad especial no tendría entonces bases objetivas y entraría en el terreno de lo puramente subjetivo.Después de 200 años de creer lo contrario no podemos esperar que esta idea sea fácilmente aceptada. La respalda, no obstante, una investigación objetiva que tendría que ser refutada por otra investigación igualmente objetiva.",
    "Una manera común de clasificar los hechos históricos es mediante el año en que tuvieron lugar. Así, al final de cada año es costumbre hacer un recuento de los acontecimientos más significativos ocurridos a lo largo del mismo. Clasificar a los hechos históricos por año es sin duda conveniente. Es, sin embargo, al mismo tiempo arbitrario, pues depende del calendario particular que usemos. En este respecto, el calendario más ampliamente empleado en la actualidad es el gregoriano, vigente desde finales del Siglo XVI, y que reemplazó al calendario juliano implantado en Roma hace 2 milenios.Para nosotros la unidad básica de medida del tiempo es el día, o sea el periodo medio que transcurre entre dos pasos del Sol por el mismo punto en el firmamento. Aparte del día, hay otros periodos de tiempo que nos son significativos: el periodo de rotación de la Tierra alrededor del Sol –que determina las estaciones de año y que es fundamental para, por ejemplo, la agricultura– y el periodo de las fases de la Luna. Consecuente con esto, los calendarios que se han inventado han tomado en cuenta ya sea al periodo lunar, al periodo solar, o bien a ambos.   Se tienen, sin embargo, algunos problemas para elaborar un calendario solar o lunar. Éstos tienen que ver con el hecho de que tanto el periodo de traslación de la Tierra alrededor del Sol, como el periodo de las fases de la Luna, no constan de un número exacto de días. En efecto, tenemos que la Tierra tarda en girar alrededor del Sol un poco más de 365 días, mientras que dos fases iguales de la Luna distan entre sí poco más de 29 días.   El calendario juliano está basado en el ciclo solar y asume que una vuelta al Sol le toma a la Tierra exactamente 365.25 días. De este modo, considerando 365 días por año se produce un adelanto anual por un cuarto de día que el calendario juliano compensa introduciendo un año de 366 días –año bisiesto– cada cuatro años. No obstante, se producen errores, pues el periodo de traslación de la Tierra alrededor del Sol no es 365.25 días sino un poco menos. La diferencia es de unos 11 minutos, los cuales se acumulan año con año y llevan a un retraso entre las fechas indicadas por dicho calendario y los eventos astronómicos que le sirvieron inicialmente de referencia. En particular, el Domingo de Pascua –regido por una combinación de los ciclos lunar y solar– fue fijado por el Concilio de Nicea del año 325 d.c. como el primer domingo después de la primera luna llena de primavera. Al final del Siglo XVI, sin embargo, el inicio de la primavera tal como lo indicaba el calendario juliano se había retrasado cerca de 10 días con respecto a su fecha astronómica real.Para corregir este desfase de fechas, el papa Gregorio XIII instauró en 1582 el calendario gregoriano. Este calendario, que rige hasta nuestros días, le suprimió diez días al mes de octubre de 1582. Suprimió, además, los años bisiestos múltiplos de 100 –como lo son 1700 y 1900– con la excepción de aquellos múltiplos de 400 –como lo fue el año 2,000–. No obstante, aunque mejoró sustancialmente a su predecesor, el calendario gregoriano no es perfecto, y acumula un error de 26 segundos por año. Una inconveniencia más significativa que el retraso anual de casi medio minuto  –y que el calendario gregoriano comparte con su inmediato antecesor– es que el día de la semana que corresponde a una fecha determinada cambia año con año. En relación a esto, dos investigadores de la universidad norteamericana Johns Hopkins, en Baltimore, Maryland, proponen un nuevo calendario que solucionaría este problema, es decir, que fijaría un día de la semana para cada fecha particular.El calendario propuesto comprende doce meses divididos en 4 trimestres. Cada uno de estos trimestres está formado por dos meses de 30 días y un mes de 31 días para un total de 91 días. El año consta así de 364 días, lo que lo hace más corto que el año gregoriano por uno o dos días, según se trate de un año normal o uno bisiesto. Para compensar, cada cuatro o cinco años se añade una semana al final del año. Con este esquema a cada día de cada año le corresponderá efectivamente un mismo día de la semana. Esto será en particular cierto para las festividades que tienen una fecha determinada. No lo será, sin embargo, para aquellos días festivos móviles, como el Domingo de Pascua, que depende del ciclo lunar y que seguiría flotando.Según sus autores, el nuevo calendario simplificaría mucho la planeación de actividades anuales que ahora tiene que ser hecha siguiendo el calendario particular de cada año. Consideran, asimismo, que tendría grandes ventajas en operaciones financieras que requieran de contar días; por ejemplo, el cálculo del interés que se aplicaría a un determinado préstamo. Pensando en nuestro país nos preguntamos si con este nuevo calendario podríamos fijar de una manera racional –y de una vez por todas– el día de la semana en que ocurran nuestros días festivos –moviéndolos de fecha en caso necesario– y evitar de esta manera la proliferación de “puentes” que nos gusta establecer a la menor provocación. Para encontrar una respuesta a esta pregunta sería necesario primeramente que se adopte el nuevo calendario, lo que, sin embargo, no es claro que vaya a ocurrir.",
    "Cuando los marcianos invadieron nuestro planeta, según la novela “La guerra de los mundos” del escritor británico H.G. Wells, se encontraron con un enemigo inesperado: los microbios terrestres. Para  fortuna nuestra y desgracia de los invasores, el enemigo resultó tan formidable que éstos últimos terminaron aniquilados en unos pocos días, a pesar de que contaban con una avanzada tecnología. El exterminio ficticio de marcianos, que habría ocurrido hace poco más de un siglo –la novela de Wells fue publicada en 1898–, fue precedido por buen número de pandemias que llevaron al exterminio –ése sí real– de millones de personas. Una de las más famosas fue la epidemia de peste bubónica conocida como la “Muerte negra”, que en el Siglo XIV llevó a la tumba a una buena parte de la población de Europa–. No hubo, por otro lado, que esperar mucho tiempo después de la publicación de la  “Guerra de los mundos” para que se diera la siguiente epidemia de grandes proporciones: la epidemia de influenza llamada “Gripe española”, que en 1918 mató a cerca de 20 millones de personas. En la actualidad hay virus circulando en el mundo que los expertos consideran tienen potencial para provocar una pandemia. Uno de éstos es el virus de la gripe aviar H5N1, que fue noticia hace algunos años cuando logró infectar a personas que habían estado en contacto cercano con aves enfermas en el sudeste asiático. Se sabe que en humanos este virus es letal con una tasa de mortalidad mayor al 50%. Afortunadamente no es muy contagioso y solamente se sabe de un total de unas 600 personas infectadas; además, éstas lo han sido fundamentalmente a través del contacto con aves y no mediante un contagio entre personas.De hecho, la Organización Mundial de la Salud, que mantiene una vigilancia cercana sobre el virus de la gripe aviar, no considera que en estos momentos haya motivo de una particular preocupación por la posible ocurrencia de una pandemia de gripe aviar. No obstante, se sabe ahora que el virus H5N1 puede mutar y volverse altamente contagioso, manteniendo al mismo tiempo una alta tasa de mortalidad. En efecto, durante una reunión de virólogos realizada el pasado mes de septiembre en la isla de Malta, un grupo de investigadores del Centro Médico Erasmus de Rotterdam reportó la creación de una variante del virus H5N1 capaz de trasmitirse a través del aire, al igual que lo hacen los virus de la gripe común. Aunque los estudios fueron realizados con hurones, se considera que pueden extenderse a humanos. Se tendría así un virus capaz de propagarse fácilmente entre personas pero con una tasa de mortalidad superior al 50%.  Este virus, además, contiene mutaciones que pueden producirse de forma natural, lo que implica que podría aparecer sin intervención nuestra.Un artículo con los resultados de esta investigación ha sido sometido para publicación a la revista “Science”. En una situación inédita, no obstante, el Consejo Nacional Científico Consultor sobre Bioseguridad del Gobierno de  los Estados Unidos ha solicitado al editor de dicha revista que se supriman del artículo detalles que ayudarían a reproducir el virus. El argumento es que una información detallada de las técnicas empleadas pudiera  ser usada por grupos terroristas para la fabricación de armas biológicas.“Science”, al igual que los autores del artículo de referencia, han sido receptivos a esta solicitud, con la condición de que los detalles de la investigación sean difundidos entre otros grupos trabajando en el tema. En una entrevista concedida al periódico “New York Times”, sin embargo, Ron Fouchier, uno de los autores del artículo, considera que será difícil ocultar detalles de su trabajo una vez que se hagan del dominio de otros investigadores pues “Tan pronto como usted comparta información con más de 10 personas, la información estará en la calle”. Considera de este modo que el articulo debe publicarse con todos los detalles incluidos.Hay quien critica que una investigación que condujo a crear un virus tan peligroso se haya llevado a cabo en un laboratorio universitario, sin las medidas de seguridad indispensables como las que se encontrarían en una instalación militar. Hay quien pone en duda incluso la pertinencia de desarrollar un supervirus capaz de producir una pandemia.  Fouchier por su lado afirma que han demostrado la posibilidad de existencia de un virus de influenza altamente letal, al mismo tiempo que han encontrado las mutaciones que son responsables de su peligrosidad. Esto ayudaría a identificar brotes del supervirus que pudieran devenir en una pandemia. Ayudaría, asimismo, al desarrollo de vacunas y tratamientos para combatirlos.Es decir, ahora que conocemos más a fondo a un potencial enemigo natural, dado el caso podríamos defendernos mejor –aunque este enemigo por lo pronto sea hipotético, pues su existencia depende de que ocurran en la naturaleza ciertas mutaciones del virus H5N1.O en su defecto, podríamos usar el nuevo conocimiento contra un enemigo artificial que pudiera ser creado por un grupo terrorista, empleando conocimientos generados por una investigación llevada a cabo para fabricar un virus letal, y evitar así tener una muerte propia de un extraterrestre.",
    "La revista “Science” contiene en su edición del pasado jueves 15 de diciembre un interesante artículo, muy a tono con el año electoral que se avecina en nuestro país. En dicho artículo, publicado por un grupo de investigadores de universidades en Europa y los Estados Unidos, se describen los resultados de una investigación relativa a los procesos de toma de decisiones grupales en asuntos de interés colectivo. En particular, el estudio fue dirigido a averiguar la influencia que en la decisión del grupo tienen aquellos miembros poco informados acerca del asunto a convenir.  Los resultados de la investigación, encabezada por Iain Couzin de la Universidad de Princeton, indican que, si bien una minoría con ideas firmes acerca del asunto a acordar puede guiar el sentido de la decisión colectiva, esto no es necesariamente el caso si en el grupo hay un mínimo de miembros poco informados.Para ser más precisos, en un grupo formado por una mayoría y una minoría con opiniones encontradas e igualmente fuertes, los puntos de vista que  se impondrán serán ciertamente aquellos de la mayoría. La opiniones de la minoría, en cambio, prevalecerán si éstas tienen una carga emotiva lo suficientemente alta; es decir, las decisiones del grupo serán orientadas por un grupo minoritario intransigente. Esto último, sin embargo, no será necesariamente cierto si además existe un tercer grupo de individuos poco informados, que por naturaleza son altamente influenciables y tienden a seguir a la mayoría. Así, la influencia de la minoría intransigente es debilitada en beneficio del grupo mayoritario, que verá la balanza inclinarse  a su favor. Este efecto crece en la medida en que el grupo de desinformados se hace más grande hasta alcanzar un cierto tamaño, a partir de la cual no pueden actuar de manera coherente y lo mismo escogen una opción que la otra.Las conclusiones anteriores fueron alcanzadas tanto mediante estudios teóricos como a través de experimentos realizados con peces dorados, del tipo empleado como carnada. Para esto, lo peces fueron entrenados para seguir un blanco de color azul o amarillo. Los peces tienen una tendencia natural a seguir al color amarillo, de modo que el grupo entrenado para seguir a este color corresponde a la minoría con opinión intransigente, mientras que aquellos entrenados para seguir al color azul formaría la mayoría informada. El grupo de desinformados corresponde a un grupo de peces sin entrenamiento.Para llevar a cabo el experimento los peces fueron liberados en un tanque de agua, dentro del cual se colocaron los dos blancos a seguir. En una primera prueba se colocaron 6 peces entrenados para ir hacia el azul y 5 peces para ir hacia el amarillo. El resultado fue que estos últimos dominaron y todo el cardumen se dirigió de manera preferencial hacia el amarillo. Esta tendencia, sin embargo, se revirtió cuando se añadieron 10 peces sin entrenamiento de modo que ahora todo el grupo se dirigió de manera preferencial hacia el color azul;  es decir, hacia lo que dictaron las mayorías.Extendiendo los resultados anteriores, obtenidos con peces, a los procesos humanos, resultaría que la existencia de una mayoría sin opiniones fuertes y por lo tanto influenciable –que normalmente tiene para nosotros una connotación negativa– jugaría después de todo un papel importante en el momento de tomar decisiones por consenso. En palabras de Iain Couzin, recogidas en la página web de la Universidad de Princeton, “Pensamos que estar bien informado es bueno  y que estar mal informado es malo, pero eso es una construcción humana. Los grupos de animales rara vez están en un estado de rebeldía y podemos ver mucho consenso” y añade,  “Estos experimentos indican que hay una función evolutiva en estar desinformados, que es quizá tan activa como la de estar informados”. De este modo, los grupos desinformados anularían a las minorías intransigentes y apuntalarían a la democracia. Las conclusiones del artículo pudieran parecer absurdas pues ¿cómo es posible que el estar desinformado no sea necesariamente algo negativo sino que incluso cumpla con la función positiva de generar consensos? Dichas conclusiones están, no obstante, sustentadas en datos científicos. Por otro lado, deberán ser refutadas o confirmadas por otras investigaciones antes de ser ampliamente aceptadas o desechadas de plano y esperamos ver esto en un futuro cercano. Por lo pronto, sin embargo, resultan fascinantes.",
    "En el número del pasado jueves de la revista “Science” encontramos dos artículos que hablan de ciencia y educación de posgrado en dos países, China y Arabia Saudita. Si bien ambos países hasta hace algunos años no se distinguían en estos dos campos, en la actualidad los están impulsando fuertemente, cada uno a su manera. China y Arabia Saudita son dos naciones muy diferentes. Esta última, con una población que no llega a los 30 millones de habitantes, es el mayor productor mundial de petróleo. China, por su lado, cuenta con una población de más de 1,300 millones de personas –que es en números redondos el 20% de la población total del mundo–, y constituye la segunda economía más grande del planeta después de los Estados Unidos. Arabia Saudita con sus petrodólares y sobre todo China con su enorme población están ciertamente haciendo ruido en el mundo de la ciencia. Arabia Saudita por un lado ha invertido grandes recursos en los últimos años para hacerse de una infraestructura de investigación de clase mundial. En la “King Abdullah University of Science and Technology” (KAUST), que es una Universidad exclusivamente para estudios de posgrado, tiene recursos que oscilan entre 10,000 y 20,000 millones de dólares. La universidad fue inaugurada en 2009 y se espera que alcance una población de 2,500 estudiantes y 250 profesores.El artículo de “Science” del pasado jueves sobre el desarrollo científico de Arabia Saudita toca, no obstante, un aspecto que no es necesariamente positivo. Éste se refiere a la “compra” de prestigio académico para lograr una mejor posición de las listas de clasificación de universidades a nivel mundial –que últimamente se han puesto de moda–. Así, la  “King Saud University” en Riayadh ha lanzado un programa para contratar a tiempo parcial profesores destacados de universidades de prestigio en otras partes del mundo, con el compromiso de que en los artículos técnicos que publiquen dichos profesores incluyan a la universidad saudita como su segunda institución de adscripción. Esto, aun si en el trabajo de investigación que da origen a la publicación la universidad árabe no hubiera tenido una participación real.Esta práctica ha triplicado en tres años el número de artículos de investigación en los que aparece la“King Saud University” lo que la ha ayudado a escalar cientos de lugares en las clasificaciones mundiales de universidades. No deja, sin embargo, de ser una simulación y ha provocado  críticas en el sentido de que en poco ayuda y aun perjudica los esfuerzos dirigidos a desarrollar una capacidad científica real como el que se está llevando a cabo en KAUST.     China, por su lado, no solamente se ha convertido en los últimos años en la segunda economía del mundo sino que también es en la actualidad el segundo país, después de los Estados Unidos, que más artículos científicos produce por año. Además, las universidades chinas están aumentado rápidamente su generación de graduados con grado doctoral. En la actualidad  producen cerca de 50,000 por año en todos los campos del conocimiento, lo que da a China el primer lugar en este respecto, incluso por encima de los Estados Unidos.Los chinos invierten actualmente el 1.4% de su producto interno bruto en gastos de investigación y desarrollo, y esto los ha convertido en el segundo país que más recursos invierte en este respecto, solamente superado por nuestros vecinos del norte.En el lado negativo, “Science” aduce que el sistema de educación e investigación chino adolece de defectos. Entre estos se encuentran el bajo nivel de algunos graduados con grados doctorales, así como también el que sea más importante en el momento de conseguir apoyos para un proyecto de investigación, los contactos que se tengan que los méritos técnicos del proyecto en cuestión. Los defectos anteriores son propios de un sistema científico en estado de desarrollo y posiblemente en México no nos resulten sorprendentes. Lo que si resulta contrastante con nuestro país es el grado de apoyo que tanto China como Arabia Saudita han dado a la ciencia como un elemento indispensable para transitar hacia una economía moderna basada en el conocimiento. En efecto, si bien en México se han fomentado ya por varias décadas los estudios de posgrado con un generoso programa de becas tanto en el país como en el extranjero, las oportunidades de empleo para los recién doctorados no son igualmente generosas. Además, las probabilidades de conseguir apoyos económicos para proyectos de investigación han empeorado en los últimos años, sobre todo para los investigadores recién graduados.    En marcado contraste con otros países también está el porcentaje del producto interno bruto que México dedica a la ciencia y la tecnología que es de sólo el 0.4% y que, según planes de hace algunos años, debería haber alcanzado ya un 1%. Debemos ser, sin embargo, optimistas y confiar que nuestro país tome pronto el rumbo que muchos han ya tomado. En todo caso, debemos tomar en cuenta que la esperanza muere al último. Y confiar en que no tenga una muerte prematura.",
    "Durante el campeonato de mundial de fútbol celebrado en Inglaterra en el año 1966, el equipo de Corea del Norte derrotó 1-0 a Italia, que era uno de los equipos más fuertes de la competencia. Posteriormente, en cuartos de final, aunque Corea del Norte perdió 5-3 con Portugal –equipo que finalizó en tercer lugar en el campeonato–, tuvo a los portugueses 3-0 abajo en algún momento del partido. Antes de todo esto no se sabía nada de Corea del Norte en cuanto a fútbol, de modo que su actuación en Inglaterra –había también empatado con Chile– fue grandemente sorpresiva. Así, no tardó en surgir una broma según la cual los coreanos habían ganado debido a que durante la interrupción del medio tiempo el entrenador había sustituido por completo a los jugadores por otros de refresco sin que nadie, incluido el árbitro, se hubiera dado cuenta. Esta broma tiene su base en un hecho real, pues si bien tenemos grandes habilidades para identificar por medio de su cara a aquellas personas con las que tenemos alguna familiaridad, hacerlo con personas otra raza nos resulta más difícil.  Esto es, por supuesto, general y válido para todo el mundo, incluidos los coreanos. Así, si el fútbol se hubiera originado en Corea del Norte en lugar de Inglaterra y si el campeonato del mundo de 1966 se hubiera realizado en aquel país y no en este último, bien pudiera haberse originado la misma broma pero en sentido inverso. Esto, si se hubieran dado una serie de victorias de cierto equipo desconocido de la región occidental del planeta sobre un equipo fuerte del Asia oriental.Los expertos saben que el cerebro humano tiene una región especializada en el reconocimiento facial de personas. Es decir, si un estímulo visual proviene de la cara de una persona el cerebro procesa la información de una manera diferente a como lo hace si el estímulo proviene de cualquier otro objeto. Esta característica la compartimos con otros mamíferos y es posible que esto no sea sorprendente. En cambio, si es de sorprender que en un artículo publicado el pasado jueves en la revista “Science” por un grupo de investigadores de la Universidad de Michigan en Ann Arbor, se  llega a la conclusión que cierta especie de avispas – “polistes fuscatus”–  también la comparte.Los experimentos reportados en dicho artículo fueron realizados con avispas que se sabe pueden identificarse visualmente entre ellas de manera individual por medio de marcas faciales que varían de un individuo a otro.  Las pruebas se realizaron en un túnel en forma de T con una altura suficientemente baja para no permitir el vuelo de las avispas, que de esta manera son obligadas a permanecer en contacto con el piso durante todo el experimento. Esto último es importante pues como parte del mismo las avispas son sujetas a choques eléctricos como un medio de obligarlas a tomar decisiones. Para este propósito se electrifica el piso del túnel, con la excepción de una “zona segura” en uno de los brazos de la T. El experimento consiste en entrenar a las avispas para encontrar en que brazo de la T se encuentra la zona segura, la cual, sin embargo, se cambia de manera aleatoria hacia uno u otro lado de la T en cada prueba. Cada avispa repite la prueba 40 veces.  Al inicio del experimento la avispa es colocada en la antesala del túnel en la parte inferior de la T. Después de un cierto tiempo de adaptación, se abre la puerta de acceso y se le permite la entrada al túnel, al fondo de cual hay dos imágenes de avispas de su misma especie, una en cada brazo de la T.  La imagen que se coloca en la región de la zona segura siempre es la misma, de modo que ésta constituye la clave para llegar a dicha zona. Una vez en el fondo del túnel la avispa tiene que decidirse si va hacia la izquierda o hacia la derecha en busca de la zona segura. Si sabe reconocer las caras de avispa en las imágenes que le son mostradas, entonces rápidamente aprenderá a encontrar el camino hacia la seguridad. De otro modo tendrá siempre un 50% de probabilidad de llegar a la zona equivocada.Además de llevar a cabo pruebas con imágenes de avispas de su misma especie, también se realizaron pruebas con imágenes de otros insectos, o bien con imágenes de avispas de su misma especie pero deformadas de alguna manera –quitándole la antenas, por ejemplo. Como resultado de los experimentos, encontraron que cuando se utilizaron imágenes de individuos  de su misma especie, las avispas equivocaron menos frecuentemente el camino hacia la seguridad, que cuando se cambiaron estas imágenes por otras deformadas o de especies diferentes. Esto demuestra que las avispas “polistes fuscatus” son especialistas en reconocimiento de caras de avispas al igual que lo somos nosotros de caras humanas.Queda por averiguar si las “polistes fuscatus” también hacen chistes a costa de otras avispas diferentes a ellas mismas.",
    "El planeta Marte, uno de los más cercanos a la Tierra, ha sido noticia en los últimos días. Por un lado, nos enteramos que la nave rusa Phobos-Grunt, lanzada al espacio el pasado 8 de noviembre, tiene problemas para llevar a cabo la misión que le fue encomendada. Esta nave fue enviada a explorar el satélite marciano Fobos y, de acuerdo con los planes originales, debía descender en dicho satélite, tomar muestras de materiales de su superficie y regresar con ellas a la Tierra. Una pérdida de comunicación con la estación controladora, sin embargo, impidió que se encendieran los cohetes propulsores que llevarían a la nave desde una órbita terrestre hasta el satélite marciano. Sin el impulso de estos cohetes, el Phobos-Grunt reingresará inevitablemente a la atmósfera de nuestro planeta en fecha cercana. Una buena noticia al respecto, surgida en los últimos días, es que se había logrado establecer contacto con la nave. Este contacto, no obstante, ha sido esporádico y no está claro si será posible revivir a Phobos-Grunt. En todo caso, aun si se lograran encender sus cohetes propulsores, la nave no podría alcanzar a Fobos por haber pasado ya la ventana de tiempo en que esto hubiera sido posible. La misión, entonces, tendría que ser redirigida a un objetivo alcanzable –la Luna, por ejemplo. Otra noticia sobre Marte fue el lanzamiento exitoso el día de ayer –a un costo de 2,500 millones de dólares– del explorador “Curiosity” hacia este planeta. El Curiosity es un vehículo de la NASA con un peso de alrededor de una tonelada, el cual explorará por un periodo de dos años la superficie marciana impulsado por energía nuclear. De acuerdo con el sitio de Internet de esta agencia espacial, el vehículo está dotado de seis ruedas de 50 centímetros de diámetro que se mueven de manera independiente y que le permitirán avanzar sin dificultad por terreno pedregoso. Según el mismo sitio, sobre terreno plano y firme podrá recorrer una distancia máxima de 150 metros en una hora. Esto no es mucho para nuestros estándares en la Tierra pero sin duda es impresionante para un robot controlado de manera remota a una distancia de 250 millones de kilómetros –hay que considerar que esta distancia es tan grande que las órdenes que se envíen al explorador desde nuestro planeta tardarán alrededor de 14 minutos en llegar. El “Curiosity” lleva una serie de sofisticados instrumentos para el análisis del suelo marciano hasta una profundidad de medio metro y, según la NASA, buscará establecer si existen o han existido en el pasado en Marte las condiciones necesarias para el desarrollo de vida. El explorador lleva, además, una buena cantidad de cámaras para la obtención de imágenes a todo color, e incluso de video, del paisaje marciano, y desde este punto de vista quizá lo podríamos concebir como una extensión de nuestro sentido de la vista; es decir, el equivalente a un telescopio de gran potencia que nos permitirá observar con detalle objetos extraordinariamente lejanos –al igual que los robots que lo antecedieron pero con un mayor detalle.Sabemos lo importante que han sido los telescopios –como una extensión de nuestra vista– para el avance científico. En efecto, cuando Galileo Galilei apuntó en el año 1610 su telescopio hacia Júpiter descubrió cuatro puntos brillantes en movimiento continuo alrededor del planeta. Para Galileo fue claro que estos puntos brillantes, que de ninguna manera son visibles a simple vista, son satélites de Júpiter –los satélites Galileanos, como ahora los conocemos–, que lo orbitan de manera similar a como los planetas se mueven alrededor del Sol. El descubrir que Júpiter forma con sus satélites un “sistema solar en miniatura” contribuyó a superar teorías según las cuales la Tierra es el centro del Universo a cuyo alrededor giran todos los cuerpos celestes.  De manera similar al telescopio, el microscopio como una extensión de nuestros sentidos –en este caso hacia lo muy pequeño– jugó un papel esencial en el avance científico. En particular, el holandés Anton van Leeuwenhoeck empleando microscopios que él mismo construyó con gran maestría, descubrió en el Siglo XVII la existencia de las bacterias, que son, por supuesto, invisibles sin esta extensión de la vista.       En estos momentos, según las últimas noticias, el “Curiosity” se dirige sin novedad hacia el planeta Marte, al cual arribará en agosto del siguiente año. Asumiendo que todo resulte como está planeado, en menos de doce meses podremos disfrutar de imágenes detalladas de la superficie de Marte, sin importar que este planeta esté a una distancia tal que a simple vista sólo lo percibamos como un punto de luz. Dado que no serán éstas las primeras imágenes de la superficie marciana que hayamos recibido, posiblemente no quedemos tan impactados como Galileo o Leeuwenhoeck cuando hicieron uso de sus extensiones de vista. No hay duda, sin embargo, de que resultarán fascinantes.",
    "Frases tales como “los fieros combatientes”, “una escala de violencia casi incomprensible” y “los encuentros son breves y brutales”, sin duda nos traen a la mente imágenes de alguna de las muchas guerras que se han dado a lo largo de nuestra historia. Estas frases, no obstante, no corresponden a confrontación humana alguna; lejos de esto fueron extraídas del artículo “Las hormigas y el arte de la Guerra” en el que Mark Moffett, del Museo Smithsoniano de Historia Natural, describe una guerra entre hormigas de la que fue testigo en las selvas de Malasia. Dicho artículo fue publicado en el último número de la revista “Scientific American”.De acuerdo con Moffett, al igual que los humanos, varias especies de hormigas libran guerras feroces, tanto contra especies diferentes como contra otras colonias de su misma especie. Con motivaciones, además, no muy distintas a las nuestras. Entre éstas se cuentan la de asegurar territorios y alimentos y, en algunos casos, la de conseguir trabajadores esclavos.  De manera sorprendente, además, las hormigas emplean algunas tácticas de ataque similares a las usadas por nuestros estrategas militares. Según Moffett, por ejemplo, las hormigas merodeadoras que habitan en el sudeste asiático atacan a sus enemigos en grupos compactos que recuerdan a las falanges griegas. Como sabemos, las formaciones militares conocidas como falanges fueron inventadas en las ciudades-estado griegas varios siglos antes de nuestra Era. En la falange los soldados marchaban de manera compacta en formaciones rectangulares con profundidades de decenas de filas. La formación presentaba al enemigo un frente compacto de combatientes armados con espadas y picas de hasta siete metros de longitud apuntando horizontalmente hacia adelante. Además de la protección individual con armaduras, cascos y escudos, los soldados en la falange llevaban cubiertos sus flancos y espaldas por sus mismos compañeros en la formación. Durante el ataque los soldados de la primera fila eran empujados hacia la línea de batalla por sus compañeros que marchaban atrás de ellos.   La formación en falange fue la responsable de que una coalición de ciudades griegas encabezada por Esparta, con sólo 7,000 soldados, haya logrado detener a 250,000 soldados persas en al batalla de las Termópilas en el año 480 antes de nuestra Era. Esta batalla se dio en una región de desfiladeros que se adaptaba especialmente bien  a la formación en falange y en la que los persas tuvieron muchas dificultades para imponer su superioridad numérica.  ,Si bien las hormigas merodeadoras no usan armaduras ni picas en sus incursiones guerreras, sí se lanzan en grupo al ataque, ciegamente y con gran ferocidad, al estilo de los espartanos en la Grecia antigua. Escribe Moffett que, con sus mandíbulas como arma mortal, las diminutas hormigas actuando en grupo son capaces de acabar con animales miles de veces más grandes que ellas y que “en Gabón una vez vi un antílope en una trampa que fue devorado vivo por una colonia de hormigas”. Además, según Moffett, las hormigas merodeadoras tienen una estrategia definida sobre quien va por delante en la batalla. En efecto, sucede que las  hormigas de esta especie pueden diferir grandemente en tamaño entre ellas –hasta unas 500 veces– y son las más pequeñas y débiles –aunque mucho más numerosas– las que están en la primera línea de batalla. Solamente hacia el final del combate es cuando las hormigas más grandes entran en acción. Aunque en este punto las hormigas merodeadores difieran de las costumbres espartanas –cuyas tropas estaban formadas exclusivamente por aquellos que eran considerados ciudadanos de Esparta–, el que los más débiles sirvan como carne de cañón no ha sido una estrategia militar poco frecuente.  Moffett aventura que la vocación guerrera es un efecto del tamaño del grupo de individuos en cuestión. Así, es propia de colonias de 500,000 hormigas y no de grupos pequeños de estos insectos. De la misma manera, se ha dado a lo largo de los últimos miles de años entre grupos humanos numerosos, pero no se habría dado durante la época en la que nuestros antepasados vivían como cazadores-recolectores en grupos pequeños. Apunta igualmente que, en cuanto a impulsos guerreros se refiere, los humanos nos parecemos más a las hormigas que a los chimpancés –nuestros parientes vivos más cercanos– que no viven en grandes grupos. No obstante, y por supuesto, la similitud entre humanos y hormigas tiene un límite. Y en efecto, como sabemos, la batalla de las Termópilas tuvo un fin trágico para el grupo de soldados griegos que se mantuvo firme en su enfrentamiento con los persas. El culpable de la debacle –que abrió las puertas de la región griega a la invasión persa– fue Efialtes, quién traicionó a los espartanos indicando a los persas un camino por el cual pudieron penetrar hasta la retaguardia de aquellos. Así, bajo dos fuegos, los espartanos y sus aliados terminaron por sucumbir.    En contraste con la especie humana, en una colonia de hormigas difícilmente pudiera surgir un traidor que vendiera a sus congéneres. En este punto no hay ninguna duda de que efectivamente existe una diferencia entre especies.",
    "¿Cuál es el automóvil más pequeño del mundo? A raíz de que se dispararon los precios del  petróleo en la década de los años setenta se ha buscado reducir el consumo de gasolina fabricando automóviles más pequeños y haciéndolos al mismo tiempo cada vez más eficientes. Mediante una búsqueda en Internet nos enteramos que están a la venta –o a punto de estarlo– un buen número de automóviles de pequeñas dimensiones fabricados en Alemania, Japón, Francia, Noruega y los Estados Unidos, ente otros países. Se encuentran automóviles –algunos eléctricos y otros de combustión interna– desde los 2.5 metros hasta los 3.3 metros de largo, con pesos  alrededor de los 1000 kilogramos.El automóvil que se lleva el récord, sin embargo, es el modelo Peel P50, fabricado por una empresa británica entre 1962 y 1965 –paradójicamente, en una época en las que no se tenían demasiadas preocupaciones por el ahorro de gasolina–. El Peel P50 es realmente pequeño: mide 1.3 metros de largo, 1 metro de ancho, 1.2 metros de alto y pesa solamente ¡59 kilogramos! Además, sólo puede transportar a una persona –en una posición nada cómoda, por cierto.El Peel P50 tiene ventajas. Por su tamaño podemos, por ejemplo, encontrar fácilmente lugar para estacionarlo. Y si no fuera éste el caso, podríamos levantarlo por uno de sus extremos y –como si se tratara de una maleta con ruedas– llevarlo con nosotros a lugar al que vayamos –para ver estas posibilidades, consulte el video de la BBC al respecto que puede ser encontrado en Internet–. En el lado negativo, un automóvil de las dimensiones del Peel P50 resulta peligroso para su ocupante, que puede resultar fácilmente muerto en un accidente de tráfico –sobre todo en ciudades desordenadas como las nuestras–. Esta, quizá, es la razón por la que no tuvo mucho éxito.Por otro lado, si redefinimos de manera más amplia a un automóvil esencialmente como un vehículo auto propulsado, sin que tenga necesariamente la capacidad de transportar personas, entonces el Peel P50 no es el automóvil más pequeño del mundo. Además, resulta que esta distinción no recae, como podríamos quizá haber esperado, en algún juguete motorizado, sino en un mecanismo sorprendentemente pequeño.En efecto, en un artículo publicado el pasado 10 de noviembre en la revista “Nature”, un grupo de investigadores de Holanda y Suiza publicaron un artículo en el que describen la fabricación de un automóvil –en el sentido amplio definido anteriormente– formado por una sola molécula. Este increíblemente pequeño automóvil, que cuenta con dos ejes y dos ruedas por eje, tiene dimensiones de sólo 4x2 nanómetros –un nanómetro es la mil millonésima parte de un metro–. Es, además, eléctrico, como algunos de sus congéneres de tamaño macroscópico. Para moverse, sin embargo, no usa baterías, sino que funciona con la corriente eléctrica que se le proporciona a través de un alambre metálico terminado en una punta extremadamente afilada. Otra diferencia con sus contrapartes de mayor tamaño es que la corriente eléctrica le debe ser proporcionada de una manera peculiar: por saltos, cada salto produciendo media vuelta en las ruedas. Empleando esta técnica, sus creadores lograron que el nano automóvil avanzara unos 6 nanómetros en línea recta sobre una superficie de cobre.  Hacer que el diminuto automóvil avanzara 6 nanómetros puede ser poco en nuestra escala macroscópica. No lo es, por supuesto, si tomamos en cuenta las extraordinarias dificultades que se presentan para manipular un objeto de tan pequeñas dimensiones y en particular para lograr que las cuatro ruedas se muevan en consonancia a fin de que el automóvil avance en línea recta. El nano automóvil descrito líneas arriba podría quizá parecer una curiosidad científica, si bien extraordinariamente asombrosa. Lejos de esto, y según los expertos, representa un avance importante en el campo de la nanotecnología, demostrando que es posible desarrollar nano motores capaces de inducir un movimiento sobre una superficie y para los cuales se avizoran muchas aplicaciones en el futuro que ahora sólo pertenecen al campo de la ciencia ficción. Así, como escribe uno de los autores del artículo de referencia en una publicación anterior: “El motor molecular es un elemento central en las máquinas moleculares y es concebible que estos motores y máquinas jueguen un papel tan prominente en la revolución nano tecnológica del Siglo XXI como aquel que jugaron sus contrapartes macroscópicas –las máquinas de vapor y de combustión interna– al provocar la revolución industrial del Siglo XIX”.  Es decir, a pesar de que los nano automóviles no sean capaces de transportar pasajeros –del tamaño nuestro– es posible que tengan un impacto futuro considerablemente más grande sobre nuestros descendientes que el que han tenido  los macro automóviles en el tiempo presente.",
    "De acuerdo con la Organización de las Naciones Unidas, durante la semana que acaba de terminar nació el habitante número 7,000,000,000  (siete mil millones) del planeta en donde nos tocó vivir. No es posible, por supuesto, saber quien fue el o la afortunada que llegó al mundo de manera tan llamativa,  pero, de manera simbólica, la ONU ha concedido este honor a Danica May Camacho, nacida en Filipinas en los últimos minutos del pasado 30 de octubre.Al inicio de la era cristiana la población del mundo era de unos 200 millones de personas,  número que se ha venido incrementando paulatinamente hasta nuestros días, excepto en épocas de catástrofes poblacionales como la debida a la epidemia de peste bubónica conocida como la Muerte negra –que en el Siglo XIV habría reducido la población de Europa hasta en un 50 %–. Históricamente, sin embargo, este crecimiento fue relativamente lento, hasta que se aceleró de manera notable durante el siglo XX. En efecto, tenemos que, por ejemplo, entre los años 1700 y 1850 la población del mundo creció de 650 millones a 1300 millones de personas; es decir, tuvo un periodo de duplicación de unos 150 años. Esto contrasta con el correspondiente periodo de sólo 40 años que se observa a partir de la segunda mitad del siglo pasado. Así, mientras que la población del mundo en 1950 era de aproximadamente 2,500 millones de personas, en 1990 había ya rebasado los 5,000 millones, alcanzando a partir de ahí, en poco más de dos décadas, la actual cifra de 7,000 millones.La explosión poblacional del mundo –que mucha alarma genera– es debida al aumento de la esperanza de vida al nacer, la cual se ha más que duplicado en los últimos cien años. Esta última, a su vez, ha sido influenciada por varios factores: la disminución de la mortalidad infantil, una mejora en las condiciones de higiene de la población y en general el desarrollo de procedimientos y medicamentos cada vez más eficientes para el tratamiento de enfermedades. La comprensión que hemos alcanzado sobre los mecanismos por medio de los cuales nos enfermamos ha sido ciertamente clave para aumentar nuestra esperanza de vida –y provocar de paso la explosión poblacional–. Sin entender estos mecanismos no es posible desarrollar métodos para combatir las enfermedades. Durante la epidemia de peste bubónica en el siglo XIV, por ejemplo,  no se entendía en absoluto como se contagiaba la enfermedad, que algunos llegaron a pensar podía adquirirse a través de la vista. Se aventuró también que la epidemia había tenido su origen en la conjunción de Marte, Júpiter y Saturno ocurrida el 20 de marzo de 1345. Sabemos ahora lo disparatado de estas explicaciones y no nos sorprende que los europeos de la época no hayan sido capaces de desarrollar tratamientos efectivos contra la epidemia, como lo atestigua la gran mortandad que provocó.En la actualidad, el conocimiento que se tiene sobre el origen de las enfermedades es cada vez más sofisticado y en consecuencia los tratamientos y medicamentos de que disponemos para combatirlas son cada vez más efectivos. En relación a esto, en un comunicado de prensa del pasado 2 de noviembre, el Consejo Nacional de Investigación de los Estados Unidos dio a conocer un reporte de un comité de expertos formado con el propósito de explorar la factibilidad y necesidad de una “nueva taxonomía de enfermedades humanas basada en la biología molecular”. De acuerdo con dicho reporte, la integración –en una base de datos electrónica– del conocimiento actual y futuro sobre las enfermedades humanas a un nivel molecular, juntamente con la gran cantidad de datos clínicos personalizados de pacientes de los que se dispone actualmente, haría factible el desarrollo de una nueva clasificación –o taxonomía– de enfermedades basada no en síntomas y signos físicos como se hace en la actualidad, sino en los procesos moleculares que originan las enfermedades. Esto resultaría en una clasificación más detallada, la cual distinguiría entre enfermedades que en el esquema actual están agrupadas en una misma categoría. De este modo, empleando la información genética del paciente, lo mismo que los factores medio ambientales a los que ha estado expuesto, el médico  podría realizar diagnósticos más precisos y por consecuencia prescribir tratamientos específicos a la enfermedad que serán más efectivos.   La base de datos sería, además, dinámica, incorporando de manera continua nuevos datos clínicos de pacientes en la medida en que estén disponibles, así como nuevos resultados relevantes de la investigación biomédica.Independientemente de que en un futuro cercano dispongamos o no de una nueva taxonomía para las enfermedades humanas, es claro que el conocimiento médico se hará cada vez más y más sofisticado y nos alejará de manera vertiginosa de aquellas épocas en las que el origen de las enfermedades era un completo misterio. Lo anterior para nuestra fortuna, aunque pudiera agravar la explosión demográfica. Esto último sería, sin embargo, harina de otro costal.",
    "¿Quiénes fueron los primeros pobladores de América? Como sabemos, en comparación con otros continentes, América se pobló hasta fechas relativamente recientes. Comúnmente se asume que los pueblos americanos se originaron en una avanzada que cruzó desde Asia hacia América hace unos 15,000 años por el puente de Beringia –aunque no hay un acuerdo unánime con respecto a la fecha–. Este puente, que se formó entre Alaska y Siberia por  el descenso en el nivel de los océanos, permitió al final de la última glaciación el tránsito de personas entre los dos continentes. Al subir nuevamente este nivel, sin embargo, el puente de Beringia desapareció, aislando a la avanzada que se internó en nuestro continente. De este modo, las culturas americanas evolucionaron de manera independiente, sin contacto con el Viejo Mundo. A lo largo de la última mitad del siglo pasado se consideraba que la cultura Clovis, que floreció por un corto periodo de tiempo hace unos 13,000 años, fue la primera en América. Esta cultura está caracterizada en gran medida por el desarrollo de una tecnología para fabricar puntas de piedra  –conocidas como “puntas Clovis”– con características únicas. Esta puntas fueron utilizadas en implementos de caza y requirieron de una gran habilidad manual por parte de los artesanos que las fabricaron.  La cultura Clovis recibe su nombre del pueblo homónimo en Nuevo México, en donde se encontraron objetos de esta cultura  por primera vez en 1929. Para subsistir, los Colvis se dedicaban a la caza de la megafauna, que incluía mamuts y mastodontes de varias toneladas de peso. Tuvieron tanto éxito en esta actividad que, según algunos expertos, terminaron por extinguir a estos grandes animales. De haber sido así, puesto que la cultura Clovis fue de muy corta duración –unos doscientos años– la extinción de mamuts y mastodontes habría sido muy rápida. En este contexto se habla de una  “blitzkierg” –guerra relámpago– de Clovis contra mamuts y mastodontes.    No todo mundo está de acuerdo, sin embargo en que la megafauna se haya extinguido de manera tan repentina. En efecto, en los últimos años se han encontrado evidencias –cada vez en mayor número– de que los Clovis no fueron en realidad los primeros pobladores del Nuevo Mundo y que habrían existido culturas anteriores que igualmente contribuyeron a acabar con la megafauna. Así, ésta habría tenido un final más lento que el que resultaría de una guerra relámpago. En relación a esto último, en un artículo publicado el pasado 21 de octubre en la revista “Science” por un grupo de investigadores encabezados por Michael Waters de la Texas A&M University en los Estados Unidos, se llega a la conclusión que hubo cazadores de megafauna que antecedieron a los Clovis por cerca de 1,000 años. Estas concusiones están basadas en un estudio de un esqueleto de mastodonte encontrado cerca de la costa del Pacífico en la frontera ente los Estados Unidos y Canadá, el cual muestra una punta de hueso de mastodonte –de un animal diferente– alojada en una de sus costillas y que evidentemente fue producto de un ataque con propósitos de caza. A este esqueleto se le ha asignado una antigüedad de 13,800 años por medio de la técnica del carbono 14, la cual resulta anterior a la cultura Clovis. Los cazadores del mastodonte tendrían que haber pertenecido entonces a una cultura pre-Clovis. Existe actualmente un debate de expertos entre los que defienden la Clovis fue  primera cultura de nuestro continente y aquellos que sostienen la existencia de culturas anteriores –de las que hay evidencias, no solamente en Norte América, sino en algunos lugares de   América del sur, notablemente en Monte Verde, Chile–. Al margen del debate, sin embargo, lo que sí es incontrovertible es que los mamuts y los mastodontes americanos terminaron por extinguirse. En este respecto, y aunque la extinción pudiera haberse debido a causas naturales –tales como un clima cambiante– los Clovis y sus predecesores se apuntan como posibles culpables de haber contribuido a disminuir la biodiversidad del planeta.De haber sido así, sin embargo, no podríamos ser demasiado severos con ellos, pues ciertamente necesitaban cazar para poder sobrevivir. Por el contrario, tendríamos que admirarlos por la tecnología que desarrollaron para fabricar armas que les permitían cazar animales muchas veces mayores a su tamaño.  Después de todo, al haber cosas más urgentes que hacer, a la conservación de la biodiversidad no se le podría haber dado en la época de los Clovis la menor importancia.",
    "El gran desarrollo tecnológico e industrial que experimentó el mundo a lo largo del siglo pasado y lo que va del presente, juntamente con el acelerado crecimiento de la población del planeta –que se incrementó en más de 200 % en los últimos 50 años– ha llevado a una cada vez mayor contaminación a escala global. En este respecto, entre las fuentes importantes de polución contamos a los materiales plásticos, los cuales una vez desechados tienen tiempos de vida largos pues, por su naturaleza artificial, no están sujetos a los mismos procesos de degradación de los materiales naturales. Los materiales plásticos son un producto de la primera mitad del siglo XX. Uno de los más famosos, el nailon, fue creado por la compañía DuPont en 1938. La fabricación de medias femeninas fue una de las primeras aplicaciones que se le dio al nuevo material. Éstas fueron inmediatamente adoptadas y pronto se convirtieron en una necesidad.Al entrar los Estados Unidos en la Segunda Guerra Mundial, sin embargo, DuPont dejó de fabricarlas a fin de dedicar toda su producción de nailon a la fabricación de paracaídas y otros productos militares. Esta situación cambió al finalizar el conflicto, reanudando DuPont la producción de medias para beneficio de las mujeres, aunque no con el volumen suficiente para satisfacer la gran demanda existente. En efecto, en tiendas que las habían puesto a la venta, la disparidad entre demanda y oferta de medias provocó incluso motines y peleas entre mujeres para conseguirlas. Esto ilustra la gran novedad que los materiales sintéticos representaron en su tiempo, novedad que llevó incluso a Fats Waller –el famoso cantante y pianista– a componer la canción “Cuando los nailons florezcan nuevamente”, aludiendo a los tiempos de guerra cuando no se fabricaban las medias de nailon.Las ventajas que ofrecen los plásticos no se limitan, por supuesto, a la fabricación de medias femeninas. Por el contrario, como sabemos, los plásticos han sustituido a otros materiales en un gran número de aplicaciones –basta con echar una ojeada a nuestro alrededor para convencernos–, calculándose en alrededor de 250 millones de toneladas la producción anual de plásticos en el mundo. Esta gran producción, conjuntada con sus grandes tiempos de vida, ha contribuido inevitablemente a la polución del planeta. Un ejemplo de esto es la gran concentración de basura plástica –conocida como la “Gran mancha de basura de Pacífico” – que se localiza en el norte del Océano Pacífico. Esta mancha fue descubierta en 1977 y está formada por desechos plásticos llevados ahí por las corrientes marinas.  Un artículo aparecido en la revista “Environmental Science and Technology”, publicado por un grupo de investigadores de universidades de Irlanda, Australia y el Reino Unido, señalan una nueva fuente de contaminación marina por plásticos: las lavadoras de ropa. Según estos investigadores,  una prenda hecha con telas de poliéster y acrílico desprende hasta unas 2000 fibras de pelusa cada vez que se lava a máquina. Así, al desaguar la lavadora esta pelusa llega al drenaje de la ciudad y de ahí al mar, contribuyendo a incrementar su polución.  Las conclusiones anteriores fueron resultado de un estudio llevado a cabo en 18 playas a lo largo de todo el mundo, incluyendo playas en Australia, Japón, Chile, Mozambique, los Estados Unidos, Portugal y el Reino Unido. El objetivo del mismo fue el de identificar la presencia de fibras plásticas en la arena de la playa y en dado caso determinar su composición y su procedencia.Como resultado, en todas las playas estudiadas encontraron fibras plásticas en números  que iban desde 2 hasta 31 por cada cuarto de litro de arena. Los números más grandes corresponden a playas con una mayor densidad de de población a su alrededor –en Europa, Japón y la costa este de los Estados Unidos, entre otros lugares–. Además, las fibras encontradas fueron fundamentalmente de poliéster y de acrílico, como se esperaría de corresponder éstas a pelusas de ropa. Además, para recabar evidencia adicional, investigaron los depósitos a la salida del drenaje en busca de fibras plásticas y las que encontraron resultaron ser también en gran medida de poliéster y de acrílico.Con toda esta evidencia, los autores del artículo referido concluyen que el mar se está efectivamente contaminando por la pelusa de ropa generada por las lavadoras domésticas. No es claro, sin embargo, cuáles son los efectos que tiene esta contaminación sobre el medio ambiente; en particular si se está afectando a la vida marina.De acuerdo con Mark Browne, quien es el autor líder del artículo referido líneas arriba, esto último es importante averiguarlo. Considera además que, en tanto se aclaren las cosas, el poliéster y el acrílico deben ser considerados presuntos culpables. Mucho han avanzado los materiales plásticos desde su invención y hoy ocupan un lugar preponderante en el mundo –para bien o para mal–. Al mismo tiempo han perdido novedad, al grado que si Fats Waller viviera –murió de pulmonía a bordo de un tren en los Estados Unidos antes de que terminara la guerra– con seguridad no le cantaría al nailon ni a ninguno de sus primos hermanos.",
    "Como sabemos, a mediados del Siglo XIV Europa sufrió una epidemia de peste bubónica conocida como la “Muerte Negra”. La bacteria que la causó llegó al continente europeo a bordo de un barco proveniente del Mar Negro, desembarcando por un puerto del sur de Italia. Una vez en tierra firme, la bacteria rápidamente se expandió hacia el norte, sembrando la muerte a su paso y llegando a Escandinavia en el curso de unos pocos años. Se estima que en la epidemia podría haber muerto hasta a un 50% de la población de Europa. En los siglos que siguieron a la Muerte Negra se han dado de manera recurrente nuevas epidemias de peste bubónica. En 1665, por ejemplo, la Gran Plaga de Londres mató a 100,000 londinenses y provocó la huida de la ciudad de muchos de sus habitantes, incluido el Rey Carlos II de Inglaterra. Las bacterias que han producido las epidemias posteriores a la Muerte Negra, sin embargo, no han tenido la misma virulencia, particularmente las que circulan por el mundo en la actualidad.Dada la mortalidad y rápida diseminación de la Muerte Negra, no es difícil entender que causara un gran terror entre la población, máxime cuando no se conocían ni sus causas ni su vía de trasmisión. Hoy sabemos que la peste bubónica es dispersada por ratas infectadas por la bacteria “Yersinia pestis” y que es trasmitida  a los humanos mediante la picadura de pulgas parásitos de aquellas. En la Edad Media no tenían este conocimiento, en particular, no sabían de la existencia de las bacterias ni de su papel como agentes infecciosos. De este modo, difícilmente hubieran podido descubrir la cadena de trasmisión de la enfermedad que llevaba de las ratas a los humanos. Así, dieron por buenas algunas explicaciones sobre el origen de la epidemia que hoy nos parecen absurdas. Creían, por ejemplo, que la enfermedad se debía a los malos olores en el aire y que podían prevenirla eliminándolos del ambiente. O bien, y esto debió resultar todavía más atemorizante, que podía haber contagio por medio de la vista. Había también explicaciones más sofisticadas, como aquella ofrecida por un grupo de sabios al Rey Felipe VI de Francia según la cual la epidemia tenía su origen en la conjunción de Marte, Júpiter y Saturno ocurrida el 20 de marzo de 1345.Nos es ahora claro que dichas explicaciones sobre el origen de la Muerte Negra están muy lejos de la verdad y de ninguna manera nos sorprende que las medidas que en su tiempo se tomaron para prevenirla o curarla hayan resultado por lo general en un rotundo fracaso.En contraste, en la actualidad sabemos lo suficiente acerca de las enfermedades infecciosas y en particular acerca de la bacteria “Yersinia pestis”, como para que ésta no constituya ya un peligro mayor. El conocimiento que se tiene sobre dicha bacteria, además, se incrementa a medida que avanzan las investigaciones, llevadas a cabo a veces con técnicas que hasta hace pocos años se hubieran antojado casi mágicas. Como un ejemplo de esto, el pasado miércoles 12 de octubre, un grupo internacional de investigadores encabezado por Kristen Bos de la Universidad McMaster en Canadá y por Verena Schuenemann de la Universidad de Tübingen en Alemania, publicaron un artículo en la revista británica “Nature” en el que reportan la reconstrucción del genoma de la bacteria causante de la “Muerte Negra”. El material genético para el estudio de la bacteria –con más de seis siglos de antigüedad– fue obtenido de los dientes de víctimas de la epidemia enterradas en un cementerio de Londres, Inglaterra. Así, fue posible averiguar las intimidades de una bacteria seis siglos después de que provocara una mortandad.Los resultados de la investigación referida muestran que las pestes actuales son provocadas por bacterias originadas en aquellas que provocaron la Muerte Negra, que sería de este modo la madre de todas las pestes. Además, los investigadores  no encuentran grandes diferencias entre las bacterias medievales y las modernas que pudieran explicar la gran virulencia de aquellas en comparación con las actuales. La diferencia en este respecto debería ser entonces debida a causas externas a la bacteria misma, tales como las mejores condiciones de higiene y alimentación actuales, en comparación con las que prevalecían en la Europa medieval.Además del Rey de Inglaterra, otro de los que salieron huyendo de la peste de 1665 en Londres fue Isaac Newton –considerado por muchos el más grande científico que ha existido– quien, al cerrar la Universidad de Cambridge por la epidemia se mudó a Whoolsthorpe, su pueblo natal, en espera de que pasara la contingencia. Ahí pasó dos años altamente fructíferos –según él los más productivos de su vida– en los que desarrolló ideas y teorías que resultaron de gran trascendencia para el desarrollo posterior de la ciencia. Así, en este caso la peste bubónica tuvo una contribución positiva.No obstante, aun viéndola por su lado bueno, es claro que la peste bubónica no es de ninguna manera nuestra amiga y no podemos sino congratularnos de que hoy en día no constituya un peligro mayor.",
    "Fue en 1879 cuando un arqueólogo aficionado descubrió las famosas pinturas de la cueva de Altamira, cerca de Santillana del Mar en el norte de España. Esta cueva consiste de varias cámaras, la más famosa de las cuales –la llamada Gran Sala o Sala de los Polícromos–, muestra en su techo una gran cantidad de imágenes en color de bisontes en diferentes posiciones, venados, jabalíes y caballos, que vivieron en esa parte de Europa hace miles de años. Aunque en un principio hubo controversia entre los expertos acerca de la autenticidad de dichas pinturas –dada su perfección artística–, al despuntar el Siglo XX se aceptó finalmente que tenían un origen prehistórico.  Hoy se sabe que las pinturas de Altamira tienen una antigüedad que ronda los 14,000 años.\t Después de superada la controversia sobre su autenticidad, no pasó mucho tiempo antes de que se organizaran las primeras visitas guiadas a la cueva, las cuales –dado  lo espectacular de las  pinturas que aloja– tuvieron un gran éxito. Éste, sin embargo, a la larga resultó en detrimento de las pinturas mismas. En efecto, en el transcurso de los años el número de personas que anualmente visitaba la cueva aumentó rápidamente, alcanzando 50,000 en 1955 e  incrementándose hasta 175,000 en 1973. Con esto, la cueva de Altamira, aislada del exterior durante 13,000 años por un derrumbe en su entrada, mostró signos de contaminación por bacterias y hongos que pusieron en riesgo la integridad de las pinturas prehistóricas. Así, se dio la voz de alarma y Altamira fue cerrada al público en 1977. En 1982 fue reabierta, pero limitando a 11,000 el número de visitantes por año. Como fue claro que aun con esta reducción continuó el deterioro de las condiciones ambientales de la cueva, Altamira fue cerrada nuevamente en 2002 y así ha permanecido hasta la fecha.  Con el fin de impulsar el turismo local, existen desde 2010 planes de reanudar las visitas públicas a la cueva de Altamira. Un artículo publicado el pasado jueves en la revista “Science” por un grupo de investigadores españoles, sin embargo, lo desaconseja. Esto último debido a que, como resultado de las visitas turísticas masivas, la cueva de Altamira sufre de contaminación microbiana. En relación a esto, los investigadores españoles han identificado cuatro diferentes bacterias, distribuidas a lo largo de la cueva. La mayor parte se alojan en la galería de entrada de las misma, pero algunas han llegado ya hasta la Gran Sala, en donde hay pinturas que muestran manchas verdes por crecimiento bacteriano. Un factor que ha contribuido a contaminar el ambiente en el interior de la cueva –de acuerdo con el artículo de referencia– ha sido el empleo de luz artificial para la iluminación de las pinturas. Esto ha provocado la proliferación de bacterias capaces de llevar a cabo la fotosíntesis –empleando la luz de las lámparas de iluminación para este propósito –, y que son las que han contaminado las pinturas de la Gran Sala.Además de la contaminación por bacterias, se observa en la cueva de Altamira una contaminación por hongos llevados al interior por insectos, o bien hongos que crecen ahí sobre las heces de roedores.  Los investigadores españoles temen que, si bien en estos momentos en que la cueva está cerrada pareciera ser que el problema de contaminación del sitio de Altamira está estabilizado, éste se reactive de reabrirse la cueva al público. En particular, el flujo de personas hacia el interior de la cueva provocaría el ingreso de nutrientes desde el exterior, lo que dispararía el crecimiento bacteriano. Igualmente, el movimiento de aire que provocaría dicho flujo transportaría  bacterias alojadas en las paredes de la galería de entrada de la cueva hacia el interior de la misma.Podría de esta manera reproducirse en Altamira el desastre ocurrido en la cueva de Lascaux, en Francia. Esta cueva es un sitio de arte prehistórico de importancia similar a la de Altamira –de hecho, ambos sitios han sido llamados la “Capilla Sixtina de la prehistoria” –, la cual sufrió una grave contaminación por hongos motivada por un intenso flujo de turistas y que no ha podido ser superada.  Hace 14,000 años no había cámaras fotográficas y eso, desgraciadamente, nos priva del placer de apreciar imágenes –con el detalle que una fotografía nos podría proporcionar– de un pasado remoto muy diferente al tiempo en que nos ha tocado vivir. A falta de fotografías, sin embargo, los artistas de Altamira nos han legado pinturas de sorprendente realismo, las cuales nos permiten echar un vistazo a su mundo. Altamira, además, y quizá esto es lo más importante, guarda secretos acerca de las sociedades prehistóricas que los expertos están todavía por descubrir.Esto último será posible, no obstante, sólo si Altamira logra librarse de aquellos que quieren que siga siendo un negocio. h",
    "¿A qué hora del día está una persona de mejor humor? ¿Hay algún día de la semana o estación del año en el que haya un mayor entusiasmo por ir al trabajo?  Scott Golder y Michael Macy, sociólogos de la Universidad Cornell en los Estados Unidos buscaron respuestas a preguntas como éstas mediante el estudio de millones de mensajes enviados a través de la red social Twitter. Los resultados de dicho estudio fueron publicados el pasado jueves en la revista “Science”. En los últimos años hemos sido testigos de la extremadamente rápida expansión de las llamadas redes sociales. Facebook, por ejemplo, alcanzó este mes los 800 millones de usuarios registrados; es decir, más del 10 % de la población del mundo. Dado su carácter masivo, las redes sociales constituyen para los sociólogos un medio de valor inestimable para acceder a millones de personas de diferentes culturas a lo largo y ancho del mundo. Twitter, en particular, permite a sus usuarios el envío de textos cortos de no más de 140 caracteres en los que se incluye de manera implícita información personal del remitente. Durante su estudio, Golder y Macy analizaron 500 millones de estos textos –enviados por 2.5 millones de personas en 84 países a lo largo de dos años– en busca de palabras que reflejaran su estado de ánimo. Dado que se conoce el momento preciso en que los mensajes fueron enviados, fue posible determinar el cambio de estado de ánimo colectivo  a lo largo del día, de una semana e incluso a lo largo de todo un año. Para tal propósito, los investigadores emplearon un programa de computadora que analiza textos y busca palabras que revelen emociones positivas, tales  como entusiasmo o placer. Para cuantificar el estado de ánimo de una persona en particular en un  momento dado, dividieron el número de palabras de este tipo que aparecieron en su mensaje por el número total de palabras contenidas en el mismo.Los resultados que arrojó el estudio son a la vez interesantes y esperables. Se encuentra, por ejemplo, que los días de mejor ánimo son los sábados y domingos, que son precisamente los días en los que no se trabaja, lo que, por supuesto, no resulta     sorprendente. En contraste –lo que no resulta tampoco sorprendente–, según Golder y Macy el estado de ánimo positivo baja sensiblemente el lunes –al regresar a trabajar– y lo hace todavía más el martes, que es el peor día de la semana en este respecto, posiblemente por la lejanía del sábado. A partir del martes y ante la perspectiva de un fin de semana cada vez más cercano, el estado de ánimo positivo sube gradualmente.En referencia a nuestro país y de acuerdo a la costumbre –en algunos casos– de hacer “san lunes”, en primera instancia hubiéramos esperado que el día menos favorable para el trabajo fuera precisamente el lunes y no el martes como encuentran Golder y Macy. Es posible, sin embargo, que el factor determinante en México para no asistir al trabajo al iniciar la semana no sea un pobre estado emotivo sino alguna otra  indisposición resultado de los días de descanso.Golder y Macy encuentran, además, que el estado de ánimo positivo varía a lo largo del día. Así, éste es alto durante las primeras horas de la mañana, baja gradualmente conforme avanza el día alcanzando su mínimo durante la tarde, y vuelve a ser alto en horas de la noche. Se encontró también que, en comparación con los días de trabajo, durante sábados y domingos el máximo de estado de ánimo positivo durante la mañana ocurre unos dos horas más tarde, como consecuencia de que en los fines de semana se alarga el periodo de sueño. Las variaciones de estado de ánimo a lo largo del día ocurren tanto en los días de trabajo como en sábados y domingos, lo que indica que no son debidos a presiones laborales, y por el contrario siguen al ritmo circadiano y al periodo de sueño. La obligación de ir al trabajo, en cambio, si sería la causa de las variaciones de ánimo observados durante la semana. Esto se confirma por el hecho de que en los Emiratos Árabes Unidos, en donde la semana de trabajo empieza el domingo y termina el jueves, los días con estados de ánimo altos son viernes y sábado, en lugar de sábado y domingo. Golder y Macy muestran, además, que estamos ante un fenómeno universal: datos para cuatro regiones del mundo, Estados Unidos-Canadá, Gran Bretaña-Australia, India y el África de habla inglesa, indican que sus habitantes experimentan cambios similares de estado de ánimo a lo largo de la semana.De acuerdo a lo anterior, México, en donde tenemos a lo largo del año varios periodos vacacionales y “puentes”, incluyendo el ya tradicional “Guadalupe-Reyes”, podría ser uno de los países más felices del mundo.Esto posiblemente sería cierto si no fuera porque el trabajo resulta un ingrediente indispensable si hemos de generar riqueza y alcanzar un mejor nivel de vida.",
    "En la última semana los medios de comunicación han dedicado una considerable atención al  regreso a la Tierra del satélite de investigación atmosférica UARS de la NASA, después de 20 años de permanencia en el espacio. La NASA confirmó el reingreso del satélite UARS –siglas en inglés de Satélite de Investigación de la Atmósfera Superior– de seis toneladas de peso, en un punto no determinado del pacífico norte durante la madrugada del sábado pasado. De acuerdo a informaciones no oficiales, habría caído cerca de Calgary, Canadá.No se esperaba que el satélite de seis toneladas, del tamaño de un autobús, cayera tal cual a la superficie de la Tierra; por el contrario la NASA predijo que la mayor parte de la masa del satélite se desintegraría por el roce con la atmósfera. En la caída, no obstante, las partes del satélite más resistentes a la fricción generarían 26 fragmentos con un peso total de media tonelada –uno de ellos de unos 150 kilos– que alcanzarían la superficie de la Tierra a gran velocidad. Así, la perspectiva de ser alcanzado de manera repentina por un proyectil venido del cielo,  provocó de manera natural cierto nivel de alarma y fue objeto del interés de los medios.  De acuerdo con la NASA, sin embargo, la probabilidad de que alguien resultara herido por un fragmento del UARS era muy pequeña. Según sus cálculos,  dicha probabilidad era de apenas 1 en 3,200 –que es más o menos la probabilidad de sacarse el premio mayor de la Lotería Nacional comprando trece series de billetes–. Además, dividiendo esta probabilidad por el número de personas que viven en el mundo –unos siete mil millones– la probabilidad de que una persona en particular sea alcanzada por un fragmento del UARS resulta ser insignificante.Hace más de tres décadas el reingreso a la Tierra del satélite Skylab de 75 toneladas atrajo también una gran atención de los medios. El Skylab fue la primera estación espacial norteamericana. Fue puesta en órbita en 1973 y reingresó a la Tierra seis años después. Los restos de la estación espacial cayeron cerca del pueblo de Esperance, en una apartada región en el occidente de Australia. Por cierto, para disgusto de los australianos, en esa ocasión la NASA hizo declaraciones desafortunadas asegurando que los únicos que habían sido puestos en peligro por los restos del Skylab fueron los canguros.Desde el año 1957, cuando la entonces Unión Soviética lanzó al espacio el satélite Sputnik 1, se han puesto en órbita alrededor de 6,000 satélites artificiales de los cuales unos 900 están actualmente en operación. Una buena parte de éstos circulan  a la Tierra en órbitas cuya altura se mide en cientos de kilómetros. Aunque a tales alturas la atmósfera está muy enrarecida y la resistencia que el aire opone a los satélites es muy pequeña, ésta no es cero, de modo que los satélites se ven frenados de manera continua. Esto obliga a estar permanentemente recolocándolos en órbita, ya que de otro modo de manera inevitable regresarían a la superficie de la Tierra. Durante el tiempo que permanecen en órbita, por otro lado, los satélites están expuestos a sufrir choques catastróficos entre ellos –como ocurrió en febrero de 2009, cuando colisionaron el Cosmos-2251 ruso y el Iridium 33 norteamericano a una velocidad superior a los 40,000 kilómetros por hora– o más comúnmente al chocar con fragmentos de basura espacial. Esta basura, que viaja a velocidades extremadamente altas, es producto de la destrucción de otros satélites o bien de material que ha sido dejado en órbita por alguna razón a lo largo de la historia espacial. Al chocar dos objetos en el espacio se multiplican los fragmentos en órbita y de esta manera se incrementa la probabilidad de que ocurran más choques. Se produce de esta manera una reacción en cadena que, según los expertos, podría elevar el número de fragmentos de basura hasta niveles que impidieran futuras misiones espaciales. Para evitar choques entre satélites y no contribuir al incremento de la basura espacial, es necesario entonces hacerlos descender una vez que  hayan superado su vida útil. Así, el satélite UARS al ser dado de baja en 2005, fue transferido a una órbita más cercana a la superficie de la Tierra, con lo que se incrementó su frenado por el rozamiento con el aire y se precipitó de este modo su caída.Al regresar el UARS a la Tierra se evitó una posible colisión en el espacio con la consecuente formación de más basura espacial. Al mismo tiempo, sin embargo, se posibilitó la colisión de los fragmentos del satélite con alguno de los habitantes del planeta Tierra, aunque de aceptar los cálculos de la NASA, la probabilidad de que esto ocurra es pequeña. Hay que recordar, no obstante, que durante la caída del Skylab en Australia un adolescente de 17 años recogió del patio de su casa en Esperance un fragmento de la estación espacial, mismo que se apresuró a presentar al periódico San Francisco Examiner a fin de reclamar el premio de 10,000 dólares que dicho periódico había ofrecido a aquel que llevara un pedazo del Skylab en un plazo de 48 horas.",
    "Los restos fósiles de animales que vivieron en nuestro planeta en tiempos prehistóricos nos permiten echar una ojeada a la vida en la Tierra  tal como era hace millones, decenas de millones o incluso centenas de millones de años. Desgraciadamente, esta ojeada no nos proporciona toda la información que quisiéramos, pues los restos que se encuentran por lo general son solamente de los huesos y partes duras del animal en cuestión, ya que sus tejidos blandos resisten poco el paso del tiempo. De este modo si bien los paleontólogos saben de la existencia en nuestro planeta de animales extintos hace ya mucho tiempo, no tienen una tarea fácil tratando de reconstruir la apariencia física que tenían en vida.Una anécdota al respecto se dio en las primeras décadas del Siglo XIX. En 1822, el médico y paleontólogo inglés Gideon Mantell encontró en Cuckfield, Inglaterra, un colmillo enorme que estimó perteneció a un animal desconocido de unos 18 metros de largo. El descubrimiento posterior del esqueleto de un animal similar dejó en claro que se trataba de una especie extinta: algo así como una iguana gigante que Mantell bautizó con el nombre de Iguanodon. En la reconstrucción del Iguanodon –que pudo ser vista en la Gran Exposición de Londres de 1851– Mantell colocó uno de los huesos encontrados –que no hallaba donde fijar– en la nariz del modelo, como si fuera un cuerno de rinoceronte. Hoy se sabemos que Mantell se equivocó y que dicho hueso era en realidad un dedo pulgar, que era empleado por el Iguanodón posiblemente como espolón defensivo.Para solaz de los paleontólogos, sabemos que se han encontrado atrapados en ámbar y preservados por decenas de millones de años, insectos e incluso pequeñas lagartijas y ranas –hecho que fue la inspiración de películas hollywoodenses de los años noventa–. El ámbar preserva los detalles del espécimen atrapado de una manera más delicada que la fosilización por otras vías, y proporciona una imagen más cercana de la especie tal como  era en su momento. En relación a esto último, el pasado 16 de septiembre un grupo de investigadores de la Universidad de Alberta en Canadá, encabezados por Ryn McKellar,  publicaron un artículo en la revista “Science” en la que describen los resultados de un estudio llevado a cabo con muestras de plumas de animales prehistóricos preservadas en ámbar. La preservación de las plumas resultó tan delicada que es incluso posible adivinar el color que tuvieron en su momento así como su estructura microscópica. Los especímenes, procedentes de la localidad de “Grass Lake” en Alberta, pertenecen al Museo Real de Paleontología de la Universidad de Alberta y tienen una antigüedad de unos 80 millones de años. Los investigadores encontraron una variedad de tipos en las plumas estudiadas. Algunas son parecidas a las de aves voladoras y acuáticas actuales, y consideran que pertenecieron a aves primitivas. Otras consisten de filamentos simples o de matas de filamentos conocidos como protoplumas de 2 centímetros de longitud, las cuales se sabe que corresponden a un estado primitivo de evolución de las plumas de las aves modernas. Es interesante mencionar que para las protoplumas no existe un equivalente actual.Aunque no se han encontrado huesos de dinosaurios directamente asociados con las muestras de ámbar, McKellar y colaboradores adelantan de manera hipotética que las protoplumas pertenecieron  a dinosaurios de pequeño tamaño. Además, de acuerdo con los mismos, éstas no estaban relacionadas con el vuelo, sino que tenían la función de mantener caliente al dinosaurio.La paleontología es una ciencia que depende de los hallazgos de fósiles de animales o plantas fallecidos en tiempos remotos. El que de manera azarosa se preserve un organismo muerto a través de decenas o cientos de millones de años de tal manera que pueda ser estudiado por los paleontólogos es, por supuesto, extremadamente infrecuente.  Y, sin embargo, la paleontología nos ha proporcionado un panorama con un gran número de detalles acerca de la historia de la vida en la Tierra, panorama que se expande día con día. Ahora sabemos con cierta seguridad, por ejemplo, que no todos los dinosaurios se extinguieron hace 65 millones años cuando colisionó en la península de Yucatán un meteorito con la Tierra, sino que algunos sobrevivieron y que las aves modernas son sus descendientes. Sabemos también que, al igual que estas últimas, algunos dinosaurios lucían un plumaje de colores.Con seguridad a muchos el panorama del pasado de la vida en nuestro planeta, tal como nos lo presenta la paleontología, les resulta fascinante. Aunque a veces los paleontólogos se equivoquen y coloquen huesos en el lugar equivocado.",
    "Charles Darwin defendió la hipótesis según la cual nuestra especie humana y los grandes simios comparten un antepasado remoto. Darwin asumió, además, que dicho antepasado vivió en África, pues es en este continente en donde habitan chimpancés y gorilas, que serían nuestros parientes vivos más cercanos en la evolución. Dados los prejuicios de la época, no es sorprendente que estas ideas hayan sufrido un fuerte rechazo, pues implicaban que los humanos no ocupamos, después de todo, un lugar especial en el Reino Animal –chocaba, además, la afirmación que nuestra especie se hubiera originado en África, que no concordaba con ideas racistas muy extendidas en Europa, aun hoy en día.No mucho tiempo después, no obstante, las especulaciones de Darwin empezaron a recibir sustento. Así, en 1924 el antropólogo Raymond Dart de la Universidad Witerwatersrand en Sudáfirca descubrió en una cantera en el noroeste de este país el cráneo fósil de un infante de unos 3-4 años de edad y 2.8 millones de años de antigüedad, el cual presentaba características intermedias entre simios y humanos, incluyendo la de caminar erguido en dos pies. Este fósil, sin embargo, no fue reconocido como intermedio por la comunidad científica de manera inmediata.En 1936, el paleontólogo Robert Broom encontró en Sterkfontein, Sudáfrica, los restos fósiles de un adulto de antigüedad equivalente a los descubiertos por Dart. A éste siguieron otros descubrimientos realizados por el mismo Broom, que llevaron finalmente al reconocimiento amplio de los fósiles sudafricanos como formas intermedias en el camino evolutivo hacia los humanos modernos.Broom era médico de profesión y paleontólogo por afición. En 1903  perdió su puesto como profesor de la Universidad  Stellenboch, en Sudáfrica, por sostener la teoría de la evolución. Esto lo orilló a practicar su profesión como médico para sobrevivir, dedicando  su tiempo libre a la paleontología. El episodio nos da una idea del clima prevaleciente en la época con respecto a estos temas. Hoy en día se han descubierto numerosos fósiles que muestran la línea evolutiva que ha dado origen a nuestra especie “Homo sapiens”, incluyendo a Lucy, el famoso esqueleto de 3.2 millones de años de antigüedad descubierto en Etiopía en 1974, así como a las huellas de creaturas bípedas de 3.5 millones de años de antigüedad que, preservadas en ceniza volcánica endurecida, fueron encontradas en Tanzania en 1978.La semana que acaba de terminar nos trajo la noticia del más reciente descubrimiento de fósiles pre humanos, el cual fue llevado a cabo por un grupo de investigadores encabezados por Lee Berger de la  Universidad Witerwatersrand. Dicho descubrimiento está descrito en una serie de artículos publicados por la revista “Science” el pasado jueves 8 de septiembre. En tales artículos se describen dos esqueletos fósiles incompletos descubiertos hace tres años en Malapa, Sudáfrica, no lejos de Sterkfontein en donde Robert Broom encontró sus primeros fósiles. Se ha determinado que los esqueletos de Malapa tienen una antigüedad de alrededor de 2 millones de años. Las creaturas a las cuales pertenecieron  –un niño de alrededor de 12 años y una hembra adulta– tenían un cerebro pequeño, brazos largos y un cuerpo de chimpancé. Podían, además, flexionar las manos al igual que los simios, lo que los habría habilitado para trepar árboles. Al mismo tiempo, sin embargo, tenían características humanas, incluyendo dedos cortos y un pulgar alargado que les permitía manipular objetos, tal como lo hacemos nosotros. Igualmente, aunque su cerebro era solamente un poco más grande que el de un chimpancé, aparentemente tenía una organización interna que anticipaba a la humana. Dado que la antigüedad de los fósiles corresponde al momento en que se piensa apareció el género humano, Berger especula –aunque no lo asegura– que podría tratarse del antecesor directo de dicho género.Los fósiles de Malapa, con su mezcla de rasgos humanos y pre humanos, resultan sin duda fascinantes y los expertos no dudan que eventualmente arrojarán luz sobre el proceso evolutivo de género humano y de nuestra propia especie, aunque por lo pronto tal parece que han generado más preguntas que respuestas. En relación a esto último, no es claro, por ejemplo, si son realmente un antecesor directo del género humano o si corresponden a una línea evolutiva que finalmente se extinguió.En todo caso, buscar y encontrar una respuesta a ésta u otras preguntas no llevará con seguridad a que alguien pierda su trabajo como resultado de prejuicios absurdos.Por cierto, Robert Broom, después de una fructífera y larga existencia, murió el 6 de abril de 1951 a la edad de 85 años, Murió momentos después de finalizar un manuscrito sobre paleontología. Habría exclamado: “Ahora esto está terminado…y yo también”.",
    "Como nos lo enseñaron en la escuela, calcular el área de una superficie cuadrada o rectangular es fácil: solamente hay que multiplicar su ancho por su largo. Hacerlo con una superficie irregular, en contraste, es considerablemente más difícil –tratemos, por ejemplo, de calcular el área de una superficie limitada por cuatro lados, todos de diferente longitud.  Sin embargo, dado que la medición de una superficie tiene una gran importancia práctica –para conocer la extensión de una propiedad agrícola, por ejemplo–, los métodos para hacerlo se desarrollaron hace ya un buen número de años. Los sumerios y los egipcios, por ejemplo, sabían cómo medir superficies en épocas que se remontan a varios miles de años en el pasado. En el caso de Egipto, después de una inundación del Río Nilo –que ocurría de manera periódica–, había que delimitar nuevamente las parcelas agrícolas.     En este lado del Atlántico, sabemos que los mayas tenían un conocimiento matemático avanzado que, entre otras cosas, incluía el uso del cero. Hace algunos años, además, se descubrió que los aztecas eran igualmente competentes en el tema. En referencia a esto último, en días pasados, apareció un artículo publicado en la revista norteamericana “Proceedings of the National Academy of Sciences”, en el que investigadores de la UNAM y de la Universidad de Wisconsin reportan los resultados de un estudio llevado a cabo con el “Códice Vergara”. Este códice –alojado en la Biblioteca Nacional de Francia– fue elaborado por expertos indígenas en 1543-1544; es decir, durante los primeros años del periodo colonial. En el mismo se incluye un censo de predios agrícolas en el pueblo de Tepetlaoxtoc, cerca de Texcoco en el Estado de México. Como sabemos, el reino de Texcoco integró la triple alianza juntamente con Tenochtitlán y Tlacopan como parte del Imperio Azteca. Dado que en el manuscrito se proporcionan tanto la superficie de cada predio como las dimensiones de sus colindancias, los investigadores se dieron a la tarea de intentar adivinar los métodos utilizados para calcular sus superficies. El estudio se hizo con 386 predios, de los cuales 90 tenían una superficie cuadrada y 32 una superficie rectangular. El resto de los predios estudiados, un total de 264, tenían una superficie irregular, aunque todos con cuatro colindancias. En el caso de los predios cuadrados y rectangulares, las superficies consignadas en el códice coinciden en cada caso de manera exacta con el producto largo por ancho correspondiente, lo que no deja duda del procedimiento que se siguió para calcularas. Como comentan los autores del artículo de referencia, esto demuestra que las personas que elaboraron los cálculos –que se asume hipotéticamente eran indígenas–  tenían una idea clara del concepto abstracto de área.Para calcular la superficie de un cuadrilátero irregular es necesario emplear métodos más complicados. Una técnica aproximada –usada por los sumerios– consiste en multiplicar los promedios de cada uno de los dos pares de lados opuestos. En este caso, además de la operación de multiplicación, se requiere de las operaciones de suma y división. Existen adicionalmente otras técnicas que también requieren de  varias operaciones aritméticas. Se encontró que, aunque no es posible determinar cuál fue el método particular empleado en  cada caso por los autores del Código Vergara para calcular la superficie del predio, los valores consignados en dicho código –área y largo de colindancias– tienen sentido en el  86% de los casos. Además, la imprecisión en las áreas calculadas sería menor al 10%. Esto, de acuerdo a los autores del artículo, sugiere que dichos valores fueron calculados empleando un método que requirió la realización de varias operaciones matemáticas.Así, los aztecas habrían contado con técnicas de agrimensura que no se quedaban atrás en comparación con las practicadas en Europa, no solamente en el Siglo XVI, sino incluso cientos de años después.    Y sin embargo, según los autores del artículo referido, los conquistadores españoles no aprovecharon la experiencia indígena e introdujeron de la península a la Nueva España un sistema caótico de agrimensura que habría hecho decir al primer Virrey Antonio de Mendoza en 1535 que “en esta ciudad no existe una manera de medir la tierra”.",
    "La educación es ciertamente uno de los bienes más valiosos que una persona pueda poseer y que determina en buena medida su calidad de vida. Se piensa, por otro lado, que la educación que un niño recibe en sus primeros años de vida es fundamental para su rendimiento escolar posterior. La pregunta sobre la edad óptima para que un infante inicie su educación formal es entonces de la mayor importancia. Como resultado de estudios científicos llevados a cabo en las últimas décadas, algunos expertos piensan que la educación formal de los niños debe empezar en el nivel preescolar. En relación a esto, un estudio, publicado el pasado 15 de julio en la revista “Science” por un grupo de investigadores de las universidades de Minnesota y de Misuri en los Estados Unidos encabezados por el psicólogo Arthur Reynolds, muestra que la educación preescolar –impartida a partir de los tres años de edad– puede influir en la calidad de vida adulta en aspectos tales como nivel salarial, consumo de drogas y problemas con la justicia.Dicho estudio fue realizado en la ciudad de Chicago, Illinois,  con un total de 1,539 niños de familias de bajos recursos –mayormente de raza negra–. De éstos, un primer grupo de 989 niños participó en un programa de educación para infantes –Child Parent Center Education Program–, iniciando a los tres años de edad. Dicho programa busca “interactuar con niños y sus padres a una edad temprana a fin de desarrollarles habilidades de lenguaje y autoconfianza, y demostrar que, si se les da la oportunidad, estos niños pueden cubrir de manera exitosa todas las demandas de la actual sociedad tecnológica y urbana”. El programa enfatiza habilidades en lenguaje y en matemáticas y fue impartido por profesores con grado de nivel licenciatura y certificados en educación temprana de niños.Un segundo grupo de 550 niños del total de 1,539 –que sirvió como referencia– asistió a la escuela solamente a partir del jardín de niños; algunos lo hicieron a tiempo completo y otros a medio tiempo. El objetivo del estudio fue el de averiguar si existía una diferencia en la calidad de vida entre los dos grupos de  participantes al alcanzar una edad de 28 años. Se encontró que éste es el caso. Resultó, por ejemplo, que aquellos niños que gozaron de una educación formal desde los tres años tuvieron un mejor desempeño académico y se graduaron de la escuela preparatoria en un mayor porcentaje, en comparación con aquellos niños que iniciaron su educación en el nivel de jardín de niños. Igualmente, se encontró que al llegar a los 28 años de edad los participantes del primer grupo gozaban de un mejor salario y que un mayor porcentaje de éstos tenía un seguro médico. Una educación más temprana llevó también a un menor consumo de drogas y a menos problemas con la justicia.No obstante, de acuerdo con Jeffrey Mervis –editor de noticias de “Science”–, si bien los resultados del estudio referido demuestran sin duda el efecto positivo de la educación temprana en el grupo particular de niños estudiados, no es claro que dichos resultados puedan ser extrapolados a otros grupos poblacionales –considera, en particular a las poblaciones blanca y latina–. Dada esta incertidumbre, resulta complicado implementar el programa en gran escala en los Estados Unidos por su alto costo, que alcanza varios miles de dólares por alumno. El estudio de Reynolds y colaboradores podría ser relevante para nuestro país, que cuenta con un gran porcentaje de población marginada. En México la educación preescolar es obligatoria. Sabemos, no obstante, que la educación primaria y secundaria tiene graves deficiencias –como lo indica el papel poco destacado que han tenido nuestros estudiantes de nivel secundario en la prueba PISA  de la Organización para la Cooperación y el Desarrollo Económico que cubre tres  áreas de competencia: lectura, matemáticas y ciencias naturales–, de modo que resulta incierto predecir la magnitud del beneficio que la educación preescolar aportará a nuestros niños, y si ésta se acercará al obtenido por Reynolds y colaboradores en condiciones más controladas. En cualquier caso, ahora sabemos que, dadas las condiciones adecuadas, la población marginada en México podría quizá alcanzar mejores niveles educativos y calidad de vida mediante la educación temprana.",
    "De acuerdo con la leyenda, el rey Hierón II de Siracusa solicitó a Arquímedes que averiguara si la corona que recientemente había adquirido era realmente de oro como se suponía. El rey sospechaba que el artesano que la fabricó lo había engañado y que la corona era parcialmente de plata.Arquímedes habría imaginado una solución al problema mientras tomaba un baño, al observar que el agua en la bañera subía de nivel –desplazada por su cuerpo– en la medida en que se sumergía. Entendió de manera repentina que para determinar la composición química de la corona bastaba con sumergirla en un recipiente con agua y medir el incremento del nivel que producía; es decir, puesto que la plata es menos densa que el oro, de sumergirla en agua desplazará un volumen de líquido mayor que un peso equivalente de oro.Si bien es improbable que las cosas se hubieran dado tal como nos las describe la leyenda –basada en un escrito, varios siglos posterior al hecho, del arquitecto romano Vitruvius–, la anécdota nos ilustra una experiencia humana frecuente: la concepción de ideas o procedimientos para resolver un determinado problema como resultado de una “chispa de inspiración”. Arquímedes era especialmente eficiente en este respecto, habiendo creado numerosos dispositivos e ingenios mecánicos.Durante mucho tiempo consideramos que la capacidad de concebir soluciones creativas era exclusiva de los humanos. Ahora, sin embargo, sabemos que  es compartida en algún grado por ciertos animales. Es conocido, por ejemplo, que los chimpancés cazan termitas como alimento y para este propósito, localizan un termitero y extraen los insectos introduciendo una vara delgada –especialmente preparada– a través de un orificio en las paredes del mismo. Incluso, de ser necesario, perforan previamente un orificio en la pared del termitero con una vara más gruesa. Esta “tecnología”, que es trasmitida de padres a hijos, fue seguramente ideada por algún antepasado chimpancé en un arranque de inspiración,Los elefantes poseen igualmente una gran inteligencia. En un artículo publicado hace unos días en la revista en línea PLoS ONE por investigadores de la Universidad de la Ciudad de Nueva York y del Parque Zoológico Smithsoniano, se reportan los resultados de un estudio realizado con tres elefantes asiáticos de 61, 33 y 7 años de edad. El estudio demostró que el elefante más joven, de nombre Kandula, confrontado con el problema de alcanzar comida fuera de su alcance, fue capaz de discurrir soluciones creativas.En un primer experimento, en el patio donde habitaba Kandula se colocaron diversas frutas de las que acostumbraba comer. Las frutas fueron suspendidas de un cable a una altura más allá de lo que Kandula podía alcanzar. Adicionalmente, se colocó en el suelo cerca del sitio en donde se encontraba la comida, un cubo sobre el cual el elefante podía subir las patas delanteras para alcanzar las frutas. Aunque desde el inicio del experimento Kandula se interesó en la comida, no hizo el intento de usar el cubo para alcanzarla en sus primeras oportunidades. En algún momento, sin embargo, se le “prendió el foco” y comprendió la utilidad del cubo.A partir de este punto, el elefante siempre usó el cubo para alcanzar la fruta, no importando que lo hubiesen colocado –o incluso escondido– lejos de donde se encontraba aquella. Cuando fue necesario incluso arrastró al cubo con la trompa hasta colocarlo en la posición adecuada. Además, cuando en un experimento diferente el cubo fue sustituido por una llanta de tractor, Kandula la usó de manera similar al primero. En otro experimento en el que el cubo y la llanta fueron retirados del patio, el elefante usó en su empeño una bola del tamaño aproximado a la del cubo que estaba a su alcance. Esto a pesar de que con las patas arriba de la bola estaba en una situación inestable y en peligro de caer.  Dado que es evidente que no existe una división tan tajante entre humanos y algunos animales como en algún momento lo consideramos, éstos últimos son ahora motivo de algunas consideraciones de parte nuestra. Así, por ejemplo, los experimentos con elefantes citado líneas arriba tuvieron que ser aprobados por el Comité para el Cuidado y Uso de los Animales de la Universidad de la Ciudad de Nueva York, así como por el Instituto Smithsoniano.En nuestro país, hace algunos días nos enteramos por la prensa que se presentará en fecha próxima a la Asamblea Legislativa del Distrito Federal una iniciativa de ley para penalizar el maltrato a los animales hasta con seis años de cárcel. De aprobarse, la ley terminaría en el Distrito Federal con actividades tales como las corridas de toros, en las que se maltratan animales en un grado que no admite comparación con el hipotético maltrato a los elefantes en los experimentos relatados aquí. Lo más probable, sin embargo, es que aun en caso de aprobarse la ley quede en letra muerta, como frecuentemente acontece en México.Al menos hasta que los animales nos den mayores pruebas de sus capacidades intelectuales.",
    "El pasado martes 9 de agosto se conmemoró el 66 aniversario del bombardeo atómico de Nagasaki al final de la Segunda Guerra Mundial –precedido tres días antes por un ataque similar a Hiroshima–. Dependiendo de la fuente, se han estimado hasta en 250,000 las víctimas nucleares de Hiroshima y Nagasaki, que murieron en forma instantánea al momento de la explosión o a lo largo de los meses subsecuentes. Sabemos, además, de la suerte de los “Hibakusha”—sobrevivientes de los bombardeos atómicos— aquejados de un número de enfermedades asociadas a su exposición a las radiaciones nucleares.   Dada esta terrible experiencia, uno hubiera quizá esperado que  el Japón fuera el último país en el mundo interesado en la tecnología nuclear; y no solamente para uso militar, sino también para aplicaciones civiles tales como la generación comercial de energía eléctrica. No obstante, presionados por su escasez de recursos energéticos, los japoneses han construido 54 reactores nucleares en su territorio de los que obtienen, en números redondos, el 30% de la energía eléctrica que consumen. Japón es, de este modo, un país que irónicamente depende en buena medida de la energía del átomo.    El accidente de los  reactores de Fukushima ocasionado por el temblor y subsecuente tsunami que asolaron la costa norte del Japón el pasado mes de marzo, sin embargo, han cambiado el estado de ánimo de los japoneses en relación a la energía nuclear. Una encuesta telefónica llevada a cabo por la agencia Kyodo news el pasado mes de julio, señala que un 70% de japoneses apoyan la idea de un país sin energía nuclear. Un “Hibakusha” sobreviviente de Nagasaki, citado por The New York Times expresó: “Me hubiera gustado haber tenido el coraje para hablar antes en contra de la energía nuclear”.  Por su parte, en la ceremonia de conmemoración del bombardeo de Hiroshima el pasado 6 de agosto, el primer ministro japonés afirmó: “La política energética del país está siendo revisada de manera fundamental, después de una reflexión profunda del mito que la energía nuclear es segura”. Las reacciones nucleares liberan cantidades extraordinarias de energía, decenas de millones de veces más grandes que las generadas por la combustión ordinaria. Esto hace que la temperatura que se alcanza en el centro de una explosión nuclear sea de decenas de millones de grados centígrados –en comparación con los cientos de grados centígrados generados por la combustión del gas de la estufa–. Así, los cuidados que ponemos al manejar sustancias explosivas a fin de prevenir accidentes, deben ser llevados a un grado extremo en el caso de la energía nuclear si no queremos originar un desastre. En la práctica, desgraciadamente, como lo prueba Fukushima -y con anterioridad los accidentes de “Three Mile Island” en los Estados Unidos y  de Chernobyl en Ucrania–, los accidentes en las plantas nucleares eventualmente ocurren por error humano o por deficiencias en los procedimientos de seguridad. Además, la energía nuclear genera desechos radiactivos de alta peligrosidad que necesitan ser confinados de manera segura. Si bien esto puede en principio llevarse a cabo,  en la práctica no siempre ocurre.  Así, uno de los factores que agravaron la crisis de Fukushima fue que el combustible usado –que era almacenado en el edificio de uno de los reactores siniestrados– quedó expuesto al aire liberando grandes cantidades de radiación. El fuego ordinario, con temperaturas de cientos de grados –mucho mayores que nuestra temperatura corporal–, es por supuesto incompatible con la vida. Sin embargo,  hemos hecho uso del fuego ya por cientos de miles de años y lo tenemos en gran medida domesticado; aunque a veces ciertamente se nos sale de control y de ahí la frase “jugando con fuego”.En contraste, el fuego nuclear, millones de veces más intenso que una llama ordinaria y, en consecuencia, millones de veces más mortífero, nos está resultando muy difícil de domesticar. Quizá sea solamente cosa de darnos más tiempo, pues solamente han transcurrido siete décadas desde que lo empezamos a usar –tanto para  bien como para  mal. No obstante, en tanto progresamos en nuestro intento de domesticar al fuego nuclear, tendríamos quizá que aprender a usar otras fuentes de energía más amigables, tanto con nosotros como con el medio ambiente. El Sol, que es de hecho la fuente de la vida, es sin duda la opción más natural. De otro modo seguiríamos jugando con un fuego muy peligroso.",
    "El novelista japonés Soseki Natsume publicó en 1905 la novela “Yo soy un gato”, en la que el protagonista, un gato que gozaba de poco aprecio entre los humanos –aun de parte de su amo, al grado de que ni nombre le había dado–, describe sus vivencias en la sociedad japonesa cien años atrás. Los gatos, no obstante y como bien sabemos, no tienen la habilidad de hablar –y mucho menos la de escribir–, de modo que el gato de la novela no es otra cosa que un recurso del autor para exponer su visión del Japón de la época. Entre los animales que sabemos sí pueden hablar –aunque no escribir– contamos a los loros o pericos, que en cautiverio pueden imitar notablemente bien la voz humana. En realidad, los pericos posiblemente puedan hacer más que eso, como lo demostró Alex, un loro gris africano, el cual fue entrenado por la sicóloga Irene Peperberg de la Universidad Harvard a lo largo  de 30 años, hasta la muerte del ave por causas naturales en 2007. Alex tenía un vocabulario de 150 palabras en inglés, podía contar hasta seis y clasificar objetos por color y forma. Era capaz, además combinar correctamente palabras, lo que implica una forma primitiva de sintaxis En un artículo recientemente publicado en la revista “Science” por un grupo de investigadores encabezado por Karl Berg de la Universidad Cornell en los Estados Unidos, se presentaron nuevas evidencias acerca de las capacidades comunicativas de los loros. De manera específica, en dicho artículo se describen los resultados de un estudio llevado a cabo en Venezuela con loros verdes en estado salvaje, mediante el cual se descubrió que los padres dan nombres a cada de sus crías cuando están todavía en el nido con el fin de identificarlas. Los polluelos aprenden su nombre por imitación, de manera similar a como lo hacen los niños pequeños. No esperaríamos, por supuesto, que los nombres que reciben las crías fueran algo así como Juan o Pedro. Lejos de esto, los padres proporcionan a los polluelos un patrón de sonidos emitidos a gran velocidad, el cual es ligeramente modificado por cada uno de ellos. Este sonido los identificará posteriormente, incluso después de dejar el nido, el cual visitan todavía por algún tiempo para ser alimentados.Con el objeto de averiguar si el nombre adquirido por cada cría era innato o aprendido de los padres, los investigadores intercambiaron huevos entre nidos en algunos casos, encontrando que los polluelos efectivamente adquirían su nombre por imitación de los padres y no por su herencia genética. De acuerdo con Berg y colaboradores, los loros podrían ayudar a desentrañar el origen del habla humana, asunto en el que los expertos no se ponen todavía de acuerdo. Entre las dificultades que enfrentan se señala, por ejemplo, que mientras que la paleontología cuenta con restos fósiles para interpretar el pasado de la vida en la Tierra, no existen los correspondientes “fósiles lingüísticos” que ayuden a explicar el origen del lenguaje. La investigación de los loros y su capacidad de imitar voces –lo mismo que el estudio de los ruiseñores y otras aves canoras que aprenden a cantar por imitación–  podría ser de gran utilidad para explicar el proceso mediante el cual adquirimos nuestra capacidad de hablar.  Sabemos que otros animales aparte de los loros son capaces de establecer comunicación entre sí: los grandes simios empleando gritos y gestos y los delfines emitiendo sonidos, por ejemplo. Es conocido también que gatos y perros entienden cuando son llamados por su nombre. No obstante, resulta interesante que ningún mamífero muestre facilidad para el habla, a pesar de estar evolutivamente cercanos a nosotros; en particular los grandes simios, de los que estamos separados por apenas unos seis millones de años, están muy lejos de nuestras capacidades en este sentido. Los loros, en contraste, de los que nos separan 300 millones de años de evolución, si tienen la capacidad de aprender sonidos. Aun más, sabemos ahora que no solamente lo pueden hacer en cautiverio, sino que regularmente lo hacen en  estado salvaje.  A la luz de los nuevos descubrimientos científicos y dado que los gatos podrían estar quedándose atrás en cuanto a habilidades de comunicación, si Soseki Natsume reviviera podríamos quizá sugerirle que reescribiera su novela y le pusiera por título “Soy un perico”. Con esto posiblemente estaría más a la altura de los tiempos.",
    "Según reportes científicos de los últimos meses, nuestro mexicanísimo maguey, además de su papel  como materia prima para la producción de tequilas y mezcales –papel muy apreciado por muchos, por cierto–, tiene potencial para la fabricación de combustible para automóviles –con lo irreverente que esto pueda sonar–. En efecto, según los expertos, el agave presenta muchas virtudes que podrían ser aprovechadas para mitigar la crisis energética y climática por la que atraviesa el mundo.Se argumenta, por ejemplo, que un incremento en la producción de agave no desplazaría tierras agrícolas destinadas a la producción de alimentos, situación que, según sus críticos, sí ocurre con la producción de alcohol a partir del maíz –que representa en los Estados Unidos un 10% del total de combustible para automóviles que se consume en el país–. El agave, en efecto, está adaptado a climas secos, poco propicios para la agricultura y un incremento en la producción de esta planta podría llevarse a cabo en tierras que de otro modo no estarían bajo cultivo. Así, según se argumenta, el uso del agave como materia prima para fabricar combustibles no entraría en conflicto con la producción de alimentos. Además, puesto que el agave no es un alimento de la misma importancia que el maíz –excepción hecha de aquellos que aseguran que al pulque le falta un grado para ser carne–, su uso como energético estaría relativamente libre de las críticas que se hacen por el uso del maíz para la producción de energéticos en lugar de dedicarlo al consumo humano.Otra ventaja clave del agave es su baja demanda de agua durante el crecimiento, que no pone presión sobre las reservas de este líquido del planeta. Los altos requerimientos de agua y la contaminación del suelo por fertilizantes son argumentos que se esgrimen en contra de los biocombustibles. Aunado a todo lo anterior, según un artículo publicado el pasado 28 de julio en la revista “Energy and Enviromental Science” por científicos de la Universidad de Oxford en el Reino Unido: “el alcohol obtenido del agave es superior o al menos comparable al del maíz y al de la caña de azúcar en términos de energía y balance de gases de invernadero”.  Tomando en cuenta lo anteriormente expuesto, parecería que un recurso para el desarrollo económico de las zonas áridas en nuestro país sería el cultivo del agave como materia prima para fabricar biocombustibles. Las ventajas aparentes de dicha planta para este propósito no son, sin embargo, suficientes para asegurar su desarrollo en esta dirección, el cual depende de manera crucial de factores económicos. Al respecto, la factibilidad económica para la producción de alcohol combustible se analiza en un artículo recientemente publicado en  línea en GCB Bioenergy, por investigadores de la Universidad de Illinois en Urbana-Champaign encabezados por Héctor M. Núñez. Estos investigadores encuentran, desafortunadamente, que tomando como base los costos asociados al cultivo de agave tal como se hace en México para la fabricación de tequila y mezcal, resultaría demasiado caro producir alcohol combustible a partir de esta planta. Esto en comparación con los costos para producirlo a partir del maíz o la caña de azúcar –producción esta última que se hace ya por varias décadas y con gran éxito en el Brasil–. Para aprovechar todas las ventajas que promete el agave se tendrían entonces que hacer más eficientes tanto su cultivo como el proceso de producción de alcohol.Por otro lado, Núñez y colaboradores señalan que durante la producción de tequila se aprovecha solamente el 62% de la piña del agave, además de que no se usan otras partes de la planta, incluyendo las raíces, las hojas y los desechos de la piña. Aprovechar íntegramente la planta de agave constituye entonces una vía para reducir los costos de producción de alcohol combustible.Estamos, así,  en una situación que guarda cierta semejanza con la que encontró Brasil en la década de los setentas cuando decidió desarrollar la tecnología para la producción de biocombustible a partir de la caña de azúcar, algo que consiguió hacer con éxito notable. Brasil en esa ocasión fue presionado por el alza desmesurada que sufrieron los precios de petróleo por el embargo de los países árabes. México, que en estos momentos sufre una disminución de su producción petrolera, podría encontrar una fuente de riqueza en regiones áridas que poco ofrecen en materia de agricultura convencional.  Faltaría por ver si cristaliza la oportunidad. De darse el caso, descubriríamos que los magueyes sirven para otra cosa aparte de hacernos andar a gatas.",
    "Con el aterrizaje de la nave espacial Atlantis el pasado 21 de julio a las 5:57, tiempo de Florida,  se puso fin a la era de los transbordadores espaciales, la cual se había iniciado el  12 de abril de 1981 con el lanzamiento del transbordador “Columbia”. Durante esta era, que se extendió por treinta años, se llevaron a cabo un total de 135 misiones al espacio con resultados contrastantes. Un recuento, en cifras, de los logros del programa de transbordadores espaciales fue publicado hace unos días por la revista “Scientific American” en su edición electrónica. Apunta, por ejemplo, que  a través de dicho programa se colocaron en órbita más de 1,600 toneladas de carga, al mismo tiempo que se regresaron a la Tierra más de 100 toneladas de desechos. Los transbordadores también contribuyeron sustancialmente a la construcción de la Estación Espacial Internacional, además de que pusieron en órbita el telescopio espacial.El programa de transbordadores espaciales, no obstante,  constituye posiblemente el más controvertido de los programas que ha llevado a cabo la agencia espacial norteamericana. Esto no solamente por el saldo trágico que arrojó –de cinco vehículos construidos con capacidad de vuelo orbital se perdieron dos, el “Challenger” en 1986 y el “Columbia” en 2003, con un total de 14 muertos–, sino porque no se alcanzaron todos los resultados esperados. Y sobre todo, por el costo final del programa, que alcanzó la asombrosa cifra de 200,000 millones de dólares. Si dividimos esta cantidad por el total de misiones llevadas a cabo obtenemos una cifra todavía más notable: un costo de cerca de 1,500 millones de dólares por misión.Estas cifras –por su enorme magnitud– son difíciles de apreciar sin hacer algunas comparaciones ilustrativas. Podemos notar, por ejemplo, que el costo por misión del programa de transbordadores es equivalente al costo de construcción de la torre Khalifa en Dubai –que con sus 828 metros de altura es con mucho el edificio más alto del mundo–. Notamos también que dicho costo es mayor a lo que se invirtió para construir el nuevo estadio de los Dallas Cowboys en Arlington, Texas.Podemos igualmente señalar que, asumiendo un costo de construcción por unidad de un millón de pesos, lo invertido en el programa de transbordadores hubiera bastado para construir más de dos millones de casas habitación, cantidad suficiente  para alojar a la población de una ciudad varias veces más grande que la ciudad de San Luis Potosí.      El retiro de los transbordadores espaciales, por otro lado, ha dejado a los Estados Unidos sin la capacidad de realizar vuelos espaciales tripulados. Para este propósito, la NASA está desarrollando la cápsula “Orion” que podrá alojar a cuatro astronautas en misiones de hasta tres semanas. Este proyecto, sin embargo, tomará algunos años en cristalizar y la actual crisis económica por la que atraviesan los Estados Unidos no es prometedora en este sentido.El gobierno norteamericano confía igualmente, en el desarrollo de naves espaciales comerciales para la realización de vuelos orbitales tripulados. La iniciativa privada norteamericana, no obstante, sólo cuenta en la actualidad con la capacidad para realizar vuelos suborbitales de hasta unos cien kilómetros de altura. Para enviar astronautas a la Estación Espacial Internacional en un futuro cercano, los Estados Unidos tendrán entonces que depender de Rusia, que es el único país que los puede realizar de manera rutinaria.  El proyecto de transbordadores espaciales no queda bien parado, según la opinión de sus críticos, comparado con proyectos anteriores de gran envergadura altamente exitosos. Dos de estos, el proyecto “Manhattan”, que desarrolló la bomba atómica en la etapa final de la Segunda Guerra Mundial y el proyecto “Apollo”, que llevó a los norteamericanos a la Luna, tuvieron un costo menor que el proyecto de transbordadores espaciales. En particular, en el proyecto “Manhattan” se invirtió el equivalente a unos 20,000 millones de dólares actuales; es decir, apenas un décimo de lo que costó el programa que finalizó el pasado 21 de julio.El concepto empleando en el transbordador espacial” –es decir, una nave orbital reusable, con alas que le permiten regresar a la superficie de la Tierra y aterrizar como un avión– no ha sobrevivido. Así, el concepto de la nave “Orion” está más cercano  a las naves “Apollo” que a los transbordadores espaciales, que demostraron se caros y peligrosos. Al final, tal parece que la NASA, está sufriendo una “cruda espacial” después de la  fiesta y borrachera presupuestal que significó el programa de transbordadores espaciales. La fiesta, por lo demás, no resulto todo lo divertido que se esperaba.",
    "¿Cuál es el nombre de pila del papa Benedicto XVI? Sin duda habrá quien de manera inmediata conteste acertadamente. Más comúnmente, sin embargo, habrá quién no lo recuerde en ese momento –y lo tenga, quizá, en la punta de la lengua– o de plano no lo sepa. Afortunadamente, para estos últimos dos casos tenemos a nuestra disposición la red Internet. En efecto, tecleando en una computadora o en un teléfono inteligente las palabras “Benedicto XVI”, nos aparece una liga a la Wikipedia en donde obtenemos de manera expedita el nombre completo del papa: Joseph Aloisius Ratzinger –aunque en la biografía oficial de la página del Vaticano aparece solamente como Joseph Ratzinger y en otros sitios como Karl Joseph Aloisius Ratzinger–. Nos enteramos, además, que nació en Marktl an Inn, Baviera, Alemania, el 16 de abril de 1927 y que se ordenó sacerdote el 29 de junio de 1951, entre muchos otros de sus datos biográficos.Lo que se aplica a la biografía del papa se aplica igualmente a prácticamente toda la información pública, de modo que unos pocos tecleados en la computadora es todo lo que nos separa de todo un universo de información –aunque debamos tener cuidado con las fuentes consultadas, pues en Internet también encontramos información falsa. Dada la facilidad de obtener información a través de la red, no es sorprendente que estemos adoptando a Internet como una especie de extensión de nuestra memoria, tal como lo reportan en un artículo publicado el pasado jueves en la revista “Science”, la psicóloga Betsi Sparrow de la Universidad Columbia en Nueva York y sus colaboradores de las universidades Harvard y de Wisconsin. De acuerdo con Sparrow, Internet nos está haciendo “flojos” para memorizar hechos que puedan ser  consultados a través de la red. Así, no tenemos necesidad de memorizar todo lo concerniente a Joseph Ratzinger o a cualquier otro personaje, si en un minuto y sin esfuerzo podemos extraer desde nuestra “extensión de memoria” lo que necesitemos saber acerca del  tema. Sparrow y colaboradores llegaron a esta conclusión mediante varios estudios realizados con estudiantes de las universidades Harvard y Columbia. En un experimento se hicieron preguntas a los participantes los cuales tenían que ser respondidas con un sí o con un no –por ejemplo, “El ojo de un avestruz es más grande que su cerebro”–. El experimento no pretendía que los estudiantes contestaran realmente las preguntas, sino averiguar si su intención para hacerlo era consultar Internet. Se encontró que este es efectivamente el caso. En un segundo experimento, a los participantes les fueron mostradas una serie de preguntas y se les permitió que las escribieran en la computadora. A la mitad del grupo se le dijo que lo que habían escrito se guardaría en la memoria de la máquina, mientras que al otro grupo se les mencionó que sería borrado. En seguida, se les pidió que escribieran de memoria las preguntas. El resultado fue que aquellos a los que se les dijo que lo que habían escrito sería borrado recordaban mejor la serie de preguntas que aquellos que sabían que podrían disponer posteriormente de la información guardada en la computadora. Esto indica que este último grupo fue más “flojo” para memorizar la información, confiando en que estaba almacenada en otro lado.Los autores del artículo, no obstante, señalan que lo más notable de sus descubrimientos es que en lugar de memorizar datos, dirigimos nuestro esfuerzo a memorizar procedimientos para encontrar el lugar en el que están guardados. Esta conclusión fue alcanzada mediante un tercer experimento en el que a los participantes se les mostró nuevamente una serie de preguntas y luego se les permitió que las anotaran en la computadora. Las preguntas fueron en seguida divididas en tres grupos. Un primer grupo fue guardado en la memoria general de la máquina, un segundo grupo fue almacenado en una serie de carpetas con nombres dados y un tercer grupo fue borrado. Al igual que en el caso anterior, las preguntas eliminadas fueron más fácilmente recordadas por los participantes. Estos, sin embargo, no tuvieron ningún problema para recordar en qué carpeta había sido guardada determinada pregunta.La explosión de Internet se inició hace escasamente dos décadas. En este corto tiempo, no obstante, esta tecnología ha tenido un enorme impacto social. Sabemos, por ejemplo, que la difusión de información y de noticias ha tenido una gran influencia en movimientos sociales recientes –en Egipto, por ejemplo, en donde contribuyó a la salida del Presidente. El estudio de Sparrow y colaboradores apunta hacia una nueva dirección de impacto de Internet: la colectivización de nuestra memoria individual.",
    "Los casi 7,000 millones de seres humanos que habitan el planeta Tierra están divididos por partes casi iguales en hombres y mujeres. Tomando en cuenta que el mecanismo mediante el cual nos reproducimos es de carácter sexual, el que haya igual número de hombres que de mujeres no nos resulta  sorprendente y rara vez nos preguntamos por sus causas. En la naturaleza, no obstante, hay organismos que se reproducen de manera asexual y esto pareciera tener ventajas desde el punto de vista evolutivo. Tenemos, por ejemplo, que la reproducción asexual podría ocurrir al doble de rapidez que la reproducción sexual, ya que ésta requiere de una división de la población entre hembras y machos lo que la hace menos eficiente. Así, los machos, que no cuentan con la capacidad de reproducirse, estarían ocupando lugares que de otro modo podrían ser ocupados por individuos que sí lo podrían hacer. La reproducción sexual tiene entonces un costo que debe ser compensado en términos evolutivos de algún modo.Las ventajas de la reproducción sexual pueden ser entendidas por medio de la hipótesis de la Reina Roja, desarrollada en 1973 por el biólogo evolucionista Leigh Van Valen. Según esta hipótesis, las especies están continuamente evolucionando a fin de adaptarse a un medio ambiente continuamente cambiante. El nombre de la hipótesis de la Reina Roja fue inspirado por el personaje homónimo de la novela de Lewis Carroll “Alicia a través del espejo” publicada en 1871. En un pasaje de esta novela, Alicia se encuentra corriendo apresuradamente junto a la Reina Roja, quién la apremia a correr todavía más rápido. Lo curioso es que Alicia encuentra que a pesar de correr tan rápido como puede, los objetos a su alrededor no parecen cambiar de lugar. “Pero ¿cómo? ¡Si parece que hemos estado bajo este árbol todo el tiempo! ¡Todo está igual que antes!”, exclama Alicia al final de la carrera. Así, al igual que Alicia corriendo y viendo que nada cambia, las especies tienen que estar continuamente evolucionando simplemente para mantenerse en la misma condición de adaptación al medio y no extinguirse. Como lo expresa la Reina Roja: “Lo que es aquí, como ves, hace falta correr todo cuanto uno pueda para permanecer en el mismo sitio. Si se quiere llegar a otra parte hay que correr cuando menos dos veces más rápido”.  Aplicada a la reproducción sexual, la hipótesis de la Reina Roja nos dice que a través de la mezcla de los genes del padre y de la madre se genera una diversidad genética que contribuye a la defensa de la especie contra organismos parásitos, los cuales a su vez están también evolucionando permanentemente. Los organismos que se reproducen de forma asexuada, en contraste, evolucionan más lentamente y se adaptan al medio de manera menos eficiente.  En un artículo publicado el pasado jueves en la revista “Science” por un grupo de investigadores de la Universidad de Indiana, Estados Unidos, se reportan los resultados de un estudio que apoyan la hipótesis de la Reina Roja. En el mismo se emplearon gusanos C. elegans, los cuales pueden reproducirse tanto sexual como asexualmente. En condiciones normales, el 20% de los C. elegans se reproduce sexualmente y el resto lo hace de manera asexuada. Durante los experimentos los gusanos fueron expuestos a la bacteria S. marcescens que se sabe les es tóxica.Para el estudio se prepararon tres grupos de gusanos. Un primer grupo  se puso en contacto con la bacteria y a ambos, gusanos y bacterias, se les permitió que evolucionaran libremente. Como resultado, se encontró que el porcentaje de C. elegans que se reproducía sexualmente se incrementó del 20% inicial a un 80-90%. Un segundo grupo de gusanos se puso en contacto con  bacterias S. marcescens que se mantuvieron sin evolucionar. En este caso, después de un incremento inicial en el porcentaje de gusanos que se reprodujeron sexualmente, éste disminuyó posteriormente y se estabilizó en 20%. Un tercer grupo de gusanos evolucionó sin contacto con las bacterias, manteniendo el porcentaje de reproducción sexual en 20%.    En un experimento adicional se creó un grupo de gusanos que solamente se podría reproducir de manera asexuada. Después de exponerlos a las bacterias S. marcescens, el grupo de gusanos se extinguió en 20 generaciones. Los resultados del estudio muestran que la presión que los microbios S. marcescens ejercen sobre los gusanos C. elegans hace que estos cambien su modo de reproducción dominante de asexual a sexual, y que esto es esencial para su supervivencia en presencia de las bacterias S. marcescens. La reproducción sexual en general podría ser entonces producto de un proceso de adaptación por la presencia de organismos parásitos, sin la cual la especie terminaría por extinguirse.Adicionalmente, el estudio echaría abajo la noción de que los machos ocupan espacios costosos desde un punto de vista evolutivo. Noción que, por lo demás y con seguridad, resultaría poco popular entre la población masculina.",
    "“El robot viajero “Opportunity” de la NASA envió hoy temprano las primeras imágenes del sitio en donde descendió, revelando un paisaje negro y surrealista, diferente a cualquier cosa que hayamos visto antes de Marte”. Así iniciaba el comunicado de prensa de la agencia espacial estadounidense del 25 de enero de 2004, en el que anunciaba el descenso exitoso del robot “Opportunity” a la superficie de Marte después de un viaje de casi siete meses desde la Tierra. El “Opportunity”, juntamente con su gemelo el “Spirit”, fue enviado a Marte por la NASA para realizar una exploración de su superficie que ha obtenido resultados sin precedentes. Tanto el “Opportunity” como el “Spirit” tuvieron una curiosa forma de descender a la superficie marciana: bajaron envueltos en balones inflados, rebotando un buen número de veces antes de detenerse. Fue así que –después de saltar 26 veces– el “Opportunity” terminó  en el fondo de un pequeño cráter de 20 metros de diámetro y tres metros de profundidad, en donde encontró un paisaje calificado de surrealista por la NASA.     Imágenes posteriores trasmitidas por las dos sondas mostraron una superficie marciana árida e inhóspita de tonalidad rojiza, que luce extraña al mismo tiempo que recuerda algunas de las regiones más secas de nuestro planeta. Esta similitud, sin embargo, es sólo aparente, pues las condiciones de la superficie de Marte son completamente diferentes a las que prevalecen en la Tierra. Tenemos así que la atmósfera marciana es mucho más tenue que la atmósfera terrestre –menos del 1% en densidad– y que está en un 95% compuesta de dióxido de carbono.  También, como Marte está más alejado del Sol que la Tierra, su superficie es considerablemente más fría que la de nuestro planeta, con una temperatura medida que ronda los –50 grados centígrados. La tenue atmósfera marciana, además, no proporciona suficiente abrigo para moderar la variación de temperatura entre día y noche, variación que puede alcanzar los cien grados centígrados.   Marte es entonces un lugar inhóspito en el que no podríamos sobrevivir de no contar con equipo especial para respirar, al igual que con equipo para aislarnos del frío extremo que ahí prevalece. No obstante lo anterior, y aunque no pudiéramos caminar sobre la superficie de Marte tal como lo hacemos en la Tierra, las imágenes de alta resolución que han enviado las sondas “Spirit” y “Opportunity” y que muestran el paisaje marciano en gran detalle son fascinantes. En ellas podemos ver montañas en la lejanía o nos podemos asomar al interior de un cráter y observar las formaciones rocosas que afloran en sus paredes. De la misma manera, acercamientos fotográficos de rocas marcianas nos permiten casi palparlas.Marte es nuestro vecino cercano, aunque está a una distancia tal que se requieren varios meses para viajar hasta allá. Es, además, el planeta más parecido la Tierra –con todo y sus marcadas diferencias, incluyendo el que no tenga agua líquida– y los detalles de su superficie pueden ser observados desde la Tierra. No es sorprendente entonces que Marte haya ejercido siempre una gran fascinación sobre nosotros y que en más de una ocasión se haya fantaseado sobre la existencia de civilizaciones marcianas e incluso con visitantes o invasores de Marte a nuestro planeta. En comparación, Venus, nuestro otro vecino cercano, tiene condiciones climáticas terribles: una atmósfera cien veces más densa que la de la Tierra –que no permite observar su superficie–  y una temperatura en la misma mayor a los 400 grados centígrados, lo que ciertamente le quita atractivo como planeta para explorar.Para continuar con la exploración de Marte, tiene la NASA en puerta el lanzamiento a Marte de la sonda “Curiosity”, un nuevo robot móvil del tamaño de un automóvil y capacidades considerablemente mayores que las de los gemelos “Spirit” y “Opportunity”: “Curiosity” tendrá una mayor movilidad y podrá sortear obstáculos de 75 centímetros de altura y avanzar a una velocidad de 90 metros por hora.  La sonda está programada para lanzarse entre el 25 de noviembre y el 18 de diciembre del presente año para arribar a Marte a mediados de 2012. Entre otras tareas, “Curiosity” se dedicará a buscar moléculas orgánicas que pudieran dar un indicio de la existencia de vida en Marte, tanto en el presente como en una época pasada.Esperemos que “Curiosity” llegue el año que viene sin novedad a la superficie marciana, algo que, sin embargo, no podemos asegurar si hemos de tomar en cuenta que desde 1975, cuando la NASA llevó a cabo el primer descenso controlado sobre la superficie de Marte, más de un tercio de las misiones a este planeta han terminado en fracaso.De fallar el nuevo robot se habrán ido al bote de la basura buena parte de los 2,500 millones de dólares que costó su desarrollo. Al mismo tiempo, nos habremos perdido, entre otras muchas cosas, de posibles nuevas imágenes surrealistas de la superficie marciana.",
    "Según nos relata Artemio de Valle Arizpe, en su novela “La Güera Rodríguez“, en una ocasión en que María Ignacia Rodríguez de Velazco y Osorio –mejor conocida como la Güera Rodríguez–, se encontró con el hijo de un personaje de gran alcurnia, se abalanzó sobre él y le pegó tal mordida en un brazo que casi le arranca el pedazo. Hizo esto para demostrarles que todavía conservaba su dentadura, pues papá y vástago se habían dado  a la tarea de propalar un rumor según el cual la Güera era tan vieja que ya ni dientes tenía. Ésta, después de la mordida, le habría espetado –apuntándole con el índice– al azorado hijo de su papá: “Mira niño….. dile al trasto viejo que ahora funge como marido de tu señora mamá, que todavía muerdo”.Si bien después de este episodio quedó demostrado, sin duda alguna, que la Güera Rodríguez tenía una muy buena dentadura, del relato de de Valle Arizpe no queda claro si la María Ignacia fue víctima de un infundio en cuanto a su edad, o si por el contrario contaba con dientes de calidad excepcional que le habían durado más de lo normal.De haber vivido en la actualidad, la Güera Rodríguez no hubiera tenido que recurrir a métodos tan drásticos para hacer constar que no era tan vieja –de haber sido éste el caso– como decían sus malquerientes. En efecto, según un artículo publicado el pasado 22 de junio en la revista electrónica PLoS ONE, por un grupo de investigadores de la Universidad de California en Los Ángeles, a partir de una muestra de saliva es posible determinar la edad de una persona con una precisión de cinco años. En la medida que envejecemos el DNA sufre cambios de diversa índole. Mediante un estudio de la saliva de 34 pares de gemelos masculinos idénticos, con edades entre 21 y 55 años, los investigadores de la Universidad de California fueron capaces de encontrar cambios en ciertos lugares del DNA que están estrechamente relacionados con la edad. De este modo, el estado del DNA en dichos lugares nos indicará la edad de la persona. Los resultados fueron corroborados por un segundo estudio con 60 personas, 31 hombres y 29 mujeres, con edades entre los 18 y los 70 años de edad.   Como sabemos que la muerte es inevitable, los signos de envejecimiento no nos son por lo general agradables –siendo comprensible la irritación de la Güera Rodríguez por las historias acerca de su edad–. Así, como un medio para enmascarar el envejecimiento, ha florecido la industria de las cirugías para rejuvenecer. En este sentido, de acuerdo con estadísticas de la Asociación Norteamericana de Cirujanos Plásticos, durante el año de 2010 se realizaron 13 millones de intervenciones de cirugía estética –incluyendo aquellas mínimamente invasivas–. Es de notar igualmente que hace unos días se anunció que “Food and Drug Adminstration” aprobó un tratamiento novedoso para remover arrugas alrededor de la boca y la nariz a base de las células de la piel del paciente.Sabemos ciertamente que la muerte es inevitable. A lo largo del último siglo, sin embargo, con el advenimiento de nuevos tratamientos y procedimientos médicos y quirúrgicos, así como con una mejora en las condiciones de higiene, la esperanza de vida se ha incrementado sustancialmente en la mayor parte de los países del mundo, rondando los 80 años en algunos casos. En el futuro existe la posibilidad de que la esperanza de vida se incremente todavía más; en relación a esto, hay investigadores que anticipan –aunque esto es altamente controversial– que la esperanza de vida algún día alcanzará los 800 años.No sabemos si esto ocurrirá algún día. Lo que sí sabemos es que el conocimiento científico nos ha cambiado drásticamente la vida. Al respecto podemos referir otro pasaje de la novela de de Valle Arizpe en el que la Güera Rodríguez, encita y a punto de dar a luz, invita a seis señores que “pasaban muy tranquilos por la calle” a presenciar su parto. La razón de este insólito comportamiento fue la necesidad de contar con testigos del alumbramiento de su hija, pues su embarazo había sido puesto en duda por los parientes de su segundo marido –recientemente fallecido–, quienes alegaban que lo fingía para quedarse con la herencia del difunto. De haber nacido dos siglos después, la Güera Rodríguez hubiera podido simplemente solicitar un examen genético para demostrar el origen de su hija. Aunque quizá no se hubiera divertido tanto.",
    "La idea de que la Tierra se está calentando, posiblemente como resultado de acciones nuestras, tiene entre el público en general –como es natural– tanto quien la apoye como quien la rechace. Al respecto, si en estos momento hiciéramos una encuesta, con los calores que hemos sufrido en las últimas semanas posiblemente encontrásemos más apoyadores que detractores de esta idea –aunque en medio de un invierno más frío que lo usual, posiblemente pensáramos diferente.  El calentamiento global, por otro lado, no es de ninguna manera un asunto de encuestas de opinión; lejos de esto, hoy en día es un hecho apoyado por datos científicos duros y por tanto es difícil de refutar. Está también bien establecido que con toda probabilidad dicho calentamiento es debido a la emisión de dióxido de carbono a la atmósfera por la quema de combustibles fósiles –petróleo, gas y carbón– que ha ocurrido en los últimos dos siglos a partir de la revolución industrial. La cantidad de dióxido de carbono en la atmósfera es diminuta –menos de 0.04 por ciento – y sin embargo tiene una influencia determinante sobre el clima de la Tierra. Es tan grande su influencia que incluso ha sido llamado el “termostato del planeta”: si su concentración sube o baja, la temperatura de la superficie terrestre de manera correspondiente sube o baja. Este es el comportamiento que ha seguido la Tierra a lo largo del último medio millón de años. Hay que mencionar, por otro lado, que es riesgoso culpar al calentamiento global de un evento climático en particular –la actual ola de calor, por ejemplo–. Dicha culpabilidad tendría que establecerse, en su caso, en base a una estadística que demuestre que por un periodo de tiempo suficientemente largo, a partir de que la temperatura de la Tierra empezó a elevarse de manera más marcada hace medio siglo, ha habido un incremento en la frecuencia con que ocurren dichos eventos.En caso de que se encontrara que el calentamiento global esté efectivamente incrementando la frecuencia de eventos climáticos fuera de lo normal tendríamos motivos de alarma y enfrentaríamos una perspectiva futura nada halagüeña.  La mala noticia es que desgraciadamente hay evidencia de que éste es el caso. Tenemos, por ejemplo, que en un artículo publicado en el año 2006 en la revista “Journal of Geophysical Research” por un grupo de investigadores de varias instituciones alrededor del mundo –incluyendo México– encabezados por L.V. Alexander de la Universidad Monash en Australia, se muestra que el número de noches por año con temperaturas cálidas en el 70 % de la superficie terrestre estudiada, aumentó significativamente entre los años 1951 y 2003, al mismo tiempo que disminuyó el número de noches frías en ese mismo periodo. Los climatólogos predicen también que por el calentamiento global en el futuro padeceremos de olas de calor más intensas, frecuentes y de mayor duración. Predicen igualmente que por el desequilibrio climático algunas regiones de la superficie de la Tierra se harán más húmedas –al aumentar la temperatura terrestre hay una mayor evaporación de agua hacia la atmósfera y por lo tanto ocurren mayores precipitaciones pluviales–, al mismo tiempo que otras se volverán más secas.Cabe preguntarse si el calentamiento global puede revertirse. Se sabe que en gran medida éste se debe a la emisión de dióxido de carbono a la atmósfera por la quema de combustibles fósiles. Como un medio para combatir el calentamiento global debe entonces moderarse el uso de dichos combustibles. Los combustibles fósiles, sin embargo, aportan en la actualidad más del 85 % de la energía consumida por el mundo y cabría esperar que deshacernos de ellos no constituirá una empresa sencilla. Esto, por más que con toda seguridad aumentará sustancialmente en el futuro la generación de energía por fuentes renovables –solar y eólica–. De hecho, según proyecciones del Departamento de Energía de los Estados Unidos, el uso del carbón se incrementará sustancialmente en las próximas décadas, lo mismo que el del gas natural.  Nos tendremos pues que ir acostumbrando a olas de calor más intensas y frecuentes, lo mismo que a sequías e inundaciones más severas, entre otros fenómenos climáticos extremos. Y todo por culpa de un incremento de la concentración de dióxido de carbono en la atmósfera de apenas un 0.01 por ciento. Hay veces que la pregunta ¿qué tanto es tantito? tiene respuestas sorprendentes.",
    "En lo que quizá parezca una verdad de Perogrullo, de un estudio reciente llevado a cabo por investigadores de la Universidad Estatal de Michigan, encabezados por William Schmidt –y publicado el pasado 10 de junio en la revista “Science”– podríamos concluir que los mejores maestros de matemáticas de la escuela secundaria son por lo general aquellos que más saben del tema –aunque habrá, sin duda, sus excepciones. Lo anterior se desprende de este estudio en el que se comparan los resultados de una prueba llevada a cabo en el año 2010 con el propósito de estudiar y evaluar el proceso de formación de maestros de matemáticas para el nivel preuniversitario, con los resultados de un examen realizado en 2003 para evaluar el desempeño de los estudiantes de secundaria. Ambas pruebas fueron llevadas a cabo en 16 países por la Asociación Internacional para la Evaluación de Logros Educativos –IEA, por sus siglas en ingles–. Los resultados obtenidos muestran que una mejor preparación en matemáticas redunda, años más adelante, en maestros mejor calificados en esta materia. Dado que el artículo de referencia se centra en la educación en los Estados Unidos, los autores hicieron énfasis en una comparación del proceso de formación de profesores de matemáticas en ese país, con los correspondientes procesos llevados a cabo en Taiwán y Rusia, los países más exitosos en este respecto. Se hace notar que los estudiantes de secundaria estadounidenses no son especialmente exitosos en las pruebas internacionales de matemáticas –la prueba PISA de la OCDE, por ejemplo– y ocupan posiciones de media tabla en listas que son encabezadas por algunos países asiáticos –China-Hong Kong y Corea, entre otros– seguidos de algunos países europeos. En consonancia con lo anterior, el artículo en cuestión señala que los Estados Unidos no producen suficientes maestros de matemáticas competitivos a nivel internacional. En particular, afirma  que en la preparación de los maestros de matemáticas en los Estados Unidos se da un énfasis mayor a aspectos pedagógicos –tanto específicos al área de la matemáticas como a conocimientos pedagógicos en general– en detrimento de los conocimientos matemáticos; esto en comparación con el menor énfasis que el que se da a la pedagogía en los países con los estudiantes más exitosos, en los que se favorece el conocimiento matemático. Resultan así en los Estados Unidos  maestros preparados de manera deficiente en esta materia, los cuales a su vez entrenan estudiantes que resultan con conocimientos en matemáticas también deficientes. Algunos de estos se convertirán posteriormente en maestros de matemáticas, cerrando así un círculo vicioso. Señalan los autores del artículo de referencia que para competir en nivel de conocimientos matemáticos con países como Singapur y Taiwán sería necesario que  los futuros profesores de matemáticas de los Estados Unidos sean escogidos de entre el 25% de egresados de la escuela preparatoria con mejores calificaciones. Consideran, sin embargo, que esto no será posible si no se incrementan los salarios de los maestros en ese país.El único país latinoamericano incluido en el estudio fue Chile, que ocupó uno de los últimos lugares junto con Botswana y las Filipinas. México no participó, pero de haberlo hecho posiblemente hubiera tenido un desempeño similar al de Chile. En efecto, como es sabido, nuestro país ocupó el último lugar en la prueba de matemáticas PISA 2003 entre los países miembros de OCDE, y en el artículo de Schmidt y colaboradores se muestra que esta última prueba concuerda en gran medida con la prueba de matemáticas de la IEA.El conocimiento pedagógico es sin duda alguna una de las más importantes herramientas con que cuenta un maestro de nivel preuniversitario para trasmitir sus conocimientos. Al respecto, en no pocas ocasiones hemos oído la frase “sabe mucho pero no sabe enseñar”, con la que un estudiante expresa su insatisfacción con respecto a alguno de sus maestros. La pedagogía, sin embargo, tiene sentido solamente si el maestro domina los conocimientos que se supone debe enseñar, y como lo señalan Schmidt y colaboradores, no siempre se les da a dichos conocimientos el énfasis adecuado.        Al igual que empezamos, terminamos este artículo con una afirmación que podría igualmente sonar a verdad de Perogrullo: “para enseñar algo hay que primero aprenderlo”.",
    "Como lo explicó el Dr. Mario Molina, premio Nobel de Química 1995, durante su presentación el pasado jueves al recibir el doctorado Honoris Causa de la Universidad Autónoma de San Luís Potosí, la emisión de gases de invernadero por el uso indiscriminado de combustibles fósiles ha elevado su concentración en la atmósfera terrestre a niveles sin precedente en cientos de miles de años. Esto ha llevado a un incremento concurrente de la temperatura de la superficie de la Tierra que, entre otras cosas, ha reducido el volumen de hielo en las regiones ártica y antártica y ha aumentado  la ocurrencia de eventos climáticos extremos en todo el mundo. Así, aunque no es posible asegurarlo con un cien por ciento de certeza, muy probablemente el desarrollo industrial del mundo ha tenido también un impacto negativo global que está cambiando la faz del planeta.No es el clima, sin embargo, la única víctima del desarrollo acelerado que ha experimentado el mundo en los últimos doscientos años desde el inicio de la revolución industrial; otra víctima en este respecto ha sido las reservas de agua dulce del planeta, que en muchas regiones están decayendo a una velocidad mayor con la que son reemplazadas. Un ejemplo de esto se da en la región noroeste de la India, en la frontera con Pakistán –con un área de medio millón de kilómetros cuadrados–, en donde los acuíferos han declinado su nivel entre 2002 y 2008 a una razón de 30 centímetros anuales. Lo anterior se desprende de un estudio llevado a cabo por investigadores de la NASA y de la Universidad de California en Irvine a partir de datos satelitales. La técnica empleada para dicho estudio está basada en la medición de los cambios en la fuerza de gravedad de la Tierra por el desplazamiento de grandes volúmenes de agua. Se emplearon para el estudio datos proporcionados por dos satélites gemelos propiedad de la NASA, los cuales fueron puestos en órbita en marzo de 2002 con el objeto, precisamente, de estudiar la fuerza gravitacional terrestre. Los satélites orbitan a una altura de 500 kilómetros sobre la superficie de la Tierra, manteniendo una separación entre ellos de unos 220 kilómetros. Esta separación, sin embargo, sufre cambios minúsculos cuando un acuífero cambia su volumen de agua y modifica la gravedad de la Tierra. Así, la disminución del volumen del acuífero en un cierto periodo de tiempo puede ser conocida a través del cambio en la separación de los dos satélites, cambio que puede ser medido con una gran precisión por medio de rayos de microondas intercambiados entre los mismos. Además, según Mathew Rodell, del Goddard Space Flight Center de la NASA, es posible detectar cambios tanto en aguas superficiales como en aguas profundas, haya o no haya luz de día o claridad en el cielo, pues “la gravedad atraviesa todos los materiales”.  Medir el cambio en el volumen de agua de un depósito subterráneo de manera remota y desde el espacio parece cosa de magia. Existe hoy en día, sin embargo, el conocimiento y las tecnologías necesarias para llevarlo a cabo. Entre estas últimas se cuentan no solamente aquellas que permiten poner dos satélites en órbita y medir de manera precisa los cambios de distancia entre ellos, sino también a la tecnología de cómputo sin la cual no sería posible analizar los datos entregados por los satélites a fin de desentrañarlos y obtener la información de interés.   No está por demás señalar la gran conveniencia de una técnica para medir desde el espacio –por métodos sorprendentes y no convencionales– el estado de los acuíferos del mundo, aunque esta técnica sea necesariamente complementaria a otros métodos de medición a nivel de la superficie de la Tierra.Por otro lado, mediciones por satélite similares a las realizadas en el noroeste de la India indican que en el Valle de San Joaquín, en California, Estados Unidos, el nivel de las aguas subterráneas está también descendiendo paulatinamente, lo que significa que la velocidad con que se les está extrayendo el agua es más grande que aquella con que ésta es reemplazada por medios naturales.   Estos estudios, al igual que otros realizados con técnicas diferentes, indican que en muchos lugares del mundo los mantos acuíferos se están agotando por la sobre explotación a la que están sujetos y que en las próximas décadas el mundo bien podría sufrir una crisis por la falta del vital líquido. Cuando esto ocurra tendremos una indicación más de que, efectivamente, hemos estado abusando del planeta por un buen número de años.",
    "De cuando en vez nos enteramos de celebridades –artistas de cine o cantantes, por ejemplo– que aumentan su fama por haber sido encarceladas por manejar ebrias o por haber cometido delitos, en ocasiones graves. Se da incluso el caso de personas famosas que no tienen más habilidad que la de provocar alborotos y de vivir de los rumores y chismes que generan. Para algunas celebridades, la generación de escándalos se ha convertido ciertamente en un negocio lucrativo.Este fenómeno –que no es nuevo, por lo demás– ilustra la fascinación que sentimos por los rumores y por el chismorreo, actividad esta última que, convendríamos, es practicada de manera muy extendida y entusiasta. Una afición tan común, no hay duda, debería tener una explicación científica en términos de nuestra evolución social. Se pronuncia al respecto un artículo reciente publicado en el número del 19 de mayo pasado en la revista “Science” por un grupo de investigadores de varias universidades norteamericanas, encabezados por Eric Anderson de la Northeastern University, Boston, Massachusetts. En dicho artículo se describen experimentos llevados a cabo a fin de averiguar como la mala fama de un individuo influye en la visibilidad que alcanza. En las pruebas participaron un total de 66 personas a las que les mostraron imágenes de caras neutras –sin expresión–, asociando a cada cara mostrada –a la manera de un rumor o chisme– un comportamiento social específico. Este comportamiento podía ser negativo –por ejemplo, “le arrojó una silla a un compañero de clase” – o positivo –“ayudó a una señora anciana con sus bolsas de mandado”–. El comportamiento asociado a las imágenes podía ser también socialmente neutro. A los participantes les fueron igualmente mostradas caras sin expresión que no fueron asociadas a algún tipo de comportamiento.Los experimentos fueron llevados a cabo empleando una técnica en la cual al participante se le presentan simultáneamente dos imágenes diferentes. Dichas imágenes se colocan de modo que cada ojo vea solamente una de ellas –el ojo izquierdo la imagen izquierda y el ojo derecho la imagen derecha–. Por un proceso que no es posible controlar de manera consciente –denominado “rivalidad binocular”–, el cerebro fija su atención en sólo una de las imágenes suprimiendo la otra. Esto lo hace alternando uno y otro ojo. El tiempo que el cerebro fija su atención en una de las imágenes es una medida de la importancia que le brinda a la misma.   Como resultado se encontró que los participantes dedicaron más tiempo a ver las imágenes con una connotación negativa en comparación con aquellas asociadas a un comportamiento neutral o incluso positivo. De manera similar, el cerebro mostró preferencias por los “mal comportados”  por encima de aquellos acerca de los cuales no había información sobre sus cualidades morales. Así, un mal comportamiento ayudó a tener una mayor notoriedad.   De acuerdo con Anderson y colaboradores, el chismorreo –positivo o negativo– cumple el propósito de proporcionar información acerca de una persona sin necesidad de interactuar directamente con ella, lo que nos permite vivir en grupos grandes de personas. Esta información es importante en el momento de evaluar la conveniencia de entablar una determinada amistad; o bien, si fuera el caso, de encender luces rojas y evitar a la persona en cuestión. En este contexto, se podría quizá esperar que una información positiva acerca de una persona fuera tan importante como una negativa. De hecho, sin embargo, los chismes personales negativos son más frecuentes que los positivos, lo que indicaría que en nuestra evolución social ha sido más importante cuidarnos las espaldas que entablar amistades. En palabras del artículo de referencia “Es fácil imaginar que una selección preferente para percibir a las personas malas nos puede proteger de embusteros y tramposos, permitiéndonos que los tengamos más tiempo bajo observación y obtener así una mayor información acerca de su comportamiento”.    El chismorreo es una actividad que, pese a su amplia difusión, no goza de prestigio social –aunque a algunos profesionales del mismo les represente un buen negocio–. De acuerdo a los expertos, sin embargo, el chismorreo juega un papel importante proporcionando cohesión social.  Reconocer esto último posiblemente nos ayude a reducir cargos de conciencia si en algún momento llegamos a sorprendernos “cohesionando” nuestra sociedad.",
    "El terremoto del pasado 11 de marzo que devastó la costa pacífica de Tohoku, en el norte de la isla japonesa de Honshu, y que ha sido el más violento experimentado por Japón desde que se llevan registros, podría ser seguido por otro sismo de magnitud equivalente pero con el agravante de que su epicentro estaría más cercano a la región de Tokio –que concentra a un cuarto de la población japonesa–. Esta posibilidad es considerada en un artículo publicado esta semana en la revista “Science” por un grupo de investigadores de los Estados Unidos y de Francia, encabezados por Mark Simons del Instituto Tecnológico de California.   Los sismos de gran magnitud se producen por las fallas geológicas a lo largo de la frontera entre dos placas tectónicas. Como es conocido, la corteza de la Tierra está constituida por placas que están en permanente movimiento, chocando ente ellas, alejándose y/o “frotándose” una contra la otra. De este modo, en las fronteras entre dos placas se producen grandes esfuerzos, acumulándose paulatinamente una gran cantidad de energía, la cual al liberarse repentinamente provoca un sismo cuya magnitud depende de la energía almacenada en la falla antes de su ruptura. Así, entre más tiempo se mantenga estable una falla geológica sin producir un temblor, mayor será la probabilidad de que en el momento de su ruptura se produzca un terremoto de gran magnitud.El Japón está localizado en la confluencia de cuatro placas tectónicas y por lo mismo es uno de los países con más actividad sísmica del mundo. El terremoto del 11 de marzo pasado tuvo su epicentro cerca de la frontera entre la placa tectónica del Pacífico y la placa de Ojotsk en la región de océano pacífico enfrente de la costa de Tohoku. Estas dos placas se mueven una contra la otra, con el resultado de que la placa del Pacífico se hunde al mismo tiempo que se desliza por debajo de la placa de Ojotsk. Hacemos notar que sobre esta última placa está asentada la parte norte de la isla de Honshu. La costa noreste de la isla de Honshu ha tenido históricamente una gran actividad sísmica, con sismos de magnitudes 7 y 8, por la falla debida a las placas del pacífico y de Ojotsk. Esta falla se extiende en el océano pacífico hacia el sur hasta la altura de la  ciudad de Tokio. Los expertos, sin embargo, fueron tomados por sorpresa por la magnitud del sismo del 11 de marzo, que ha sido el más intenso desde que se llevan registros y que significó movimientos de las placas tectónicas hasta de 60 metros en algunos lugares. Es, de hecho, necesario remontarse en los registros históricos hasta el año 869 de nuestra Era para encontrarse, en la misma región, con un sismo de magnitud comparable –con tsunami incluido– a la del terremoto de Tohoku de marzo pasado.  Lo anterior ha llevado a postular a Simons y colaboradores que existen puntos a lo largo de la falla enfrente de la costa pacífica japonesa que son capaces de acumular enormes energías a lo largo de cientos o miles de años antes de experimentar una ruptura, algo que antes del sismo del 11 de marzo no era considerado como una posibilidad. Lo que resulta alarmante, de acuerdo a Simons, sin embargo, es la posibilidad de que algo como esto pueda estar ocurriendo en el segmento sur de la falla que produjo el terremoto de Tohoku, lo que podría producir un sismo de magnitud comparable al del 11 de marzo, pero a una menor distancia de Tokio, lo que podría tener serias consecuencias. Reconoce, sin embargo, que el conocimiento de las fallas geológicas sobre las cuales está asentado el archipiélago japonés no es lo suficientemente sólido –sobre todo después del sismo no esperado de Tohoku y que desconcertó a los expertos– para ofrecer una predicción confiable al respecto.Como quiera que sea, echando un vistazo a la distribución de placas tectónicas sobre la superficie terrestre –que puede ser fácilmente consultada en Internet–, uno no puede dejar de concluir que la localización geográfica del Japón, asentado en las inmediaciones de cuatro fronteras tectónicas, difícilmente podría haber sido peor. Esto ha forzado al Japón a realizar esfuerzos para mitigar el impacto de los frecuentes terremotos que sufre. Y si a una localización geográfica poco afortunada añadimos el que Japón, además de ser un país montañoso y con una alta densidad de población, no es de ninguna manera un país rico en recursos naturales –lo que incluye los energéticos–, no puede uno dejar de admirar su éxito como país, no obstante los problemas que ahora enfrenta por el sismo del pasado mes de marzo.Japón, efectivamente, no es un país rico en recursos naturales. Excepto, por supuesto, en japoneses –si los clasificamos como recursos naturales– de los cuales tiene 130 millones. Eso posiblemente haya hecho la diferencia.",
    "Tal parece que el transbordador espacial “Endeavour” quiere retrasar lo más posible el momento de su jubilación: estaba programado para despegar en su último vuelo al espacio el pasado 29 de abril, pero fallas técnicas han obligado a suspender dicho despegue en tres ocasiones. Al momento actual su lanzamiento está programado para mañana 16 de mayo. El vuelo del “Endeavour” será el penúltimo del programa de transbordadores espaciales de la NASA, el cual cerrará con el vuelo del transbordador  “Atlantis” el próximo 28 de junio. Un nuevo retraso en el lanzamiento del “Endeavour”, sin embargo, podría obligar a retrasar igualmente el último viaje del “Atlantis”.   Al término del programa, la NASA habrá llevado a cabo 135 viajes al espacio desde el vuelo inaugural del “Columbia” en abril de 1981. Entre otras muchas realizaciones, el programa de transbordadores espaciales puso en órbita el telescopio espacial Hubble en el año 1990, que ha permitido realizar observaciones astronómicas sin precedente y que estará en servicio hasta 2014. El programa de transbordadores ha contribuido igualmente a la construcción de la estación espacial internacional. La NASA construyó un total de cinco transbordadores con capacidad de vuelo orbital, dos de los cuales tuvieron un final trágico con la muerte de todos sus ocupantes: el “Challenger”, poco después de despegar el 28 de enero de 1986, y el “Columbia”, a su reingreso a la atmósfera el 1 de febrero de 2003. Con dos catástrofes en 133 vuelos –sin contar las dos misiones que están aún pendientes– los transbordadores no se han mostrado de ninguna manera como el medio más seguro de viaje.  No obstante sus desventuras, el programa de transbordadores espaciales ha sido uno de los más glamorosos de la NASA. En el momento de su lanzamiento los transbordadores quizá puedan parecer grotescos; sin embargo, una vez que se desprenden del enorme tanque de combustible y de los dos cohetes impulsores, siempre han lucido –con sus alas en delta, sus colores blanco y negro y su aterrizaje suave a la manera de un avión– considerablemente más atractivos que otros vehículos espaciales –las cápsulas Apollo que llevaron a los norteamericanos a la Luna, por ejemplo. Además, los transbordadores espaciales fueron –y lo siguen siendo– los únicos vehículos que se pueden reutilizar en varios vuelos orbitales –en realidad, se reutilizan sólo el vehículo orbital y los dos cohetes impulsores, pues el tanque de combustible se desecha después de cada lanzamiento–. El récord en este sentido lo tiene el transbordador “Discovery” –hoy ya fuera de servicio– con 39 vuelos orbitales realizados.El glamour y la distinción, sin embargo, tienen su costo y los transbordadores de la NASA no son la excepción en este respecto. En efecto, un estudio llevado a cabo por Roger Pielke y Radford Byerly de la Universidad de Colorado –reportado en el número del 7 de abril pasado de la revista “Nature” –, arroja que el costo total del programa de transbordadores de la NASA ronda a los 200,000 millones de dólares –en dólares de 2010–. De este modo, si se divide esta cantidad por el total de vuelos realizados, se obtiene que cada viaje de un transbordador al espacio tuvo un costo promedio de unos 1,500 millones de dólares.El programa de transbordadores espaciales pretendía, además, desarrollar la tecnología necesaria para realizar vuelos frecuentes de rutina al espacio a un costo bajo. En este sentido es evidente que los resultados estuvieron por debajo de las expectativas, pues solamente se realizaron un promedio de 4.5 vuelos por año, a lo largo de los 30 años de operación del programa, a un alto costo por vuelo. Hubo también periodos de inactividad de más de dos años después de los accidentes del “Challenger” y del “Columbia”. Con la terminación del programa de transbordadores, la NASA pierde la capacidad de llevar a cabo vuelos orbitales tripulados. Al mismo tiempo, no obstante, el sector privado norteamericano está ingresando al negocio espacial. En diciembre de 2010, la compañía “SpaceX”, con base en California,  se convirtió en el primer ente privado en colocar una cápsula en órbita y posteriormente recuperarla. Igualmente, otras compañías privadas están buscando desarrollar el “turismo espacial”, ofreciendo a la venta vuelos suborbitales. Con  costos entre 100,000 y 200,000 dólares por viaje, sin embargo, este turismo no será precisamente de masas.Si todo sale como planeado, el día de mañana a las 7:56 a.m. despegará el “Endeavour” para su último vuelo orbital y una vez que regrese a la Tierra se unirá al “Discovery” en calidad de jubilado, quedando en espera de un hogar definitivo en un museo norteamericano. Quedará así un último transbordador en activo y pendiente un último vuelo por realizar; con esto se cerrará la era de los transbordadores espaciales.Al respecto, Pielke y Byerly escriben: “El transbordador espacial es el programa norteamericano más caro jamás realizado. Ahora que llega a su fin, debemos celebrar su éxitos, y extraer lecciones para futuras empresas humanas en el espacio”.",
    "Como ha sido ampliamente difundido, el pasado martes 3 de mayo ocurrió una explosión en una mina de carbón cerca de Sabinas, Coahuila, con el resultado de, hasta ahora, 11 mineros muertos. La mina accidentada es del tipo conocido como “pocito”, a la que los mineros ingresan y salen –con el rostro cubierto de carbón– por un único acceso: un estrecho pozo vertical, suspendidos por un cable accionado por un motor de camión. Las minas de “pocito” cuentan con una tecnología primitiva, que ha sido calificada como más propia del siglo XIX que de la época actual.       Se sabe que las minas de carbón son especialmente peligrosas por el gas metano –gas grisú– que se encuentra asociado a las mismas. Si este gas alcanza una determinada concentración puede sobrevenir una explosión. Para evitar esto las minas tienen que estar adecuadamente ventiladas, lo que supuestamente no habría sido el caso en la explosión del pasado martes. A pesar de los peligros a los que se exponen, los mineros acceden a trabajar en las minas de “pocito” porque, en primer lugar, no disponen en la localidad de fuentes de trabajo y, en segundo lugar, porque aparentemente reciben un pago más alto que los trabajadores de otras minas menos rústicas.    El carbón fue la fuente de energía que posibilitó la revolución industrial en la Inglaterra de finales del siglo XVIII. Constituyó, además, el principal energético que impulsó a lo largo del siglo XIX la  expansión industrial en Europa y Norteamérica. Hoy en día, aunque las fuentes de energía se han diversificado, el carbón sigue siendo uno de los  energéticos más importantes. Tenemos, por ejemplo, que el 40% del total de energía eléctrica generada a nivel mundial proviene de la combustión del carbón.El carbón como fuente de energía, sin embargo, tiene mala fama. Se sabe, por ejemplo, que su combustión genera más bióxido de carbono –principal causante del calentamiento global– que otros combustibles como el gas y el petróleo. Puede, además, generar bióxido de azufre –responsable de la lluvia ácida–, así como contaminación con residuos tóxicos como el mercurio. Se ha reportado incluso que alrededor de las plantas carbo-eléctricas los niveles de radiación pueden ser más altos que en las inmediaciones de una planta nuclear –en condiciones normales de operación, por supuesto–. Esta radiación –que no representaría un problema de salud pública por lo débil de la misma– provendría de trazas de los elementos químicos radiactivos uranio y torio presentes en las cenizas resultantes de la combustión del carbón.  No obstante su mala fama, sin embargo, el carbón tiene gran importancia estratégica para países como los Estados Unidos, que cuenta con el 25% de las reservas a nivel mundial y China, que es el principal productor y consumidor de carbón. Para algunos otros países como Polonia y Sudáfrica, más del 90% de la electricidad que consumen proviene del carbón. En México la contribución del carbón a la generación de electricidad es relativamente baja, constituyendo menos de 10% del total. Este hecho se explica porque las reservas carboníferas de nuestro País –localizadas fundamentalmente en Coahuila– son relativamente pequeñas, constituyendo alrededor del 0.15 % de total mundial. El consumo de carbón en México, sin embargo, está aumentando mediante su importación. La Comisión Federal de Electricidad tiene, además, planes para la construcción de nuevas plantas de energía que operarán con carbón importado. Esto último ha recibido críticas de parte de grupos ecologistas, que consideran va en contra de las políticas del gobierno federal para reducir la emisión de gases de invernadero en México.Parece difícil, sin embargo, que el carbón disminuya su papel en la generación de energía a nivel mundial en un futuro inmediato. Por el contrario, posiblemente aumente en la medida en que se agoten los hidrocarburos y sea necesario buscarles un sustituto, que bien podría ser el carbón. En este sentido hay que hacer notar que se estima que las reservas de mundiales de este mineral durarán más de cien años a la tasa de consumo actual. Habría también que tomar en cuenta los problemas que está experimentando en el Japón la energía nuclear –presentada frecuentemente como una opción “no contaminante” para sustituir a los combustibles fósiles– que podrían frenar su expansión en muchos países.  En el caso de nuestro País, –que no cuenta con reservas sustanciales de carbón– una vez que se nos agote el petróleo nos convertiremos en importadores de energéticos–¿carbón?–, como de hecho ya lo estamos haciendo para generar electricidad. Nuestra necesidad de carbón, sin embargo, no puede justificar la operación en Coahuila de minas trasladadas –como en una máquina del tiempo– cien años desde el pasado.",
    "Las hormigas de la especie “Solenopsis invicta”, que son originarias de Sudamérica pero que han infestado el sur de los Estados Unidos desde la década de los años 30, tienen una curiosa –y al mismo tiempo efectiva– manera de sobrevivir a las inundaciones: se agrupan y forman con sus cuerpos una balsa flotante que navega hasta que encuentran tierra firme y pueden desembarcar en forma segura. Las hormigas son entonces al mismo tiempo la balsa y los pasajeros. La balsa-hormiga está formada de dos capas, una capa inferior que permanece sumergida en agua y que está constituida por hormigas que se enlazan firmemente unas con otras por medio de sus patas y mandíbulas –y están por tanto inmóviles– y una capa o capas superiores que sobresale del agua formado por hormigas libres de moverse. Si bien es posible que las hormigas del fondo  hubiesen preferido viajar en el piso superior de la balsa, éstas optan por sacrificarse por el bien de todo el grupo. El sacrificio, sin embargo, es solamente durante la travesía, pues al final de la misma, y en el supuesto de que la balsa llegue a un lugar seguro, todas las hormigas –las “de arriba” y las “de abajo”– tienen un final feliz.De todo esto surge una primera pregunta: ¿cómo es que las balsas-hormiga se las arreglan para flotar? Como sabemos, las balsas ordinarias están construidas con materiales ligeros –como la madera– que flotan por sí solos. Las hormigas, sin embargo, son más densas que el agua y en principio no podrían flotar. Para encontrar una respuesta a esta y otras preguntas, un grupo de científicos del Instituto Tecnológico de Georgia en los Estados Unidos llevó a cabo un estudio detallado del comportamiento de las hormigas de referencia cuando se les coloca en agua. Los resultados del estudio fueron publicados en forma electrónica el pasado 25 de abril en las Memorias de la Academia Nacional de Ciencias de los Estados Unidos. Se encontró que las balsas-hormiga emplean como flotadores bolsas de aire atrapado entre los cuerpos de sus integrantes –lo que es equivalente a los flotadores de aire que usan los niños cuando están aprendiendo a nadar.Este resultado, además, responde a una segunda pregunta relativa a la manera en que las hormigas de la capa sumergida pueden sobrevivir por días o semanas en esa posición: lo hacen porque toman el aire que necesitan para respirar de dichos flotadores. Están entonces en buena medida secas a pesar de estar por debajo de la línea de flotación. Así, en efecto, las hormigas “de abajo” se sacrifican pero hasta cierto punto.Tal como asientan los autores del artículo de referencia, “Sin tomar en cuenta su tamaño  diminuto y sus deficiencias, las balsas de hormigas tienen características atractivas con respecto a los dispositivos para flotar hechos por el Hombre. De manera simultánea proporcionan cohesión y flotabilidad a sus pasajeros, además de ser repelentes al agua. Pueden ser construidas rápidamente –en aproximadamente minuto y medio– sin ninguna clase de equipo. Pueden transportar a miles de millones de pasajeros con cero bajas. Pero quizá lo más impactante es que son auto ensambladas”.     Además, cuando una hormiga de la capa superior es removida, su lugar es automáticamente ocupado por una hormiga de la capa inferior. Las balsas-hormiga tienen entonces la capacidad de “auto curarse” después de sufrir una “herida”. Las capacidades de auto cura y auto ensamble son, de acuerdo a los autores del artículo que nos ocupa, “características de los seres vivos”. Así, las hormigas de la especie “Solenopsis invicta” cuando están amenazadas por el agua –y al margen de lo caótico de sus movimientos–, se organizan y actúan como un “súperorganismo” para sobrevivir.El comportamiento organizado de las hormigas “Solenopsis invicta” está en aparente  contraste con nuestro comportamiento como humanos, el cual está en ocasiones lejos de ser cooperativo. Pensemos, por ejemplo, en las guerras que han sido una constante en la historia de la civilización, en la compañías generadoras de energía nuclear que anteponen las ganancias a la seguridad de la población, o en la cámara de diputados caracterizada por los constantes desencuentros entre sus miembros. Las hormigas, no obstante, son “máquinas” considerablemente más simples que un humano y tienen muchas menos opciones para actuar en una determinada circunstancia. Esperaríamos entonces que se pusieran más fácilmente de acuerdo, como efectivamente ocurre.      No todo, sin embargo, es color de rosa con las hormigas “Solenopsis invicta” –que de hecho son rojas–. En efecto, si bien algunas de las hormigas “de abajo” en las balsas estuvieron siempre ahí, otras residan originalmente en la parte alta y habrían descendido, no por cuenta propia, sino “obligadas” por sus congéneres. El artículo que nos ocupa incluye videos  fascinantes del proceso de formación y de la fortaleza de las balsas-hormiga que se pueden consultar en la siguiente dirección electrónica: http://www.pnas.org/content/early/2011/04/20/1016658108/suppl/DCSupplemental",
    "El pasado 20 de abril se cumplió un año de la explosión de la plataforma petrolera “Deepwater Horizon” en el pozo Macondo frente a la costa de Luisiana. El accidente resultó en 11 muertos y en el derrame de 5 millones de barriles de petróleo en el Golfo de México, lo que lo convierte en un accidente aun más grande que el del tristemente célebre pozo Ixtoc 1, que entre junio de 1979 y marzo de 1980 derramó tres millones de barriles de petróleo en el Golfo de Campeche antes de poder ser sellado. En la medida en que se agotaron los pozos petroleros en tierra firme, se empezaron a explorar y explotar yacimientos en el mar, primero en aguas someras –el pozo Ixtoc 1 estaba a una profundidad de 50 metros– y después en aguas profundas, como fue el caso del pozo Macondo que se encontraba a una profundidad de 1,500 metros. A tales profundidades las condiciones para la perforación de un pozo petrolero son severas, entre otras cosas por las grandes presiones que ocurren en el fondo marino –a 1,500 metros de profundidad la presión es unas 150 veces más grande que la presión a nivel del mar– y por las bajas temperaturas imperantes que son del orden de unos pocos grados centígrados. La perforación del pozo Macondo estaba siendo realizada por la compañía British Petroleum (BP), la que contrató para este propósito a las compañías Halliburton y  Transocean, dueña esta última de la plataforma “Deepwater Horizon”. Estas compañías cuentan con la tecnología necesaria para perforación en aguas profundas; las prácticas que siguieron en Macondo no fueron, sin embargo, las adecuadas. Esta es una conclusión alcanzada por el comité de siete personas nombrado por el presidente Obama para determinar las causas del accidente de Macondo. De acuerdo con el reporte de dicho comité –hecho público en enero del presente año– “Las causas inmediatas del aumento repentino en la presión del pozo pueden ser atribuidas a errores identificables hechos por BP, Halliburton y Transocean, que revelan tales fallas en el manejo de riesgos que ponen en entredicho la cultura de seguridad de la industria entera”. El reporte señala igualmente la falta de coordinación entre las compañías participantes en la perforación del pozo Macondo, las cuales no compartieron ente sí información crítica acerca de las condiciones del pozo previo al accidente. El comité concluye que “La pérdida explosiva del pozo Macondo hubiera podido ser evitada”. La revista “Science”, comentando en su número del 6 de enero de 2011 el reporte del comité de referencia escribe: “Durante la perforación del pozo, BP se enfocó en maximizar ganancias y no necesariamente en la seguridad”. Por otro lado, en un incidente también relacionado con la industria energética, desde el pasado 11 de marzo –como resultado del tsunami que golpeó la costa noreste de Japón– hay una situación de emergencia con los cuatro reactores nucleares de la planta  nuclear Fukushima I que han dejado escapar grandes cantidades de radiación. Esto ha obligado al gobierno japonés a ordenar la evacuación obligatoria de 140,000 personas de un área de 20 kilómetros alrededor de la planta y a elevar gradualmente el índice de gravedad del accidente –el periódico español El País ha calificado a Fukushima de “Chernóbyl en cámara lenta”– desde 4 hasta el máximo posible de 7.Los reactores nucleares de Fukushima fueron dañados por el tsunami que inundó la planta e inutilizó sus sistemas de enfriamiento que son críticos para evitar fugas de radiación.Los tsunamis no son raros en la costa noreste de Japón. La agencia de seguridad nuclear japonesa no consideraba, sin embargo, que representaran un peligro mayor para la planta nuclear. En declaraciones al periódico Washington Post, el sismólogo japonés Yukinobu Okamura, quien fungió como miembro de un panel de expertos formado en 2009 por la agencia nuclear japonesa para evaluar la seguridad de los reactores japoneses, señaló –apoyado en datos históricos–  la posibilidad de que un tsunami de gran magnitud golpeara la planta de Fukushima. Su señalamiento no fue, sin embargo, tomado en consideración por la agencia.Macondo y Fukushima han sido en el último año dos accidentes de gran magnitud asociados a la industria de generación de energía que según los expertos hubieran podido evitarse de haberse hecho una evaluación correcta de los riesgos involucrados. Ambos accidentes han originado desastres ambientales en un cada vez más atribulado planeta.De manera coincidente con el aniversario del desastre de Macondo, el pasado 22 de abril se celebró el “Día Internacional de la Madre Tierra”, decretado por el Programa de las Naciones Unidas para el Medio Ambiente”. Con las dos emergencias ambientales ocurridas en un lapso de un año, sin embargo, la Madre Tierra está en esta ocasión un poco más maltrecha que de costumbre y tiene poco que celebrar.Quizá, el 22 de abril pasado debíamos haberle deseado un “pocos días de estos”.",
    "Hace medio siglo, para ser más precisos el 16 de mayo de 1960, nació el láser en los laboratorios Hughes de Mailbu, California, y con esto uno de los inventos que más han cambiado nuestra vida cotidiana. Esta afirmación, que podría quizá parecer exagerada, es, no obstante, precisa. Una manera de apreciar la importancia de los láseres hoy en día es imaginando que desaparecen de manera repentina de la faz de la Tierra. De este modo nos harían sentir su ausencia y se volverían paradójicamente visibles –de manera similar a como lo hicieron los latinos californianos en la comedia hollywoodense “Un día sin mexicanos”.  Sin láseres no podríamos, por ejemplo, escuchar música en nuestro aparato de discos compactos, que emplea precisamente un láser para reproducir los sonidos codificados en el disco. Si tal fuera nuestro propósito, tendríamos probablemente que desempolvar nuestra antigua grabadora –si es que todavía la conservamos– y nuestros viejos casetes de audio –los que hayan sobrevivido al maltrato al que frecuentemente eran sometidos durante su reproducción–. Una cosa similar nos sucedería si pretendiésemos ver una película en un formato DVD o Blu-ray: tendríamos que recurrir a los obsoletos casetes de video VHS.En los supermercados estaríamos probablemente obligados a hacer largas colas en la caja registradora, pues la lectora de barras –que con un rápido vistazo a la etiqueta del producto determina su precio y su clasificación– depende de los láseres para funcionar y si éstos desaparecieran la cajera tendría que ingresar manualmente el precio y la descripción de los productos que deseáramos adquirir.  Y sin embargo, si bien es mucho más conveniente escuchar música o ver una película en un formato de disco que en uno de audio o de video casete –y siempre es, por supuesto, preferible salir rápidamente del supermercado a hacer largas colas–, en donde los láseres han tenido su mayor impacto es en el ámbito de las telecomunicaciones, particularmente en la red Internet. De hecho, sin los láseres no existiría esta red –al menos no con la extensión que tiene ahora– y en consecuencia no podríamos consultar la página del clima con datos para el día de hoy, ni podríamos “chatear” –lo que, como tantas cosas, tiene aspectos positivos y negativos–, a veces a miles de kilómetros de distancia. No podríamos tampoco hacer transferencias bancarias de manera instantánea, ni acceder de manera también instantánea a una enciclopedia masiva –la “wikipedia”– que está en continuo proceso de evolución y autocorrección. De hecho, un día sin láseres equivaldría en buena medida a un día sin internet.No existirían tampoco aplicaciones médicas de los láseres tales como la cirugía oftálmica para corrección de la miopía o el tratamiento de tumores, ni  aplicaciones industriales que emplean láseres de alta potencia para cortar metales con gran precisión. En la primera semana del presente mes de abril se celebró la décima segunda edición de la semana académica del Instituto de Investigación en Comunicación Óptica de la UASLP. Haciendo honor a la importancia de los láseres, esta vez el ciclo de conferencias giró alrededor del tema “El láser y sus aplicaciones” y se contó con la participación de destacados conferencistas, nacionales y extranjeros.Uno de estos conferencistas fue el Dr. Daniel Malacara, quien es uno de los pioneros de la Óptica en nuestro País. El Dr. Malacara, fundador y primer director del Centro de Investigaciones en Óptica de la ciudad de León, Guanajuato, nos habló sobre la exitosa fabricación de láseres en nuestro País, proyecto que él llevó a cabo hace ya casi medio siglo en la UNAM, muy poco tiempo después de la invención de este dispositivo en California. Aunque no perseguía fines comerciales –los láseres fueron fabricados como herramientas para su laboratorio de investigación, pues el costo de un láser en la década de los sesentas hacia prohibitiva su adquisición comercial– se vendieron varios láseres a instituciones mexicanas, notablemente el Instituto Mexicano del Petróleo.  Como lo expuso en su conferencia, la fabricación de láseres en México en una época tan temprana pudo haber dado origen a una industria mexicana de este tipo de dispositivos. No se dio, ya que el Dr. Malacara fue invitado por el Dr. Guillermo Haro a incorporarse al entonces recién creado Instituto Nacional de Astrofísica, Óptica y Electrónica en Tonantzintla, Puebla, ofrecimiento que aceptó dejando su laboratorio en la Ciudad de México. Una cosa por otra, pero perdimos de este modo la posibilidad de contar en México con una industria de láseres, dispositivos que, sin exageración, se cuentan entre los desarrollos tecnológicos que más han contribuido a dar forma a la época actual. En efecto, tan omnipresentes son los láseres hoy en día, que si de manera repentina desaparecieran de la faz de la Tierra –al igual que los mexicanos en California– no cabe duda de que serían perfectamente visibles.",
    "Según la policía japonesa, como lo informa la agencia de noticias Kyodo, en un radio de 20 kilómetros alrededor de la planta nuclear de Fukushima, podría haber hasta 1,000 cuerpos de víctimas del tsunami del pasado 11 de marzo que no han podido ser recuperados por temor a la radiación. Dichos cuerpos estarían severamente contaminados por las emisiones radiactivas de la planta  y aun si fueran recuperados se tendrían dificultades para su manejo. No podrían ser cremados, pues cabría la posibilidad de esparcir una nube radiactiva de humo en la atmósfera, ni podrían tampoco ser enterrados, ya que contaminarían el suelo. Tendrían entonces que ser  descontaminados, lo que los dañaría en forma severa.Lo anterior refleja de manera dramática los peligros que representa la industria nucleoeléctrica, que maneja materiales altamente tóxicos que pueden ser dispersados en el medio ambiente por un accidente o contingencia con resultados desastrosos. El accidente de Fukushima ha incrementado la oposición en algunos países a la industria nucleoeléctrica, aunque muchos consideran, por el contrario, que es una alternativa viable a las plantas termoeléctricas para la generación de energía eléctrica; esto, a fin de limitar la emisión de los gases de invernadero a la atmósfera causantes del calentamiento global.Si no es la energía nuclear ¿qué otras fuentes de energía están disponibles para tal propósito? La más obvia es la energía solar, que es abundante y prácticamente inagotable. En efecto, sobre la superficie de la Tierra incide una cantidad de energía varios miles de veces superior a la consumida a nivel global; lo seguirá haciendo, además, por varios miles de millones de años. La energía solar es adicionalmente limpia, pues no contamina ni emite gases de invernadero.La radiación del Sol puede ser capturada empleando espejos para calentar un fluido y con éste mover una turbina y un generador de electricidad –de manera análoga esto último a como lo hacen los reactores nucleares–. Puede ser igualmente capturada por medio de celdas solares para producir directamente energía eléctrica. El Sol es la causa última del viento y de la lluvia que alimenta ríos y llena represas, de modo que la energía hidroeléctrica y la energía eólica pueden verse como formas particulares de energía solar.  Se dice también que la energía solar es  gratis. Esto sólo cierto si no tomamos en cuenta que la construcción de los dispositivos e instalaciones para su captura y conversión requirió de consumir una cierta cantidad de energía. Así, lo que es importante es cuanta energía entrega una instalación dada a lo largo de toda su vida de trabajo, en comparación con la energía empleada en construirla. Esta energía varía grandemente según el tipo de instalación. Así, una planta a base de celdas solares genera en todo su tiempo de operación –unos 25 años– entre 3 y 6 veces la energía que fue empleada en construirla. Una central hidroeléctrica, en contraste, puede generar más de 200 veces la energía necesaria para construirla.      Igualmente, la energía solar será limpia  solo en la medida que descontemos los gases de invernadero generados durante la construcción de la instalación solar. Hoy en día la energía que se consume en el mundo proviene mayormente de los combustibles fósiles –gas, carbón y petróleo– con un 87% del total. Las dos siguientes fuentes más importantes de energía son la nuclear y la hidroeléctrica, cada una contribuyendo con alrededor del 6% de la energía consumida a nivel global. En contraste, las energías renovables, con la excepción de la hidroeléctrica, contribuyen con apenas un 1% de total de la energía consumida por el mundo.   Hay países, sin embargo, en donde las energías renovables juegan un papel importante. Por ejemplo, el pasado 1 de abril el diario español “El País” destacó que la energía eólica ocupó por primera vez durante el mes de marzo del presente el primer lugar en producción de energía eléctrica en España. La energía eólica contribuyó con el 21% de la electricidad total generada, contra el 19% de la energía nuclear y el 17% de la hidroeléctrica.  El cambio climático que está experimentando nuestro planeta demanda una reducción de la emisión de los gases de invernadero a la atmósfera. Esto requiere del uso de fuentes limpias y renovables de energía basadas en la radiación solar. La energía nuclear, como nos lo enseña la experiencia de Fukushima, está lejos de constituir una fuente de este tipo.En realidad, puede convertirse en una fuente de contaminantes altamente tóxicos.",
    "Después de la puesta en marcha de las primeras plantas nucleares comerciales para la generación de electricidad en la década de los años cincuenta, hubo un gran optimismo en que la energía nuclear resolvería todos los problemas energéticos que se preveía ocurrirían en el futuro ante el eventual agotamiento de los combustibles fósiles. De acuerdo con esta visión, la energía nuclear creció rápidamente hasta alcanzar los 400 reactores en operación en la década de los años ochenta. Esta visión, sin embargo, ha cambiado y en el último cuarto de siglo el ritmo de crecimiento de la industria nucleoeléctrica se ha frenado considerablemente. En efecto, en la actualidad existen en operación 440 reactores nucleares que implican un incremento de sólo un 10 % en los últimos veinticinco años.Como quiera que sea, la industria nuclear –localizada en 30 países, incluido México–, produce el 14 % de la electricidad consumida a nivel global. El primer país generador de energía nucleoeléctrica es Estados Unidos, con 104 reactores nucleares en operación que le proporcionan alrededor del 20% de la electricidad que consume. Francia, con 58 reactores es el segundo país en este rubro en el mundo, mismos que le generan el 80% de la electricidad consumida. Japón, que al igual que Francia no tiene petróleo, ha impulsado fuertemente la energía nuclear en su territorio y es el tercer país con más centrales nucleares. Si bien, según los expertos, existen varias causas para la baja en el ritmo de crecimiento de la industria nuclear, se considera que una de ellas fue el accidente de la planta nuclear “Three Mile Island” en el estado de Pensilvania, Estados Unidos, ocurrido en el año de 1979. En dicho accidente, se produjo la fusión parcial del núcleo del reactor por una falla en el sistema de enfriamiento, lo que originó una fuga de radiactividad. Aunque el reactor pudo ser finalmente controlado, el accidente dio argumentos a quienes se oponen a la operación comercial de los reactores nucleares por considerarlos peligrosos. Los problemas que está enfrentando la central nuclear de Fukushima después del sismo que golpeó a Japón el pasado 11 de marzo, han avivado el debate sobre la seguridad de la industria nuclear. En este contexto, la canciller alemana Angela Merkel aboga por abandonar la energía nuclear e ir por las energías renovables lo antes posible. En los Estados Unidos, el presidente Obama ha ordenado una revisión detallada de las 104 plantas nucleares norteamericanas. En Japón, en donde el tema nuclear es muy sensible por ser el único país que ha sufrido un bombardeo con armas atómicas, la resistencia a las plantas nucleares crecerá con seguridad en cuanto pase la emergencia.Japón es un país que no tiene petróleo y que es en consecuencia vulnerable a las fluctuaciones en el mercado petrolero mundial. Es entendible entonces que, al igual que Francia que no tiene tampoco petróleo, haya recurrido a la núcleoelectricidad para satisfacer sus necesidades energéticas. Esto a pesar de estar asentado en una de las regiones de más alta actividad sísmica en el planeta. Después de todo, Japón cuenta con una gran capacidad tecnológica y una cultura de prevención de desastres que, entre otras cosas, le ha llevado a construir altos edificios en Tokio que resistieron al cuarto sismo más intenso desde que se lleva registro, así como a implementar un sofisticado sistema de alerta de sismos que seguramente –pese al elevado número de víctimas que aumenta cada día– salvó muchas vidas.  Dado lo anterior, no es clara la razón por la que la central nuclear de Fukushima estaba en una situación tan vulnerable. Como se ha informado, los reactores nucleares de dicha central se apagaron automáticamente en el momento del sismo como se suponía debía ser. Con esto se activó la planta diesel de emergencia para mantener funcionando el sistema de enfriamiento, esencial para evitar el sobrecalentamiento y eventual fusión del núcleo del reactor. Con el tsunami que siguió al sismo, sin embargo, se dañó dicha planta, lo que activó un tercer sistema de respaldo a base de baterías. Éste funcionó solamente por ocho horas, al final de las cuales se desató el maremágnum que ha llevado a intentar enfriar a los reactores casi a cubetazos de agua. Podríamos quizás preguntarnos sobre la conveniencia de instalar un reactor nuclear en la costa con una protección mínima contra las olas, en un país que ha sido históricamente tierra de sismos y tsunamis –palabra está última incluso de origen japonés–. Al respecto, nos viene a la mente la “La gran ola de Kanagawa”, pintura universalmente conocida de Hokusai, artista japonés del Siglo XIX, en la que se muestra, con el monte Fuji, al fondo, una gigantesca ola  a punto de engullir a tres embarcaciones. Tal parece que el desastre de Fukushima podría haberse evitado y en cambio se tiene una industria nuclear amenazada con cambios drásticos de políticas energéticas alrededor del mundo.¿Constituirá Fukushima un tsunami para la energía nuclear como se empieza a vislumbrar?",
    "El 3 de febrero de 1975 las autoridades chinas ordenaron la evacuación de la ciudad de Haigcheng en el norte del país. Al día siguiente, la ciudad fue golpeada por un terremoto de magnitud 7.0 en la escala de Richter, matando a 2,000 de sus habitantes. De acuerdo con la agencia científica del gobierno norteamericano “US Geological Survey”, la evacuación de Haigcheng salvó un estimado de 150,000 vidas del millón de habitantes con que la ciudad contaba en esos momentos. El evento constituye la primera predicción exitosa de un terremoto de gran magnitud.Los sismólogos chinos fueron puestos en alerta sobre la posible ocurrencia de un macro-sismo en Haigcheng por indicios tales como cambios en la elevación del terreno y en los niveles de las aguas subterráneas, observados meses antes de que el terremoto finalmente ocurriera. Se convencieron de la inminencia del mismo por un incremento en la actividad sísmica y fue entonces que recomendaron la evacuación de la ciudad. Desgraciadamente, los científicos chinos no han sido igualmente exitosos en otros casos. No pudieron predecir, por ejemplo, el sismo de magnitud 7.5 que el 17 de julio de 1976 devastó a la ciudad china de Tangshan matando a 250,000 personas. Tampoco lo hicieron con el terremoto de magnitud 7.9 que el 12 de mayo de 2008 –previo a la Juegos Olímpicos de Beijing– golpeó  a la región de Sichuan en el suroeste de China resultando más de 70,000 víctimas fatales.En realidad, la mayoría de los expertos concuerda en que los conocimientos geofísicos de que disponen en la actualidad no son suficientes para predecir con un mínimo de certeza la inminencia de un terremoto. Es difícil, además, hacerlo de manera empírica como sucedió en Haigcheng, pues  un terremoto de gran magnitud no siempre está precedido por indicios como los que alertaron a los expertos chinos. Lo más que los sismólogos pueden hacer es dar una estimación de la probabilidad de que ocurra un terremoto de gran magnitud en un determinado lugar sobre la base de su historia sísmica. Estas predicciones, sin embargo, no siempre resultan acertadas. A falta de un adecuado conocimiento de la geofísica de los terremotos que permitan su predicción precisa, queda el recurso de alertar a la población –o los operadores de instalaciones o servicios susceptibles de sufrir daño– una vez que un macro-sismo ha ocurrido, a fin de que tome las medidas pertinentes. Con este propósito,  varios países asentados en zonas sísmicas –incluyendo a México– se han dado a la tarea de construir sistemas de alerta de  macro-sismos. En este respecto destaca Japón, que ha mantenido funcionando desde 2007 una red de 1,000 sismógrafos distribuidos a lo largo de su territorio. Esta red permite alertar sobre el arribo de una onda sísmica a una determinada población con una anticipación que depende de la distancia de dicha población al epicentro del sismo.  La tecnología que permite realizar esto hace uso del hecho que en un terremoto se generan dos clases de ondas sísmicas, las ondas P y las ondas S. Las ondas P –que son relativamente poco dañinas– viajan desde el epicentro del sismo a aproximadamente el doble de la velocidad de las ondas S –que son las causantes de la mayor destrucción–. Una vez que se produce un sismo, las primeras ondas en arribar al sismógrafo más cercano son entonces las ondas P. El sismógrafo analiza su amplitud y decide si las ondas S que le siguen corresponden a un sismo de gran magnitud, en cuyo caso emite una alarma que se propaga a gran velocidad a través del sistema de telecomunicaciones del país. No obstante, puesto que el sistema requiere de algunos segundos para determinar la magnitud del sismo antes de enviar la alerta –y así evitar falsas alarmas–, para poblaciones cercanas al epicentro esta alerta puede llegar después del sismo.En el caso del sismo del pasado 11 de marzo en el norte de Japón, la ciudad de Tokio, localizada a  400 kilómetros del epicentro, recibió la señal de alerta con una antelación de más de un minuto a la llegada del sismo. En poblaciones cercanas al epicentro, de acuerdo con la agencia meteorológica japonesa –responsable del sistema de alerta– queda por determinarse que tan útil fue la alarma. El terremoto del pasado 11 de marzo de magnitud 9.0 en la escala de Richter ha sido el de mayor intensidad sufrido por Japón. Dada esta circunstancia, los expertos consideran que el número de víctimas relativamente bajo –hasta ahora cerca de 2,000, aunque el número se está elevando rápidamente– se debió, entre otros factores, al sistema de alerta temprana que fue probado por primera vez con un sismo de grandes dimensiones.Dos mil muertos, no obstante, son muchos muertos, y si a esto le añadimos los graves problemas que está experimentando la planta nuclear de Fukushima, es claro que no nos libraremos de los desastres causados por los terremotos hasta que no tengamos un conocimiento profundo de las causas que los originan y seamos capaces de predecir cuándo ocurrirán.",
    "Según noticias aparecidas en la prensa británica en días pasados, en restaurantes y puestos callejeros de la Gran Bretaña se vende de manera ilegal carne de chimpancé. Ésta es transportada al país desde las selvas ecuatoriales africanas a través de una “conexión francesa” vía el aeropuerto Charles de Gaulle de Paris. De acuerdo con Marcus Rowcliffe, investigador de la Sociedad Zoológica de Londres –citado por el diario británico “Daily Mail”–, cinco toneladas de carne “salvaje” –“bushmeat”– llegan semanalmente de África al aeropuerto parisino para su distribución en Europa. Del total, un 1% corresponde a carne de grandes simios: gorilas y chimpancés. La carne de chimpancé, según Rowcliffe, no solamente se destina al consumo en restaurantes y puestos de comida, sino que también se le da usos medicinales y como un símbolo de estatus social.La reacción de los británicos a las informaciones anteriores, tal como fueron expresadas en comentarios enviados al diario “Daily Mail”, van desde expresiones de desagrado por una práctica calificada de “caníbal”, pues “los chimpancés son 99% humanos”, hasta comentarios racistas como el de un lector que escribió que “si los africanos o britanico-africanos querían comer carne salvaje se regresaran al África en donde abunda”, afirmando al mismo tiempo que “los británicos no comen ese tipo de carne”.  Otro lector que vivió en Gran Bretaña se congratula de haber dejado a “ese país olvidado por Dios” y cambiado su residencia a Nueva Zelanda, nación en donde “se cuida al medio ambiente y a las especies en peligro de extinción”. Un lector más no se sorprende y afirma que la venta de carne selvática “se ha dado por mucho tiempo”, carne que “por cierto apesta y deja un horrendo mal aliento”. No faltó tampoco quien lo tomara a broma, como aquel lector que escribió que “ahora entiende cómo es que puede treparse a los árboles a bajar al gato”, lo que pensaba “se debía a las vitaminas”.  Todo este asunto podría efectivamente tomarse a broma si no fuera porque involucra a una especie animal que está en peligro de extinción. En relación a esto, la Unión Internacional para la Conservación de la Naturaleza (UICN) tiene incluido al chimpancé en su Lista Roja de especies en peligro. Calcula que en la actualidad hay una población total entre 170,000 y 300,000, la cual, sin embargo, se está reduciendo a un ritmo acelerado. La UICN estima que en un lapso de tres generaciones –60 años– la población total de chimpancés se ha reducido en más de un 50%. Entre las causas del declive de la población de chimpancés citadas por UICN sobresale el incremento en las actividades de tala de árboles y de minería en la región del centro y oeste de África que ha devastado su hábitat natural. Estas actividades, además, han resultado en la construcción de caminos que han abierto a la caza del chimpancé regiones que antes eran inaccesibles y en las que se mantenían protegidos. Otra causa importante del declive en el  número de chimpancés son las enfermedades como el Ébola que ha provocado en algunas regiones una reducción de población de hasta un 90%.La caza de chimpancés puede ser vista también desde otro ángulo. Se sabe que entre los primates, el chimpancé es nuestro pariente más cercano, del que nos separamos evolutivamente en tiempos relativamente recientes –hace unos cinco millones de años–.  Compartimos de esta manera el 99% del ADN. Por lo mismo, compartimos también otras cosas, y al igual que nosotros los chimpancés viven en grupo, utilizan herramientas, tienen conciencia de sí mismos y son capaces de manejar símbolos –cazan, además, otros primates que ocupan evolutivamente un nivel inferior–. Se ha incluso planteado que las sociedades de chimpancés pudieran reflejar de algún modo una etapa temprana en el desarrollo social humano, lo que haría patente nuestra cercanía como especies. Dados los puntos de coincidencia entre chimpancés y humanos –con todo y las enormes diferencias en sofisticación intelectual que existe entre ambas especies–, es entendible que algunos califiquen la caza de chimpancés como un “asesinato” y al acto de consumir su carne como uno de canibalismo. Los problemas de extinción que enfrentan los chimpancés por la caza indiscriminada y la destrucción de su hábitat –compartida por los otros grandes simios, gorilas y orangutanes, mas graves incluso en el caso de estos últimos–, amenaza con acabar con nuestros parientes vivos más cercanos. Nos privaríamos así de la oportunidad de estudiar en vivo una especie cercana a la nuestra que pudiera ayudarnos a entender nuestra propia evolución.  Así, lo mejor que podemos hacer como especie es evitar consumir carne de chimpancé. Con esto le haremos un gran favor a las generaciones futuras.  Evitaríamos además un posible mal aliento y la sensación de que cometemos un acto de canibalismo, aun ante la posibilidad de que disminuyera nuestra agilidad para trepar árboles.",
    "La escritora norteamericana Pearl S. Buck,  Premio Nobel de Literatura 1938, publicó en 1931 la novela “La buena tierra” con la que ganó el Premio Pulitzer un año después. En esta novela Buck relata la historia de Wang Lung, campesino chino que a base de trabajo, determinación y habilidad, logró superar la condición de pobreza en la que estaba sumido en su juventud hasta  alcanzar un considerable bienestar económico en sus años maduros. En un determinado momento de su vida, sin embargo, una sequía prolongada provocó una hambruna que lo colocó en una situación desesperada y que hizo incluso que su esposa estrangulara a su hija recién nacida por no tener manera de alimentarla. Para huir de la catástrofe, Wang Lung decidió trasladarse en tren junto con su familia a una ciudad hacia el sur –cualquier similitud con situaciones que ocurren en nuestro País con la migración centroamericana no es mera coincidencia–, en donde sobrevivió trabajando de cochero tirando de un “rickshaw” y su familia pidiendo caridad. La hambruna –ficticia– relatada en “La buena tierra” tiene, por supuesto, un sustento real –Pearl S. Buck vivió buena parte de su vida en la China previo a la revolución comunista de 1949– pues sabemos que este tipo de sucesos han sido algo recurrente en la historia de China y del mundo en general. La hambruna ocurrida en Irlanda a mediados del Siglo XIX, por ejemplo, provocó la muerte o el éxodo, a los Estados Unidos y a otras partes del mundo, de dos millones de irlandeses. En África las hambrunas son endémicas y el año pasado la región del Saheli, al sur del desierto del Sahara, sufrió de una escasez severa de alimentos que tuvo que ser paliada con ayuda internacional.En las últimas décadas ha habido preocupación de parte de muchos especialistas por la posibilidad de que la población mundial haya ya sobrepasado, o lo haga en un futuro cercano, la capacidad del planeta para alimentarla y dotarla de un mínimo de  condiciones de bienestar. Dado este caso, ocurrirían hambrunas que no serían ya fenómenos locales que pudieran aliviarse mediante migraciones –ya no tendríamos a donde ir– u otras medidas de ayuda por parte de los países ricos, sino que constituirían problemas básicamente sin solución.  La capacidad de la Tierra para albergar a la raza humana fue el tema de un simposio organizado por la Asociación Americana para el Avance de la Ciencia durante su congreso anual llevado a cabo en Washington, D.C., en días pasados. El simposio reunió especialistas de universidades y organizaciones en los Estados Unidos y Gran Bretaña que trataron temas tales como la  estimación de los miles de millones de personas que nuestro planeta puede mantener y si, como humanos, deberíamos buscar  florecer o simplemente sobrevivir. Se consideraron también  nuestras reservas de tierra para la producción de alimentos y la búsqueda de caminos para estabilizar la población mundial. Se estima que la población mundial, que en estos momentos es de alrededor de 7,000 millones, alcanzará un máximo de 9,000 millones en el año 2050 y después empezará a descender –una gran proporción del incremento poblacional vivirá en países en desarrollo que están creciendo sustancialmente más rápido que los países desarrollados–.  En 40 años el planeta tendría entonces que alimentar a 2,000 millones más de personas. En realidad, si hemos de ser justos, serían más de 2,000 millones, pues los recursos del planeta están mal distribuidos –en algunos casos, muy mal distribuidos– entre los países desarrollados y aquellos en desarrollo, y es un hecho que una proporción significativa de los habitantes del mundo están alimentados de manera deficiente. Así, Jonathan Foley de la Universidad de Minnesota en los Estados Unidos, uno de los participantes en el simposio, considera que se tendría que doblar la producción mundial de alimentos en 40 años.Foley duda, sin embargo, que esto sea posible y hace notar que la agricultura en la actualidad ya usa el 40 % de la superficie de la tierra cultivable y el 70 % del total de agua consumida. Genera, además, el 30 % de los gases de invernadero que se emiten a la atmósfera.Pareciera ser entonces que, a menos que se desarrollen tecnologías agrícolas para hacer un uso más eficiente del agua de irrigación y de la tierra cultivable, así como para reducir la emisión de gases de invernadero a la atmósfera, las generaciones futuras tendrán que apretarse el cinturón.",
    "Las ideas de Giordano Bruno, que incluían la noción de un universo con múltiples mundos habitados, le ocasionaron hacia finales del Siglo XIV graves problemas con la Inquisición; tan graves que fue declarado hereje  y quemado vivo el 17 de febrero del año 1600 en la plaza romana “Campo de` Fiori”. El episodio es recordado por un monumento en dicha plaza, en el que se ilustran, grabadas en bronce, varias etapas del juicio inquisitorial de Bruno.Después de publicar en la segunda mitad del siglo XIX “El origen de la especies” y el “Origen del Hombre”, Charles Darwin fue igualmente atacado de manera feroz. A diferencia de Bruno, no obstante, Darwin no sufrió consecuencias personales tan drásticas y murió por causas naturales el 19 de abril de 1882, siendo enterrado en la Abadía de Westminster junto a Isaac Newton y otros notables intelectuales. Y sin embargo, aunque recibió numerosos honores, sufrió también duros cuestionamientos a su obra, que incluso duran hasta nuestros días –como es el caso del movimiento creacionista en los Estados Unidos. Las ideas de Bruno y de Darwin –al igual que las de Nicolás Copérnico y Galileo Galilei que defendían que la Tierra no ocupa el centro del sistema solar– implicaban que la especie humana no es un caso especial en el Universo, lo que chocaba con las ideas dominantes de la época en que fueron expresadas. En la actualidad, cuatro siglos después del juicio de Giordano Bruno y a 150 años de la publicación del “Origen de las especies”, aceptamos sin dificultad –aunque no de manera generalizada– que no somos una especie única y que muy posiblemente haya numerosas civilizaciones, incluso más avanzadas que la nuestra, en otras partes del Universo. No constituimos, pues, un caso aparte.  Así, el que la supercomputadora “Watson” fabricada por la compañía IBM en los Estados Unidos haya vencido –la semana que recién termina– en el concurso “Jeopardy” de la televisión norteamericana a dos previos campeones de dicho concurso, no ha provocado reacciones violentas por parte de alguna tenebrosa institución. Esto, a pesar de que apunta a la posibilidad de fabricación de una máquina inteligente que supere a la capacidad del cerebro humano. “Jeopardy” es un concurso que mide amplitud de conocimientos, velocidad de respuesta, y capacidad para analizar preguntas expresadas con juegos de palabras y en un lenguaje ambiguo. Esta última habilidad, propia del cerebro humano, no era una característica de las computadoras hasta la aparición de “Watson”. La supercomputadora “Watson” consiste de un arreglo de 2880 procesadores “Power 7” desarrollados por IBM. Físicamente está alojada 10 gabinetes del tamaño de un refrigerador. En su memoria tiene guardadas 200 millones de páginas de datos, incluyendo el contenido completo de la Wikipedia. Es de notar que “Watson” es autónoma y no está conectada a la red Internet. Es de notar también que “Watson” tiene una conexión con San Luìs Potosí, pues en el diseño del procesador Power 7 participó un egresado de la UASLP, quién desarrolló el sistema de corrección de errores que protege a la memoria de la supercomputadora.  Se han planeado ya aplicaciones para la tecnología desarrollada en la construcción de “Watson”. Se ha propuesto, por ejemplo, aprovechar la capacidad de análisis de textos de la supercomputadora para leer e interpretar artículos científicos y así resolver el problema que experimentan los investigadores debido al crecimiento acelerado en el número de artículos científicos, lo que hace difícil o imposible procesarlos de manera adecuada. Se ha pensado también que podrá ser utilizada en el diagnóstico de enfermedades. Para esta aplicación la supercomputadora “leerá” toda la información médica disponible en artículos de investigación, los almacenará en su memoria y los utilizará para emitir un diagnóstico.Según los expertos, se avizoran problemas tecnológicos a resolver en los próximos años para incrementar aun más la capacidad de cómputo de los procesadores. Hay quienes piensan, sin embargo, que las computadoras superarán la capacidad del cerebro humano hacia el final del siglo actual. Si este fuera el caso, en el curso de pocos siglos habríamos transitado desde un estado de desarrollo social en el que se quemaba a los herejes que ponían en duda nuestra posición en el Universo, a una sociedad con la suficiente tecnología para construir máquinas que nos superen intelectualmente –al estilo de HAL, la computadora de la película “2001 Odisea del Espacio” que mata a los tripulantes de la nave espacial que estaba a su cuidado. Aunque no es posible predecir el futuro, el éxito de la supercomputadora “Watson” en el concurso “Jeopardy” es una indicación de el futuro será testigo de máquinas que nos superen en inteligencia.",
    "El Siglo XXI nos ha traído un crecimiento vertiginoso en el volumen de tráfico conducido a través de la red Internet, que se ha convertido en una herramienta de primera necesidad en numerosas actividades cotidianas. Internet, con su enorme capacidad para trasmitir información, ha modificado nuestros hábitos y costumbres y facilitado numerosas tareas, algunas de las cuales de otro modo hubieran sido imposibles de llevar a cabo. Incluso ha tenido consecuencias políticas en varias regiones del mundo –la última en Egipto, en donde el pasado viernes las redes sociales, basadas en Internet,  precipitaron la dimisión del Presidente.  ¿De qué dimensión es el volumen de información trasmitido por Internet?  De manera más general\t ¿cuál es la capacidad de la sociedad actual para guardar y comunicar información? Estas preguntas son abordadas en un artículo publicado el pasado 10 de febrero en la revista “Science”, escrito por Martin Hilbert de la Escuela de Comunicación y Periodismo de la Universidad del Sur de California y Priscila López de la Universidad Abierta de Cataluña. Entre otras cosas, encuentran que si se vaciara en discos compactos toda la información de que disponíamos hasta 2007 –en libros, fotografías, videocasetes, discos duros de computadoras, discos ópticos, etc.– y se apilaran los discos grabados uno sobre el otro, se alcanzaría una altura que sobrepasaría a la Luna.   En otra conclusión, que resulta igualmente sorprendente, los autores del artículo referido estiman que durante 2007 se envió por televisión y otros medios “unidireccionales” un volumen de información –gran parte de la cual seguramente muchos estaríamos de acuerdo en que no tiene gran valor– equivalente al que resultaría si cada persona en el mundo recibiera 174 periódicos diarios –que, por supuesto, no podría leer.  Los números anteriores, además, están creciendo a ritmo acelerado. El volumen de información almacenada se duplica cada 34 meses, mientras que el volumen de información trasmitida lo hace cada 40 meses. Hilbert y Priscila López estimaron también la capacidad de computación del mundo –por medio de la cual se transforma la información–, la que calculan crece de manera exponencial, doblándose cada 18 meses, la mitad del tiempo que le toma hacerlo a la capacidad de almacenamiento y al volumen de información trasmitida. En este sentido, Hilbert señala que más que una revolución en las comunicaciones, de la que comúnmente se habla, estamos ante una revolución en la computación.No obstante, y a pesar de lo asombroso de los números anteriores, nuestras capacidades tecnológicas para guardar, comunicar y transformar información resultan insignificantes comparadas con las correspondientes capacidades de la Naturaleza. En efecto, en el artículo referido, los autores concluyen que el número de operaciones por segundo que pueden realizar todas las computadoras del mundo no es muy diferente al número de impulsos nerviosos ejecutados por el cerebro humano en un segundo. De la misma manera, el volumen total de información almacenada en la actualidad en el mundo es significativamente menor que la información contenida en todas las moléculas de ADN de un humano. Desde el punto de vista del almacenamiento y transformación de la información, de acuerdo con Hilbert no somos entonces sino “aprendices” de la Naturaleza.Aprendemos y avanzamos, no obstante, muy rápido –mucho más que la Naturaleza– y Hilbert estima en una entrevista publicada por la revista “Scientific American” que hacia finales del siglo actual nuestra capacidad de cómputo será equivalente a la de todos los cerebros humanos en conjunto. Estima, igualmente, que el volumen de información almacenada en el mundo equivaldrá en esos momentos al del ADN combinado de todos los seres humanos. Quedarían por verse, por supuesto, lo acertado de estas predicciones.   Al margen de especulaciones sobre el futuro tecnológico del mundo,  sin embargo, Hilbert –que es un científico social– hace consideraciones interesantes en la entrevista concedida a “Scientific American”. Entre otros comentarios, señala que se están gastando en el mundo 3.5 millones de millones de dólares en mejorar nuestros actuales dispositivos para almacenar, comunicar y transformar información, al mismo tiempo que hay países de África que dedican solamente 50 dólares al año por persona en educación. Así, en el entrenamiento de una persona –una  “máquina” que tiene una capacidad de procesamiento de información muy superior a la de nuestros actuales dispositivos tecnológicos– se dedican recursos que en esta perspectiva –y en otras, por supuesto– resultan absurdamente pobres.  En este respecto es muy válida la pregunta que hace Hilbert: “¿Qué pasaría con la evolución social si empezáramos finalmente a explorar las capacidades informáticas de la humanidad?”.",
    "El Río Amazonas, que nace en los Andes peruanos y desemboca en el Océano Atlántico después de recorrer 6,800 kilómetros, es el más caudaloso –y también el más largo– del mundo. Vierte anualmente en el Océano Atlántico el equivalente al 20% de las reservas mundiales de agua dulce. La cuenca amazónica cubre una extensión de 7 millones de kilómetros cuadrados de los cuales el 80% –casi tres veces la superficie de nuestro País– corresponden a la selva amazónica. Dadas sus enormes proporciones, lo que ocurra en la selva amazónica puede llegar a tener un impacto global. En este respecto, en los años 2005 y 2010 la cuenca del Amazonas sufrió sequías severas que se suponía podrían producirse solamente “cada cien años”. Estas sequías altamente atípicas provocaron una baja considerable en los niveles de algunos afluentes del Río Amazonas y en relación a esto, nos encontramos en los medios escritos con numerosas fotografías de embarcaciones y peces muertos yaciendo sobre lechos secos de ríos. Esto ha tenido consecuencias graves para las poblaciones ribereñas que usan los ríos como vías de comunicación. Iquitos, por ejemplo –ciudad en la selva amazónica peruana que tuvo un gran desarrollo hace un siglo gracias a la fiebre del caucho y que cuenta en la actualidad con casi medio millón de habitantes–, solo tiene comunicación con el exterior por vía fluvial o aérea y sufrió grandemente las sequías de 2005 y 2010. De la misma manera, la sequía del año pasado redujo el nivel del Río Negro –afluente del río Amazonas, que confluye con el mismo en la ciudad brasileña de Manaos– a su nivel más bajo desde que se empezó a medir hace cien años. Esto lo convirtió en innavegable, aislando a las poblaciones asentadas en sus orillas.En una escala global, las consecuencias que las sequías de 2005 y 2010 puedan tener sobre nuestro planeta están aún por verse. Hay que notar, sin embargo, que la selva amazónica  –llamada el “pulmón del mundo”– tiene un papel relevante como regulador del dióxido de carbono en la atmósfera –causante del cambio climático– y que dichas sequías pueden comprometer su capacidad para seguir actuando como tal. Sabemos que a través del proceso de fotosíntesis las plantas absorben el dióxido de carbono de la atmósfera y lo convierten –juntamente con otros nutrientes– en materia orgánica. La selva es entonces un sumidero de dióxido de carbono. La sequía, sin embargo, produce la muerte de un cierto porcentaje de los árboles de la selva, reduciendo su capacidad como “pulmón” del planeta. Además, al descomponerse, los árboles muertos liberan dióxido de carbono contribuyendo positivamente al incremento de la concentración de este gas en la atmósfera. Un factor adicional en este sentido son los incendios forestales intencionales empleados en la deforestación, los cuales son agravados por la sequía.  En un artículo publicado el pasado 4 de febrero en la revista “Science”, un grupo de investigadores británicos  y del Instituto de Investigaciones Ambientales de la Amazonia de Brasil, reportan un estudio de la sequía de 2010 empleando datos de satélite. Concluyen que afectó a un área de 3 millones de kilómetros cuadrados de selva –una vez y media la superficie de México– principalmente en tres regiones: suroeste de la Amazonía, norte-centro de Bolivia y el estado brasileño de Mato Grosso. En comparación, la sequía de 2005 fue menos severa, afectando una superficie de poco menos de 2 millones de kilómetros concentrada en el suroeste de la cuenca del Amazonas.   Aunque no está claro el origen preciso de los eventos de 2005 y 2010 en la Amazonia, los autores del artículo referido arguyen que varios modelos de clima predicen un incremento, tanto en frecuencia como en severidad, de las sequías en la región amazónica por el aumento paulatino en la concentración de gases de invernadero en la atmósfera, la cual ha venido ocurriendo desde hace medio siglo por la quema de combustible fósiles. Las sequías podrían entonces estar originadas, en último término, por acciones nuestras. No hay, sin embargo, resultados definitivos en este respecto y el asunto es todavía motivo de discusión entre los expertos. Es,no obstante, motivo de preocupación que un evento de sequía que anteriormente se pensaba podría ocurrir una vez cada cien años, se haya repetido en solamente cinco. La preocupación aumenta al leer la frase con la que se cierra el artículo de los investigadores británicos y brasileños “Si los eventos de sequía continúan, la era de una selva amazónica intacta que actúa como un amortiguador del incremento de dióxido de carbono en la atmósfera podría ser cosa del pasado”.",
    "En días pasados Egipto solicitó a Alemania –una vez más desde 1930– la devolución del famoso busto de la reina Nefertiti, que tiene más de 3,300 años de antigüedad y que ha permanecido en el país europeo desde 1913. Dicho busto fue descubierto por el arqueólogo alemán Ludwig Borchardt en  Amarna, en la ribera oriental del Río Nilo, en 1912 y trasladado a Alemania en base a un acuerdo establecido en el sentido de dividir entre Egipto y Alemania las piezas arqueológicas que fueran descubiertas durante las excavaciones de Borchardt. Zahi Hawass, jefe del Consejo Superior de Antigüedades de Egipto, sin embargo, sostiene que recientemente descubrieron documentos que demuestran que los egipcios fueron engañados por el arqueólogo alemán, quién habría cubierto de barro el busto de Nefertiti para ocultar su importancia histórica y artística, y apartarlo de este modo de la atención de los egipcios.   Lo anterior no es reconocido por las autoridades alemanas que afirman, a su vez, contar con documentos que demuestran precisamente lo contrario. Sostienen que el busto pertenece de manera legal a su país y se niegan a devolverlo a su lugar de origen. Al respecto, el alcalde de Berlín, ciudad en donde se encuentra el museo de antigüedades egipcias que aloja al busto de la reina, declaro que “Nefertiti se queda en Berlín. No se saqueó nada. Hubo un acuerdo justo y no hay razón para devolverla a Egipto”.  No es claro quién tiene la razón, pero una simple vista de la imagen del busto policromado de Nefertiti, de rasgos perfectos y cuello alargado, y que ejerce de manera inmediata una atracción casi magnética, nos convence que de haberlo apreciado en toda su magnitud los egipcios no podrían haber estado de acuerdo en que saliera de su país –a menos, por supuesto, de que hubieran existido otro tipo de motivaciones.   La solicitud egipcia a Alemania, por otro  lado, no ha resultado oportuna dados los súbitos problemas políticos que ha experimentando Egipto en los últimos días y que podrían reforzar los argumentos en contra de la devolución del busto. Según la agencia Reuters, algunas de las escenas de protesta más violentas se han dado en plazas y calles cercanas al Museo Egipcio del Cairo y aunque se reporta que el ejército egipcio ha resguardado sus instalaciones, no está claro si están fuera de peligro.Al respecto, nos viene a la mente el saqueo que sufrió el Museo Nacional de Irak cuando se produjo la toma de Bagdad por el ejército norteamericano en abril de 2003. De acuerdo con informaciones periodísticas de la época, dicho museo –que aloja piezas arqueológicas que representan cinco mil años de historia del lugar que fue la cuna de nuestra civilización– sufrió un saqueo extensivo a lo largo de tres días durante los cuales fueron robados 170,000 objetos –número que ha sido reducido a 15,000 en estimaciones recientes–. Este saqueo fue posible debido a que el ejército norteamericano no protegió el museo a pesar de la solicitud en ese sentido de los directivos del mismo.  En el saqueo, además, participaron ladrones que sabían lo que robaban y que iban preparados para abrir cerraduras y para transportar objetos de varios cientos de kilogramos de peso. Era de esperarse, entonces, que muchos de los objetos robados aparecieran posteriormente en el mercado negro internacional de antigüedades. En efecto, en el año de 2006 el gobierno norteamericano recuperó en los Estados Unidos y entregó a Irak la estatua de un rey sumerio que había sido robada del museo de Bagdad. La estatua tiene un peso de cientos de kilogramos y una antigüedad de 4,400 años. Los objetos arqueológicos son parte de la herencia cultural de un pueblo y desde este punto de vista, independientemente de cómo el busto de Nefertiti haya salido de Egipto –legal o ilegalmente, con o sin el consentimiento del gobierno egipcio– debe  regresar a su lugar de origen. Hay quién defiende, por el contrario, que debería estar en el lugar en el que pueda ser visto por el mayor número de personas, que en este caso posiblemente sea Berlín. Quien así piensa, es posible que también considere válido el argumento de la seguridad de la pieza arqueológica, que en estos momentos seguramente será mayor en Berlín que en el Cairo.Esto, sin embargo, no ha sido siempre así. Durante la Segunda Guerra Mundial el busto de Nefertiti tuvo que ser retirado en 1943 del museo en Berlín en donde se le mantenía y puesto en un lugar más seguro. Esta decisión fue afortunada pues el museo fue posteriormente destruido.  Tendríamos entonces que ser cuidadosos con los argumentos para justificar que objetos que son parte de la herencia cultural de un pueblo –bustos, estatuas, obeliscos, frisos etc.– residan fuera de su lugar de origen.",
    "Imagínese ingiriendo sin límite todos los días la comida que más le gusta hasta sentir que ha saciado su apetito –comida que quizás sea alta en carbohidratos o grasas de todo tipo y esté rebosante de colesterol–; imagine, además, que el comer sin límite no le trae consecuencias graves a su salud. Todo esto suena bien, pero, por supuesto, va en contra de la evidencia médica con que se cuenta. Y sin embargo, un estudio publicado recientemente en la revista “Science” (10 de diciembre de 2010) por investigadores  de la Universidad Carnegie Mellon, en los Estados Unidos, encabezados por Carey Morewdege, nos indica que después de todo quizás sea posible comer todo lo que nos gusta sin atentar contra nuestra salud. Esta es la noticia buena, la mala es que parte de la comida que ingiramos tendría que ser virtual; es decir, estaría solamente en nuestra imaginación. En el estudio de referencia, que lleva por título “Pensamientos en lugar de comida: al imaginar el consumo se reduce el consumo real”, se concluye que imaginar de manera repetida que se ingiere un determinado alimento resulta al final en un menor consumo del mismo. De este modo, si se quiere disminuir la ingesta real de un alimento en particular, sin llegar a padecer hambre, bastaría con imaginar varias veces que lo consumimos. Estas conclusiones fueron  alcanzadas mediante una serie de experimentos en las que se pidió a un grupo de personas que imaginaran ingerir un alimento específico, para investigar después como esto afecta al apetito por ese alimento en particular.En uno de los experimentos, un total de 51 personas fueron divididas en tres grupos y se les pidió que imaginaran realizar un total de 33 acciones. A un primer grupo se le solicitó que imaginara insertar de manera secuencial 33 monedas en una máquina lavadora de ropa. A un segundo grupo se le pidió que imaginaran insertar 30 monedas en la lavadora y que además imaginaran ingerir tres lunetas de chocolate de colores –de acuerdo con los autores del estudio, las acciones de imaginar depositar una moneda en la ranura de la lavadora y de ingerir una luneta se procesan de manera similar en el cerebro, de modo que ambos grupos de personas llevaron a cabo un mismo número de operaciones–. Un tercer y último grupo imaginó insertar tres monedas en la máquina lavadora e imaginó ingerir 30 lunetas. Al final, a los tres grupos de personas se les presentó una copa con 40 gramos de lunetas y se les permitió comer tantas como quisieran.Se encontró que los dos primeros grupos –el que no imaginó consumir lunetas y el  que imaginó consumir tres–, ingirieron aproximadamente la misma cantidad de “lunetas reales”. El “consumo” de solamente tres “lunetas virtuales” no afecto entonces el apetito por las mismas. En contraste, el grupo que “ingirió” 30 “lunetas virtuales” consumió solamente la mitad de las “lunetas reales” de los dos primeros grupos. Cuando  comemos un alimento que nos apetece –por ejemplo, una barra de chocolate– nos sabe mejor el primer bocado que el décimo. Esto es debido a que en la medida en que consumimos el alimento nos “habituamos” al mismo, disminuyendo nuestro deseo de seguir ingiriéndolo. Los resultados de Morewedge y colaboradores nos indican que podemos habituarnos a un determinado alimento no solamente si lo consumimos en forma real, sino también si solamente lo hacemos en nuestra imaginación. De acuerdo con los autores del artículo de referencia, sus resultados pueden aplicarse al desarrollo de métodos más efectivos para reducir el ansia por el consumo de drogas, así como por el consumo de alimentos no saludables. Como parte de una dieta, nuestro apetito podría ser entonces saciado de manera sana mediante una combinación juiciosa de “comida real” –como es lo usual– y de “alimentos virtuales”. Antes de sentarnos a la mesa podríamos entonces imaginar que comemos tal o cual alimento de manera repetida –¿treinta veces?– y esto –de acuerdo al trabajo de Morewedge y colaboradores– bastaría para que disminuyamos la cantidad de dicho alimento que después ingiramos. Hay que hacer notar, sin embargo, que tendríamos que hacer la misma operación para cada tipo de comida que vayamos a consumir, pues el mecanismo por el que nos habituamos a determinado alimento –de manera tanto real como virtual–, funciona solamente para ese alimento en particular. ¿Sería este un procedimiento práctico para bajar de peso? No es claro que lo sea, pero si tenemos problemas de sobrepeso y suficiente tiempo antes de la comida, quizás valdría la pena probarlo.",
    "¿Qué  tan severas pueden ser las consecuencias del cambio climático? Nadie lo sabe con seguridad, pero pudiera ser que muchos de los acontecimientos que forjaron la historia europea en los últimos dos mil años –incluyendo la caída del Imperio Romano de Occidente y el surgimiento de la Muerte Negra que asoló a Europa en la Edad Media–, hayan estado ligados en cierto grado a variaciones climáticas relativamente rápidas. Cuando menos a esta conclusión llegó un grupo de investigadores europeos y norteamericanos –encabezados por Ulf Buntgen del Instituto Federal Suizo de investigación sobre Bosque y Nieve–, en un artículo publicado el pasado 13 de enero en la revista “Science”. En dicho artículo, Buntgen y colaboradores reportan un estudio acerca de la variabilidad del clima europeo a lo largo de los últimos 2,500 años. Por causas naturales, el clima de nuestro planeta está continuamente sufriendo cambios. Sabemos, por ejemplo, que durante la Edad Media la temperatura terrestre tuvo un valor promedio relativamente alto, lo que fue seguido por un periodo, conocido como “Pequeña Edad de Hielo”, durante el cual dicha temperatura tuvo un ligero descenso. Estos cambios, sin embargo, fueron lentos y ocurrieron a lo largo de cientos de años. Buntgen y colaboradores, por el contrario, estaban interesados en averiguar cómo varió el clima año con año a lo largo del último par de milenios. Para averiguarlo, estudiaron más de nueve mil muestras de madera de roble provenientes de Europa Central, tanto de ejemplares vivos, como de árboles muertos que vivieron a lo largo del periodo de interés. Sabemos que el tronco de un árbol muestra anillos concéntricos cuyo número corresponde a su edad. Los anillos aparecen debido a que en latitudes con un marcado contraste entre estaciones, hay una gran variabilidad a lo largo de año en la velocidad con la que un árbol engrosa su tronco, lo que a su vez produce cambios en la densidad de la madera a lo largo del anillo, marcando así su inicio y su final. Además de indicarnos su edad, los anillos contienen información acerca de las condiciones climáticas en las cuales creció el árbol. Para desentrañarla, Buntgen y colaboradores llevaron a cabo un estudio sobre cómo la humedad y la temperatura afectan el crecimiento de los anillos y con esta información pudieron reconstruir la historia climática de un árbol dado. Un árbol, por supuesto, nos cuenta su propia historia. Sin embargo, si combinamos las historias –traslapadas en el tiempo– de un conjunto de árboles crecidos a lo largo del periodo de interés, tendremos la historia completa. Esto fue lo que hicieron Buntgen y colaboradores para los últimos 2,500 años. Encontraron que existe una correlación entre periodos de tiempo durante los cuales ocurren variaciones climáticas marcadas, y la ocurrencia de eventos históricos de gran magnitud. Encontraron, por ejemplo, que el Imperio Romano de Occidente prosperó durante una época relativamente húmeda y cálida al inicio de nuestra Era, mientras que la invasión de los bárbaros, que llevó al colapso de dicho Imperio, ocurrió durante un periodo en que la humedad y la temperatura tuvieron un descenso marcado. Encontraron también que la aparición de la epidemia conocida como “Muerte Negra”, que devastó a Europa durante los años 1348-1351 matando a cerca de la mitad de la población, estuvo acompañada de un descenso en la temperatura. Lo mismo ocurrió en el Siglo XIX con las migraciones de Europa a los Estados Unidos.El estudio de Buntgen y colaboradores es, por supuesto, relevante dadas las condiciones actuales de incremento acelerado en la temperatura global que está experimentando nuestro planeta, y podría ayudarnos a entender las consecuencias que este incremento pueda tener en el futuro.Como lo señalan los autores del artículo de referencia, la historia humana es lo suficientemente compleja como para considerarla producto de una sola causa –el cambio climático–, y por el contrario debe ser explicada tomando también en cuenta otro tipo de “factores socio-culturales que interactúan con el cambio climático de manera compleja”. Y, sin embargo, mencionan que: “Aunque las poblaciones modernas son potencialmente menos vulnerables a las fluctuaciones del clima que lo que lo fueron las sociedades del pasado, no son ciertamente inmunes a los cambios en temperatura y precipitación que se predicen, especialmente considerando que las migraciones a hábitats más favorables como una respuesta adaptativa no serán una opción en un mundo cada vez más poblado.”Para nosotros, sin duda, los hallazgos de Buntgen y colaboradores son especialmente relevantes, dadas las sequías que se pronostican ocurrirán en nuestro País en el futuro.",
    "En días pasados los medios de comunicación dieron a conocer los resultados de una encuesta sobre percepción pública de la ciencia en México. La encuesta fue llevada a cabo en forma conjunta por el CONACyT y el INEGI en el año 2009. Como resultado, entre otras cosas, ahora sabemos que más de un 80% de nuestros compatriotas piensa que confiamos mucho en la fe y muy poco en la ciencia. Sabemos también que casi un 60% piensa que los científicos pueden ser “peligrosos” –no está claro en que sentido– debido a sus conocimientos. En una aparente contradicción, sin embargo, cerca del 80% opina que en México debería haber más personas trabajando en áreas de investigación. Como quiera que sea y al margen de encuestas, contradicciones o interpretaciones, es claro que en nuestro País están muy extendidas las creencias y prácticas que se califican como no científicas, incluyendo la cura de enfermedades con hierbas y conjuros, la astrología y las dietas mágicas.La revista “Science” hizo eco de la encuesta del CONACyT en un artículo publicado el pasado 5 de enero al que tituló, “Encuesta: los mexicanos afirman creer en espíritus, no en ciencia”. Si bien el  cuerpo de artículo describe de manera razonablemente equilibrada los resultados de dicha encuesta, el título  resulta algo  tendencioso, pues las creencias y prácticas esotéricas no son exclusivas de nuestro País. En días pasados, por ejemplo, nos enteramos por los medios de comunicación que las brujas en Rumania están muy irritadas porque a partir del pasado primero de enero tienen que pagar impuestos por las ganancias que les genere su práctica. Argumentan que ganan muy poco –unos 110 pesos por consulta– y que gravarlas resulta estúpido. Su irritación es tal que han amenazado con lanzar hechizos y conjuros sobre el Presidente rumano, incluyendo el uso de excremento de gato y perros muertos. Amenazan también con arrojar mandrágora en el Rio Danubio. El Presidente, por su lado, aparentemente se viste de morado ciertos días de la semana para mantener a raya al diablo.En los Estados Unidos las prácticas esotéricas no son tampoco algo inusual. En una encuesta Gallup del año 2005 se encontró que tres de cada cuatro estadounidenses cree en la existencia de los fenómenos paranormales. Así, un 41% de la población estadounidense cree en la percepción extrasensorial, un 37% en las casas encantadas, un 32% en fantasmas y espíritus, y un 25% en la astrología. Las creencias esotéricas no son, entonces, enteramente producto de un pobre nivel educativo –como el que desgraciadamente tiene nuestro País– y se presentan igualmente en los países desarrollados.La brujería, la caza de fantasmas o la astrología son actividades que por lo general no pretenden pasar por científicas –por más que, en su caso, esta última haya sido el antecedente de la física moderna–. En una encuesta realizada en el año 2008 por la Fundación Nacional de la Ciencia norteamericana, por ejemplo, el 63% de los encuestados manifestó que la astrología “no tenía nada de científica”, mientras que el 28% creía que era “algo científica”.  En contraste, otras áreas como el creacionismo o los esfuerzos para negar el cambio climático, que no siguen una metodología científica, sí tienen esta pretensión. Este tipo de áreas, que califican como seudociencias tienen en  los Estados Unidos un buen número de seguidores, no obstante que es el país que lleva el liderazgo tecnológico en el mundo. En este punto es interesante recordar el llamado incidente de Roswell, que involucró el supuesto accidente de una nave extraterrestre en Roswell, Nuevo México en 1947. De acuerdo con esta historia, que fue alimentada con propósitos probablemente comerciales, la fuerza área norteamericana estaría en posesión de la nave accidentada, incluyendo los cadáveres de sus ocupantes. Lo curioso es que Roswell no está muy lejos de Trinity Site, en donde se llevó a cabo en julio de 1945 la primera explosión atómica de la historia. Resulta curioso porque la explosión en Trinity Site –que fue resultado del mayor esfuerzo científico jamás realizado– no hay duda que ocurrió, a diferencia del incidente de Roswell que se manejó a base de rumores y suposiciones. No obstante, para algunas personas son igualmente reales ambos episodios.México es un país que tiene ciertamente muchas carencias educativas que resultan en creencias y prácticas esotéricas. Éstas, no obstante, no son exclusivas del mundo en desarrollo y de ahí que el título del artículo de “Science”, que busca caracterizarnos como habitantes del Medioevo, no es de apreciarse.No es afortunado tampoco el último párrafo que reza: “De acuerdo con la encuesta mexicana, los científicos son tanto temidos como respetados, quizás no de manera diferente a los viejos sacerdotes aztecas.” Es desafortunado porque, para empezar, la mayor parte de los mexicanos no son descendientes de los aztecas sino  de otros grupos indígenas.",
    "El último mes del año que acaba de concluir nos trajo una controversia científica que, como signo de los tiempos, se ha dado en gran medida a través de blogs de Internet. La controversia tuvo su origen en un artículo publicado el pasado 2 de diciembre en la revista “Science” por un grupo de científicos de diversos centros de investigación en los Estados Unidos, incluyendo  el Laboratorio de Astrobiología de la NASA, en el que se reportan los resultados de una investigación llevada a cabo con bacterias extraídas del Lago Mono en California. Los investigadores encontraron que dichas bacterias pueden crecer en un ambiente rico en arsénico –que es el caso del Lago Mono– y que, además, son capaces de incorporar este elemento químico –que sabemos tiene un alto grado de toxicidad en condiciones normales– a su material biológico en sustitución del fósforo. A raíz de su publicación, sin embargo, el artículo de referencia ha recibido muchas críticas de parte de otros especialistas –publicadas en medios masivos de comunicación y en blogs de Internet–. Los críticos reconocen que, si bien el haber encontrado bacterias que pueden tolerar muy altas concentraciones de arsénico constituye un descubrimiento importante por sí mismo, no consideran que los autores hayan proporcionado suficiente evidencia de que dichas bacterias le den un uso biológico –fabricando proteínas e incorporándolo incluso a su ADN.En condiciones normales estas críticas no tendrían nada extraordinario y más bien serían parte del proceso estándar de validación por el que debe pasar cualquier resultado científico –que debe sortear todos los obstáculos y aclarar todas las dudas que suscite antes de ser aceptado por la comunidad científica–. Lo que en buena medida hace especial al caso presente es la manera como la NASA decidió presentarlo: por medio de una conferencia de prensa en la que se darían a conocer resultados de “un descubrimiento astrobiológico que impactará la búsqueda de evidencia de vida extraterrestre”,      La revista británica “Nature” –competidora de “Science” y una de las revistas científicas más prestigiosas– ha sido especialmente crítica de las “fanfarrias” y  “trompetas” con que la NASA anunció el descubrimiento, y hace notar que era esperable que dicho anuncio provocara reacciones “debido a que muchos científicos piensan que la NASA es dada a hacer anuncios extravagantes en el campo de la astrobiología”.  “Nature” es también crítica de la actitud de silencio que tomaron los autores del artículo a raíz de las críticas recibidas. En un editorial publicado el pasado 15 de diciembre que tituló “Se requiere una respuesta”, “Nature” escribe: “Quizá usted haya visto que científicos de la NASA se atribuyen haber descubierto una bacteria que puede reemplazar el fósforo en su ADN con arsénico. Quizá haya oído que esto pudiera ayudar en la búsqueda de extraterrestres. Incluso quizá oyó que dicha bacteria es por sí misma un extraterrestre. Lo que usted no ha visto u oído es una respuesta detallada de la NASA y de los científicos involucrados a las críticas que se han hecho a su trabajo. A la vista de la atención mundial en el artículo, que la NASA y los autores fomentaron de manera deliberada, los investigadores han metido la cabeza en la arena digital”.    Felisa Wolfe-Simon, primer autor del artículo referido y quien  había  guardado silencio alegando que la discusión debería llevarse a cabo, no en los medios masivos de comunicación o en blogs de Internet, sino en el seno de las revistas científicas, como tradicionalmente se ha hecho, publicó en su blog personal el pasado 16 de diciembre –un día después de la editorial de “Nature”– un alegato apoyando su artículo original. En el mismo se incluyó, sin embargo, la frase: “las bacterias del Lago Mono pueden sustituir un pequeño porcentaje de su fósforo por arsénico”, en lugar una sustitución total como podría haberse entendido de la conferencia de prensa original.El caso NASA-Wolfe-Simon incluye un aspecto de relativa novedad en el campo de la discusión científica que es el relativo al uso de la blogosfera para dirimir una controversia –cuya utilidad, por otro lado, estaría por demostrarse–. No obstante, la blogosfera, por su naturaleza informal, no podrá sustituir a la publicación formal en revistas científicas. Con seguridad durante el año que acaba de iniciar tendremos resultados de otros investigadores –que se publicarán en revistas científicas y no en blogs personales– confirmando o desmintiendo los hallazgos de Wolfe-Simon y colaboradores. Solamente después de este proceso se podrá establecer de manera sólida la existencia de bacterias que se alimentan de arsénico. Con relación a este procedimiento, por supuesto, no hay nada nuevo bajo el Sol. Tampoco lo habría, quizás, en cuanto a exagerar la importancia de una investigación, ya que de los resultados obtenidos podría derivar la consecución futura de fondos para continuarla. Parecería, sin embargo, que en el caso presente se les pasó un poco la mano.",
    "El pasado 11 de diciembre concluyó en Cancún, QR,  la convención de la ONU sobre cambio climático. Esta convención siguió a la realizada a finales del pasado año en Copenhague, Dinamarca, la cual fue marcada por el poco entendimiento entre los países desarrollados y los países en desarrollo sobre los compromisos a establecer para combatir el cambio climático. En esta ocasión, dada la experiencia de Copenhague, no  había grandes expectativas  con respecto a los acuerdos que podrían alcanzarse  y quizá por esto se considera que la conferencia tuvo éxito pese a sus modestos resultados.    De acuerdo con The New York Times, “El acuerdo fue limitado con respecto a los grandes cambios que los científicos dicen que son necesarios para evitar el peligroso cambio climático en las siguientes décadas. Pero establece las bases para medidas más enérgicas en el futuro, si las naciones son capaces de sobreponerse a los argumentos emocionales que han paralizado las negociaciones sobre el cambio climático en años recientes”. Por su parte, The Washington Post, señala que “Algunas de las cuestiones más espinosas tales como el futuro del   Protocolo de Kyoto difícilmente fueron discutidas. La conferencia de Cancún fue más acerca de rescatar el complejo marco de negociación climática de la irrelevancia o incluso del colapso”.  El principal resultado de la conferencia de Cancún fue entonces que no fue un caos y que hubo un mínimo de acuerdos.El hecho, sin embargo, es que dichos acuerdos –que incluyen un fondo de 100,000 millones de dólares de ayuda para que los países en desarrollo combatan el cambio climático– no son obligatorios y por tanto nada asegura que finalmente se cumplan. Según BBC News, “el pacto es mucho menos que el acuerdo comprensivo que muchos países querían en la cumbre de Copenhague del año pasado y que continúan buscando. Deja abierta la pregunta de si sus medidas, incluyendo la reducción de emisiones, serán legalmente obligatorias”.El calentamiento global, por supuesto, avanza independientemente de los desencuentros y discusiones entre países durante las cumbres climáticas. Según estadísticas de la NASA, en las tres últimas décadas la temperatura promedio de la Tierra se ha estado elevando a un ritmo de 0.2 grados centígrados cada diez años. Como resultado, la última década ha sido la más caliente desde 1880, fecha a partir de la cual se tienen registros de temperatura a escala global. Si bien el calentamiento global es un hecho, las consecuencias que traerá a nuestro planeta en las décadas por venir tienen un cierto grado de incertidumbre. Esto es debido a lo complejo del ecosistema terrestre lo que dificulta  las predicciones de los científicos del clima que no siempre se ponen de acuerdo. Como un ejemplo de esto, podemos mencionar el estudio publicado el pasado 10 de diciembre en la revista “Science” por el climatólogo Andrew Dessler de la Texas A&M University, sobre el efecto que tienen las nubes sobre el calentamiento global. Las nubes, dependiendo de las circunstancias, pueden tanto elevar como disminuir la temperatura de la Tierra. En efecto, tenemos que éstas reflejan la luz del Sol que incide sobre nuestro planeta impidiendo que llegue a la superficie y contribuyendo así a disminuir su temperatura; pueden, no obstante, producir el efecto contrario, pues también reflejan el calor que es radiado por la superficie de la Tierra, el cual de otra manera se perdería en el espacio –podemos convencernos de este último fenómeno si consideramos   que las noches claras tienden a ser frías, precisamente por la falta del escudo protector que proporcionan las nubes–. De este modo, dependiendo de cuál de los dos posibles efectos domine, las nubes pueden contribuir tanto positivamente como negativamente al calentamiento global. Dessler en su artículo concluye que el calentamiento global modifica las nubes de tal manera que éstas generan más calentamiento, produciendo así una suerte de círculo vicioso en el que cada ciclo agrava el problema de cambio climático. Esta conclusión, sin embargo, es disputada por Roy Spencer de la Universidad de Alabama quien sostiene que las nubes no son una consecuencia del calentamiento global y que por tanto contribuyen negativamente al mismo al bloquear la radiación del Sol.La controversia Dessler-Spencer tiene, además, otro ingrediente –que no tiene nada que ver con la ciencia del clima– pues este último acusa a la revista “Science” de haber publicado el artículo del primero durante la conferencia de Cancún a fin de influir en sus resultados, cosa que tanto la revista como Dessler niegan.Este último aspecto ejemplifica las dificultades que han enfrentado las reuniones cumbre sobre el cambio climático. Estas dificultades, más que de tipo científico –que la ciencia en todo caso tiene métodos eficientes para resolver– son de naturaleza económica y política. Esto no es sorprendente si consideramos que la energía que mueve al mundo proviene  en casi un 90% de fuentes fósiles de combustible y que un cambio drástico hacia fuentes de energía limpias tendría un elevado costo económico.",
    "Como fue ampliamente difundido, la semana que hoy termina se dieron a conocer los resultados de la prueba PISA que llevó a cabo la Organización para la Cooperación y el Desarrollo Económicos (OCDE) en el año 2009. Esta prueba mide los conocimientos y habilidades de estudiantes de 15 años de edad en tres áreas: lectura, matemáticas y ciencias. La prueba se realiza cada tres años, evaluando siempre las tres áreas, pero poniendo de manera alternada énfasis en una de ellas. La primera prueba PISA se llevó a cabo en el año 2000, enfatizando el área de lectura. Las siguientes se realizaron en 2003 (matemáticas) y 2006 (ciencia). En 2009 se destacó nuevamente el área de lectura.En la prueba PISA 2009 se consideraron siete niveles de calificación para la habilidad lectora. El nivel 6 corresponde a la máxima calificación alcanzable, mientras que los niveles 1a y 1b son para los desempeños más bajos. De acuerdo a la OCDE, los estudiantes que alcanzan los niveles 5 y 6 “forman una reserva de talento que ayudará a los países a competir a nivel global en la economía del conocimiento”. El nivel 2, por otro lado, es considerado el mínimo aceptable, de tal manera que los estudiantes que no logran alcanzarlo “batallan para realizar actividades de la vida diaria que involucran la lectura”; además de que “la evidencia de anteriores pruebas PISA muestra que es improbable que esos estudiantes adquieran el hábito de aprender de manera permanente o de tener un buen desempeño en el mercado de trabajo”.                                                              Juntamente con los 34 países miembros de la OCDE –organización de la cual México es miembro desde 1994–, participaron en la prueba PISA-2009 28 países no miembros de esta organización, además de las ciudades chinas de Shangai, Hong Kong y Macao, en calidad de “economías colaboradoras”.  La prueba PISA-2009 fue dominada por los países asiáticos y las “economías colaboradoras” chinas. Así, encontramos que, integrando las tres áreas, lectura, matemáticas y ciencia,  los cinco primeros lugares fueron ocupados, en ese orden, por Shangai, Corea del Sur, Finlandia, Hong Kong y Singapur. Cabe también destacar que en el área de matemáticas, Shangai superó ampliamente al segundo lugar que fue Singapur. México en contraste, ocupó el lugar 48 entre los 65 participantes y el último lugar entre los 34 países miembros de la OCDE. Cabe señalar, sin embargo, que los resultados que ha obtenido nuestro país desde el año 2000 muestran una evolución positiva en el área de matemáticas, evolución que la OCDE clasifica por como “estadísticamente significativa” –es decir, que de realizarse nuevamente, no produciría resultados radicalmente diferentes–. México también muestra una evolución positiva en las áreas de lectura y de ciencia. En estos casos, no obstante, la OCDE no otorga el mismo grado de confiabilidad a los resultados por el margen reducido de mejora en los indicadores; esto es sobre todo cierto para el área de ciencia.   El papel de México a lo largo de las pruebas PISA está analizado en un documento de la OCDE que se ocupa de manera expresa de nuestro país. En dicho documento la OCDE destaca, por ejemplo, que México es el país con el incremento absoluto mas grande en el desempeño en matemáticas entre 2003 y 2009. Hay que notar, no obstante, que en términos absolutos la calificación de los estudiantes mexicanos es la más baja dentro de grupo de la OCDE, de modo que la mejora en su desempeño, si bien es grande en términos relativos, de manera absoluta sigue siendo muy pobre. Consideraciones similares pueden hacerse con respecto a las áreas de lectura y ciencia. Las estadísticas pueden también enfocarse a los dos grupos en los extremos de la escala: los que están por abajo del nivel 2 y aquellos en los niveles 5 y 6. Estas estadísticas muestran también la gran distancia que hay entre nuestro país y otros miembros de la OCDE. Así, mientras que en México el porcentaje de estudiantes que están por abajo del nivel 2 en el área de matemáticas es cercano al 50%, en países como Finlandia y Corea del Sur dicho porcentaje está por abajo del 10%. De manera similar, tenemos que México es el país de la OCDE que en 2009 tuvo en matemáticas el menor porcentaje –inferior al 1 %– de estudiantes en los niveles más altos. En contraste, entre el 25 % y el 50% de los estudiantes de Taiwán, Hong Kong y Shangai califican en los niveles 5 o 6. Los estudiantes de la escuela secundaria en México, de acuerdo al informe PISA, han mejorado sus habilidades en matemáticas en la última década, mientras que en las áreas de lectura y ciencias podemos decir que no hay evidencias de que hayan empeorado –lo cual ya es ganancia–. Dado su bajo desempeño promedio, sin embargo, las tendencias positivas tendrán que mantenerse por un buen número de años si hemos de alcanzar los niveles de los países desarrollados, por más que algunos de ellos muestren una caída en sus indicadores.El dicho que reza: “Mas tiene el rico cuando empobrece que el pobre cuando enriquece”, tiene entonces plena validez.",
    "En días pasados se publicaron en la prensa escrita, así como en numerosos sitios de Internet, un buen número de artículos y comentarios en los que se especulaba sobre la posibilidad de que la NASA hubiera encontrado vida extraterrestre. De haber sido esto cierto, habría constituido uno de los mayores descubrimientos científicos de la historia; descubrimiento que habría, además, coronado los esfuerzos de la agencia espacial norteamericana, que por muchos años ha buscado evidencias de vida en el sistema Solar fuera de nuestro planeta. Los artículos referidos tuvieron su origen en un comunicado de la NASA del día 29 del pasado mes de noviembre, en el que citaba a una conferencia de prensa que se llevaría a cabo el 2 de diciembre siguiente en sus instalaciones en Washington, con el objeto de “discutir un descubrimiento astrobiológico que impactará la búsqueda de evidencia de vida extraterrestre”. De manera predecible, un comunicado de prensa redactado de esta manera, aunado a la estrategia de la NASA de darlo a conocer algunos días antes de precisar la naturaleza del descubrimiento –al estilo de una novela de suspenso–, hubo de generar notas periodísticas y comentarios como los que vimos en días pasados.  Al final, el descubrimiento en cuestión –publicado en la revista “Science” por investigadores de varias universidades y agencias de investigación norteamericanas, el mismo día 2 de diciembre en que se llevó a cabo la conferencia de prensa en Washington–, si bien resulto tener una gran importancia científica, distó de satisfacer las expectativas generadas en el público. En particular, no tuvo que ver con extraterrestres, sino con microbios que habitan el Lago Mono, en el estado de California, cerca de los límites con el estado de Nevada.  De manera específica, en dicho artículo –que tuvo como primer autor a Felisa Wolfe-Simon del Instituto de Astrobiología de la NASA– se describen experimentos de cultivos biológicos llevados a cabo con bacterias extraídas del Lago Mono. Este lago se caracteriza por una gran salinidad y concentración de arsénico y las bacterias que en él habitan están, en consecuencia, adaptadas de algún modo a vivir en contacto con este elemento, que sabemos es un veneno letal para los seres vivos . Los experimentos reportados demostraron que las bacterias del Lago Mono pueden efectivamente crecer en ambientes ricos en arsénico –lo hacen igualmente en ambientes ricos en fósforo, como es el caso común–. Lo más interesante, sin embargo, es que en un ambiente rico en arsénico, dichas bacterias incorporan este elemento a su estructura biológica en sustitución del fósforo –incluyendo el ADN–. Así, para las bacterias del Lago Mono, el arsénico, lejos de ser un veneno mortal, es un alimento sustancioso. El arsénico es un elemento con propiedades químicas que guardan una cierta semejanza con las del fósforo. Este hecho es precisamente lo que lo hace comúnmente un veneno para un organismo vivo, pues es capaz de introducirse al mismo de manera subrepticia y una vez dentro causar daño. La similitud de propiedades químicas entre el arsénico y del fósforo, por otro lado, fue lo que motivó a Wolfe-Simon y colaboradores a pensar que el primero podría ser un sustituto del segundo como componente del material biológico, dadas ciertas condiciones ambientales como las que prevalecen en el Lago Mono. Se sabe que el fósforo, juntamente con el oxígeno, el nitrógeno, el hidrógeno, el carbono y el azufre, constituyen los principales componentes de la materia viva. Con el nuevo descubrimiento, resulta que esta lista no es después de todo definitiva; al menos en lo que respecta al fósforo. De este modo, el estudio de Wolfe-simon y colaboradores nos da una indicación de que el fenómeno de la vida es más complejo de lo que se había pensado y que posiblemente podría darse en condiciones ambientales extremas –extremas, por supuesto, desde nuestra perspectiva terrestre–. Así, nuestro catálogo de sitios para buscar vida extraterrestre –por lo pronto dentro del Sistema Solar– no estaría necesariamente limitado a aquellos lugares en los que prevalecen condiciones ambientales análogas a las de nuestro planeta.Si bien la estrategia de la NASA para publicitarlo puede resultar controvertida, el descubrimiento de Wolfe-Simon y colaboradores tiene, después de todo –y aún sin una conexión directa–, relevancia para la búsqueda de vida extraterrestre.  Los resultados de Wolfe-Simon tienen, de este modo, un incuestionable valor científico. Confirman, además, el dicho popular que reza: “Veneno que no mata fortalece",
    "San Luis Potosí está considerado a nivel nacional como un Estado con un desarrollo significativo en investigación científica en un número de disciplinas, particularmente en el área de la Física. Históricamente nuestro Estado fue uno de los primeros en México en donde se cultivó esta disciplina, siendo la Escuela de Física de la UASLP –que  inició sus actividades docentes en marzo de 1956–, la tercera de su tipo en México; solamente la Universidad Nacional y la Universidad de Puebla le antecedieron en este respecto.La historia del inicio de la Física en San Luis Potosí fue precisamente el tema de amena charla impartida en la Facultad de Ciencias de la UASLP el pasado día 23 de noviembre por el Fís. Candelario Pérez Rosales –el profe Cando, como se le llamaba de manera afectuosa–, quien fue uno de los fundadores de la Escuela de Física de nuestra Universidad. Como lo comentó en su charla, esta Escuela fue creada en el año de 1955 por iniciativa del Dr. Gustavo del Castillo y Gama, quien había regresado hacía pocos meses a nuestro País al término de sus estudios doctorales en Física en la Universidad Purdue, en Indiana, Estados Unidos. El Dr. del Castillo fue apoyado en su empresa por Candelario Pérez, recién desempacado en México proveniente también de la Universidad Purdue.Como parte de su plan, además de fundar la Escuela de Física, que se encargaría de la enseñanza de esta disciplina, Gustavo del Castillo creó de manera simultánea el Instituto de Física –que fue el segundo en México, después del Instituto de Física de la UNAM–, iniciando un proyecto de investigación en el área de la física de altas energías. No es difícil imaginar las dificultades que Gustavo del Castillo y Candelario Pérez hubieron de enfrentar para introducir en San Luis Potosí una disciplina que en ese tiempo debió haber lucido “exótica” y fuera de lugar –por más que la Física gozara en la época de un gran prestigio por haber contribuido significativamente a la victoria de los Estados Unidos y sus aliados en la Segunda Guerra Mundial–. Contaron, sin embargo, con el apoyo de la Universidad, de PEMEX y del Instituto Nacional de la Investigación Científica –el antecesor del CONACyT– y en pocos años lograron resultados notables a nivel nacional en cuanto a la fabricación de equipo de investigación, resultados que aun hoy en día se antojan difíciles de igualar. La Escuela de Física –hoy Facultad de Ciencias– estuvo alojada desde su fundación y hasta el año de 1971 en dos salones de la planta alta del Edificio Central del Universidad –la de Física no era, obviamente, una carrera de masas–. En dichos salones se encontraban el salón de clase, la biblioteca, las oficinas del Director y de la secretaria y…. ¡el Laboratorio de Rayos Cósmicos! Este último instalado por Gustavo del Castillo, un constructor de instrumentos de primera línea. Dicho laboratorio consistía esencialmente en un ingenio para detectar rayos cósmicos venidos del espacio exterior –lo que se conocía como una “cámara de niebla”.   La operación de esta cámara –que fue construida en poco más de un año– está descrita por Candelario Pérez en su libro “Física al Amanecer”: “Era impresionante contemplar en acción ese incansable autómata que fotografiaba, día y noche, trayectorias de partículas que eran producto de la radiación cósmica. En medio de una densa y sofocante oscuridad, salpicada por el parpadeo de los focos  indicadores del control automático, se escuchaba la explosión estruendosa que producía el aire al salir repentinamente de la cámara de expansión; luego, un destello cegador iluminaba el interior de la cámara de niebla, al tiempo que la cámara fotográfica registraba los eventos nucleares; después se oía el corrimiento de la película fotográfica y la cámara quedaba en espera del siguiente disparo”.  Gustavo del Castillo, sin embargo, dejó la Universidad en 1959 para aceptar un puesto como investigador en los Estados Unidos y la Física en San Luis Potosí quedó bajo el cuidado de Candelario Pérez –quien, para tal efecto, decidió regresar de Estrasburgo, Francia, en donde acababa de iniciar estudios de posgrado en Física–. El cuidado fue, además, exclusivo, pues durante el año de 1959 se encargó de impartir ¡todos los cursos de la carrera! En ese año crucial, la Física en San Luis Potosí dependió entonces de una sola persona.     En lo personal no tuve la fortuna de coincidir con Candelario Pérez en San Luis Potosí, pues a mi ingreso a la Escuela de Física en enero de 1967, hacia pocos meses que se había trasladado a la Ciudad de México para aceptar un puesto de investigador en el entonces recién creado Instituto Mexicano del Petróleo. Tuve, sin embargo, la suerte de que al finalizar mis estudios, en enero de 1971, me aceptara para trabajar como parte de su grupo de investigación en dicho Instituto, en donde tuve mi primer contacto con esta actividad.Se me dio de este modo la oportunidad de conocer y tratar a una persona que fue clave para el desarrollo que posteriormente experimentó la Física en San Luís Potosí, que la sostuvo en un momento crítico, y sin cuyo concurso la historia de la investigación en esta disciplina en la UASLP hubiera sido muy diferente.",
    "Como es del dominio público, en el manifiesto conocido como Plan de San Luis, emitido desde San Antonio, Texas, Francisco I. Madero convocó al levantamiento armado contra el Gobierno de Porfirio Díaz el 20 de noviembre de 1910, precisamente a las seis de la tarde. Este hecho fue comentado en el programa del pasado lunes del noticiero matutino de Carmen Aristegui, aventurándose la explicación de que el levantamiento había sido fijado a una hora tan avanzada del día a fin de que no interfiriera con la tradicional siesta que se acostumbra tomar en México después de la comida del mediodía. Si bien esta es una posible explicación, una más simple –y por tanto más probable– es que, dadas las inclinaciones espiritistas de Madero, alguno de los espíritus con los que tenía comunicación le haya indicado que tal fecha y tal hora eran las más adecuadas para iniciar la sublevación. En dado caso, no sería la primera vez que nuestra costumbre de tomar siestas haya tenido grandes consecuencias para el País. Como bien sabemos, el 21 de abril de 1936 el  general texano Samuel Houston derrotó a la fuerzas de Antonio López de Santa Anna en la batalla de San Jacinto, cerca de la actual ciudad de San Antonio, Texas, derrota que llevó a la independencia de Texas y, con el tiempo –después de la Guerra México-Estados Unidos–, a la pérdida de la mitad del territorio nacional. En la batalla de San Jacinto las tropas de Santa Anna fueron sorprendidas por Houston cuando se encontraban durmiendo la siesta, siendo derrotadas –y posteriormente masacradas– en sólo 18 minutos, a pesar de doblar en número a las tropas enemigas.Dormir la siesta, sin embargo, no es necesariamente una mala práctica –sólo que hay que escoger el momento adecuado para llevarla a cabo–, como ha sido demostrado en años recientes a través de diversos estudios científicos. Por ejemplo, una investigación llevada a cabo de manera conjunta por la Escuela de Salud Pública de la Universidad Harvard y la Escuela de Medicina de Atenas, Grecia, llevado a cabo con 24,000 hombres y mujeres durante seis años, encontró que aquellos que durmieron siestas de al menos 30 minutos y por al menos tres veces por semana, tuvieron un 37 por ciento menos riesgo de morir de una dolencia cardiaca que aquellos que no lo hicieron. Este porcentaje se elevó a un 64 por ciento entre personas que trabajan.Se ha encontrado también que tomar una siesta breve reduce la fatiga y mejora la concentración y, por consecuencia, la eficiencia en el trabajo. Esto sigue siendo  cierto –aunque menos efectivo– aún si la siesta se toma sentado en una silla y reclinado sobre un escritorio, como lo comenta la Sociedad Británica de Fisiología (Research Digest Blog, abril de 2010). En contra de la opinión prevaleciente en algunos países industrializados, en donde se ha considerado tradicionalmente que las siestas son propias de flojos y haraganes, se ha sugerido incluso que a los trabajadores se les debería dar un cierto tiempo de descanso para que puedan tomar una siesta breve después de la comida del mediodía, con lo que presumiblemente se incrementaría su productividad. Siguiendo esta idea, MetroNap, una compañía con presencia en varios países, ofrece a “compañías medianas y grandes” equipos en renta –sillones de aspecto futurista y con bocinas para escuchar música– diseñados para tomar siestas breves. De acuerdo con la página web de MetroNap, “una siesta de 15-20 minutos puede incrementar la energía por hasta 8 horas” –no queda claro, sin embargo, si MetroNap, tiene dormitorios en sus instalaciones para aumentar la productividad de sus propios empleados–. MetroNap ofrece también la posibilidad de tomar una siesta en sus instalaciones; una de éstas se localiza en el edificio “Empire State” en la ciudad de Nueva York, la cual ofrece siestas de 20 minutos por 14 dólares. El poder de las siestas era aparentemente conocido por Leonardo da Vinci hace ya 500 años, quién, de acuerdo a la leyenda y para aumentar su productividad, habría dormido solamente un total de hora y media por día, en siestas de 15 minutos cada dos horas. De acuerdo con la revista Science (vol 249, 1990), esta práctica de sueño fue adoptada por un artista contemporáneo de 27 años quien pudo mantenerla por seis meses, durmiendo un promedio de 2.7 horas diarias –después de lo cual hubo de suspenderla por “no saber qué hacer con todo el tiempo que le quedaba libre, ya que no era otro Leonardo”.¿Se extenderá en el futuro la práctica de tomar siestas breves en fábricas y oficinas, presumiblemente para aumentar la productividad, y como un bono extra la esperanza de vida? No lo podemos saber en estos momentos, pero de ser este el caso, en México, al igual que en otros países en donde se practica la siesta, nos habremos adelantado a nuestro tiempo.",
    "¿En donde se encuentra la computadora más rápida del mundo? Hasta hace unas semanas la respuesta esta pregunta  habría sido: “En los Laboratorios Nacionales de Oak Ridge, en Tenesí, Estados Unidos”. Ahí, se aloja la supercomputadora “Jaguar XT5”, construida por la compañía norteamericana Cray, capaz de operar a una velocidad de 1.7 petaflops, es decir, de realizar 1,700 millones de millones de cálculos en un segundo –lo que es equivalente a cientos de miles de computadoras de escritorio–. A partir del 28 de octubre pasado, sin embargo, Jaguar XT5 fue desplazada por la supercomputadora china “Tianhe-1A”, que es un 40 % más rápida. Tianhe-1A fue construida por la Universidad Nacional de Tecnología de Defensa de China y se encuentra alojada en el Centro Nacional de Supercómputo en la ciudad de Tianjin en el norte de ese país. Aunque no es la primera vez que las computadoras norteamericanas se han visto rebasadas por máquinas extranjeras –en diferentes ocasiones en el pasado el título de computadora más rápida del mundo ha recaído en máquinas japonesas–, desde el año 2004 las computadoras más rápidas han sido todas norteamericanas. Además, en esta ocasión los Estados Unidos fueron superados por China, un país que es la segunda potencia económica mundial, que está en pleno ascenso y pisándoles los talones. Esto último es, por supuesto, muy serio, tratándose de un área tecnología de tanta importancia estratégica como la es la de la computación. De acuerdo con Wu-chun Feng, profesor de computación del Instituto Tecnológico de Virginia, citado por The New York Times: “ Lo que asusta es que el dominio de los Estados Unidos en el área del cómputo de alto rendimiento está en riesgo” y que “Se puede argüir que esto golpea el fundamento de nuestro futuro económico”.  Tianhe-1A fue diseñada y construida por los científicos e ingenieros chinos a un costo de 80 millones de dólares, empleando partes fabricadas por dos empresas norteamericanas: 14,000 procesadores de Intel y 7,000 de la compañía NVIDIA. La supercomputadora china es entonces norteamericana en cuanto a sus componentes básicos, pero definitivamente china en cuanto a la tecnología desarrollada para integrar dichas componentes en una máquina que ha resultado ser la más rápida jamás construida. Además, de acuerdo a un artículo aparecido  en el número del pasado 5 de noviembre en la revista Science, los chinos están probando en la computadora Tianhe-1A su propia versión de los procesadores de Intel. Por lo demás, ya anteriormente los Estados Unidos se han visto superados por una potencia rival en un área tecnológica de gran importancia estratégica. En efecto, el 4 de octubre de 1957, con el lanzamiento del primer satélite artificial por la entonces Unión Soviética, los Estados Unidos se vieron ampliamente rebasados en tecnología espacial en plena Guerra Fría. La supremacía soviética fue confirmada el 12 de abril de 1961 cuando fue puesto en órbita Yuri Gararin, el primer hombre en el espacio. En esa ocasión, los Estados Unidos apenas pudieron responder con un vuelo tripulado suborbital 23 días después. Como sabemos, no obstante, el Presidente Kennedy hizo en el verano de 1962 un compromiso público para llevar a un norteamericano a la superficie de la Luna antes de terminar la dècada, lo que fue finalmente logrado en julio de 1969.Lo cierto es que China está invirtiendo fuertemente en investigación y desarrollo, Así, entre 1996 y 2008, incrementó por un factor de 2.6 el porcentaje del PIB invertido en este rubro, mientras que en el año de 2009 tuvo un impresionante crecimiento de este porcentaje entre 30 % y 40 %. Hay que hacer notar, además, que la economía china está creciendo alrededor del 10 %, de modo que los recursos invertidos en investigación y desarrollo crecen a un ritmo todavía mayor. Al mismo tiempo, el número de investigadores en China creció cerca del 80 % entre 1995 y 2004.  Sin embargo, de acuerdo con Jack Dongarra, especialista en cómputo de la Universidad de Tenesí, citado por Science, es probable que Tianhe-1A pierda pronto su título de computadora más rápida del mundo, en virtud de que hay 5 supercomputadoras en desarrollo en los Estados Unidos y Japón que podrían alcanzar velocidades de 10 petaflops.Ya sea que se cumpla o no la predicción anterior, la gran tasa de crecimiento científico y tecnológico que está experimentando China, al igual que otros países asiáticos, les representará a los países desarrollados de occidente una competencia tecnológica cada vez mas intensa en un gran numero de áreas.",
    "El día 15 del pasado mes de octubre, un grupo de investigadores del Instituto Tecnológico de Georgia, en los Estados Unidos, enviaron para su publicación a la revista “Physics of Fluids” un trabajo de investigación en el que describen experimentos llevados a cabo para estudiar la manera como un perro mojado se seca el agua sacudiendo enérgicamente su cuerpo. Como sabemos, los perros –al igual que otros mamíferos como las ratas y los osos– se desprenden del agua atrapada en su pelo haciendo oscilar y retorciendo su cuerpo a una gran velocidad. Los investigadores del Tecnológico de Georgia grabaron en video perros y ratas mojadas, y midieron la velocidad de sus sacudidas reproduciendo el video en cámara lenta. Encontraron que dicha velocidad depende del tamaño del animal. Así, un perro labrador sacude su cuerpo entre cuatro y cinco veces cada segundo, mientras que, de manera respectiva, un ratón y una rata lo hacen 18 y 27 veces en el mismo intervalo. En contraste, un oso mojado se sacude y contorsiona 4 veces por segundo. Como lo explica uno de los investigadores involucrados en el estudio, para un mamífero con pelo es esencial tener una manera de sacudir el exceso de agua de su cuerpo por un medio mecánico – sacudidas y contorsiones–, pues de otro modo tendría que evaporarla empleando el calor del cuerpo y entraría rápidamente en un estado de hipotermia que le ocasionaría la muerte –pensemos, por ejemplo, en la sensación de “frío” que experimentamos cuando nos frotamos la piel con alcohol y dejamos que éste se evapore tomando calor de nuestro cuerpo, o en la  alta probabilidad que tenemos de enfermarnos si dejamos que la ropa mojada se nos seque sobre el cuerpo. No es, por otro lado, sorprendente que para liberarse del agua los animales más grandes necesiten sacudirse a una menor rapidez que los animales más pequeños, pues las fuerzas que tienden a liberar a una gota de agua que esté, por ejemplo, sobre el lomo de un oso, son mayores que las correspondientes fuerzas sobre la misma gota de agua en el lomo de una rata.  Reportajes del estudio referido tuvieron difusión en la televisión norteamericana y, en consecuencia, alcanzaron una gran notoriedad, misma que se vio reflejada en la gran cantidad de artículos que con este tema aparecieron en medios masivos de comunicación de todo el mundo. La naturaleza del estudio y sus conclusiones se prestaron, por supuesto, a su difusión amplia, pues difícilmente encontraríamos a alguien que no haya sido testigo del curioso y predecible comportamiento de un perro mojado –muy diferente, por cierto, al de nuestra propia especie, que ha perdido el pelo en la mayor parte del cuerpo a lo largo de cientos de miles de años de evolución y puede librarse más fácilmente del exceso de agua. Es positivo que los medios masivos de comunicación se ocupen de un tema científico o tecnológico, ya que esto puede ayudar a enfatizar el papel esencial que la ciencia ocupa en el mundo actual y que no siempre resulta claro entre el público en general, y en particular entre aquellos que están en posición de tomar decisiones con respecto a su apoyo público. Martin Rees, astrónomo de la Universidad de Cambridge, por ejemplo, señala que la ciencia  y la tecnología tenían en el Siglo XIX en Inglaterra una mayor visibilidad social y aprecio público que la que tienen en la actualidad, como lo atestiguan los museos nacionales que consumían, en términos relativos, mayores recursos que los que actualmente se les dedican.Los artículos en medios masivos de comunicación relativos al estudio de los investigadores del Tecnológico de Georgia, sin embargo, se enfocaron más a aspectos banales que a la posible relevancia, teórica o práctica, de los resultados de dicho estudio. Algunos de ellos, por ejemplo, se refirieron en tono de broma a la supuesta costumbre de los perros a esperar a que una persona esté cerca de ellos para empezar a sacudirse el agua; idea o prejuicio que es posible sea producto de los pocos casos en que un perro moja a una persona, al azar, sin tomar en cuenta  todos aquellos casos en que no sucede así, que son la mayoría pero que no se dan a notar.               Ocurre, por otro lado, que los investigadores hoy en día están sujetos a fuertes presiones para conseguir apoyo económico para llevar a cabo sus proyectos, y desde este punto de vista hay una tendencia a exagerar los resultados de las investigaciones, tanto por parte del investigador mismo como de la universidad en la que labora. En el caso presente, se dispone solamente de un video en el que se muestran diversos animales sacudiéndose el agua y no a un artículo completo. No se podría entonces emitir una opinión al respecto.     En tanto tenemos más información, podemos disfrutar del video de referencia (HYPERLINK \"http://arxiv.org/abs/1010.3279\" \\t \"_blank\"http://arxiv.org/abs/1010.3279)  en el que se muestran imágenes en cámara lenta –incluso de rayos x– de ratones, perros y un oso mojados y sacudiéndose el agua.",
    "“¿Su nombre? preguntaron voces ocultas. Sale, contestó,  mareado y con nauseas. Leonard Sale. ¿Ocupación? urgieron las voces. ¡Hombre del espacio! replicó, solo en la noche. Bienvenido, dijeron las voces. Bienvenido, bienvenido. Desaparecieron.”Así escribe Ray Bradbury al inicio de su cuento de ciencia ficción “Tal vez soñar” –publicado originalmente en 1948 con el título de “Dormido en Armagedón”–, en el que relata el trágico destino de Leonard Sale, un astronauta cuya nave se accidentó en el espacio y terminó en la superficie de un asteroide. A su llegada al asteroide –que estaría situado entre las órbitas de Marte y Júpiter–, Sale descubrió que le era  imposible dormir, pues apenas cerraba los ojos los nativos del lugar –de naturaleza puramente espiritual– tomaban su cerebro como campo de batalla para dirimir una batalla definitiva y largamente retrasada. Al final el sueño lo venció, no obstante sus esfuerzos para permanecer despierto en espera de ser rescatado, lo que le significó la muerte.La posibilidad de vida extraterrestre es algo que ejerce una gran fascinación y ha sido tema de innumerables cuentos y novelas de ciencia ficción. Una de las más famosas es “La guerra de los mundos”, escrita por George H. Wells en 1898 –en donde describe la invasión de la Tierra por los marcianos– y cuya dramatización en la radio por Orson Welles en 1938 provocó pánico en la ciudad de Nueva York. Esto, a pesar de que al inicio de la trasmisión se advirtió que se trataba de una ficción, lo que revela nuestra fácil disposición para aceptar la existencia de vida extraterrestre.Los extraterrestres, por otro lado, no son solamente motivo de fascinación para el lego sino que también lo son para los expertos. El proyecto SETI –siglas de Search for Extraterrestrial Intelligence– se ha ocupado desde la década de los años 1970 de escudriñar el cielo en busca de señales de radio provenientes de una civilización interestelar avanzada. El programa SETI, que no ha producido todavía resultados positivos, fue originalmente apoyado por la NASA y actualmente está a cargo de la Universidad de California Berkeley. Cinco millones de computadoras en todo el mundo, conectadas en una red a través de Internet, están contribuyendo actualmente al proyecto SETI.   En otro esfuerzo para descubrir vida extraterrestre, en marzo de 2009 la NASA puso en órbita alrededor del Sol la nave Kepler, que tiene como objetivo específico el descubrimiento de exoplanetas –es decir, de planetas orbitando estrellas fuera del sistema solar– con condiciones para desarrollar vida. Para que sea capaz de sustentar vida, un planeta debe estar a una distancia adecuada de su estrella, dentro de la llamada zona habitable, de manera que permita la existencia de agua líquida, que es esencial para la vida tal como la conocemos. El planeta no debe estar ni muy cerca de la estrella, lo que lo haría demasiado caliente, ni tan lejos que lo convierta en un lugar muy frío. Actualmente se sabe de la existencia de un medio millar de exoplanetas, la mayor parte de los cuales son planetas gigantes al estilo de Júpiter. Esto último, sin embargo, no significa que los planetas pequeños, del tamaño de la Tierra, sean poco abundantes, sino que refleja el hecho de que estos son más difíciles de detectar. En efecto, en un artículo publicado el pasado 29 de octubre en la revista “Science” por investigadores de universidades en los Estados Unidos, China y Japón, se concluye que en realidad los planetas pequeños son más comunes que los de mayor tamaño, y que probablemente una cuarta parte de todas las estrellas cuenten con un planeta de un tamaño comparable al tamaño de la Tierra, si bien demasiado cerca de la estrella para desarrollar vida. Habrá, sin embargo, planetas de tamaño comparable a la Tierra que estén dentro de la zona habitable. En relación a esto, el artículo referido anteriormente predice que la nave Kepler, que está buscando planetas en unas 156,000 estrellas, encontrará al término de su misión de 120 a 260 planetas similares a la Tierra con capacidad de desarrollar vida.Dado el enorme número de estrellas que podemos observar, muy probablemente encontraremos, tarde o temprano, evidencia de la existencia de vida –o de condiciones para su desarrollo– fuera del sistema solar. Las enormes distancias interestelares impedirían, sin embargo, un contacto directo entre civilizaciones. En nuestra vecindad inmediata hay ciertas probabilidades de encontrar vida en Marte. Contemplando esto, la NASA tiene actualmente en curso varios proyectos de exploración de este planeta. La vida que pudiera encontrarse en Marte, o en otros lugares del sistema solar, sin embargo, sería sólo a nivel microbiano. Las posibilidades de establecer contactos con nuestros vecinos lejanos, tal  como nos los describen las novelas de ciencia ficción, son entonces muy remotas y quedan solamente en el campo de nuestra fantasía.",
    "En las pruebas PISA que ha llevado a cabo la Organización para el Desarrollo y la Cooperación Económica (OCDE) en la última década, y que tienen como propósito evaluar el aprovechamiento académico de los estudiantes de la escuela secundaria –en las áreas de matemáticas, ciencias y lectura–, México ha ocupado el último lugar entre los países miembros de esta organización. Esto no es sorprendente si se toma en cuenta que la OCDE está formada mayormente por países desarrollados y que un sistema educativo deficiente es precisamente una característica de los países que no han alcanzado todavía esta categoría, como es el caso de nuestro país.En el contexto anterior, resulta interesante el artículo publicado el pasado 14 de octubre en la revista “Science” por Mary Pyc y Katherine Rawson, investigadoras de la Universidad Estatal de Kent, en Ohio, Estados Unidos. En dicho artículo se reportan experimentos mediante los cuales se concluye que los exámenes, más allá de su papel como herramienta para evaluar el aprendizaje, constituyen en sí mismos un vehículo para facilitar la memorización. Los educandos deberían, entonces, recibir tantos exámenes como fuera posible –lo que probablemente no entusiasme demasiado a la mayor parte de los estudiantes de la escuela secundaria. Los experimentos de Pyc y Rawson se centraron en el aprendizaje de la lengua africana swahili. Fueron llevados a cabo con 118 estudiantes de la Universidad Estatal de Kent a los que se les proporcionaron 48 pares de palabras inglés-swahili, cada par con un mismo significado, y las cuales se les pidió memorizaran. La hipótesis que guió el estudio fue que el proceso de memorización de una palabra en un idioma distinto al nativo es ayudado por una palabra mediadora en el idioma propio, la cual suena parecido a la palabra que se quiere traducir. Asumieron, además, que la efectividad del mediador se incrementa con los  exámenes. Para ilustrar el punto, Pyc y Rawson ponen como ejemplo el par de palabras “Wingu-Cloud”,  que significan nube en ambos swahili e inglés. La palabra mediadora escogida por muchos participantes fue “wing”, que significa “ala” en inglés y que suena parecido a “wingu”. La asociación entre “wingu” y “cloud” vendría entonces del hecho de que los aviones tienen alas y vuelan entre nubes. Al inicio del estudio, a los participantes se les pidió que encontraran un mediador para cada uno de los 48 pares de palabras que les fueron presentadas, después de lo cual fueron divididos en dos grupos de estudio que trabajaron a lo largo de una semana. Un primer grupo se dedicó simplemente a estudiar y memorizar las palabras en swahili, mientras que al otro, además del estudio de memorización, se le aplicaron una serie de exámenes a lo largo de la semana de trabajo. Para terminar, a todos se les aplicó un examen final.Como resultado, los estudiantes del segundo grupo, que habían estudiado y además pasado por los exámenes intermedios, tuvieron un mejor desempeño que aquellos del primer grupo que se dedicaron solamente a estudiar. Así, cuando se les preguntó la traducción al inglés de una palabra en swahili de las 48 estudiadas, los del segundo grupo contestaron tres veces mejor que los del primero. Aquellos pudieron también recordar y emplear mejor los mediadores que habían escogido a lo largo de la semana de estudio, confirmando la hipótesis en el sentido que los exámenes intermedios mejoraron la efectividad de dichos mediadores.  La explicación que Pyc y Rawson dan a este hecho es en el sentido de que los exámenes intermedios proporcionaron a los estudiantes que los tomaron una oportunidad de discriminar a los mediadores que funcionaban de aquellos menos efectivos, teniendo de este modo mejores herramientas para el examen final.   Como siempre sucede en ciencia, no obstante, las conclusiones obtenidas de un cierto estudio no son ampliamente aceptadas hasta que no son confirmadas por otros investigadores, y en este sentido el resultado del estudio de Pyc y Dawson no sería la excepción. De lo que si hay mayor evidencia y aceptación, sin embargo, es que los exámenes, más allá de constituir la herramienta principal para evaluar a un estudiante, contribuyen por sí mismos al proceso de aprendizaje.De este modo, una de las posibles vías para abandonar nuestro nada honroso, aunque explicable, último lugar en las pruebas PISA –aunque no necesariamente la más popular–, consistiría en  mantener a nuestros estudiantes en una sucesión continua de exámenes.",
    "La semana que acaba de terminar fue de premios Nobel. Se adjudicaron los premios de Medicina, Física, Química, Literatura y el Premio Nobel de la Paz. Como sabemos, el premio de Literatura se otorgó  este año a Mario Vargas Llosa –que, por cierto, hace algunos años recibió de la UASLP el doctorado Honoris Causa–, escritor peruano nacionalizado español. Como sucede cada año, el premio de Literatura recibió mayor atención que los premios en ciencias. Esto es natural, pues el número de lectores de, por ejemplo –y sobre todo–, Vargas Llosa, es considerablemente mayor que el número de lectores de textos científicos tales como aquellos sobre películas de grafeno de los ganadores del Premio Nobel de Física del presente año.Más allá de su impacto mediático, no obstante, los premios Nobel en ciencias se otorgan a trabajos que en no pocas ocasiones han representado descubrimientos y avances que han cambiando nuestra vida. El descubrimiento del grafeno –que está compuesto de átomos de carbono, al igual que el grafito que se encuentra en la punta de los lápices– les representó a Andrei Geim y Konstantin Novoselov, investigadores de la Universidad de Manchester, el Premio Nobel de Física 2010.  El grafeno se presenta en la forma de láminas de carbono extremadamente delgadas, con un espesor de exactamente un átomo –y que es, por supuesto, el menor posible–. El descubrimiento del grafeno se llevó a cabo de una manera asombrosamente simple: arrancando láminas de este material de un pedazo de grafito por medio de una cinta adhesiva. El grafeno en un material con propiedades inusuales derivadas en gran medida de su extrema delgadez. Si pudiéramos reducir nuestro tamaño hasta alcanzar dimensiones atómicas e hiciéramos de una lámina de grafeno nuestro hábitat, nos encontraríamos con una situación extraña, tal como la descrita por el escritor inglés Edwin A. Abbot en su libro “Flatland” –que podríamos traducir como “Mundo plano”–, publicado en 1884. “Flatland” es un mundo de dos dimensiones que contrasta con el nuestro tridimensional en donde podemos distinguir entre el largo, el ancho y el alto de un objeto.  En “Flatland” solamente existen largo y ancho, y sus habitantes, con formas de triángulos, cuadrados, pentágonos y en general de polígonos planos –con un número de lados según su categoría social–, no conciben la tercera dimensión. De este modo, entre ellos se perciben solamente como líneas rectas.  Al igual que “Flatland”, el grafeno es un mundo en donde todo ocurre en dos dimensiones.Se piensa que el grafeno podría tener un sinnúmero de aplicaciones, entre ellas la de sustituir al silicio en la fabricación de los “chips” que se utilizan, por ejemplo, en los procesadores y las memorias de las computadoras. En un “chip” se requiere controlar corrientes eléctricas muy pequeñas por medio de compuertas eléctricas diminutas. Un “chip” integra millones de estas compuertas, las cuales adquieren dimensiones cada vez más reducidas en la medida que avanza la tecnología. Se piensa, sin embargo, que ésta alcanzará pronto un límite, más allá del cual el silicio –el elemento base en la fabricación de los “chips” – tendrá que ser sustituido por otro material. Una de los candidatos para este propósito es precisamente el grafeno, que potencialmente podría ser la base de “chips” con características superiores a los actuales de silicio. Las promesas tecnológicas del grafeno, sin embargo, son tomadas con cautela por los expertos, pues su extrema delgadez lo convierte en un material muy difícil de manipular. En el pasado hemos visto no pocos casos de descubrimientos científicos que prometen grandes aplicaciones que a la postre han resultado difíciles de concretar. Entre estos contamos a la superconductividad de alta temperatura –motivo del Premio Nobel de Física 1987– y a la fusión nuclear. Aquella prometía la fabricación de conductores de electricidad con cero resistencia para un sinnúmero de aplicaciones, lo que no se ha logrado, mientras que la fusión nuclear ha ofrecido ya por muchas décadas energía ilimitada a partir del agua de mar, sin tampoco obtener resultados prácticos. Hay incluso quienes no reaccionaron de manera entusiasta al premio Nobel otorgado a Geim y Novoselov. En un artículo publicado en el número del pasado 8 de octubre de la revista “Science”,  que llevó por título: “Todavía en su infancia, un cristal bidimensional recibe un premio”, se señala que dicho premio constituye una anomalía si se le compara con otros reconocimientos pasados, los cuales premiaron el descubrimiento de un nuevo efecto físico o un desarrollo que en el curso de décadas llevó a aplicaciones ubicuas. Por su lado, Walter de Heer, del Instituto Tecnológico de Georgia, en declaraciones a la revista “Scientific American”, consideró que el premio es prematuro y que el grafeno todavía necesita demostrar su potencial tecnológico.Resulta, pues, que en el presente año no solamente el Premio Nobel de la Paz ha suscitado controversia.",
    "Las computadoras en su infancia sufrían de una suerte de autismo severo que, por un lado, las hacía extraordinariamente eficientes para la realización de tareas tales como las de sumar o multiplicar números extremadamente grandes a una enorme velocidad, mientras que, por otro lado, eran incapaces de realizar tareas que el cerebro humano lleva a cabo de manera rutinaria. Como explicaba mi profesor de computación hace ya casi 40 años: si quieren que una computadora haga algo para ustedes, deben darle instrucciones precisas; así, si le piden que –de manera figurada– se beba el contenido de una botella, tienen que indicarle primero que le quite el corcho, pues de otra manera intentará tomársela tapada. En contraste, sin necesidad de que le sea explícitamente indicado, una persona normal entendería que no podría beber de una botella sin destaparla –a menos que, por ejemplo, haya recibido un fuerte golpe en la cabeza o se encuentre de alguna manera en estado inconveniente, en cuyo caso el cerebro no funcionaría de la manera apropiada. Aunque las computadoras no se han hecho inteligentes a la velocidad a la que hace medio siglo se pronosticaba que lo harìan, si han avanzado un largo trecho. Así, en el año de 1996 en un encuentro de ajedrez a seis juegos, la computadora “Deep Blue” de la compañía IBM venció en la partida inicial al entonces campeón mundial Garry Kasparov, aunque después el campeón le ganó tres juegos a la computadora y resultó vencedor al final. Al año siguiente, no obstante, “Deep Blue” hizo historia y venció apretadamente a Kasparov en un segundo encuentro. Las computadoras de este modo invadieron un terreno que hasta entonces había estado reservado al cerebro humano.Durante los últimos tres años, IBM ha estado trabajando en un proyecto que ha dejado corto a “Deep Blue”: el desarrollo de Watson, una computadora que sería capaz de participar con éxito en el programa de concursos por televisión “Jeopardy!”. Este popular concurso de la televisión norteamericana  enfrenta a tres participantes, a los cuales se les hacen preguntas sobre diferentes temas, teniendo la opción de ofrecer una respuesta apretando un timbre –la oportunidad le es dada a aquel participante que lo aprieta primero–. Las preguntas se hacen en la forma de una respuesta  –por ejemplo, “Este platillo mexicano está hecho de carne y vegetales cubiertos de masa y envueltos en una hoja de maíz”–, mientras que las respuestas tienen que expresarse en la forma de una pregunta –“¿Qué es un tamal?” –. Cada respuesta exitosa implica una ganancia monetaria para el participante, que se convierte en pérdida si por el contrario es incorrecta.De acuerdo a información publicada en su portal, la compañía IBM está convencida de que Watson ha alcanzado un desarrollo tal que tiene posibilidades de participar de manera exitosa en “Jeopardy!”, y  tiene planeado llevarla a concursar en el programa televisivo en algún momento en lo que resta del añoPara tener éxito en “Jeopardy!”, un participante  requiere no sólo tener acumulada en su cerebro –natural o artificial– una gran cantidad de información sobre numerosos temas –Watson tiene almacenados en su memoria decenas de millones de documentos–, sino que, además es necesario tener la capacidad de procesarla a una gran velocidad. En particular, el participante tiene que decidir en una fracción de segundo si podría ofrecer una respuesta correcta a la pregunta  en turno, ya que una respuesta incorrecta le significa una pérdida monetaria. Para una computadora, además, el concurso “Jeopardy!” presenta el reto de entender preguntas expresadas en lenguaje común, con todas sus ambigüedades y falta de precisión, e incluso en algunos casos con juegos de palabras que tienen que ser interpretados en base a un cierto contexto. Estas son habilidades propias del cerebro humano en las que las computadoras no destacan de manera particular. Adicionalmente, un punto clave durante el concurso es que antes de apretar el botón de timbre para dar una respuesta, el participante tiene que estar razonablemente seguro de que la que va a ofrecer es la correcta; esto con el objeto de evitar penalizaciones. Para hacer esta evaluación, Watson hace un análisis de todas las sus posibles respuestas, escogiendo, en su caso, la que tiene más alta probabilidad de ser cierta.De acuerdo a información publicada en su portal, IBM tiene planeado llevar a la computadora Watson al concurso “Jeopardy” en algún momento en lo que resta de año. Para prepararla, ha realizado concursos de prueba en sus instalaciones en Nueva York, invitando a antiguos participantes en dicho concurso. El autor Clive Thomson, en un artículo publicado el pasado 10 de julio en el magazine dominical del periódico “The New York Times”, menciona que Watson ha tenido un desempeño notable en los concursos de prueba derrotando a sus oponentes.De llevarse a cabo, durante el concurso “Jeopardy” Watson tendrá una ventaja sobre sus oponentes humanos: no se pondrá nerviosa ni tendrá problemas emocionales. Así, aún en el caso de que derrote a sus adversarios, seguirá siendo una máquina relativamente poco compleja",
    "Tuve  la oportunidad de asistir la semana que acaba de terminar a la Conferencia Iberoamericana de Óptica que se celebró en la ciudad de Lima, Perú, y que reunió por una semana a especialistas  de países de América Latina, España y Portugal. Es quizá pertinente hacer la aclaración que la óptica es una disciplina muy amplia que se refiere no solamente al diseño de aparatos e instrumentos ópticos, tales como microscopios y telescopios, sino que también es la ciencia que ha dado origen a un gran número de tecnologías y dispositivos de una gran complejidad, entre los que se encuentran los láseres y las fibras ópticas que se emplean en los sistemas actuales de telecomunicaciones, los sensores de luz usados en las cámaras fotográficas digitales y que han reemplazado al negativo fotográfico, y las pantallas planas para televisores y computadoras, por mencionar solamente algunas de sus muchas aplicaciones. En la conferencia de Lima se presentaron 370 ponencias y carteles y se impartieron 12 conferencias plenarias. Casi de manera coincidente con la celebración de la Conferencia Iberoamericana de Óptica, la revista “The Economist” publicó en su número del pasado 11 de septiembre un extenso reporte  sobre América Latina. Dicho reporte llevó por título “El patio trasero de nadie: el ascenso de América Latina”,  y para ilustrarlo “The Economist” destacó en su portada un mapa de todo el Continente Americano dibujado de cabeza, con la Patagonia en la parte superior y Canadá y los Estados Unidos en la parte inferior. Así, América Latina  habría dejado de ser –o estaría a punto de hacerlo– el patio trasero de los Estados Unidos, como comúnmente se caracteriza  a la región. América Latina latina ha tenido en los últimos años un crecimiento económico acelerado. Según cifras de “The Economist”, el país latinoamericano que en este respecto lleva la delantera es Panamá, cuyo producto Interno Bruto (PIB) creció a un 5.8 % anual promedio en la última década, seguido de Perú con un 5.1 %.  Otros países latinoamericanos con altos porcentajes de crecimiento del PIB son: Ecuador (4.8 %), Colombia (3.8 %), Chile y Bolivia, ambos con un crecimiento del  3.7 %, Venezuela con 3.5 %, y Argentina y Brasil con un 3.3 % –México no está bien situado en este respecto: de los 18 países latinoamericanos incluidos en el reporte de “The Economist”, México ocupa el último lugar con un lejano 1.8 %–. La demanda de materias primas y alimentos por la rápida expansión económica de Asia, notablemente de China, han contribuido a dicho crecimiento.Tomando como ejemplo al Perú, notamos que es el primer productor mundial de plata y el segundo de cobre, además de que se encuentra entre los cinco mayores productores mundiales  de zinc, plomo y oro. Asimismo, es el primer productor de harina de pescado, y exportador importante de productos agrícolas. Perú se ha beneficiado por el incremento al precio de sus productos de exportación en fechas recientes, y el gran número de obras viales de reciente construcción que es posible encontrar en la ciudad de Lima es indicativo del auge económico que experimenta  el País. A pesar de su crecimiento económico reciente, no obstante, América Latina no muestra un desarrollo educativo a la par, indispensable, tal como lo señala “The Economist, para lograr una estabilidad económica menos dependiente del valor de las materias primas. En efecto, en la prueba PISA practicada en el año de 2006 por la OCDE para medir el grado de conocimiento que sobre ciencia tienen los estudiantes del nivel secundario –incluyendo su habilidad para usar el conocimiento científico para resolver problemas de la vida diaria–, los seis países latinoamericanos   examinados –Argentina, Brasil, Chile, Colombia, México y Uruguay– tuvieron un desempeño mediocre: de 57 países participantes, el mejor colocado fue Chile que ocupó el lugar número 40. El volumen de la actividad científica que se lleva a cabo en Latinoamérica es igualmente   insuficiente. Tenemos, por ejemplo, que el número de artículos científicos publicados en la región     es solamente un 4 % del total publicado a nivel mundial. El poco peso específico que tiene la ciencia latinoamericana se vio reflejado en el Congreso Iberoamericano de Óptica, pues de las doce conferencias plenarias que se presentaron, solamente una estuvo a cargo de un investigador latinoamericano –de una institución mexicana, por cierto–; las once restantes fueron impartidas por especialistas de países desarrollados.  Hay que notar, no obstante, que la contribución científica de Latinoamérica está creciendo rápidamente y que su porcentaje de artículos publicados con respecto al total mundial se cuadruplicó en los últimos 30 años. De este modo, habría que esperar que el subdesarrollo científico latinoamericano pueda superarse en un futuro no muy lejano. El conocimiento que en esta materia se desarrolle en nuestros países, sin embargo, no podrá ser aprovechado si no se integra al aparato productivo, aspecto que en el que Latinoamérica –con la excepción en cierto grado de Brasil– todavía está en desventaja con respecto al mundo desarrollado. Mantener al mapa de América de cabeza no será entonces tarea fácil.",
    "¿Cree usted en el cambio climático? ¿Está convencido de que la temperatura de la Tierra se está elevando por causa nuestra? ¿Está preocupado porque considera que el cambio climático es un peligro real que afecta no solamente a las focas o a los osos polares sino a todos los habitantes de la Tierra –incluyéndonos a nosotros, los humanos– y que amenaza con cambiar nuestra forma de vida? Cada persona a la que se hicieran estas preguntas tendría, por supuesto, sus propias respuestas. No obstante, de acuerdo a un estudio publicado por Aaron McCright de la Universidad Estatal de Michigan en el número de septiembre del presente año de la revista “Population and Environment, es más probable obtener respuestas afirmativas si las  preguntas se dirigen a una representante del género femenino; esto al menos entre la población estadounidense.El estudio referido se realizó con resultados de encuestas Gallup llevadas a cabo en los años 2001-2008. De acuerdo al mismo, el 59 % de las mujeres consultadas afirmaron creer que el cambio climático es algo real, en comparación con solamente el 54 % de los hombres que así lo piensan. Igualmente, el 64 % de las mujeres tienen la convicción de que el cambio climático es producto de las actividades humanas, mientras que sólo el 59 % de los hombres comparten esta opinión. Se encontró, asimismo, que el 37 % de las mujeres encuestadas piensan que el cambio climático amenaza su forma de vida, frente al 28 % de los hombres que tienen esta convicción. En contraste, si bien las mujeres resultan estar mejor informadas acerca de las evidencias científicas que apoyan el cambio climático, muestran menos confianza que los hombres en la compresión de las mismas. Por otro lado, y al margen de las diferencias de opinión entre hombres y mujeres en la población norteamericana en lo que respecta al cambio climático, existe un buen porcentaje –si bien minoritario– de escépticos que no consideran que dicho cambio sea real; o bien que, de existir, obedece a causas naturales y no humanas. La creencia del público en el calentamiento global se vio afectada por el robo de información del servidor de la Unidad de Investigación sobre el Clima de la Universidad de East Anglia en el Reino Unido, ocurrida en noviembre de 2009, y que dio origen al llamado “Climagate”. La información sustraída fue distribuida por la red Internet e incluyó correos electrónicos intercambiados entre científicos del clima en varios centros de investigación del mundo, en los cuales los activistas anti-cambio climático quisieron ver evidencias de prácticas anticientíficas que intentaban presentar un panorama agravado sobre el cambio climático. Aunque las acusaciones no pudieron ser comprobadas, el Climagate alentó el escepticismo en el público sobre la realidad del calentamiento global.El último invierno, en el que se tuvieron temperaturas por debajo de lo normal en buena parte del hemisferio norte, incluyendo nuestro país, no ayudó tampoco a generar la precepción entre el público de que la temperatura de nuestro planeta está incrementándose. De haber vivido en el Ártico, no obstante, hubiéramos tenido la percepción contraria, pues en el pasado invierno allí hubo temperaturas superiores a las acostumbradas. Experiencias similares las vivimos en los últimos meses en San Luís Potosí, en donde tuvimos un invierno con días muy fríos y un mes de mayo con altas temperaturas. Hay evidencias científicas sólidas de que la temperatura promedio de la Tierra se está incrementando por el uso indiscriminado que hemos hecho de los combustibles fósiles desde el inicio de la revolución industrial. Habrá en ciertas regiones del mundo inviernos más fríos que los normales, lo mismo que veranos más calientes, pero en promedio la temperatura terrestre está elevándose lentamente. Ningún resultado científico, por otro lado, es absolutamente concluyente y será siempre susceptible de ser refutado por nuevas evidencias. Hasta ahora, sin embargo, el consenso en la comunidad científica es que el calentamiento global es un hecho real, que ha sido provocado por actividades humanas, y que si no se atenúa llevará a un incremento de la temperatura terrestre de varios grados centígrados en el año 2100, con consecuencias desastrosas para el mundo. De manera natural, por los grandes costos económicos que implica transitar hacia una economía libre de combustibles fósiles, al igual que por los intereses que serían afectados en el proceso, hay instancias en donde el cambio climático no goza de simpatías como lo demuestra el episodio del Climagate. En estas circunstancias, la opinión pública será fundamental para reducir las emisiones de dióxido de carbono a la atmósfera y sacar de apuros a nuestro planeta. Cuando esto se logre, el mundo estará en deuda con el género femenino que mayoritariamente se inclina por creer en el cambio climático –aunque, para ser justos, en este respecto el género masculino no se queda demasiado atrás.",
    "Así se titula el artículo aparecido en el número de septiembre de la revista de divulgación científica “Scientific American”, en el que se considera el efecto que han tenido las actividades humanas, tanto sobre el medio ambiente, como sobre el inventario de recursos naturales con que cuenta nuestro planeta. El artículo establece una escala temporal para diversos acontecimientos que se predice ocurrirán en los próximos 100 años a escala global y que incluyen: los efectos del cambio climático sobre la producción de alimentos y el abastecimiento de agua potable, el agotamiento de los recursos minerales y energéticos, y la extinción masiva de especies biológicas por la pesca inmoderada y la contaminación de los océanos. Como sabemos, el incremento en la concentración de gases de invernadero en la atmósfera terrestre durante la era industrial –a lo largo de los últimos 200 años, pero sobre todo en el último medio siglo–, ha provocado un incremento en la temperatura de nuestro planeta que se manifiesta por –entre otros efectos– la reducción del volumen del hielo en los polos terrestres y en los glaciares, y en el desprendimiento de grandes masas de hielo de los casquetes polares que flotan a la deriva. Se pronostica que este aumento de temperatura –aunque pequeño– producirá cambios en la distribución de lluvias, cuyo volumen se incrementaría en ciertas regiones como Siberia y el este de los Estados Unidos, pero se reducirá en otras, entre las que desgraciadamente se encuentra gran parte de nuestro país. Una disminución del volumen de lluvias afectaría tanto a la agricultura como al abasto de agua potable para consumo humano. Se pronostica que la falta de agua hará crisis en 50 años y que México sería grandemente afectado, con una reducción de 25 % en su producción agrícola.Un problema más inminente es el del agotamiento de las reservas de algunos metales de gran importancia actual como son la plata, el oro, el indio y el cobre. A los niveles presentes de extracción, según datos contenidos en el artículo de referencia, las reservas naturales de estos metales se agotarán en plazos que van desde los 18 años para el indio, hasta los 34 años para el cobre. Como sabemos, el cobre tiene amplias aplicaciones y lo encontramos en redes de distribución de energía eléctrica, en tubos conductores de agua, en sistemas de comunicación, incluyendo los teléfonos celulares, y en general en toda clase de dispositivos y aparatos eléctricos. El indio, por su lado, aunque no es un elemento químico que nos sea tan familiar como lo es el cobre, tiene una gran importancia en la fabricación de pantallas para computadoras y televisores, así como en la manufactura de celdas solares para la conversión de la energía del sol en energía eléctrica, dispositivos que serán esenciales en un futuro cercano para remplazar parcialmente a los combustibles fósiles. Otro problema que afrontamos es precisamente el del agotamiento de los combustibles fósiles. En efecto, se espera que el petróleo alcance su máximo de producción a nivel mundial en el año 2014 y que a partir de ese año entre en una etapa de declive. Incluso en el caso del carbón, del cual existen reservas considerablemente más altas, David Rutledge, del Instituto Tecnológico de California, predice que para el año de 2072 se habrán extraído, al ritmo actual de producción, el 90 % de las reservas existentes.  La pesca y la producción de alimentos del mar son actividades que se encuentra también en peligro amenazadas por lo sobrexplotación. Entre las especies marinas que están en riesgo, según el artículo referido, se encuentran el tiburón martillo y el esturión, que han reducido su población en un 90 % en las últimas décadas. Desde nuestra perspectiva y escala humanas, la Tierra se muestra como un lugar inmenso, al que han dado forma fuerzas naturales cuyo poder está muy por encima del nuestro; consecuentemente, por mucho tiempo no resultó natural pensar que mediante acciones nuestras pudiéramos perturbarla en una escala global. De manera similar, tendimos a considerar que los recursos naturales en el interior de nuestro planeta eran inmensos e inagotables. Hoy en día, a 200 años del inicio de la revolución industrial, es patente que nuestras percepciones estaban equivocadas y que mediante el uso indiscriminado de combustibles fósiles hemos contaminado la atmósfera del planeta y perturbado su equilibrio; hemos igualmente agotado el petróleo y estamos en camino de hacer lo mismo con el carbón y otros minerales esenciales para nosotros como lo son el cobre y el indio. Y a pesar de todo esto, es decir, de haber llevado al límite a nuestro planeta, no hemos podido dar condiciones de vida dignas a un porcentaje mayoritario de la población del mundo.",
    "En junio de 1997, durante un torneo previo al campeonato mundial de fútbol celebrado en Francia en 1988, el jugador Roberto Carlos del equipo de Brasil le anotó un gol a Francia   en tiro libre, el cual se encuentra entre los más notables de la historia. Desde una distancia de unos 35 metros, el brasileño pateó la pelota hacia la portería rival con la parte exterior del zapato izquierdo, de  modo tal que adquirió un movimiento de rotación además de una gran velocidad, quizá cercana esta última a los 100 kilómetros por hora. La pelota viajó inicialmente muy desviada de la portería, lo que le permitió eludir la barrera defensiva por su lado izquierdo. Una vez librada ésta, sin embargo, dio un giro inesperado hacia la arco, acabando en el fondo del mismo después de golpear el poste a la izquierda del portero. La trayectoria de la pelota resulto tan inesperada, que el portero francés permaneció estático pensando que no representaba peligro alguno.  La trayectoria curva de un balón de fútbol es debida a la rotación que se imprime al mismo en el momento de golpearlo. En el fútbol –al igual que en otros deportes– este efecto se emplea de manera común y los porteros están conscientes de que una pelota lanzada contra su arco puede seguir una trayectoria curva. En el caso del gol de Roberto Carlos contra Francia, sin embargo, el efecto se presentó de manera extrema. Tanto así, que muchos lo consideraron un “gol inexplicable o milagroso”.La explicación del gol de Roberto Carlos, no obstante, finalmente llegó. En un artículo aparecido en el “New Journal of Physics” el pasado 2 de septiembre, un grupo de investigadores de la Escuela Superior de Física y Química Industrial y de la Escuela Politécnica de Paris, reportaron un estudio llevado a cabo para determinar la trayectoria que sigue un objeto esférico –tal como una pelota de fútbol– al que se le ha dado una cierta velocidad y rotación iniciales. El artículo describe experimentos en los que se lanzan pelotas de plástico en agua y se toman fotografías de sus movimientos. De acuerdo con los resultados del estudio, un objeto lanzado con una cierta velocidad y con una cierta rotación describe efectivamente una trayectoria curva. El movimiento de un objeto en rotación a lo largo de una línea curva es un fenómeno que ha sido conocido por mucho tiempo. Este fenómeno recibe el nombre de “Efecto Magnus”, en honor al científico alemán Henrich Gustav Magnus, quién lo estudio a mediados del siglo XIX y al que se da crédito por haberlo explicado por primera vez. Magnus no estaba interesado en el fútbol soccer –que no existía todavía como tal– o en algún otro deporte. Por el contrario, su interés se centraba en aplicaciones militares: quería averiguar la razón por la que al lanzar girando una granada de artillería, ésta se desviaba hacia un lado. Lo novedoso del artículo de los investigadores franceses es su descubrimiento de que los objetos en rotación siguen, bajo ciertas condiciones, una trayectoria en espiral, la cual es, por supuesto, más complicada que la simple trayectoria curva a la que están acostumbrados los jugadores de fútbol, en particular los porteros. Una trayectoria en espiral implica que el objeto se mueve en una curva cada vez más cerrada –virando por tanto de manera cada vez más brusca–, haciendo difícil de este modo predecir su evolución. De acuerdo con los investigadores franceses, esto fue lo que confundió a su portero, quién no pudo anticipar el viraje acusado que hizo el balón  antes de introducirse a su portería. La velocidad y rotación que Roberto Carlos imprimió a la pelota, así como las distancia a la portería desde donde disparó, fueron esenciales para que el balón haya hecho lo que hizo. El gol no tuvo entonces nada de milagroso y dadas las mismas condiciones debería repetirse.La explicación dada por los científicos franceses al gol del jugador brasileño en contra de la selección establece las causas físicas por las que un balón de fútbol pudo describir una trayectoria aparentemente errática, o al menos sorprendente. Como se explica en el artículo de referencia, los resultados reportados tienen relevancia para otros deportes en los cuales también se hace uso del efecto que el giro de una pelota le impone a su trayectoria, tales como el tenis, el béisbol y el ping pong. En el caso del béisbol, por ejemplo, el efecto que un lanzador le imprime a la trayectoria de la pelota es parte fundamental de su estrategia para anular al bateador. Igualmente, el giro que este último le da a la pelota al golpearla puede hacer que vuele por más tiempo, o que más rápidamente caiga al suelo.Los jugadores expertos en hacer que la pelota haga cabriolas en el aíre, por supuesto, han desarrollado su habilidad al margen de que existan o no explicaciones científicas; y saben, aunque no sean doctos en ciencia, que no hacen milagros y que lo que han hecho lo pueden repetir.",
    "El Instituto Nacional de Antropología e Historia (INAH) anunció en su boletín del pasado 24 de agosto el hallazgo, en el cenote Chan Hol en Tulúm, Quintana Roo, de una osamenta humana con una antigüedad superior a los 10,000 años, lo que corresponde al final de la Edad del Hielo. Por la dentadura poco gastada, se sabe que los restos pertenecen a los de una persona que murió joven. El esqueleto del “Joven de Chan Hol”, como se le conoce, fue encontrado a 542 metros en el interior de una cueva sumergida a una profundidad de 8 metros. De acuerdo con el INAH, el cuerpo fue colocado en la cueva en una ceremonia funeraria. El hallazgo se une a los de los esqueletos de la Mujer de Naharon, la Mujer de las Palmas y el Hombre del Templo, que también fueron encontrados recientemente en cuevas sumergidas cercanas a Tulum y que tienen antigüedades que igualmente se remontan a la Edad del Hielo.En la época en la que vivió el Joven de Chan Hol, la cueva en donde fueron encontrados sus restos no estaba sumergida. Hace unos 13,000 años, sin embargo, terminó la última glaciación y la temperatura terrestre empezó a elevarse paulatinamente, hasta acumular hoy en día un incremento de unos 8 grados centígrados. Con el aumento de la temperatura de la Tierra los casquetes polares disminuyeron su volumen, además de que el agua de los océanos se expandió. Como resultado, el nivel del mar se elevó unos 80 metros en los últimos 12,000 años.     De acuerdo con el INAH, el esqueleto del Joven de Chan Hol –que fue recuperado en un 60 por ciento, incluyendo huesos de las extremidades, vértebras, costillas y cráneo– proporciona evidencia de las primeras migraciones humanas a nuestro continente. Como sabemos, la teoría clásica sobre el origen de la población indígena en América, que  fue ampliamente aceptada por largo tiempo, presupone que ésta es el resultado de la migración de tribus nómadas del Asia Central, que habrían alcanzado nuestro continente a final de la Edad de Hielo a través de lo que hoy es el estrecho de Bering. En esa época, y  debido a que la profundidad del mar alrededor de dicho estrecho es muy poca, se formó un puente terrestre entre Asia y América como resultado del descenso del nivel del mar. Una vez en el continente americano, los nómadas se habrían desplazado hacia el sur a través de un pasadizo libre de hielo en Canadá. De acuerdo con la teoría clásica, la cultura Clovis de Nuevo México, con una antigüedad de 13,000 años, corresponde a los primeros pobladores del Nuevo Mundo.Hoy en día, sin embargo, hay quienes ponen en tela de juicio esta teoría, pues se han hallado evidencias de pobladores en nuestro continente anteriores a la cultura Clovis. Este es el caso del sitio de Monte Verde en el sur de Chile, al que se le adjudica una antigüedad de 14,000 años. Se aduce también que hubo rutas adicionales de migración hacia el sur por vía marítima a lo largo de la costa oeste de nuestro continente, las cuales llegaron hasta el extremo sur del mismo. Hay también quien defiende que la población indígena del Nuevo Mundo es el resultado de la migración de más de un grupo étnico. Algunos investigadores piensan, por ejemplo, que hubo viajeros provenientes del sudeste de Asia –e incluso de Europa– que llegaron a nuestro continente por vía marítima. De acuerdo con el INAH, el Joven de Chan Hal –al igual que las otras osamentas encontradas en Tulum– apoya la idea que el Nuevo Mundo fue poblado por oleadas de inmigrantes al continente americano provenientes no solamente del Asia central, sino también del sudeste asiático. En relación a esto, en un boletín emitido el pasado 24 de julio, el INAH mostró una reconstrucción de la Mujer de las Palmas, realizada por un laboratorio francés sobre la base de los restos encontrados en la cueva cerca de Tulum. De ser acertada la reconstrucción, la Mujer de las Plamas –de 1.52 metros de estatura, 58 kilogramos de peso y de 44 a 50 años al morir– tenía la apariencia de un poblador de Indonesia en el sudeste asiático.La evidencia genética de que se dispone, sin embargo, no apoya la idea de que el continente americano fue poblado al final de la Edad del Hielo por varias oleadas de inmigrantes con características genéticas diversas, como lo discuten investigadores de las Universidades en Texas y Utah en el artículo “Dispersión de humanos modernos en las Américas en el Pleistoceno tardío”, publicado en la revista “Science” (14 de marzo de 2008). ¿Fue el continente americano originalmente poblado por inmigrantes del centro de Asia que cruzaron el Estrecho de Bering al final de la Edad del Hielo y se dispersaron gradualmente hacia el sur, como ha sido aceptado de manera tradicional? o, por el contrario, ¿hubo varias oleadas de inmigrantes desde diversos puntos de Asia, o incluso desde lugares tan “exóticos” como lo sería Europa? Los expertos no tienen todavía una respuesta sólida a estas preguntas, misma que seguramente no tardaremos en tener. Esperemos que en su búsqueda, el Joven de Chan Hol pueda aportar su granito de arena.",
    "La norma de la Ley de Residuos Sólidos del Distrito Federal, que entró en vigor el pasado 19 de agosto y que prohíbe que los establecimientos comerciales regalen bolsas de plástico a sus clientes, ha sido motivo de mucha polémica en los últimos días. Se señala, por ejemplo, que no se ha precisado lo que debe entenderse como plástico biodegradable. Los comerciantes también aducen que el costo de las bolsas proporcionadas a los compradores está ya incluido en el precio de los productos y por tanto estarían cumpliendo la norma. De acuerdo con datos de la Asociación Nacional de Industrias del Plástico, los establecimientos comerciales en el Distrito Federal proporcionan diariamente 32 millones de bolsas de plástico y, dados estos números, la norma sobre bolsas de plástico no biodegradable tiene como propósito atenuar su impacto sobre el medio ambiente. Los plásticos tienen comercialmente grandes ventajas, pues son baratos, fáciles de moldear y durables. Esta última característica, sin embargo, es la que los hace poco amigables al ambiente, pues una vez que son desechados pueden sobrevivir por largo tiempo sin degradarse.Hay una preocupación creciente entre los expertos sobre el destino de los productos de plástico una vez que son desechados. Los océanos y la fauna marina son particularmente susceptibles a la contaminación por plásticos. Un estudio realizado hace dos décadas en la costa de Carolina del Norte con 1,000 aves, por ejemplo, reveló que 55 % de ellas tenían  plásticos en los intestinos. En otro estudio, llevado a cabo hace quince años, se encontró que más del 80 % de la basura en el fondo de la bahía de Tokio estaba formada de materiales plásticos. Los plásticos pueden incorporarse al océano llevados desde los centros urbanos hasta la costa por ríos o drenajes, o bien pueden ser arrojados al mar desde barcos, de manera accidental o intencional. Una vez en el océano, las corrientes marinas los arrastran y acumulan en ciertas áreas bien definidas. Se sabe, por ejemplo, que en el  Océano Pacífico, a la altura del archipiélago hawaiano, hay una región de concentración de desechos plásticos conocida como “Gran mancha de basura de Pacífico”, que tiene una superficie mayor a la del estado de Texas. La radiación ultravioleta hace que los materiales plásticos en el océano se desintegren en pedazos –cada vez más pequeños hasta llegar a tamaños microscópicos–, de modo que la mancha de basura del Pacífico está formada por pequeños trozos de material plástico, algunos tan pequeños que son engullidos por la fauna marina.  En un artículo firmado por investigadores de California y Hawai, aparecido el pasado jueves en la revista “Science” –publicada por la Asociación Americana para el Avance de la Ciencia–, se reporta un estudio de la contaminación del Océano Atlántico norte por materiales plásticos. El estudio involucra observaciones llevadas a cabo en el periodo 1986-2008 y en el mismo se encontró que el 60 % por ciento de las más de 6,000 muestras tomadas de la superficie marina, contenían basura plástica con un tamaño típico del orden de milímetros. Se encontró una concentración de plásticos mayor en un área localizada a unos 1,500 kilómetros al este de la península de la Florida, con una densidad promedio de fragmentos plásticos entre 50,000 y 100,000 por kilómetro cuadrado. Un resultado sorprendente de este estudio –llevado a cabo a lo largo de más de dos décadas– es que la densidad de fragmentos plásticos en el área estudiada no ha aumentado con el tiempo. Esto a pesar del incremento por un factor de cinco que se ha dado a nivel global en la producción de plásticos en las tres últimas décadas, lo que llevaría a suponer que ha habido un incremento concurrente en la cantidad de estos materiales que llegan al océano. Aunque los autores del artículo de referencia no dan una explicación para esta discrepancia, entre las posibles causas que consideran es que los materiales plásticos pudieran haberse precipitado al fondo del mar por el proceso de fragmentación. Una segunda posibilidad es que, una vez fragmentados hasta un cierto punto, hayan sido ingeridos por la fauna marina. Podría haber entonces una contaminación de los océanos por plásticos más grave que lo que hasta el momento se evidencia.En México el 60 % de la basura en las playas está formada por plásticos, lo que no difiere substancialmente de lo que ocurre en otras partes del mundo (J.G.B Derraik, Marine Pollution Bulletin, 2002). Al margen de porcentajes y comparaciones, sin embargo, es evidente que los problemas de contaminación por plásticos han estado presentes en nuestro País ya por algún tiempo, y no solamente en las playas. Es relevante entonces la discusión que se está dado en el Distrito Federal sobre la Ley de Residuos Sólidos, ya que probablemente, y como ocurre con frecuencia con lo que sucede en la Ciudad de México, tendrá repercusiones para el resto del País.",
    "Una de las características de la época en que vivimos es la cada vez mayor complejidad de la tecnología a nuestro alrededor. La red Internet, por ejemplo, que hace apenas dos décadas prácticamente no existía, tiene una cada vez mayor capacidad para transmitir información y ejerce una influencia creciente en nuestras vidas. De la misma manera, la medicina desarrolla de manera continua nuevos fármacos, tratamientos y procedimientos quirúrgicos, los cuales han doblado nuestra  esperanza de vida en el curso de un siglo.La complejidad creciente de los dispositivos tecnológicos que nos rodean no puede explicarse sin el concurso del conocimiento científico. La electrónica actual, por ejemplo, requirió para el desarrollo acelerado que experimentó a lo largo del último medio siglo de la invención del transistor, la cual ocurrió a finales de la década de los años cuarenta en los Estados Unidos. Esta invención fue guiada por el conocimiento científico del que se disponía en la época acerca de la física de los materiales sólidos, que fue a su vez el   resultado de medio de siglo de estudios científicos básicos que se dieron principalmente en universidades del norte de Europa en la primera mitad del Siglo XX.Aunque nos sorprende la complejidad de la tecnología actual, los ingenios tecnológicos más complejos que conocemos –los seres vivos– no los hicimos nosotros sino la naturaleza. Podemos concebir a los seres vivos como “máquinas” altamente perfeccionadas –a través del mecanismo de selección natural– para cumplir con determinadas funciones, incluyendo la de la reproducción. Los seres vivos tienen estructuras con una complejidad más allá de lo que en estos momentos pudiéramos siquiera soñar en fabricar, de modo que como “ingeniero” la naturaleza nos supera ampliamente, por decir lo menos.    \t Para ser justos, sin embargo, hay que reconocer que la naturaleza goza de la enorme ventaja de no tener plazos para terminar sus creaciones, las cuales le han tomado decenas o centenas de millones de años; de hecho, nunca llegan a una forma final y se mantienen en continua evolución. Podemos aprovechar, por otro lado, la habilidad y experiencia de la naturaleza como ingeniero copiando y adaptando sus diseños para nuestros propósitos. Esto se está ya haciendo y en algunos casos de manera sistemática. Un ejemplo reciente es el relativo al desarrollo del dispositivo para capturar la energía del Sol y almacenarla en la forma de azúcares –tal como ocurre en el proceso normal de fotosíntesis, pero con una eficiencia mayor–, reportado recientemente por un grupo de investigadores de la Universidad de Cincinnati en la revista “Nano Letters” publicada por la “American Chemical Society”. El desarrollo está inspirado en el material con que hace su nido la rana Tungara, la cual habita zonas tropicales del planeta incluyendo a nuestro país. La conversión eficiente de la energía del Sol en azúcares es un paso intermedio para una  fabricación económicamente viable, a partir de dichos azúcares, de biocombustibles tales como el etanol. El desarrollo de biocombustibles, y en general de métodos eficientes para aprovechar la energía solar, es clave para el futuro energético de la Tierra. Como sabemos, el uso indiscriminado de combustibles fósiles ha incrementado la concentración de bióxido de carbono en la atmósfera, provocando la elevación de la temperatura de la superficie de nuestro planeta. Una disminución en la emisión de dióxido de carbono a la atmósfera es entonces indispensable, y la sustitución de los combustibles fósiles por biocombustibles es una de las acciones que se contemplan para este fin. La rana Tungara nos puede proporcionar de este modo una fuente de energía verde, y no por el color de la rana –que es más bien café con algunas rayas verdes–, sino por ser limpia, es decir, libre de emisiones de gases de invernadero. De manera adicional, podemos mencionar que, al igual que en el proceso natural de fotosíntesis, la conversión de energía solar en azúcares empleando el dispositivo basado en el material del nido de la rana Tungara se lleva a cabo mediante la absorción de bióxido de carbono de la atmósfera y por lo tanto contribuye a disminuir su concentración. Podría de este modo emplearse, según sus desarrolladores, en la vecindad de plantas emisoras de bióxido de carbono para “secuestrar” el carbono emitido e impedir su emisión a la atmósfera. Éste y muchos otros ejemplos nos muestran que, si bien a la naturaleza le ha tomado un tiempo considerable el desarrollo de las formas de vida que vemos sobre la Tierra –y que con seguridad, en la misma o diferente forma, habrán aparecido en otros lados del Universo– hasta el momento es evidente que nos supera ampliamente como ingeniero. En estas condiciones lo más inteligente que podemos hacer –y de hecho lo ya estamos haciendo– es aprender de sus diseños.",
    "El pasado 6 de agosto se cumplieron 65 años desde que Hiroshima se convirtió en la primera ciudad en sufrir un ataque nuclear. A las 8:15 de la mañana del 6 de agosto de 1945, un bombardero estadounidense B29 –apodado “Enola Gay” en honor a la madre de su capitán, Paul Tibbets”– detonó una bomba nuclear sobre Hiroshima. Entre 90,000 y 160,000 personas murieron en el momento de la explosión o en el curso de unos pocos meses. Tres días después –el día de mañana se cumplirán también 65 años–, una segunda bomba atómica fue lanzada sobre la ciudad de Nagasaki. A diferencia de Hiroshima, que es una ciudad plana, Nagasaki fue parcialmente protegida por colinas, con lo que la bomba solamente aniquiló entre 60,000 y 80,000 personas.La ceremonia de aniversario de la explosión sobre Hiroshima, que año con año se lleva a cabo en el Parque de la Paz de esa ciudad, tuvo en esta ocasión un significado especial, pues contó, por primera vez, con la presencia del embajador de los Estados Unidos en Japón, además del Secretario General de la ONU y de representantes de Francia y del Reino Unido,  países también en posesión de arsenales atómicos. Los bombardeos de Hiroshima y Nagasaki han sido siempre motivo de  controversia y de posiciones contradictorias, y en este respecto la ceremonia del pasado viernes en el Parque de la Paz no fue la excepción. Como apunta Eric Johnston, reportero de The Japan Times, si bien tanto el alcalde de Hiroshima como el Secretario General de la ONU urgieron al Japón a que haga su parte a fin de lograr la abolición de la armas atómicas en el mundo, no mencionaron nada con respecto a las exportaciones de tecnología nuclear japonesa a países que no han firmado el tratado de no proliferación de armas atómicas, como es el caso de la India. Con las diferentes posiciones que se tienen con respecto a los bombardeos de Hiroshima y Nagasaki es posible hacer un mosaico multifacético, que incluye tanto aspectos morales, como testimonios e intentos de racionalizar el uso de la bomba atómica en contra de la población civil. Como primer elemento de dicho mosaico es menester incluir  a los “Hibakusha”, es decir a los sobrevivientes de los bombardeos atómicos, contando historias espeluznantes acerca que lo que vivieron inmediatamente después de la explosión –visiones apocalípticas de personas con los ojos reventados o caminando como fantasmas sin rumbo fijo con la piel colgando a modo de jirones, entre otros muchos horrores– y de los problemas de salud que han padecido a los largo de su vida por efectos de la misma.Contrastamos estos testimonios con las declaraciones recientes de Gene Tibbets, hijo del piloto del Enola Gay, en las que “truena” contra la administración del Presidente Obama por haber enviado a su embajador en Japón a la ceremonia en el Parque de la Paz. En declaraciones a Fox News dijo que esto constituye una apología implícita al bombardeo de Hiroshima que su padre nunca hubiera aprobado. Afirmó: “Se esta haciendo que parezca que los japoneses fueron una pobre gente, como si no hubieran hecho nada. Atacaron Pearl Harbor y nos golpearon. Nosotros no matamos a los japoneses. Nosotros paramos la guerra”. Retrocediendo 65 años en el tiempo –a la manera de una película con “flashback”–, podemos imaginar a Robert Oppenheimer, cabeza científica del proyecto Manhattan mediante el cual se desarrolló la bomba atómica –y una persona intelectualmente superior a todos los demás participantes en dicho proyecto, según Hans Bethe, también participante y premio Nobel de Física– a Enrico Fermi, a Arthur Compton y a Enrnest Lawrence, todos premios Nobel de Física, firmando un carta en la que le expresan su recomendación sobre el uso militar inmediato de la bomba contra Japón, como único medio de acortar la guerra y salvar vidas norteamericanas. La carta fue dirigida al “Interim Committee”, un comité de alto rango formado en mayo de 1945 por el Secretario de Guerra norteamericano, el cual a su vez hizo la recomendación de que la bomba se usara sin aviso previo contra una instalación militar o fabrica rodeada de casas habitación o de otros edificios susceptibles de sufrir daños. No menos memorable es la fotografía del físico norteamericano Luís Walter Álvarez –igualmente premio Nobel de Física–, posando enfrente del bombardero que acompaño al Enola Gay en su misión a Hiroshima, y apodado, en forma irónica, “El gran artista”. Álvarez viajó como observador científico a bordo de este bombardero y realizó mediciones de la intensidad de la explosión por medio de detectores montados en paracaídas, los cuales fueron lanzados sobre Hiroshima momentos antes de la explosión.Hay que mencionar, no obstante, que si bien hubo científicos de alto nivel que aprobaron el uso militar de la bomba y que colaboraron en su desarrollo, hubo también intentos por parte de otros científicos para tratar de evitarlo. Estos intentos, sin embargo, fueron infructuosos. Más allá de un imperativo moral, como un argumento en contra del uso de la bomba se adujo que con una acción como ésta se desataría una carrera nuclear armamentista y que los Estados Unidos no tendrían fuerza moral para frenarla. Los años nos han mostrado lo acertado de estos razonamientos.",
    "Cuando en 1877 el astrónomo italiano Giovanni Schiaparelli observó el planeta Marte a través del telescopio del observatorio astronómico de Brera en Milán, le pareció ver que la superficie del planeta estaba surcada por líneas rectas que conjeturó eran canales de agua. Las observaciones de Schiaparelli entusiasmaron a Percival Lowell, fundador del observatorio Lowell en Arizona, quién a principios del Siglo XX defendió vigorosamente la idea que los canales de Schiaparelli fueron obra de una civilización inteligente y que fueron construidos para llevar agua desde los polos marcianos hasta el ecuador del planeta. Pocos, sin embargo, siguieron a Lowell en sus ideas, y hoy se sabe que los supuestos canales resultaron ser solamente una ilusión óptica.Aunque no podemos descartar que en algún momento haya existido una civilización inteligente en Marte, no tenemos ninguna evidencia de que así haya sido. No es aventurado conjeturar, sin embargo, que en Marte exista actualmente vida a nivel microbiano. De hecho, existen un número de iniciativas y proyectos de gran seriedad que buscan averiguarlo. Marte constituye un medio ambiente extremadamente agresivo para la vida tal como la conocemos. Según podemos apreciar en las imágenes que han enviado a la Tierra las sondas “Spirit” y “Opportunity”, la superficie marciana es un lugar árido y desolado que poco invitaría a visitar. Marte, además, está considerablemente más alejado del Sol que la Tierra  y por lo mismo es más frío, con temperaturas que pueden llegar hasta los –80 grados centígrados aun en el ecuador. Tiene una atmósfera muy poco densa –lo cual hace que haya una variabilidad de temperatura entre el día y la noche de hasta 100 grados centígrados–, que está compuesta casi enteramente de dióxido de carbono con una muy pequeña cantidad de oxígeno. Además, la tenue atmósfera marciana no filtra adecuadamente la radiación ultravioleta de los rayos solares, la cual sabemos es incompatible con la vida. Un estudio que en días pasados alcanzó los medios masivos de difusión fue reportado por un grupo de científicos españoles que demostraron que algunas bacterias que se encuentran en la cuenca del Río Tinto en el sur de España, sobrevivirían a las duras condiciones ambientales de Marte con excepción de la relativa a la irradiación con rayos ultravioleta. El estudio fue publicado el pasado mes de junio en la revista “Icarus” publicado por la Sociedad Astronómica Americana. Otro estudio de investigadores italianos de la Universidad de Padua, en proceso de publicación, arroja resultados similares. Podría concluirse entonces que, al abrigo de los rayos ultravioleta –en el subsuelo, por ejempo– podría existir vida microbiana en Marte.   Para determinar esta posibilidad, la NASA planea enviar la sonda llamada “Mars Science Laboratory” a Marte en el otoño de 2011, planeando que arribe un año después. La sonda tendrá capacidad de movimiento y podrá analizar muestras del suelo marciano y detectar la posible presencia de compuestos orgánicos asociados con la vida. Dentro del sistema solar Marte es el mundo más parecido a la Tierra. Venus, que es el otro planeta vecino nuestro, está más cerca del Sol y tiene una atmósfera de dióxido de carbono muy densa que lo hace  demasiado caliente y aun más inhóspito que Marte –la temperatura en la superficie de Venus es de 460 grados centígrados–. Venus no es por lo tanto afín a la vida, al menos como la conocemos en la Tierra. Más allá de la órbita de Marte, se considera que Titán, el satélite mayor de Saturno, podría tener condiciones favorables para la síntesis de compuestos orgánicos. Al igual que en la atmósfera de la Tierra, en la atmósfera de Titán predomina el nitrógeno y, a diferencia de aquella, contiene cantidades significativas de metano. Se piensa que la atmósfera de Titán es similar a la atmósfera primitiva de la Tierra antes de la aparición de la vida. Titán, sin embargo, es extremadamente frío, con temperaturas alrededor de los –180 grados centígrados. A estas temperaturas toda el agua en Titán está congelada. De hecho, en Titán el metano toma el lugar del agua: existen lagos y ríos de metano y de cuando en vez, al igual que en la Tierra, se producen lluvias, pero de metano en lugar de agua. De haber vida en Titán, esta tendría sin duda que ser muy diferente a la vida en nuestro planeta.Si hay vida extraterrestre en el sistema solar lo sabremos quizá pronto, una vez que el Mars Science Laboratory empiece a enviar información. En caso positivo, esto constituirá, sin duda, uno de los acontecimientos más significativos jamás ocurridos.  La posible vida que se encontrara, sin embargo, tendría una forma muy simple, pues en ningún lugar del sistema solar fuera de nuestro planeta encontraremos un medio ambiente propicio para el desarrollo de formas de vida superior tal como la conocemos. Otros lugares, o son demasiado fríos o calientes, o están llenos de radiación ultravioleta, o bien no tienen el oxígeno que necesitamos para respirar. En otro orden de ideas, podríamos concluir que más nos vale cuidar nuestro planeta, pues si seguimos con el curso de destrucción y contaminación del mismo que ahora llevamos, no tendríamos a la mano en nuestra vecindad ningún mundo alternativo a donde emigrar.",
    "El huracán “Alex”, que asoló el noreste de nuestro país al inicio del presente mes de julio, fue la primera tormenta tropical de la temporada de huracanes 2010 en el Océano Atlántico, la cual empezó oficialmente el pasado 1 de junio. No tuvimos que esperar mucho para que llegara la segunda tormenta, “Bonnie”, que está en curso, como bien podemos constatar por las lluvias que hemos padecido en los últimos días, encontrándose localizada en estos momentos en el norte del Golfo de México frente a las costas de Luisiana. Hay quienes sostienen que el calentamiento global, producido por la cada vez mayor concentración de gases de invernadero en la atmósfera –y que se ha traducido en un incremento de alrededor de medio grado centígrado en la temperatura de los océanos en los últimos treinta años–, es responsable de la ocurrencia de eventos climáticos “extremos”; es decir, de ondas de calor o de frío, sequías, huracanes, entre otros fenómenos, con intensidades por encima de sus valores históricos promedio. Se puede señalar, por ejemplo, que en  un artículo publicado el presente año, investigadores de la Universidad de Stanford  sostienen que debido al cambio climático, las ondas de calor como la registrada recientemente en la costa este de los Estados Unidos, serán comunes en las próximas décadas.En referencia a los huracanes, en los últimos años  hemos tenido algunos que han producido grandes devastaciones en nuestra región. En 1988 el huracán Gilberto asoló la península de Yucatán y, al igual que lo hizo el huracán Alex hace unas semanas, afectó gravemente el noreste de nuestro País. En 2005 el huracán Katrina golpeó a la ciudad de Nueva Orleáns, causando una de las peores catástrofes naturales en la historia de los Estados Unidos, mientras que en 1998 el huracán “Mitch” provocó inundaciones y mató a más de 10,000 personas en el norte de Honduras.Hay que hacer notar, además, que ha habido años con un número de huracanes muy por encima del promedio histórico. En el año de 2005, por ejemplo, ocurrieron un total de 15 huracanes, 7 con una categoría mayor (3, 4 o 5). Entre estos últimos se encuentran el Katrina y el Wilma; este último, el huracán más intenso jamás observado en el Océano Atlántico, con vientos máximos de 295 kilómetros por hora. De la misma manera, durante el año de 1950 ocurrieron 8 huracanes con una categoría 3-5, que es el número mayor registrado en el Atlántico.En las últimas décadas, no obstante, no se observa una tendencia positiva en el crecimiento del número de huracanes a nivel mundial y en particular de huracanes en el Océano Atlántico. Por ejemplo, el número de ciclones que han golpeado las costas de los Estados Unidos ha mantenido un promedio de aproximadamente 18 por década, si se incluyen todas las categorías (1-5) en la escala de Saffir-Simpson, y de 6 si consideramos solamente las mayores (3-5), sin mostrar una tendencia clara a crecer o decrecer. Las fluctuaciones observadas en el número de huracanes obedecerían de este modo a causas naturales y no estarían relacionadas con el cambio climático. No podríamos atribuir la ocurrencia de un evento climático particular, el huracán Alex, por ejemplo, al calentamiento global No obstante lo anterior, Kerry Emmanuel del Instituto Tecnológico de Massachussets, a través de un análisis del registro histórico de huracanes, sostiene que si bien éstos no han aumentado en número en los últimos años, su potencia a lo largo de su periodo de vida sí se ha incrementado en consonancia con la elevación de la temperatura de los océanos. Este punto de vista, sin embargo, es controvertido.Por su lado, el Panel Intergubernamental sobre Cambio Climático apunta que de continuar en el futuro el incremento de la temperatura de los océanos por el efecto invernadero, probablemente se incrementen de manera concurrente, tanto la velocidad máxima del viento de los huracanes, como la precipitación pluvial que ocasionen.¿Seremos testigos en un futuro de eventos climáticos extremos –huracanes, sequías, ondas de calor o de frío– causados por un calentamiento global creciente? De momento, posiblemente no existan evidencias científicas suficientes para contestar esta pregunta en un sentido o en otro. Si encontráramos en un futuro que la respuesta es positiva, sin embargo, nuestro País estaría muy pobremente equipado para sortear el problema. Esto es evidente si consideramos que la devastación causada por Alex en Monterrey se dio solamente dos décadas después del desastre similar debido a Gilbert, sin que aparentemente se hayan tomado las medidas preventivas suficientes. Si la frecuencia de los huracanes de categoría mayor aumenta en el futuro, no podríamos esperar nada positivo, ni para Monterrey y ni para otras ciudades de nuestro País.",
    "Durante los casi dos siglos que han transcurrido desde nuestra independencia política de España, hemos visto como México ha padecido grandes dificultades para lograr un desarrollo social y económico suficiente, y se ha visto ampliamente superado en este respecto por otros países. Entre éstos se incluyen a los Estados Unidos que inició su expansión en el   Siglo XIX –parcialmente a expensas de México–, y a Japón y Corea de Sur, que en la segunda mitad del Siglo XX tuvieron un crecimiento económico y tecnológico impresionante. Mientras que hay múltiples factores que han impedido que México haya dado el “salto” hacia delante, un sistema educativo insuficiente en todos los niveles, y en particular en el nivel superior,  ha sido sin duda  un elemento clave. A lo largo del México independiente se han dado un buen número de iniciativas educativas, inspiradas en ejemplos de países desarrollados. Una de las más significativas es la ley que reglamentó la instrucción pública en el Distrito Federal y sus territorios promulgada por el gobierno de Benito Juárez en 1867, y que fue elaborada bajo la dirección de Gabino Barreda siguiendo las ideas positivistas de Augusto Comte. De dicha ley se derivó la creación de la Escuela Nacional Preparatoria, que mucha influencia tuvo en el desarrollo educativo de México. No es suficiente, sin embargo, transplantar técnicas y métodos de enseñanza desarrollados en otras latitudes, sin adaptarlas al medio en el que se van a aplicar, pues los educandos no son los mismos en todas partes. En relación a esto, en un artículo reciente publicado por investigadores de la Universidad de Columbia Británica en Canadá (J. Henrich y colaboradores, “Behavioral and Brain Sciences”, 2010) se critican las generalizaciones que muchas veces se hacen sobre algunos aspectos psicológicos de  la especie humana, y se arguye que muchos estudios al respecto se llevan a cabo con poblaciones de países industrializados que no siempre resultan representativos de la población en general. Apuntan, por ejemplo, que aspectos sobre razonamiento analítico y de cooperación entre individuos están influenciados por el entorno cultural.   Incluso aspectos relativos a la percepción visual están determinados por dicho entorno y no por los genes. Por ejemplo, es bien conocida la ilusión óptica que se produce cuando se dibujan dos líneas paralelas de la misma longitud, terminados en ambos lados por puntas de flecha. Una de las líneas se termina con flechas encontradas y la otra con flechas divergentes. A pesar de que las dos líneas tienen la misma longitud, aquella que se dibuja con flechas encontradas parece tener una longitud menor que la otra. Esta ilusión óptica, sin embargo, no es general, como apuntan Henrich y colaboradores y, por ejemplo, no la experimentan los miembros de la tribu San en el sur de África.    Podríamos quizá concluir entonces que las técnicas educativas deben diseñarse de manera  específica a la sociedad en la cual van a ser aplicadas. En referencia a la educación superior en México, muchas veces y en ciertos campos del conocimiento se emplean textos elaborados en los países industrializados. Mientras que un porcentaje relativamente pequeño de estudiantes no tiene problema alguno para aprovechar dichos textos, no está claro que tan pertinentes son para el resto. Tanto Japón como los Estados Unidos, dos de los países más poderosos económicamente, cuentan con un sistema exitoso de educación de nuevos investigadores y profesionales de alta capacitación acorde con su desarrollo económico. Ambos sistemas tienen características propias, adaptadas a la idiosincrasia de japoneses o estadounidenses según el caso. Es conocido, por ejemplo, que estos últimos tienen un individualismo extremo, mientras que los japoneses tienden a pensar más en función del grupo al que pertenecen. Así, durante su entrenamiento doctoral, la tendencia es que un estudiante norteamericano se enfrente y resuelva por si mismo el problema que le fue asignado como tesis. Un estudiante japonés, en contraste, tiende a recibir más ayuda de sus compañeros de grupo y por lo mismo avanza más rápido en su trabajo de tesis. Esto último, sin embargo, tiene un costo y es posible que en promedio el graduado norteamericano tienda a ser más creativo que su contraparte japonesa. Esto último posiblemente generó la noción prevaleciente hace algunas décadas de que los japoneses “no inventan sino sólo copian”. Cierto o falso, sin embargo, esto no fue un impedimento para que el Japón haya tenido el impresionante despegue tecnológico que tuvo en décadas pasadas.       En México, en contraste, no se ha logrado todavía establecer un “estilo” de cómo educar a un estudiante de posgrado. Los mexicanos no tenemos el individualismo de los norteamericanos ni somos tan gregarios como los japoneses. Nuestro estilo educativo debería ser entonces una combinación de las prácticas japonesa y norteamericana. La tradición nuestra en la formación de doctores, sin embargo, es todavía demasiado joven e incipiente para que tenga una personalidad definida. Esperemos que no nos tome otros doscientos años en lograrlo.",
    "El campeonato mundial de fútbol de Sudáfrica está a punto de finalizar. Se ha caracterizado porque un buen número de juegos han sido muy cerrados, imperando el juego defensivo y de cautela extrema. Desde el punto de vista del promedio de goles anotados por partido, el presente campeonato ha sido uno de los más pobres jamás celebrados, siendo sólo superado en este respecto por el torneo de Italia en el año de 1990. En medio de este clima de “desolación futbolística”, ha hecho una aparición espectacular el “Pulpo Paul”, que presumiblemente tiene facultades “psíquicas” que le han permitido predecir con éxito los resultados de los seis partidos de fútbol que a la fecha ha celebrado Alemania en Sudáfrica. Para posibilitar que exprese sus predicciones, a Paul se le presentan dos recipientes transparentes, identificados uno con la bandera de Alemania y el otro con la del país adversario, en los que se colocan ostiones o algún otro molusco de los que le sirven de alimento. La predicción del ganador queda establecida cuando el pulpo escoge uno de los dos recipientes, abre la tapa y se come al molusco.No está claro, sin embargo, qué sucede a continuación: si Paul simplemente ignora el segundo recipiente para no dejar duda alguna de su predicción, o si bien  también abre el otro contenedor y se come al segundo molusco. Esto último daría pie a los escépticos para argüir que en realidad lo que el pulpo intenta hacer no es predecir el resultado del partido de fútbol sino comerse los dos moluscos –y que tiene que empezar, naturalmente, por alguno de ellos–. Habría que reconocer también que el mecanismo de predicción es imperfecto, pues ¿cómo podría Paul predecir un empate? ¿comiéndose los dos moluscos? ¿no comiéndose ninguno? Estas preguntas son irrelevantes una vez que se llega a la etapa de octavos de final, cuando tiene que haber un ganador; no los son, sin embargo, en la ronda clasificatoria previa.Para que Paul tuviera la capacidad de predecir los resultados del fútbol sería necesario que existieran los fenómenos “psíquicos” paranormales que permiten predecir el futuro. Sería necesario también que Paul tuviera un nivel de inteligencia suficiente –mismo que compartiría en cierto grado con otros pulpos de su especie– tanto para experimentar dichos fenómenos como para expresarlos de la manera adecuada –reconociendo, por ejemplo, la bandera alemana en cuanto la viera–. Además, posiblemente debería contar con conocimientos futbolísticos y estar informado de la capacidad de Alemania y de sus posibilidades en la copa. Todo esto, sin embargo, es presuponer demasiado. Una explicación más simple es que las predicciones de Paul son en realidad producto del azar. En el fútbol moderno, en el que imperan las tácticas defensivas y los equipos juegan con una gran cautela, la definición de un partido depende muchas veces de un sólo fallo defensivo o acción ofensiva afortunada. Esto hace incierta la predicción del resultado de un partido de fútbol, aun para los conocedores de este deporte. De este modo, sin dejar de reconocer que el equipo con mejores jugadores o mejores tácticas de juego tiene mayores posibilidades de éxito, una predicción al azar –equivalente a predecir el resultado que se obtiene (águila o sello) al lanzar una moneda al aire– en muchos casos podría ser casi tan buena como un producto de consideraciones más profundas.Suponiendo, para simplificar, que los resultados de fútbol son producto del azar  ¿Qué tan  probable es que por puro azar Paul haya hecho seis predicciones correctas? Lo primero que notamos es que esta probabilidad es la misma para cualquier posible combinación de resultados. Es decir, era tan probable que Paul hubiera hecho seis predicciones correctas que seis incorrectas. Era igualmente probable que hubiera predicho que Alemania ganaría todos sus partidos –ganó cinco, perdiendo solamente el segundo con Serbia–, obteniendo cinco aciertos y un fallo. Para saber qué tan probable es acertar en todos los partidos es necesario entonces averiguar cuántas posibles combinaciones de resultados puede haber. Estas combinaciones son 64, de modo que la probabilidad de acertar en los seis partidos es 1 en 64.Pareciera en primera instancia que una probabilidad de 1 en 64 es muy pequeña, lo que daría pie a pensar que, después de todo, el pulpo pudiera tener facultades paranormales. Debemos considerar, no obstante, que el caso de Paul ha sido muy publicitado, precisamente por su éxito,  y que de haber fallado –en sus dos primeras predicciones, por ejemplo–, la mayoría de nosotros no sabríamos de su existencia. Además de Paul hay muchos animales con supuestas facultades paranormales en el mundo –probablemente muchos más que 64– que no han sido exitosos y que no conocemos. De hecho, aprovechando la publicidad que generó Paul, en los últimos días han aparecido en la prensa otros animales “psíquicos”, incluyendo otro pulpo, un perico, una tortuga y una foca, haciendo predicciones sobre el resultado de la final de Sudáfrica en un sentido y en otro. Algunos fallarán y otros tendrán éxito. Esto último, sin embargo, no será prueba de que tengan facultades paranormales.   La copa mundial de fútbol es uno de los eventos deportivos que mayor audiencia captan. Sin embargo, es desconcertante constatar que –debido quizá a la ausencia de goles– un “pulpo adivino” haya captado buena parte de una atención que se supondría debía estar concentrada en aspectos futbolísticos.",
    "Aunque no han pasado aun dos décadas desde que la red Internet empezó a expandirse por el mundo, nuestra dependencia con respecto a la misma se ha vuelto crítica. Hoy en día, Internet se usa para una multitud de actividades que incluyen la realización de operaciones bancarias, el pago de impuestos, la compra de artículos a distancia, la comunicación personal y la búsqueda de información de todo tipo, por mencionar solamente algunos usos comunes de este moderno sistema de comunicación. Tanto nos hemos acostumbrado a la red Internet, que frecuentemente perdemos conciencia de cuanto hemos llegado a depender de ella. Los anterior, por supuesto, hasta el momento en que nos vemos privados de su uso por alguna razón, como sucedió en días recientes por efectos del ciclón Alex. El día que pasamos sin “red” nos da oportunidad de reflexionar sobre la Internet, uno de los desarrollos tecnológicos que más ha contribuido a cambiar el mundo. La Internet tuvo su origen en la llamada red Arpanet –financiada por el Departamento de Defensa de los Estados Unidos– que en 1969 enlazó computadoras en cuatro universidades en California y Utah para la transmisión de datos. No fue, sin embargo, hasta el año de 1991 –cuando se creó la “World Wide Web” – que Internet empezó a funcionar tal como la conocemos ahora. Para esto fue necesario, entre otras cosas, que el precio de las computadoras bajara lo suficiente para que su uso se extendiera entre la población en general, lo que ocurrió en la década de los años ochenta.Entre otras cosas, la red Internet ha revolucionado las comunicaciones a distancia y ha hecho al mundo más pequeño, de modo que hoy en día es posible establecer contacto de manera prácticamente instantánea y a un costo reducido con  personas incluso en otros continentes. Es igualmente posible accesar la versión electrónica de periódicos de buena parte del mundo, con las últimas informaciones locales. Todo esto era impensable hasta hace muy pocos añosOtra revolución que nos ha traído Internet –una de las más significativas– se ha dado en el campo de la diseminación del conocimiento. Esto con la aparición de los bancos de información “wiki”, ejemplificados por la enciclopedia electrónica Wikipedia. Esta enciclopedia, según su página web, acumula hoy en día más de tres millones de artículos en inglés de los más diversos temas, los cuales pueden ser consultados de manera gratuita. Cualquier persona puede contribuir a la Wikipedia, ya sea modificando el contenido de un determinado artículo o bien creando otro nuevo. En contraste con una enciclopedia tradicional –por ejemplo la Enciclopedia Británica–, el autor de un artículo en la Wikipedia no necesita ser un experto en el tema a tratar. Además, los artículos en esta enciclopedia no pueden referirse a temas de investigación, sino que deben corresponder a conocimiento previo que pueda ser validado. La estructura abierta de la Wikipedia la hace vulnerable a la posibilidad de que un determinado autor introduzca información falsa o poco precisa, ya sea por error, por desconocimiento, o incluso deliberadamente como un acto vandálico. Esto la ha hecho blanco de críticas por parte de aquellos que la consideran una fuente de información poco confiable. No obstante –como algo intrínseco a la apertura de la Wikipedia–, se tiene que un artículo publicado en la misma está sujeto a un proceso de modificación continua por parte de aquellos autores que consideran que contiene errores. Esto presumiblemente lleva a la eliminación de dichos errores. Los defensores de la Wikipedia hablan incluso de un proceso de evolución “darwikiniano” –análogo al darwinismo social–  por medio de la cual un artículo publicado en la Wikipedia sufre mejoras continuas que llevan a la eliminación de aquellos errores que pudiera contener. De este modo, entre más tiempo haya permanecido un determinado artículo en la Wikipedia sin sufrir modificaciones, mas confiable será su contenido.  Se hace notar, por otro lado, que en un estudio publicado en la revista “Nature” en el año de 2005 se encontró que en realidad la Wikipedia es solamente un poco menos precisa que la Enciclopedia Británica, la cual también contiene errores.  A pesar de las dudas razonables sobre la precisión de la información contenida en la Wikipedia –que aconsejan su uso de manera cuidadosa– cualquiera que la haya usado puede constatar su utilidad, al menos como una primera aproximación al tema. Una característica de los artículos publicados en la Wikipedia es que contienen referencias con vínculos a otras páginas –incluso originales, aunque no en todos los casos–  que pueden ser consultados de manera inmediata. De este modo, la rapidez con la que la Wikipedia nos  puede introducir a un determinado tema, incluyendo la consulta inmediata de sus fuentes de información es algo que no tiene precedentes. Internet tiene tantas ventajas que rápidamente la hemos adoptado y hecho parte de nuestra vida diaria –a lo bueno nos acostumbramos rápido–. Por lo mismo, la Internet ha “desaparecido” de nuestra vista, dándose a notar solamente cuando efectivamente desaparece, como sucedió en días pasados.",
    "Quienes vivimos en la Ciudad de San Luís Potosí hemos podido constatar la congestión vehicular que a lo largo de los últimos años se ha dado de manera creciente en nuestras calles, sobre todo a las horas pico. Este problema, que durante décadas ha afectado a la Ciudad de México, se ha hecho cada vez más presente en San Luís Potosí y en otras ciudades similares de nuestro País.Una ojeada a datos estadísticos del INEGI nos muestra que, efectivamente y de acuerdo con nuestras apreciaciones, el número de vehículos automotores en nuestra ciudad está creciendo de manera acelerada. Un análisis de dichos datos nos indica que este número se duplica aproximadamente cada nueve años, tendencia que se ha mantenido desde mediados de la década de los años ochenta y hasta cuando menos 2008 –último año reportado en las estadísticas del INEGI–. De este modo, a lo largo del último cuarto de siglo el número de vehículos en la ciudad de San Luís Potosí ha crecido exponencialmente con un periodo de duplicación inferior a una década, con todos los peligros que esto representa. \tAsí, mientras que en 1988 circulaban en nuestra ciudad poco más de 66,000 vehículos, este número aumentó a más 360,000 en 2008, lo que representa un incremento de más de 400 % en un lapso de  20 años. Este incremento está en marcado contraste con el correspondiente incremento de la población de la Ciudad de San Luís Potosí de menos de un 100 % en el mismo periodo.De acuerdo con los números anteriores, para manejar el creciente número de vehículos automotores, en el último cuarto de siglo la ciudad de San Luís Potosí  tendría que haber multiplicado por un factor de casi ocho la capacidad de sus calles y vías rápidas para manejar el creciente tráfico vehicular. La dificultad de lograr esto explica las también crecientes dificultades que ahora experimentamos para circular en la ciudad. Dada la marcada disminución en la producción de vehículos en México en el año 2009 por la crisis económica –de un 28%, según datos de la Asociación Mexicana de la Industria Automotriz (AMIA)–, es esperable que el ritmo de crecimiento en el número de automotores haya disminuido –lo que nos habría dado a los automovilistas una suerte de respiro: un punto positivo entre el cúmulo de problemas que nos acarrea la crisis económica–. Es posible, sin embargo, que el ritmo de crecimiento de los automotores circulantes se recupere una vez superada esta crisis –de hecho, según datos de la AMIA, la producción de automóviles alcanzó en los primeros cinco meses de 2010 las mismas cifras de los correspondientes meses de 2008, lo que significaría que la recuperación ya se alcanzó o va en camino de lograrse. Por otro lado, como nos lo demuestra el caso de la Ciudad de México, ganarle la carrera al crecimiento acelerado del número de automóviles es punto menos que imposible, pues las soluciones siempre van atrás del problema. De acuerdo a las estadísticas del INEGI, la tasa de incremento promedio del número de vehículos en el Distrito Federal en el periodo 1980-2003 fue considerablemente menor –y con altibajos– que el que actualmente se observa en nuestra ciudad. Es posible, sin embargo, que esto obedezca a una especie de límite natural o imposibilidad física para circular en un espacio reducido –o posiblemente también al proceso de descentralización de la capital del país hacia los estados que incluye la descentralización de los congestionamientos de tráfico–. A partir de 2003, sin embargo, la tasa de incremento de vehículos en el Distrito Federal alcanzó valores similares a los de San Luís Potosí. Esto pudiera deberse a la puesta en funcionamiento de los primeros “segundos pisos”. La industria automotriz es una de las más importantes en nuestro País, representando el 3% del producto interno bruto y el 3% del empleo. Frente a todas sus virtudes, sin embargo, la producción sostenida de automóviles y las facilidades para adquirirlos están congestionando las calles de nuestras ciudades y contaminando la atmósfera con desechos tóxicos y gases de invernadero. Si bien esto por ahora podría considerarse un mal necesario –dadas las condiciones de fragilidad económica del País–, en un futuro inmediato debe contemplarse seriamente un cambio en nuestro estilo de vida, privilegiando el transporte público por sobre el transporte en automóviles.  De seguir creciendo el número de automóviles en San Luís Potosí a la tasa actual, en menos de una década la ciudad tendría que duplicar la capacidad de sus calles para manejar el tráfico vehicular; y esto solamente para conservar el actual estado de cosas. De otro modo pudiéramos llegar a una situación en la que el número de automóviles en la ciudad se autolimite simplemente porque no habría suficiente espacio para su circulación. Si fuera éste el caso, seguramente en una década se considerarán a los automóviles más como una “plaga” que como una ventaja.",
    "El pasado 13 de junio la sonda japonesa Hayabusa regresó a la Tierra después de un viaje de 7 años al asteroide Itokawa. Este asteroide, que tiene la forma de un cacahuate de unos 500 metros de longitud, sigue una órbita excéntrica alrededor del Sol que lo acerca a la órbita de la Tierra y lo lleva más allá de la órbita de Marte. Aunque no se tiene todavía la certeza, se espera que la sonda haya regresado a nuestro planeta con muestras de material del asteroide. El estudio de dichas muestras daría información sobre el origen del sistema solar, pues se considera que los cuerpos celestes como el Itokawa han permanecido sin cambios desde la formación de dicho sistema, en contraste con cuerpos más grandes como la Luna o nuestro propio planeta.Hayabusa arribó al asteroide Itokawa en septiembre de 2005, descendiendo un par de veces a su superficie a recoger muestras. Debido a la lejanía de Itokawa con la Tierra, la sonda tuvo que llevar a cabo los descensos de manera autónoma, pues las instrucciones que le pudieran haber sido enviadas desde la Tierra habrían tardado varios minutos en llegar por la lejanía a la que se encontraba, lo que hubiera imposibilitado su control desde el centro de mando. Unas tres horas antes de ingresar a la atmósfera terrestre, la nave Hayabusa expulsó una cápsula con las muestras del asteroide, la cual descendió exitosamente en una región desértica del sur de Australia después de soportar enormes temperaturas por el rozamiento atmosférico. Pronto sabremos si en su interior la cápsula contiene efectivamente material de Itokawa. Aunque la sonda Hayabusa experimentó una serie de fallas que le impidieron completar todos los proyectos que habían sido planeados –por ejemplo, el envío de la sonda Minerva de apenas medio kilogramo a la superficie de Itokawa para una exploración más detallada de la misma– la misión constituyó un logro impresionante de la tecnología japonesa. Hay que notar que Japón ha formado parte del club espacial desde el año de 1970, cuando puso en órbita –por sus propios medios– su primer satélite artificial. Hoy en día son diez países, y dos compañías privadas norteamericanas, los que tienen esta capacidad. El último país en ingresar al club espacial fue Irán, en el año 2009. En una fecha no distante del regreso de Hayabusa a nuestro planeta, el 20 de abril pasado la Cámara de Diputados creó la Agencia Espacial Mexicana (AEXA). De acuerdo con el  decreto de creación, publicado en la Gaceta Parlamentaria, la AEXA tendrá como objetivo la definición, promoción y articulación de las actividades espaciales en México. Como apoyo se asienta que: “Se detectaron tres vertientes de oportunidad para México en materia espacial, entre ellas se encuentran: el desarrollo del terreno lunar gracias a nuestra capacidad probada en la industria automotriz; el desarrollo en el área textil, para la construcción de hábitats lunares, y la operación nacional de sistema de GPS, en el área de pequeños satélites.” El proyecto de creación de la AEXA fue impulsado por, entre otros, el astronauta norteamericano de ascendencia mexicana José Hernández Moreno, quién viajó a la estación espacial a bordo del transbordador Discovery en agosto de 2009.    Se ha tachado a la AEXA de innecesaria y de ocurrencia exótica en un país de grandes carencias, y de que se convertirá en un elefante blanco. Ciertamente sería absurdo plantear la posibilidad de enviar en las próximas décadas con nuestros medios un astronauta al espacio o de construir una sonda equivalente a Hayabusa, algo que  posiblemente ni aun los más decididos impulsores de la AEXA lo estén planteando –al menos  al corto plazo. Sí se contemplan, sin embargo, cosas que de la misma manera están posiblemente fuera de nuestro alcance, incluso a un mediano plazo. No es posible pensar, por ejemplo, que la industria automotriz en México, que es básicamente maquiladora, nos constituya una ventaja para fabricar un vehiculo lunar, o que nuestra industria textil nos capacite para fabricar hábitats lunares. La iniciativa de crear la AEXA, no obstante, resultará positiva si de ésta deriva un impulso a tecnologías que han sido desarrolladas para y por la industria espacial. En este grupo podemos incluir a las celdas para el aprovechamiento de la energía solar, a la predicción del clima por medio de satélites y al inventario de los recursos naturales del país de manera remota empleando satélites, entre otras muchas aplicaciones derivadas de la industria espacial. Un impulso serio a las actividades espaciales en nuestro país que nos llevara en un futuro a la realización de proyectos de cierta envergadura –aun sin pretender alcanzar la sofisticación de proyectos como el de la sonda Hayabusa–, requerirá, sin embargo, de recursos considerablemente más cuantiosos que los diez millones de pesos aprobados para el primer año de operación de la AEXA. Esta cantidad será mucha si dicho proyecto resulta solamente en la creación de una oficina burocrática, y demasiado poca para que la AEXA tenga un impacto real en el desarrollo científico y tecnológico de México.",
    "La dimensión de la copa del mundo de fútbol, que se ha celebrado cada cuatro años desde 1930 –con la excepción de 1942 y 1946 debido a la Segunda Guerra Mundial–, se ha ido incrementando de manera progresiva a partir de su cuarta edición celebrada en Brasil en 1950. Así, de los 13 equipos que participaron en Brasil, se pasó a 16 en Suiza 1954, a 24 en España 1982 y, finalmente, a 32 en Francia 1998. De la misma manera, la copa del mundo, que hasta hace cincuenta años era básicamente una competencia entre países de Europa y América, se ha extendido a países de los cinco continentes. En la presente edición que se está celebrando en Sudáfrica, de un total de 32 países participantes, 11 lo son de África, Asia u Oceanía. Juntamente con los juegos olímpicos, la copa mundial de fútbol es uno de los eventos de  mayor audiencia televisiva tiene a nivel global. En particular, la final de la copa del mundo es el evento deportivo que más audiencia logra en todo el mundo. Por otro lado, es interesante señalar que, en contraste con su expansión mundial, el fútbol  profesional como espectáculo ha ido en declive en el último medio siglo. Esto es cierto si lo medimos por el número promedio de goles –que se supone constituyen la esencia del fútbol– que se han marcado por partido en los diversos campeonatos del mundo. En efecto, encontramos que mientras que en Suiza 1954 se marcaron un promedio de 4.8 goles por partido, en el campeonato del mundo de Italia en 1990 esta cifra fue de solamente 2.2 goles en cada encuentro.  El declive más acusado en el número de goles por partido se dio en los campeonatos de 1958 en Suecia y de 1962 en Chile, cuando descendió de 4.8 goles por encuentro –Suiza 1954–, a 3.6 y 2.8 goles por partido, de manera respectiva. Curiosamente, estos fueron los torneos en los que Brasil obtuvo sus dos primeros campeonatos mundiales, en los que, lejos de ser defensivo, mostró una fuerte vocación ofensiva con jugadores de la talla de Pelé y Garrincha en su línea delantera. En Suecia, por ejemplo, Brasil anotó un total de 16 goles recibiendo solamente cuatro, demostrando que la mejor defensa es, efectivamente, el ataque –siempre y cuando se cuente con jugadores suficientemente buenos, como sobran en Brasil.A partir del campeonato mundial de Inglaterra en 1966, el número de goles por partido, aunque con altibajos, muestra un claro declive. El número más bajo en este respecto se dio en el campeonato de Italia en 1990, cuando se alcanzó solamente un promedio de 2.2 goles por encuentro, cifra no muy diferente de los 2.3 goles por partido del campeonato de Alemania-2006. En el presente torneo de Sudáfrica no pareciera ser que la situación vaya a mejorar mucho, y hasta el sábado 12 de junio, cuando se han jugado ya 5 partidos, solamente se han anotado 7 goles; es decir, un promedio de 1.4 goles por encuentro.Tendríamos, entonces, que la expansión del fútbol en todo el mundo no es por el espectáculo que proporciona, sino que, por el contrario, es en cierto modo a pesar del mismo. Pudiera ser incluso que la evolución hacia estrategias defensivas haya incentivado la expansión del fútbol. En referencia al esquema ultra defensivo “Catenaccio”, desarrollado por los italianos, John Bluem en su artículo “Evolución de los sistemas de juego” escribe: “La historia del catenaccio nos dice mucho acerca del desarrollo de las tácticas del fútbol. No hay absolutamente nada positivo acerca de su origen. Fue diseñado no para ganar juegos, sino para evitar perderlos. La Serie italiana A (primera división) ha sido por mucho tiempo una liga desbalanceada, con unos pocos clubes ricos que se llevan todos los honores. En  1947, Nereo Rocco se hizo del Triestina, un pequeño club que apenas sobrevivía. Fue Rocco quién lanzó al catenaccio al mundo del fútbol. Tuvo un inmediato y dramático éxito. En 1948 el Triestina alcanzó el segundo lugar en la liga. Notando el éxito de este equipo, otros clubes italianos empezaron a utilizar el sistema de juego catenaccio”.  De acuerdo con lo anterior, los esquemas defensivos en el fútbol tendrían el efecto de reducir la distancia entre equipos de niveles competitivos diferentes. Con un esquema de este tipo, un equipo técnicamente inferior incrementaría sus posibilidades ante un equipo de más alta calificación. A manera de ejemplo, mientras que en 1961 México fue goleado 8-0 en el estadio Wembley  por Inglaterra en partido amistoso, en la confrontación del pasado mayo entre estos dos equipos en el mismo estadio, México fue derrotado solamente por 3-1 –durante el campeonato del mundo de 1966 en Inglaterra, México enfrentó a los ingleses en partido oficial y teniendo fresca la goleada de años antes, el entrenador de nuestro equipo amontonó a tantos jugadores como pudo en la portería, renunciado prácticamente al ataque pero consiguiendo que la derrota fuera solamente por 2-0. La aparente reducción en distancias entre equipos técnicamente dispares al emplear esquemas defensivos posiblemente haya sido un factor para la difusión que ha experimentado este deporte a nivel profesional en los últimos decenios al crear una ilusión de competencia. En particular, esto ha ocurrido en México, en donde las expectativas sobre las posibilidades de nuestro equipo en la copa del mundo son posiblemente exageradas.",
    "Más de seis semanas han transcurrido desde el 20 de abril pasado cuando se incendió la plataforma de perforación “Deepwater Horizon” estacionada en el campo petrolero de Macondo frente a la costa de Luisiana. Como es del dominio público, a consecuencia del accidente la plataforma de hundió provocando un derrame de petróleo que a la fecha no ha podido ser controlado. Inicialmente British Petroleum, la compañía responsable de la plataforma, estimó que la fuga de petróleo era de unos 1,000 barriles diarios, cifra que poco tiempo después incrementó a 5,000 barriles por día. Actualmente se considera que dicha cifra está entre los 12,000 y los 19,000 barriles diarios, aunque algunos expertos consideran que podría llegar hasta los 100,000 barriles derramados cada 24 horas. Como quiera que sea, lo que si es claro ahora es que la contaminación ocasionada por el accidente del “Deepwater Horizon” en el Golfo de México, que ahora cubre una superficie mayor a la del Estado de San Luís Potosí, representa el peor desastre petrolero y ecológico en la historia de los Estados Unidos. Desde el punto de vista del volumen de crudo derramado, tiene incluso el potencial de convertirse en el peor accidente a nivel mundial, lugar que le corresponde al  pozo Ixtoc 1 de Pemex, que entre junio de 1979 y marzo de 1980 –cuando finalmente fue controlado– derramó unos 3.3 millones de barriles de petróleo en el Golfo de Campeche. A  un ritmo de 19,000 barriles diarios, el derrame del Deepwater Horizon alcanzaría en seis meses el volumen total vertido por el del Ixtoc 1; en el peor de los casos contemplados, una fuga de 100,000 barriles de petróleo por día nos llevaría a que el pozo de Macondo habría ya rebasado el volumen vertido por el Ixtoc 1. Comparando los accidentes de los pozos petroleros de Campeche y de Macondo se encuentran coincidencias: ambos se iniciaron por un aumento súbito de presión del pozo seguido de una falla del dispositivo que debe actuar en estos casos para cerrarlo y evitar una fuga descontrolada de petróleo. Las condiciones que enfrentan los ingenieros de British Petroleum para contener la fuga de crudo del pozo de Macondo, sin embargo, son considerablemente más difíciles que las que encontraron los especialistas de Pemex hace 31 años. En efecto, tenemos que mientras que la profundidad marina del Ixtoc 1 era de solamente 50 metros –lo que permitió el empleo de buzos el las labores de reparación– la boca del pozo de Macondo está a una profundidad de 1,500 metros. A esta profundidad la presión ejercida por el agua es de unas 150 atmósferas, lo que hace necesario emplear robots manejados a control remoto en lugar de buzos. Aun más, puesto que la luz del Sol no alcanza a llegar más allá de una profundidad de unos 200 metros, resulta –además de una total oscuridad– que la temperatura del agua es de solamente unos pocos grados centígrados.Sin bien posiblemente ni la alta presión ni la baja temperatura prevalecientes en la boca del pozo de Macondo representen por si solas problemas de gran envergadura, la convergencia de las dos condiciones ambientales ha resultado fatal. Esto es debido a que bajo dicha convergencia, el metano (gas natural) que se fuga del pozo se combina con el agua de mar para formar cristales de hidrato de metano. Estos cristales han sido un dolor de cabeza en los intentos de reparación de pozo averiado, tapando las tuberías por las que se ha intentado transportar a la superficie el petróleo fugado. En el quinto y último intento de controlar la fuga, el pasado jueves 3 de junio los ingenieros de British Petroleum colocaron sobre la boca del pozo de Macondo un dispositivo en forma de campana que busca captar la mayor parte del petróleo derramado y conducirlo a la superficie. La operación, sin embargo, ha tenido que hacerse de manera muy lenta a fin de evitar la formación de cristales de hidrato de metano que darían al traste con la misma. Si bien se reporta que el nuevo intento ha logrado reducir en cierto grado el derrame de petróleo, hasta la tarde de sábado 5 de junio no está claro si tendrá finalmente éxito. En todo caso representa solamente una solución temporal. La solución definitiva –se tiene la esperanza– lo constituirán los dos pozos de alivio que se perforan actualmente al lado del pozo accidentado y que se espera que se concluyan en el próximo mes de agosto. Estos dos pozos confluirán con el pozo descontrolado a unos 4,000 metros por abajo del fondo del mar. Una vez terminados, a través de los mismos se inyectará lodo de perforación al pozo accidentado a fin de bloquearlo y controlar la fuga. Este fue precisamente el procedimiento por medio del cual el pozo Ixtoc 1 fue finalmente controlado. De este modo, habría similitudes en ambos, el origen y la solución de los accidentes sufridos por ambos pozos. Hay que notar, por otro lado, que una vez terminados los pozos de alivio para el Ixtoc 1 transcurrieron tres meses antes de que la fuga fuera finalmente controlada. De extenderse las similitudes entre los dos accidentes, tendríamos entonces que esperar hasta noviembre próximo para ver finalmente al pozo Macondo bajo control. De ser así, el accidente del Deepwater Horizon constituiría, posiblemente con mucho, el peor desastre ecológico de la historia.",
    "Las décadas de los años sesenta y setenta constituyeron un periodo de auge para la industria de generación de energía eléctrica mediante la desintegración del átomo. Hace medio siglo la tecnología nuclear –entonces un desarrollo relativamente reciente– se veía como una manera de obtener energía eléctrica abundante y confiable. Algunos países como Francia y Japón –que no cuentan con reservas de combustibles fósiles– hicieron de la energía nuclear una de sus mayores fuentes de abastecimiento; Francia incluso la convirtió en la mayor. Con los años, sin embargo, el entusiasmo en la núcleo electricidad –sobre todo en los Estados Unidos– disminuyó por altos costos de construcción de un reactor nuclear, así como por el accidente, en 1979, del reactor nuclear de la Isla de las Tres Millas en Pensilvania, EUA. La situación se agravó con el desastre en 1986 de la planta de Chernóbil,  en Ucrania, que explotó y dispersó material radiactivo en un extenso radio que llegó hasta al norte de Europa. Desde hace una década, sin embargo, y a pesar de los peligros que presenta, el interés en la energía nuclear ha repuntado, motivado por los altos precios del petróleo y debido a los efectos de la contaminación atmosférica por el uso intensivo de combustibles fósiles.En la actualidad aproximadamente el 6 % del total de la energía que consume el mundo, y 15 % del total de la energía eléctrica, es de origen nuclear. Es producida en 438 reactores, dispersos en todo el mundo, aunque localizados principalmente en los Estados Unidos, Francia, Japón y Rusia, en ese orden. 54 reactores están actualmente en construcción en el mundo y 148 más en fase de planeaciónDesde el punto de vista de la contaminación atmosférica por gases de invernadero, las plantas nucleares son ciertamente una opción “verde” no contaminante. La energía nuclear, no obstante, presenta graves problemas de contaminación radioactiva, no solamente por la siempre presente posibilidad de un accidente nuclear, sino por los desechos radiactivos que generan, los cuales tienen que ser confinados en un lugar seguro. El confinamiento es necesario no solamente para evitar un contacto humano accidental con el material radioactivo, sino también para prevenir su robo por parte de grupos terroristas, algo que hoy en día no se puede descartar.   Como parte del “renacimiento” de la energía nuclear, varias compañías privadas en los Estados Unidos y Japón están desarrollando reactores nucleares de dimensiones reducidas, con megavatios o decenas de megavatios de potencia eléctrica en lugar de los miles de megavatios de una central nuclear convencional. Un fabricante japonés, por ejemplo, ha desarrollado un micro reactor de 200 kilovatios –capaz de proveer de electricidad a unas 100 casas–, el cual mide solamente 7x2 metros. Los micro reactores no estarían destinados a competir directamente con las centrales nucleares, sino que tendrían su mayor aplicación en lugares remotos alejados de las líneas de distribución de energía eléctrica, como hay muchos en los países subdesarrollados.   De acuerdo con un fabricante norteamericano, el tamaño pequeño de un micro reactor permitiría que éste fuera ensamblado y sellado en los Estados Unidos con la carga de combustible nuclear en su interior. De este modo, los usuarios en el país de destino no tendrían acceso al interior del reactor, el cual contaría con suficiente combustible para operar decenas de años sin interrupción. La recarga de combustible se haría en los Estados Unidos, país que también se encargaría de manejar los desechos radioactivos. Este esquema, sin embargo, ha provocado críticas. Por ejemplo, Peter Wilk de la organización “Physicians for Social Responsibility” señala que la opinión pública norteamericana difícilmente estaría de acuerdo: “Imagínese si los norteamericanos estarían de acuerdo en tomar los desechos generados en otros países y manejarlos aquí”.   La proliferación de micro reactores en países desarrollados y subdesarrollados implicaría multiplicar el peligro de un accidente nuclear. Presentaría, además, grandes problemas para custodiar el combustible nuclear alojado en el interior de los micro reactores, que estarían  dispersos en múltiples lugares. En estas condiciones es difícil entender las argumentaciones de la compañías que presentan a los micro reactores como una alternativa viable para la generación de energía y para frenar el calentamiento global.La energía nuclear en general y los micro reactores nucleares en particular ciertamente constituyen una opción “verde” que no genera gases de invernadero y que no contribuye al  calentamiento global. Los micro reactores, sin embargo, podrían tener consecuencias “negras” en caso de un accidente o de robo de combustible, eventualidad que ahora sería considerablemente más probable.",
    "Los seres vivos están formados por elementos químicos –carbono, hidrógeno, oxígeno, etc.– que pueden también ser constituyentes de materiales inanimados. Esto nos lleva a una pregunta elemental: si tanto la materia viva como la inanimada pueden estar formados por  los mismos átomos ¿por qué una y otra se comportan de manera tan diferente? La pregunta ha estado en el aire por mucho tiempo, sin que se haya dado una respuesta concluyente. Según el punto de vista vitalista, existe una “fuerza vital” necesaria para dar vida a la materia inanimada, lo que la haría cualitativamente diferente de su contraparte inanimada. Según el punto de vista opuesto, la materia viva está formada por un agregado de elementos químicos, sin la participación de una fuerza vital de naturaleza inmaterial o espiritual. El pasado jueves 20 de mayo, un grupo de investigadores que trabajan para la compañía  privada norteamericana, “J. Craig Venter Institute”, encabezados por J. Craig Venter, reportaron en la revista “Science” haber logrado fabricar una célula por métodos artificiales. Esto constituiría una prueba de que la materia viva no es más que un agregado de átomos –aunque, ciertamente, de extrema complejidad.   Como primer paso en la fabricación de la célula artificial se sintetizó el genoma de la bacteria “Mycoplasma mycoides”, empleando la información genética que se tenía disponible. Posteriormente, el genoma artificial se insertó en la bacteria “Mycoplasma capricolum”, remplazando a su genoma natural. Así, dado que el genoma de un organismo contiene las instrucciones necesarias para su reproducción, la célula huésped empezó a reproducirse siguiendo el patrón que le dictaba el genoma sintético. Se transformó de este modo a una célula en otra diferente. En declaraciones al diario The Independent, Venter afirmó: “Esta es la primera célula sintética que ha sido hecha, y la llamamos sintética debido a que la célula se derivó totalmente de un cromosoma sintético, hecho con cuatro botellas de substancias químicas en un sintetizador químico, a partir de información almacenada en una computadora”.Sin bien anteriormente se había logrado modificar el genoma de un organismo empleando diferentes técnicas –a través de la cruza de especies o por medio de la ingeniería genética– nunca se había hecho en la escala en la que lo lograron los científicos de J. Craig Venter, que alcanzaron lo que es considerado uno de los mayores avances de la biología y de la tecnología de manipulación genética. No todos los expertos, sin embargo, coinciden en que el grupo de Venter haya creado una célula genuinamente sintética que represente una nueva forma de vida. George Church, genetista de la Harvard Medical School, por ejemplo, señala que, aunque con pequeñas variaciones, la célula sintetizada es en realidad una copia de una célula existente y por lo tanto no califica como una  nueva forma de vida (Science, 20 de mayo de 2010). De la misma manera, Steem Ramussen, profesor de física de la Universidad del Sur de Dinamarca, hace notar que las instrucciones contenidas en el genoma no son suficientes para construir materia viva, sino que además es necesaria una cierta cantidad de energía, que es provista por el metabolismo de la célula, lo mismo que una barrera de protección –la membrana de la célula– dentro de la cual ocurre todo el proceso. La síntesis de Venter atendió a la parte del genoma, pero no a la del sustento físico para la reproducción de la materia viva. De este modo, la célula resultante sería solamente semi-sintética.    No se considera, por otro lado, que el conocimiento genético del que se dispone en estos momentos sea suficiente para sintetizar una célula completa partiendo de cero. En palabras de Jim Collins, profesor de ingeniería biomédica en la Universidad de Boston: “Francamente, los científicos no saben la suficiente biología para crear vida. Aunque el proyecto de genoma humano ha expandido la lista de partes que componen una célula, no hay un manual con las instrucciones para ensamblarlas y producir una célula viviente. Es como tratar de ensamblar un jet jumbo disponiendo solamente de su lista de partes. Aunque algunos de nosotros que trabajamos en biología sintética pudiéramos tener delirios de grandeza, nuestras metas son mucho más modestas” (Science, mayo de 2010).No obstante –delirios de grandeza al margen–, aun si en estos momentos la biología está lejos de crear vida verdaderamente artificial y tengamos que retrasar la declaración de muerte definitiva de las ideas vitalistas sobre el origen de la vida, es claro que el resultado del grupo de Venter les asesta un golpe muy fuerte. Este resultado nos anuncia que, con mucha probabilidad, tarde o temprano se alcanzará un nivel de conocimiento científico y tecnológico tal que hará posible construir organismos vivos partiendo de cero, con características físicas diseñadas a voluntad. Es posible, sin embargo, que algunos de nosotros no lo veamos.",
    "Uno de los resultados científicos más vistosos reportados en los últimos meses es el relativo a la evidencia genética del cruce de nuestra especie humana con la especie Neandertal, el cual habría ocurrido en el Medio Oriente hace unos 80,000 años. Esta evidencia, que fue publicada el pasado 7 de mayo en la revista “Science” por un grupo internacional de investigadores encabezado por Svante Paabo del Instituto Max Planck de Antropología Evolutiva en Alemania, está basado en el análisis del genoma Neandertal extraído de huesos encontrados en una cueva en Croacia, los cuales tienen una antigüedad de unos 40,000 años. De acuerdo con este estudio, el genoma del hombre moderno tiene contribuciones del genoma Neandertal, lo que demuestra su hibridación. Los primeros fósiles de la especie Neandertal fueron descubiertos en Bélgica en 1829. Un segundo descubrimiento fue hecho en 1848 en Gibraltar. No fue, sin embargo, hasta  que se realizó un tercer hallazgo en 1856 en el valle de Neander en Alemania –de donde la especie Neandertal toma su nombre– que se reconoció se trataba de una especie humana distinta a la nuestra. A partir de entonces se han descubierto numerosos sitios con restos de neandertales en Europa y en el Medio Oriente. Se sabe que la especie Neandertal emigró de África hacia el Medio Oriente y Europa hace cientos de miles de años y que permaneció en el continente europeo hasta su extinción hace unos 25,000-30,000 años. De este modo, habría convivido con nosotros en el medio Oriente y en Europa por decenas de miles de años y la pregunta obligada es qué tipo de relación tuvimos con ellos. De manera posiblemente prejuiciada, la respuesta que inicialmente se dio a esta pregunta es que nuestra especie había hecho la guerra a los –supuestamente inferiores– neandertales hasta acabar con ellos. En los últimos años, sin embargo, se ha especulado que quizá, después de todo, las relaciones entre el Homo sapiens y el Neandertal pudieran haber sido –en algunos casos– más amistosas.    Así, en 1998 fueron descubiertos en Abrigo do Lagar Velho, Portugal, los restos de un niño de cuatro años, con una antigüedad de 25,000 años, los cuales presentan rasgos anatómicos  combinados entre el Homo sapiens y el Neandertal. En un artículo científico publicado en 1999 por un grupo de investigadores portugueses y de otros países europeos, esto ha sido interpretado como evidencia del cruce entre estas dos especies humanas.    Los resultados de Paabo y colaboradores recientemente publicados añaden a las evidencias arqueológicas de Lagar Velho, nuevas evidencias –en este caso genéticas– de que el hombre moderno y el Neandertal se mezclaron en algún momento hace decenas de miles de años. De modo que, como lo menciona Paabo, el Neandertal de alguna manera no se extinguió, sino que vive entre nosotros en los genes de una parte de la población del mundo.Un resultado interesante de los estudios genéticos sobre el genoma Neandertal es que el mismo está presente en las poblaciones europeas y asiáticas pero no en las africanas. Esto sugiere que el cruce entre el Neandertal y el Homo sapiens se dio después de que estos últimos salieron de África, posiblemente en el Medio Oriente. A esta conclusión se llegó después de comparar el genoma de cinco personas nativas de África, China, Papúa Nueva Guinea y Francia con el genoma Neandertal. Así, mientras que europeos y asiáticos están emparentados con los neandertales, éste no es el caso de los africanos. El descubrimiento de nuestra relación con el Neandertal con seguridad cambiará la manera en la que nos vemos a nosotros mismos. Cuando Charles Darwin hace 150 años enunció su teoría de la selección natural y la evolución de las especies, provocó fuertes reacciones por sus implicaciones sobre nuestra posición en el mundo natural. Poco a poco, sin embargo, nos hemos adaptado a estas implicaciones. De este modo, por ejemplo y aunque no de manera universal, hoy en día aceptamos que los chimpancés y el hombre moderno tienen un antepasado común. Esta aceptación se ha facilitado posiblemente por el hecho de que el ancestro común vivió en una época extraordinariamente lejana, de varios millones de años, con lo que el parentesco resulta extremadamente lejano. Además, nuestra superioridad intelectual con respecto a los chimpancés es incontrovertible.En contraste, sabemos que no existía una diferencia marcada en capacidad intelectual ni en aspecto físico entre el hombre moderno y el Neandertal, que tenía un cerebro más grande y que incluso podría haber manejado herramientas más elaboradas. Ambas especies, además, interaccionaron en una época relativamente cercana, hace cuando mucho 25,000-30,000 años, que es apenas tres veces el tiempo transcurrido desde la invención de la agricultura y el inicio de nuestra civilización. Existe de este modo un efecto de proximidad en tiempo y capacidad intelectual que hace más que evidente que no ocupamos un lugar especial en el mundo. Tan evidente es que incluso buena parte de los habitantes del mundo provienen de una hibridación de dos especies humanas, resultado de relaciones más que amistosas.",
    "Como ha sido ampliamente cubierto por los medios de difusión, el pasado 20 de abril ocurrió una explosión en la plataforma de perforación petrolera “Deepwater Horizon”, estacionada en el Golfo de México a 80 kilómetros de la costa de Luisiana, la cual se hundió dos días después generando un derrame de petróleo que oficialmente se estima en unos 5,000 barriles (800,000 litros) diarios. Al momento de accidente, la plataforma Deepwater Horizon estaba terminando de perforar un pozo a 1,500 metros por debajo de la superficie del mar. La plataforma Deepwater Horizon es propiedad de Transoceanic, una compañía contratista para la exploración y perforación de pozos petroleros en el mar –la cual, de acuerdo a su página web, es la más grande del mundo y cuenta con 18,000 empleados–. La plataforma era operada por Transoceanic para la compañía petrolera British Petroleum. Se supone que Deepwater Horizon incorporaba medidas de seguridad estrictas que presumiblemente harían difícil un accidente como el ocurrido.De acuerdo a datos de la Universidad Estatal de Florida basados en imágenes de satélite, la superficie de mar cubierta de petróleo se expande a razón de 1,250 kilómetros por día, y había alcanzando el 5 de mayo pasado una extensión de 15,000 kilómetros cuadrados (http://oilspill.fsu.edu/trackingspill-fsu.php). De este modo, el derrame cubriría el día de hoy unos 20,000 kilómetros cuadrados, aproximadamente un tercio de la superficie del Estado de San Luís Potosí.  A un ritmo de 5,000 barriles de petróleo vertidos diariamente en el mar, el accidente de la Deepwater Horizon daría lugar en un lapso de dos años al peor derrame petrolero no intencional de la historia –el record en este sentido lo tiene el tristemente célebre pozo Ixtoc 1 de PEMEX, que entre junio de 1979 y marzo de 1980 vertió de manera incontrolada unos 3.5 millones de barriles de petróleo en el Golfo de México enfrente de la costa de Campeche–. Hay indicios, sin embargo, de que el derrame actual de petróleo es mayor que lo que se reconoce de manera oficial. Ian McDonald, profesor del Departamento de Oceanografía de la Universidad Estatal de Florida, empleando imágenes de satélite, calcula que dicho derrame es cuando menos de 26,500 barriles diarios; es decir, más de cinco veces la cifra oficial. De este modo, de no sellar las fugas, el volumen de crudo derramado por el accidente de la plataforma Deepwater Horizon podría superar al del Ixtoc 1 en menos de cinco meses. El daño al medio ambiente, sin embargo, podría ser mucho mayor que el ocasionado por el Ixtoc 1, de alcanzar el petróleo derramado las zonas pantanosas de la costa de Luisiana en el delta del Misisipi, las cuales sirven de criadero a peces y crustáceos y de refugio a numerosas especies marinas y aves migratorias. El accidente de la plataforma Deepwater Horizon podría también convertirse en un desastre ecológico con un alcance más allá de las costas de Luisiana, si el petróleo derramado es transportado por la “Loop current” hasta la costa atlántica de los Estados Unidos a través del estrecho de la Florida –la “loop current” es una corriente marina que viaja de sur a norte entre Cuba y Yucatán hasta alcanzar el norte del Golfo de México, seguido de lo cual invierte su dirección hacia el sur, rodeando la península de la Florida y arribando a su costa este–. De acuerdo con Yonggang Liu, oceanógrafo de la Universidad del Sur de Florida: “Si la capa de petróleo se mantiene donde ahora está, el efecto será local. Pero si es capturada por la Loop current los efectos sobre el medio ambiente costero podrían ser desastrosos. La Loop current se mueve hacia el norte y en estos momentos está a 70 kilómetros de la capa de petróleo. Esto es muy peligroso” (Nature News, 4 de mayo).En la medida que los precios del petróleo se han incrementado se ha hecho rentable la extracción del crudo de depósitos con condiciones de explotación cada vez más difíciles. En particular, los altos precios del petróleo posibilitaron la exploración llevada a cabo por la plataforma Deepwater Horizon que  a la postre resultó en un desastre, tanto para el medio ambiente como para British Petroleum. Para los Estados Unidos es estratégicamente importante disminuir su dependencia con respecto al petróleo importado –sólo produce el 43% del petróleo que consume–, incrementando la producción de crudo en su territorio. Sus reservas probadas de crudo, además, solamente le alcanzarán para 8 años. En este contexto, las reservas submarinas de crudo han recibido atención. De acuerdo con la “Energy Information Administration” del Departamento de Energía de los Estados Unidos, los yacimientos de petróleo submarinos en las costas de este país contendrían hasta 59,000 millones de barriles de petróleo. Esto podría cubrir el consumo de los Estados Unidos por 8.5 años, lo cual no resulta muy significativo. La explotación de los depósitos submarinos de petróleo en los Estados Unidos sufre, además, de una fuerte oposición, que seguramente se reforzará con el accidente del Deepwater Horizon. Después de todo resulta razonable preguntarse si vale la pena arriesgar el medio ambiente para extraer un petróleo que nos duraría menos de diez años. Aunque pensándolo bien, valdría también la pregunta aun si nos fuera a durar cien años.",
    "De acuerdo con Evo Morales, Presidente de Bolivia, los transgénicos y la carne de pollo que se consumen en los países desarrollados es causa de calvicie en su población masculina así como de “desviaciones en su ser como hombres”. Esto último debido a las hormonas femeninas que se añaden al alimento de las aves a fin de incrementar su crecimiento. El Presidente boliviano expresó estas opiniones durante la inauguración de la Conferencia Mundial de los Pueblos sobre el Cambio Climático y los Derechos de la Madre Tierra, llevada a cabo en Cochabamba, Bolivia, del 20 al 22 del presente mes de abril.En relación a la causa de la calvicie, Morales llegó a sus conclusiones comparando el aparente número de calvos en los países industrializados con el correspondiente número en la población indígena en Bolivia, poniéndose él mismo como ejemplo. Los comentarios de Evo Morales produjeron reacciones negativas en todo el mundo y fueron tachadas de no tener fundamento científico. Tomando un punto de vista abierto e invocando la libre discusión de ideas, sin embargo, debemos quizá considerarlas como hipótesis basadas en la observación de hechos naturales. Atendiendo al método científico, sin embargo, habría que validarlas con más observaciones o a la luz del resultado de experimentos diseñados expresamente para este propósito.  Tratadas de este modo, las afirmaciones de Evo Morales resisten poco. Por ejemplo, se puede señalar que mucho antes de que se desarrollaran los trasngénicos ya existían personas calvas. Entre éstos –de diversas épocas y con diferentes grados de calvicie– podemos contar a Sócrates, Julio César, Leonardo da Vinci, Charles Darwin, Piotr Ilich Chaikovsky, Miguel Hidalgo, Vladimir Ilich Lenin y los físicos Max Planck y Enrico Fermi. Hay evidencia, incluso, de que existieron calvos hace miles de años, como concluye un grupo internacional de investigadores encabezado por Morten Rasmussen de la Universidad de Copenhague. Este grupo analizó el ADN de los cabellos de un individuo que vivió en Groelandia hace 4000 años, encontrando que tenía una predisposición a la calvicie. El estudio fue publicado en la revista “Nature” en febrero del presente año.  Otro escollo mayúsculo que enfrentan los planteamientos de Morales es que en realidad a los pollos no se les dan de comer hormonas. Se puede señalar, por ejemplo, que  la “Food and Drug Adminstration” de los Estados Unidos prohíbe su uso en pollos –aunque si permite su uso en pequeñas cantidades en la alimentación del ganado bovino. De todo lo anterior, podemos entonces concluir que, efectivamente, Evo Morales hizo sus comentarios de manera precipitada y con poco sustento científico. La Conferencia Mundial de los Pueblos sobre el Cambio Climático y los Derechos de la Madre Tierra, según  Evo Morales, fue necesaria por el fracaso de la reunión sobre cambio climático realizada en Copenhague, Dinamarca, el pasado diciembre, y en la que no se pudieron establecer metas cuantitativas de reducción de gases de invernadero. La conferencia de Cochabamba emitió un comunicado, el “Acuerdo de los Pueblos”, en el que demanda que los países industrializados, causantes del calentamiento global, se hagan cargo de poner remedio a la crisis climática. Entre otras cosas, dicho comunicado plantea la exigencia de que los países industrializados reduzcan en un 50% sus emisiones de gases de invernadero con respecto a los niveles de 1990, así como el establecimiento de un “Fondo de Adaptación” para “enfrentar el cambio climático”, manejado por los países en desarrollo.  Se propone igualmente la creación de un “Tribunal Internacional de Justicia Climática y Ambiental que tenga la capacidad jurídica vinculante de prevenir, juzgar y sancionar a los Estados, las Empresas y personas que por acción u omisión contaminen y provoquen el cambio climático”. Se plantea también que “Es deber de los países desarrollados compartir su tecnología con países en desarrollo, crear centros de investigación para la creación de tecnologías e innovaciones propias, así como defender e impulsar su desarrollo y aplicación para el vivir bien”.Al margen de que algunos de las exigencias contenidas en el Acuerdo de los Pueblos son claramente inviables –como ciertamente lo es la reducción en un 50% de las emisiones de gases de invernadero en un lapso de 10 años–, los planteamientos del documento son moralmente justos. Es decir, si los países industrializados son mayormente los causantes de la contaminación atmosférica que padecemos, justo es que cubran igualmente la mayor parte del costo que implica remediarla.No podemos asumir, sin embargo, que una consideración moral sea suficiente presión para los países ricos. En particular no podemos esperar que de manera espontánea nos transfieran su tecnología y nos ayuden a establecer centros de investigación e innovación como se plantea en el Acuerdo de los Pueblos. Aun si se diera el caso, la mayor parte de los países en desarrollo, que no cuentan con una adecuada infraestructura científica y tecnológica, no tendrían la capacidad de absorberla. El discurso desinformado y descuidado de Evo Morales, sin dejar de reconocer que ha sido magnificado por los medios y quizá sacado de contexto, es un reflejo de esta situación. Una cosa es clara: si los países en desarrollo no se ayudan a si mismos creando sus propias infraestructuras de investigación científica y tecnológica, difícilmente alguien más las creará por ellos.",
    "Los pasados 12 y 13 de abril se llevó a cabo en Washington, D.C., con el Presidente Obama como anfitrión, una reunión cumbre promovida por los Estados Unidos sobre seguridad nuclear. A esta reunión asistieron líderes de 47 países, incluyendo al Presidente Felipe Calderón. Un tema central de discusión fue el del terrorismo nuclear, en particular, la posible detonación por grupos terroristas de un dispositivo nuclear en un área densamente poblada. El terrorismo en general ha sido una preocupación constante en los Estados Unidos, sobre todo a partir de los acontecimientos del 11 de septiembre de 2001. En referencia al terrorismo nuclear, algunos investigadores consideran que existe una probabilidad no nula de que un grupo terrorista lo suficientemente sofisticado sea capaz de construir una bomba nuclear de pequeñas dimensiones, introducirla clandestinamente a los Estados Unidos y detonarla en un área urbana, con el resultado de decenas o cientos de miles de muertos y un daño económico de grandes dimensiones.    La era nuclear se inició el 16 de julio de 1945, cuando los Estados Unidos llevaron a cabo la primera explosión nuclear de la historia en el desierto de Nuevo México, en un sitio al sur de Alburquerque. En el curso de unas pocas semanas, a esta primera explosión siguieron los bombardeos atómicos de Hiroshima y Nagasaki que pusieron fin a la Segunda Guerra Mundial. Tanto la bomba de Nuevo México como la lanzada sobre Nagasaki emplearon plutonio como explosivo, mientras que la bomba de Hiroshima fue fabricada a partir de uranio. Aunque la producción exitosa de la bomba nuclear de uranio requirió de un gran esfuerzo en un número de áreas científicas y tecnológicas, el reto principal fue la obtención de uranio con la pureza necesaria. El uranio natural está compuesto en un 99% del isótopo uranio-238 y en menos del 1% del isótopo uranio-235 –los isótopos de un elemento químico son átomos del mismo elemento pero con pesos diferentes–. Una bomba nuclear, por otro lado, requiere de uranio con una alta concentración de uranio-235, de modo que es necesario procesar el uranio natural para enriquecerlo con el isótopo 235. Purificar uranio, sin embargo, es un proceso complejo que requiere de grandes inversiones y que no está al alcance de cualquiera. Por su lado, el plutonio adecuado para fabricar una bomba nuclear es obtenido en reactores nucleares a partir del uranio-238. Al igual que en el caso del uranio, la obtención de plutonio con la pureza necesaria para fabricar una bomba nuclear es también un proceso que no puede ser llevado a cabo sin contar con una infraestructura de grandes proporciones.  De este modo, si un grupo terrorista ha de fabricar una bomba atómica, tendrá que robar o comprar subrepticiamente el uranio o el plutonio necesario. Una vez con los explosivos nucleares en su poder, sin embargo, se considera que si el grupo es lo suficientemente sofisticado podrá llevar a cabo exitosamente la fabricación de la bomba.   Así, la estrategia correcta para minimizar el riesgo de un ataque terrorista con armas atómicas (Matthew Bunn, Securing the Bomb 2100) es la de incrementar la seguridad de las reservas de plutonio y uranio enriquecido que existen en el mundo. De acuerdo con el International Panel on Fissile Materials, estas reservas son de alrededor de 1600 toneladas de uranio enriquecido y de 500 toneladas de plutonio, cantidades que permitirían fabricar 60,000 bombas nucleares de cada tipo.Una bomba de uranio requiere 25 kilogramos de uranio enriquecido mientras que una de plutonio requiere aproximadamente 8 kilogramos de este elemento. Es entonces factible la construcción de una bomba nuclear de pequeñas dimensiones y, de hecho, en las décadas de los años cincuenta y sesenta se construyeron bombas que podían ser transportadas por una persona. Habría por lo tanto motivo de alarma.   El temor a un ataque nuclear con una “bomba atómica de bolsillo”, sin embargo,  no es algo nuevo en los Estados Unidos, como lo comenta Scott Shane en un artículo aparecido en el New York Times el pasado 15 de abril. De hecho, esta preocupación ya existía al inicio de la Guerra Fría en la década de los años cincuenta, sólo que el papel que hoy juegan las organizaciones terroristas lo jugaban los soviéticos: “Cientos de páginas de documentos clasificados de los años cincuenta obtenidos por The New York Times….. nos relatan una historia sorprendentemente similar, en la cual los agentes comunistas jugaban el papel de Al Qaeda”.No habría entonces nada nuevo bajo el Sol. Y sin embargo, un ataque terrorista con explosivos nucleares cambiaría drásticamente para mal al mundo, como lo cambió la primera explosión nuclear en el desierto de Nuevo México hace 65 años.",
    "En el número de abril de 2010 de la revista “Nature Reviews Genetics”, la profesora Mariam B. Sticklen de la Universidad Estatal de Michigan publicó una nota en la que retiraba de dicha revista un artículo de su autoría publicado en el año de 2008, el cual trataba sobre nuevos métodos para la fabricación de biocombustibles. Las razones que adujo para el retiro de su publicación fue que en la misma se parafraseó un párrafo de otro artículo, publicado en la revista “Plant Science” por autores diferentes, sin dar los créditos correspondientes. La publicación retirada era un artículo de revisión de un tema de investigación –es decir, que no contenía nuevos resultados– y la falta –plagio– de la profesora de la Universidad Estatal de Michigan pareciera ser solamente menor. La falta estuvo agravada, sin embargo, por el hecho de que Sticklen tuvo acceso a una primera versión del artículo plagiado antes de su publicación, pues le había sido enviado por el editor de Plant Science para que emitiera una opinión crítica en vista de su posible publicación. El plagio de un trabajo de investigación no es un hecho infrecuente y en una búsqueda rápida en Internet nos encontramos con varios casos dados a conocer en los últimos meses. El diario China Daily, por ejemplo, informó el pasado 22 de marzo del caso del profesor Li Liansheng de la Universidad Xi´an Jiaotong –una de las más prestigiosas en China–,  quién fue expulsado de su puesto de profesor, además de que le fueron retirados los apoyos económicos a sus proyectos de investigación, acusado de haber plagiado el trabajo de otros. Nos enteramos también, por una nota publicada en el portal de CBC News el pasado 6 de abril, del caso de Ezeddin Shrif, profesor de Ingeniería de la Universidad de Regina en Canadá, quién fue acusado por un antiguo estudiante suyo de haber enviado para publicación un manuscrito a la revista “Journal of Canadian Petroleum Technology” con los resultados de su tesis de maestría sin haberlo hecho de su conocimiento y, lo que es peor, sin incluirlo en la lista de autores.Un caso más de plagio ocurrido recientemente es el de Kamran Daneshjou, Ministro de Ciencia, Investigación y Tecnología de Irán. Daneshjou, quién es también profesor del Departamento de Ingeniería Mecánica de la Universidad de Ciencia y Tecnología de Irán, fue acusado de plagio de resultados científicos y le fueron retirados artículos ya publicados en tres revistas de circulación internacional. Estos hechos, por supuesto, han tenido fuertes repercusiones políticas en Irán por ser Daneshjou parte de un gobierno cuya legitimidad es disputada por la oposición iraní.   El plagio de resultados científicos está empezando a ser estudiado de manera sistemática. Un grupo de investigadores médicos de la Universidad de Texas en Dallas encabezados por Harold R. Garner, ha estudiando la base de datos “Medline” de la Biblioteca Nacional de Medicina de los Estados Unidos –que actualmente contiene cerca de de 18 millones de fichas de artículos médicos–, buscando duplicaciones de información que indiquen posibles casos de plagio. Garner y colaboradores tomaron una muestra estadística de más de 60,000 fichas de Medline y la analizaron por medio de un programa de cómputo que ellos desarrollaron y que les permite encontrar similitudes entre textos. En base a los resultados obtenidos con su muestra estadística, el grupo de Graner hace una extrapolación a toda la base Medline y concluye que hasta 200,000 artículos del total serían lo suficientemente parecidos a otros publicados con anterioridad para ser considerados como duplicaciones. La mayor parte de los artículos duplicados comparten autores con la correspondiente publicación original y constituirían de este modo autoplagios. Habría, sin embargo hasta 5,000 artículos duplicados que no compartirían autores con el trabajo original, los cuales podrían corresponder a trabajo plagiado. La gran velocidad con que se está incrementando la cantidad de artículos publicados en la literatura científica internacional hace cada vez más difícil estar al tanto de todo lo hecho público en una determinada área. En estas condiciones la duplicación de artículos de investigación –ya sea por plagio o por autoplagio–  no es de ninguna manera bienvenida y, más allá de las consideraciones éticas que se puedan elevar en contra del plagio de ideas y descubrimientos científicos, la duplicación de artículos entorpece el trabajo de investigación.El plagio del trabajo de otros no es por supuesto exclusivo de los científicos y lo encontramos prácticamente en cualquier área en donde se ponga a prueba la creatividad humana, incluyendo la literatura y la música. En el área científica, sin embargo, es donde más fácilmente puede ser descubierto el trabajo de “copiar y pegar” que ha facilitado el advenimiento de las computadoras y que, desgraciadamente, está cada vez más extendido..",
    "El próximo mes de mayo, el láser, uno de los mayores inventos del siglo XX, cumplirá 50 años de existencia. Este dispositivo nació en mayo de 1960 en los laboratorios de investigación de la compañía Hughes Aircraft de Malibú, California. Su inventor fue Theodore Maiman, quién se adelantó a muchos otros investigadores en la carrera por construir el primer láser de la historia. Maiman publicó su descubrimiento en la revista “Nature” en agosto de 1960. La invención de Maiman constituyó un avance científico de gran importancia que llevó a la práctica ideas teóricas sobre las posibilidades de fabricar un dispositivo con las características de un láser. En un inicio, sin embargo, y no obstante el gran avance científico que constituyó,  no se le avizoraban al láser mayores aplicaciones; de hecho, fue calificado de “invento en busca de una aplicación”. Al final, a 50 años de su invención, el láser ha tenido un impacto práctico enorme, comparable al que tuvo el transisitor. La utilidad de los láseres tiene que ver con las características muy especiales de  luz que producen, la cual es radicalmente diferente de la emitida por las fuentes ordinarias –el Sol, por ejemplo–. Si comparamos la luz emitida por un apuntador láser con aquella que proviene de un foco incandescente, notamos la luz del apuntador viaja en línea recta y tiene un color puro –frecuentemente rojo–, mientras que la luz del foco viaja en todas direcciones y tiene un color rojizo, resultado de la mezcla de varios colores puros. La propagación en línea recta y la pureza de color son dos atributos que podemos encontrar en la luz láser y difícilmente en la luz ordinaria. Aprovechando su propagación rectilínea, por medio de un láser podemos, por ejemplo, medir distancias con gran precisión. Así, se han hecho mediciones muy precisas de la distancia entre la Tierra y la Luna dirigiendo un láser hacia la superficie de nuestro satélite natural y tomando el tiempo que tarda en ir y venir después de reflejarse en su superficie –para esto se emplean los espejos que los astronautas del proyecto Apolo colocaron en la superficie de la Luna hace cuarenta años–. Este tipo de mediciones difícilmente podría hacerse con luz ordinaria que se propaga en todas direcciones.  Sin láseres, el mundo sería hoy en día muy diferente.  No existirían, por ejemplo, los discos compactos (CDs) –que utilizan un láser para reproducir la música o el video grabado en ellos– y muy probablemente seguiríamos usando cintas magnéticas para escuchar música. De la misma manera, no existirán los lectores de códigos de barras que se emplean en las cajas registradoras de los supermercados, lo que haría que nuestras compras tomaran considerablemente más tiempo. No contaríamos tampoco con impresoras láser, con toda la conveniencia que representan, ni tendríamos a nuestra disposición técnicas oftálmicas que emplean láseres para corregir en un instante la miopía o el astigmatismo, o bien para fijar una retina desprendida. Los láseres se emplean también para cortar y soldar metales, en sistemas de tomografía óptica para ver el interior del cuerpo humano, y para medir la contaminación atmosférica, entre muchas otras aplicaciones. Entre las múltiples y variadas áreas de aplicación de los láseres,  las telecomunicaciones es posiblemente el campo en donde estos dispositivos han tenido su mayor impacto. En efecto, hoy en día las comunicaciones de larga distancia están dominadas por los sistemas de comunicación basados en fibras ópticas, y el láser –que es el dispositivo generador de la señal que ha de ser trasmitida por la fibra– es uno de sus elementos esenciales. El desarrollo que han alcanzado las telecomunicaciones, por otro lado, ha tenido un gran impacto social; esto es sobre todo cierto en relación a la red Internet, que ha establecido nuevas formas de interacción personal y de intercambio de conocimientos, incluyendo las llamadas “redes sociales”. En referencia a lo anterior, la red Internet ha tenido un crecimiento explosivo en las últimas dos décadas. Según datos de la Unión Internacional de Telecomunicaciones, organismo de la Organización de Naciones Unidas, el número de usuarios de Internet en los países desarrollados alcanzó en el año 2007 la cifra de 62 usuarios por cada cien habitantes. En los países en desarrollo las cifras son menores, pero en cambio el crecimiento es acelerado. Según la misma fuente de Naciones Unidas, el número de usuarios de Internet por número de habitantes en los países en desarrollo creció exponencialmente en el periodo 2000-2007, doblándose aproximadamente cada dos años y medio y  alcanzando en 2007 una cifra de 17 usuarios por cada cien habitantes. En la actualidad un cuarto de la población mundial está conectada a Internet y con seguridad, dadas las tasas de crecimiento, en pocos años este porcentaje será mayoritario. Nuestros tiempos se caracterizan por avances tecnológicos sin precedentes y el láser es uno de los más significativos. Muchas aplicaciones se le han encontrado y con seguridad muchas más se le encontrarán en el futuro a ésta, una luz muy especial.",
    "Una noticia a la que se dio gran difusión en los últimos días fue el caso del cuentahabiente que en mayo de 2004 demandó al banco Banamex por el pago de los intereses de un depósito de 400 pesos realizado en el año de 1987. El contrato del depósito tenía una vigencia de 28 días y en el mismo se especificaba su renovación indefinida en las mismas condiciones originalmente pactadas, a menos que existiesen instrucciones en otro sentido.  Como resultado de la demanda, en 2006 el Supremo Tribunal de Justicia de Chihuahua dictaminó que el banco tendría que pagar al cliente alrededor de 1400 millones pesos por concepto de intereses. Esta suma estratosférica fue resultado de la tasa anual de interés de 91.3 % pactada entre el banco y el cliente en el contrato original. La inflación en México en 1987 fue superior al 100 % y una tasa de rendimiento de 91.3 % no fue algo inusual. A partir de 1989, sin embargo, la inflación en nuestro país ha sido considerablemente menor –aún en 1995, año de la última gran crisis económica–, de modo que la renovación continua cada 28 días del contrato de depósito bancario original, ha generado un rendimiento absurdamente alto, que alcanza hoy en día los 250,000 millones de pesos.La conversión de 400 pesos en 250,000 millones en un lapso de 23 años es un ejemplo de los peligros del crecimiento geométrico o exponencial. Una cantidad que crece exponencialmente se duplica cada cierto intervalo de tiempo. En el caso que nos ocupa, una tasa de interés anual de 91.3 % con contratos a 28 días implica que el capital se duplicó aproximadamente cada diez meses. De este modo, los 400 pesos iniciales se convirtieron a los diez meses en 800 pesos y éstos en 1600 pesos al cabo de diez meses más, y así sucesivamente. En los primeros años los incrementos de capital no constituyeron problema alguno para el banco. Alcanzado un cierto punto, sin embargo, la situación se tornó crítica. Esto es evidente si consideramos que entre 2004 y 2010 los intereses acumulados pasaron de 1400 millones a 250,000 millones, siendo esta última cantidad claramente impagable –dicha cantidad, además, se duplicará en diez meses más y se cuadruplicará en menos de dos años. Así, un depósito de 400 pesos que parecía inofensivo en un inicio, se convirtió en una deuda formidable con el paso del tiempo.A pesar de que no son pocos los ejemplos de cantidades a nuestro alrededor que crecen exponencialmente –las epidemias en sus primeras etapas, la población del mundo en algunas épocas, las cadenas y esquemas de tipo Ponzi para esquilmar dinero, el poder de las computadoras, el interés compuesto que se carga a las tarjetas de crédito y posiblemente el tráfico de automóviles en algunas ciudades en nuestro país, entre otros ejemplos–, pocas veces nos ponemos a pensar en las consecuencias que nos pueden acarrear. El peligro fundamentalmente estriba en que una cantidad que crece exponencialmente no luce particularmente preocupante en sus etapas iniciales, hasta que repentinamente explota y se vuelve inmanejable. El crecimiento exponencial tiene consecuencias a primera vista insospechadas. Es sorprendente, por ejemplo, corroborar que un contrato de depósito a 28 días con una tasa anual de 91.3 % produce en el largo plazo intereses radicalmente diferentes a los que produciría un contrato con la misma tasa de interés pero con vencimiento a un año. En este último caso, un depósito bancario de 400 pesos generaría al cabo de 23 años alrededor de 1500 millones de pesos en lugar de los 250,000 millones que se generó con un contrato a 28 días.   El pasado 23 de marzo la Suprema Corte de Justicia de la Nación concedió un amparo a Banamex en contra de la sentencia del Supremo Tribunal de Justicia de Chihuahua. La decisión de la Suprema Corte se basó en una interpretación del significado de las palabras “mismas condiciones”, asentadas en el contrato de depósito original para referirse a las condiciones en las que se renovaría el contrato cada 28 días. En referencia a la tasa de interés, por “mismas condiciones” la Suprema Corte entendió la máxima tasa que fijara el Banco de México en su momento. Fuera de tecnicismos e interpretaciones jurídicas, sin embargo, las consideraciones de fondo subyacentes en las deliberaciones de la Suprema Corte fueron las que se derivan de la imposibilidad para el banco de cubrir una cantidad estratosférica de intereses, que además se duplica cada diez meses. Así, el Ministro Presidente Guillermo I. Ortiz Mayagoitia expresó: “Lo hemos dicho muchas veces, los bancos no operan este tipo de negocios con dinero propio, reciben grandes cantidades de dinero de los ahorradores….y este dinero que no es suyo es el que prestan a su vez a terceras personas…En estas condiciones si el banco presta lo que no es suyo, es donde interviene necesariamente el Estado para poner reglas claras que impidan operaciones ruinosas que podrían llevar a la quiebra a un banco que no tuviera cuidado, diligencia en la celebración de sus contratos”.El descuido en la elaboración y manejo del contrato de depósito que originó el conflicto legal –y que mereció un regaño por parte del ministro Ortiz Mayagoitia – puede atribuirse, al menos parcialmente, al desconocimiento que por lo general tenemos acerca de las consecuencias del crecimiento exponencial. Dado que a nuestro alrededor hay una buena cantidad de cosas que crecen exponencialmente, bien haríamos en tomar conciencia de dichas consecuencias.",
    "En la trama de la novela de H.G. Wells “El hombre invisible”, publicada en 1897, Griffin es un estudiante de medicina que deja la universidad para dedicarse a ciertas investigaciones que no quería compartir con su profesor. Griffin estaba interesado en desarrollar una poción para convertir en invisible a quién la bebiera y a eso dedicó todo su esfuerzo. Para sostener su trabajo de investigación incluso robó dinero a su padre, orillándolo por esto al suicidio. Al final sus esfuerzos dieron fruto y obtuvo la poción que buscaba, misma que probó consigo mismo haciéndose invisible.     Podemos ver un objeto debido a la luz que emite y que llega a nuestros ojos. Éste es el caso, por ejemplo, del Sol o de una lámpara eléctrica. Si el objeto no tiene luz propia –como más comúnmente ocurre–, podemos de cualquier manera verlo por medio de la luz que refleja. Incluso un objeto negro, que no refleja la luz, puede ser percibido por el ojo  debido al contraste que presenta con los objetos a su alrededor, o bien por la sombra que proyecta. Aunque normalmente el sentido de la vista nos da una información fidedigna acerca de los objetos a nuestro alrededor, en algunas ocasiones tenemos dificultad para percibir la existencia de un objeto si este no refleja o no absorbe suficiente luz. Pensemos, por ejemplo, en una ventana de vidrio de grandes dimensiones, la cual resulta casi invisible –y peligrosa– a menos que se le pongan algunas marcas opacas. Esto es debido a que el vidrio absorbe y refleja muy poca luz. Para alcanzar la invisibilidad, el protagonista de la novela de H.G. Wells cambió las propiedades físicas de su cuerpo de modo que no reflejara ni absorbiera la luz, haciéndolo de esta manera indetectable. En realidad, la novela de H.G. Wells, escrita hace más de cien años, planteaba algo que se antoja difícil de realizar con el tejido orgánico de nuestro cuerpo sin que pierda sus funciones. Hoy en día, un siglo después de publicado “El hombre invisible”, y con ideas muy diferentes a las planteadas por H.G. Wells, en varios centros de investigación en el mundo se está trabajando en el desarrollo de técnicas para hacer invisible un objeto. En un artículo publicado el pasado jueves en la revista “Science”, por ejemplo, investigadores del Karlsrhue Institute of Technology en Alemania y del Imperial College London en Inglaterra, reportaron un experimento en el que hicieron “desaparecer” una abolladura que había sido practicada en una delgada lámina de oro. Para lograr esto, cubrieron la superficie del oro con una capa, de 10 milésimas de milímetro de espesor, de un material hecho a base de un polímero comercial. La capa empleada para ocultar la abolladura en el oro en el trabajo reportado en “Science” es un ejemplo de los llamados “Metamateriales”. Estos son materiales artificiales –es decir, no existentes en la naturaleza– que son diseñados para cumplir una función dada –en este caso la de ocultar un objeto. Los metamateriales tienen una estructura muy compleja, aunque puedan ser fabricados a partir de materiales comerciales. En el caso que nos ocupa, por ejemplo, la capa que “desapareció” la abolladura de la lámina de oro consistía de un arreglo tridimensional de pequeños cilindros del material polímérico, cada uno de ellos con un tamaño menor a una milésima de milímetro.En contraste con lo planteado por H.G. Wells en su novela, las técnicas que actualmente se están desarrollando para hacer invisible un objeto buscan que los rayos de luz, más que atravesarlo, le den la vuelta. El objeto será de este modo indetectable pues no reflejará luz ni proyectará una sombra. No tendría así que ser transformado de ninguna manera para hacerlo transparente como ocurría en “El hombre invisible”: bastaría con rodearlo de una cubierta de un metamaterial adecuado.No debemos pensar, sin embargo, que estamos cerca de poder hacernos de un abrigo –y de un gorro como complemento– que nos permita “desaparecer” de la vista de todo mundo. De hecho, como lo expresó uno de los integrantes del grupo de investigación alemán en una entrevista a “Science”, en su grupo no están en este momento considerando una aplicación concreta de su descubrimiento. Ésta, ciertamente, estaría todavía lejana.  Por otro lado, para una persona común y corriente, la invisibilidad, si bien tendría ventajas, podría resultar peligrosa. El protagonista del “El hombre invisible”, que se desquició por el poder que le daba su condición y buscaba dominar al mundo, murió finalmente linchado por una multitud. Su invisibilidad lo hacia inmune a los efectos de la luz; no lo hizo inmune, sin embargo, a los golpes, así hayan sido propinados a ciegas.",
    "En los últimos 500 millones de años han ocurrido cinco grandes extinciones de especies en la Tierra. La última, que tuvo lugar hace 65 millones de años en algún momento durante la transición entre el periodo cretácico y el paleógeno –conocida como extinción K-Pg–, es la más famosa pues dio origen a la desaparición de los dinosaurios más grandes. Los paleontólogos averiguaron que hubo una extinción masiva de especies hace 65 millones estudiando la existencia de fósiles en las diferentes capas cercanas a la superficie terrestre. En este respecto, los geólogos saben que a mayor profundidad le corresponde una mayor antigüedad de modo que los diferentes estratos geológicos nos describen la historia de la Tierra. La extinción K-Pg, que llevó a la desaparición de más de la mitad de las especies existentes, es evidente por la ausencia de fósiles de las especies extintas en estratos geológicos inmediatamente posteriores a la frontera entre los periodos cretácico y paleógeno.Aunque existen hipótesis alternativas para explicar esta extinción masiva de especies, hoy en día la más aceptada es la que la relaciona con el choque, hace 65 millones de años, de un aerolito con un tamaño de 10 kilómetros con la Tierra. Este choque produjo un cráter de casi 200 kilómetros de diámetro, y provocó una nube de polvo y gases en la atmósfera que bloqueó casi por completo la luz del Sol por varios años. Este bloqueo impidió la actividad fotosintética por medio de la cual las plantas fabrican material orgánico, interrumpiendo de este modo la cadena alimenticia. Así, los animales herbívoros sufrieron escasez de alimento al igual que los carnívoros que se alimentan de aquellos. Además, los animales más grandes sufrieron más que los pequeños la falta de alimento y fueron en consecuencia más vulnerables. La hipótesis que liga la extinción masiva K-Pg con la caída de un aerolito en la Tierra fue aventurada  por vez primera en 1980 por el físico estadounidense de origen español Luís Álvarez –premio Nobel de Física 1968 y el mismo que a bordo del Enola Gay durante el bombardeo atómico de Hiroshima sirvió como observador científico de los efectos de la explosión– y su hijo el geofísico Walter Álvarez, ambos de la Universidad de Calfornia, Berkeley. Estos investigadores basaron su explicación en la las altas concentraciones del elemento iridio que encontraron en capas geológicas pertenecientes a la frontera K-Pg, medidas en varios sitios en Italia y Dinamarca. El iridio, que es poco abundante en la superficie de la Tierra, fue encontrado en dichas capas en concentraciones inusualmente altas. Este hallazgo apuntaba a un origen extraterrestre del iridio –que habría sido traído a la Tierra por un aerolito–, pues se sabe que este elemento es abundante en cuerpos del sistema solar como asteroides y cometas.  En la época de publicación del trabajo original de Luís Álvarez y colaboradores –aparecido en la revista “Science” con el título: “Causa extraterrestre de la extinción cretácico-terciario” –no se tenían candidatos para el sitio en donde habría caído el supuesto aerolito. Un indicio firme al respecto llegó en 1991 cuando se publicó en la revista científica “Geology” un artículo en el que se reportaba que en la península de Yucatán existe un cráter de 180 kilómetros de diámetro producto del impacto de un meteorito en una época coincidente con la frontera K-Pg . Hoy en día existen evidencias firmes que este cráter fue la causa de la extinción de especies en la frontera Cretácico-Paleógeno. El cráter tiene su centro en el mar a poca distancia del un pueblo llamado Chicxulub, cercano a Mérida. Había sido descubierto años atrás por dos ingenieros que trabajaban para PEMEX en tareas de exploración geológica, pero sólo fue identificado como tal hasta 1991. En un artículo publicado el 5 de marzo pasado en la revista “Science”, y al que se le dio una gran publicidad en los medios, un grupo de 41 científicos de varios países, incluyendo México, hacen una revisión de la información científica que se tiene acerca del impacto de Chicxulub, llegando a la conclusión de que el mismo fue la verdadera causa de la extinción masiva de especies hace 65 millones de años. El estudio incluye una revisión del estrato geológico de la frontera K-Pg  en 350 sitios, localizados tanto alrededor de Chicxulub como en  lugares tan alejados como la India y Australia. El impacto del asteroide, con un tamaño de 10 kilómetros y una velocidad de 20,000 kilómetros por hora, habría generado terremotos de grado superior a 11 en la escala de Richter, tsunamis gigantescos que habrían barrido las costas e incendios generalizados que habrían lanzado a la atmósfera enormes cantidades de polvo, hollín, y entre 100,000 y 500,000 millones de toneladas de azufre y otros gases. Esto último habría bloqueado la luz solar por años.Las consecuencias de la caída en la Tierra de un asteroide como el de Chicxulub son ciertamente aterradoras. No obstante, las probabilidades de que ocurra otro evento similar son muy pequeñas. Luís Álvarez y colaboradores en su artículo de 1980, por ejemplo, estiman que un aerolito con un tamaño mayor a 10 kilómetros cae en la Tierra cada 100 millones de años en promedio. Tendríamos entonces que esperar un tiempo considerable hasta el nuevo impacto y nadie de nosotros vivirá tanto para atestiguarlo.",
    "Como fue ampliamente difundido por la prensa a lo largo de la última semana, el terremoto de 8.8 grados en la escala de Richter que afectó a la región de Maule en el centro de Chile el pasado 27 de febrero habría acortado la duración del día en un poco más de una millonésima de segundo. A esta conclusión llegó el geofísico Richard Gross del Laboratorio de Propulsión a Chorro de la NASA, cuyos cálculos fueron reportados en un portal de esta agencia. Como un efecto adicional, de acuerdo a los mismos cálculos, el terremoto habría también desviado el eje de la Tierra unos 8 centímetros.    El terremoto del pasado mes de febrero en Chile, que es uno de los más intensos que se hayan registrado, se generó en la frontera entre las placas tectónicas de Nazca y Sudamérica  debido al deslizamiento de la placa oceánica de Nazca por debajo de la placa continental de  Sudamérica. Este movimiento produjo una redistribución de la masa terrestre, haciendo la Tierra más compacta, lo que a su vez  habría llevado a un aumento en su velocidad de rotación y en consecuencia a un acortamiento del día. El efecto es similar al que experimenta una patinadora sobre hielo cuando al girar recoge los brazos –es decir, redistribuye su masa corporal y se hace más compacta– con lo que incrementa su velocidad de giro. Otros terremotos han sido también responsabilizados de cambiar la duración del día y la orientación del eje terrestre. Por ejemplo, el terremoto de 9.2 grados ocurrido en el Océano Índico en diciembre de 2004 frente a la costa oeste de Sumatra por el deslizamiento de la placa tectónica de la India por debajo de la placa de Birmania –y que dejó 230,000 muertos por el maremoto que le siguió–, se ha estimado que cambió la duración del día en tres millonésimas de segundo. El movimiento de tierra más intenso que ha sido registrado – con 9.5 grados en la escala de Richter–, ocurrido en 1960 en la provincia de Valdivia en Chile –no lejos del epicentro del sismo del pasado mes de febrero– se estima que habría cambiado en 30 centímetros el eje de la Tierra.      La rotación de la Tierra también puede ser modificada por causas no naturales. La presa Tres Gargantas en China, por ejemplo, que estará en operación plena el próximo año, almacenará 40 kilómetros cúbicos de agua que, según la NASA, harán a la tierra ligeramente menos compacta, lo que alargará el día por 60 mil millonésimas de segundo y moverá el eje de la Tierra por unos 2.5 centímetros.Como sabemos, nuestro planeta gira sobre si mismo alrededor de un eje que está inclinado aproximadamente 23 grados con respecto al plano de su órbita alrededor del Sol. Esta inclinación, que es siempre la misma, hace que la cantidad de luz solar que, de manera alternada, reciben ambos hemisferios norte y sur varíe a lo largo del año dando lugar a las estaciones. Surge entonces la pregunta de si el cambio en la orientación del eje de rotación de la Tierra por efecto de un terremoto podría cambiar el clima terrestre.La respuesta es que no sería éste el caso, pues los efectos de un terremoto sobre la rotación de la Tierra son muy pequeños, aun en el caso de los terremotos más intensos como el ocurrido en Valdivia. De hecho, estos efectos son tan pequeños –algunos centímetros de desplazamiento del eje terrestre en comparación con la dimensiones de la Tierra, o unas pocas millonésimas de segundo de las 24 horas que componen un día– que es incluso difícil medirlos. Además, como hace notar Richard Gross en una entrevista concedida a la revista Scientific American en 2005 en relación al sismo de Sumatra, hay otras fuerzas naturales que producen cambios más marcados en la rotación de la Tierra. Entre éstas menciona a los cambios en la dirección de las corrientes marinas y, sobre todo, a los cambios en la dirección de los vientos. En este último caso el efecto sobre la rotación terrestre sería unas 300 veces más grande que el de un terremoto. Podríamos quizá entonces considerar que, al menos parcialmente, la amplia difusión que se le ha dado a los cálculos del Laboratorio de Propulsión a Chorro sobre los efectos del terremoto chileno es parte de la estrategia de relaciones públicas de la NASA. Como una entidad que funciona con recursos públicos, la NASA está obligada a dar a conocer sus logros de la manera más favorable posible a fin de obtener los recursos que necesita para operar.Tenemos entonces que si hay cambios en la rotación de la Tierra –que de hecho los hay– éstos no son debidos, de manera primordial, a los terremotos. Desde este punto de vista podemos dormir tranquilos sin el temor de que un movimiento telúrico nos acorte de manera repentina las horas de sueño. Lo que si pudiera ser motivo de preocupación –aunque no para nuestra generación– es que la rotación de la Tierra sea modificada por causas atribuibles a actividades humanas, como lo ejemplifica la presa Tres Gargantas. Ciertamente en este caso los efectos son despreciables en comparación con el tamaño de la Tierra. No obstante, al inicio de la revolución industrial hace 200 años nadie hubiera pensado que algo tan vasto como es la atmósfera terrestre pudiera ser llegar a ser contaminada de manera tan seria.",
    "En el último número de la revista científica británica “Nature” se incluyó un artículo firmado por investigadores de universidades de los Estados Unidos e Irlanda en el que se reportan experimentos neurológicos diseñados para determinar el grado de rechazo que los humanos tienen por situaciones de desigualdad social. Hoy en día se considera que el altruismo y la cooperación entre humanos ha sido una condición necesaria para el desarrollo de la civilización y por lo tanto debe haberse convertido en parte de la naturaleza humana a través de un proceso evolutivo. Incluso la guerra, que ha sido una constante en el desarrollo de la civilización, se considera un reflejo de nuestra naturaleza altruista, pues –excluyendo mercenarios– no podría de otro modo conformarse un ejército para defender una causa común exponiendo a los soldados a un peligro mortal.   El estudio de referencia aporta evidencia neurológica en favor de que los seres humanos tienen efectivamente tendencias intrínsecas que los lleva a buscar disminuir las desigualdades entre personas, aun si esto representa un costo individual. La experiencia se llevó a cabo con cuarenta personas del sexo masculino sin ningún tipo de relación entre ellas. Se formaron 20 parejas y al inicio a todos los participantes les fueron entregados 30 dólares. En seguida y a fin de crear una situación de desigualdad, a uno de los miembros de cada pareja le fueron entregados 50 dólares más. El otro miembro, en cambio no recibió ninguna cantidad adicional. Se crearon así dos grupos: “ricos” y “pobres”. Una vez en este punto se les confrontó con la posibilidad de recibir cantidades adicionales de dinero y se midió su respuesta neuronal en áreas del cerebro que se sabe responden a recompensas monetarias. Esta respuesta se determinó de manera indirecta por técnicas de resonancia magnética funcional, que detectan cambios positivos o negativos en el flujo de sangre a ciertas partes del cerebro producto de un aumento o disminución de la actividad neuronal.El estudio demostró que los miembros ricos tuvieron una reacción positiva mayor ante la posibilidad de que hubiera una transferencia adicional de dinero a su pareja pobre, que la que mostraron ante una posible transferencia monetaria a ellos mismos. Justamente la reacción contraria fue observada entre los miembros pobres de cada pareja. La reacción general fue entonces en la dirección de disminuir la desigualdad entre ricos y pobres. En otro estudio llevado a cabo igualmente mediante técnicas de resonancia magnética funcional por un grupo de investigadores en los Estados Unidos, Italia y Brasil, se midió la actividad neuronal que tiene lugar durante el acto de realizar una donación caritativa, poniendo de manifiesto las bases neurológicas del altruismo.  La creencia en una naturaleza altruista de la raza humana no siempre ha existido. En el siglo XIX, por ejemplo, la selección natural –postulada por Charles Darwin como el mecanismo que origina la evolución de las especies y que es sintetizada en la frase “supervivencia del más apto”– llevó al concepto de “darwinismo social” –no suscrito por Darwin– que trasladó la selección natural del ámbito biológico al social. El darwinismo social sirvió para apoyar doctrinas como el “laissez faire” que propugna por un capitalismo sin ninguna cortapisa por parte del Estado y que de manera inevitable lleva a la desigualdad social. Se  usó también en la eugenesia que plantea una mejora genética dirigida de la raza humana y que sirvió de fundamento ideológico al nazismo. Los resultados de los estudios descritos anteriormente indican que las enseñanzas éticas y morales sobre las virtudes del altruismo reflejan efectivamente a la naturaleza humana. No obstante, apuntan también a que tienen un fundamento último en la química del cerebro. El que la organización del cerebro determine el comportamiento –moral o inmoral– de un individuo tiene implicaciones religiosas y jurídicas profundas, pues pone en entredicho el concepto de libre albedrío. No obstante, ninguna teoría o conclusión científica puede tomarse como definitiva y resultados posteriores pueden confirmarla o refutarla. Los estudios sobre la respuesta neuronal del cerebro ante disyuntivas morales, que apuntan a una naturaleza humana benevolente hacia los demás, tendrán que ser confirmados por futuros estudios. Por lo pronto, nuestra experiencia diaria –muy lejos del rigor con que se conduce una investigación científica– nos indica que si la raza humana es por naturaleza altruista, algunos de nuestros semejantes muestran un especial empeño en ocultarlo.",
    "Del 8 al 18 del presente mes de febrero estuvo de visita en México Vernor Muñoz Villalobos, Relator Especial sobre el derecho a la educación del Consejo de Derechos Humanos de la ONU. La estancia de Vernor Muñoz incluyó visitas a Chiapas, Baja California, Nuevo León y el Distrito Federal, así como entrevistas con el Secretario de Educación, el Rector de la UNAM y la Directora del IPN. Al final de su estancia ofreció conferencias de prensa en la que dio a conocer algunas conclusiones de su visita a nuestro país, las cuales forman parte del informe que será presentado el próximo mes de junio al Consejo de Derechos Humanos de las Naciones Unidas. Entre otras cosas, Vernor Muñoz criticó la relación que mantiene la Secretaría de Educación Pública con el sindicato de maestros que calificó de “simbiosis atípica”, que si bien algunas veces ha resultado en colaboración, en otras ha representado una obstrucción. Se refirió igualmente a las oportunidades diferenciadas que en materia de educación tienen los diferentes grupos socioeconómicos en México. De manera específica señaló: \"La conclusión preliminar después de realizar esta misión es que las exclusiones de las oportunidades educativas en México tienen destinatarios muy precisos, poblaciones marginadas, que se resume en una frase: las poblaciones pobres reciben una educación pobre\".   La decisión de nombrar un Relator Especial fue tomada por la Comisión de Derechos Humanos de la ONU en el año de 1998. La encomienda de dicho relator se centra en  el derecho a la educación tal como está enunciado en el artículo 26 de la Declaración Universal de Derechos Humanos. Esta declaración establece el derecho de toda persona a recibir instrucción elemental gratuita, así como el acceso generalizado a la educación superior en función de méritos individuales. Establece también como objetivo de la educación: “el pleno desarrollo de la personalidad humana y el fortalecimiento del respeto de los derechos humanos y las libertades fundamentales”.   En consonancia con lo anterior, –según el portal Universia-México–, durante su visita a la UNAM Vernor Muñoz señaló que la enseñanza no existe para resolver los problemas de los empleadores “pues de ser así se vería reducida a los mandatos determinados por la economía mundial”, sino que su objetivo es desarrollar las capacidades humanas que tienen que ver con la filosofía, las letras y el trabajo.El objetivo último de la educación debe ser, por supuesto, el bienestar individual y colectivo de los miembros de una sociedad. El bienestar social de un país, sin embargo, está determinado en buena medida por su desarrollo económico y desde esta perspectiva la educación no debe desatender las necesidades de la economía. Como lo ha manifestado la Organización para la Cooperación y el Desarrollo Económico, el desarrollo económico de un país depende de su capacidad de innovación y ésta a su vez del nivel educativo de su población. La innovación tecnológica requiere entonces de un sistema educativo formador de científicos e ingenieros de alta calificación, que en el caso de México no es lo suficientemente eficiente. Esto se hace evidente por el hecho de que un porcentaje significativo de los estudiantes que arriban a nuestras instituciones de educación superior no tiene la suficiente preparación académica y, lo que es más grave, la suficiente motivación para realizar estudios universitarios. Se sabe que los niños en edad preescolar desarrollan actitudes negativas hacia actividades relacionadas con la ciencia en respuesta a una pobre percepción que ellos mismos tienen sobre sus propias habilidades en esta área. Estas actitudes negativas son un obstáculo para realizar exitosamente sus deberes escolares presentes y futuros alrededor de dichas actividades, y les impiden desarrollar un gusto por la ciencia que los impulse a decidirse en el futuro por una carrera científica. En relación a este punto, en un artículo aparecido en el número de marzo del presente año de la revista de divulgación Scientific American, se señala la necesidad de impulsar la educación en ciencias en el nivel preescolar. Dicho artículo cita el trabajo desarrollado por un grupo de investigadores en la Universidad Purdue en Indiana, Estados Unidos (www.purduescientificliteracyproject.org), que desarrolló un programa para introducir a niños en edad preescolar a temas de ciencia, haciendo énfasis en el método científico de buscar respuestas a los fenómenos naturales. De acuerdo con investigaciones de dicho grupo, es posible cambiar actitudes negativas infantiles hacia la ciencia con un entrenamiento adecuado.Introducir en México la educación científica en el nivel preescolar se ve lejano dados los enormes retos que enfrenta la educación elemental en nuestro país. En realidad, aún en un país desarrollado como lo es los Estados Unidos, presenta grandes dificultades. Sin embargo, la perspectiva de que los países industrializados en un futuro cercano implanten métodos educativos que los lleven a producir más y mejores científicos e ingenieros altamente calificados, es preocupante para un país como el nuestro que a estas alturas no ha logrado resolver sus graves problemas en materia de educación, como bien apuntó Vernor Muñoz.",
    "El pasado miércoles 10 de enero alrededor de las cuatro de la tarde, entre Tulancingo, Hgo. y el pequeño poblado de Ahuazotepec en Puebla –separados por poco más de 20 kilómetros–, se observó una bola de fuego cruzando el cielo a gran velocidad seguida de un estruendo. El evento causó mucha alarma al grado que el día jueves 11 el director de protección civil del Estado de Hidalgo se quejó de llevar 24 horas sin dormir, por el “rumor mediático muy grave” que se generó. Se pensó inicialmente que se trataba de un meteorito, pero el Ing. Fernando Peña de Tulancingo –promotor de la formación de la Agencia Espacial Méxicana–, citando datos del Departamento de Defensa de los Estados Unidos lo atribuyó al ingreso a la atmósfera de una parte del satélite ruso Cosmos 2421, que se fragmentó en el año de 2008 en cerca de 500 pedazos mayores a 5 centímetros. No se han encontrado, sin embargo, restos del bólido ni se ha descubierto el cráter que habría producido en el punto de impacto. Una explicación definitiva del evento por parte de expertos en la materia queda entonces pendiente. No sería remoto, sin embargo, que efectivamente se hubiera tratado de la entrada a nuestra atmósfera de un fragmento de un satélite que estuviera ya fuera de operación, y por lo mismo sin control. Desde la puesta en órbita del primer satélite artificial por la Unión Soviética en 1957 –el Sputnik 1– se han lanzado al espacio unos 6000 satélites, de los cuales solamente unos 800 están actualmente en funcionamiento. Hay, de este modo, numerosos satélites fuera de control. Además, durante su permanencia en órbita los satélites en desuso pueden sufrir explosiones o choques con otros satélites, fragmentándose e incrementando considerablemente el número de objetos en órbita y consecuentemente el número de los que reingresan a la atmósfera. En relación a esto último, se sabe que hay en órbita cerca de 19,000 objetos con un tamaño mayor a 10 centímetros, de los cuales aproximadamente el 50 % se originó en la desintegración de unos 200 satélites. De estos 19,000 cuerpos en órbita, unos 5,000 provienen del choque ocurrido en febrero de 2009 entre el satélite comercial Iridium 33 y el satélite ruso Cosmos 2251, así como de la desintegración intencional del satélite chino Fengyun-1C llevado a cabo en enero de 2007 con propósitos de experimentación.Los satélites en desuso y los fragmentos que se generan a partir de los mismos constituyen, por definición, basura espacial. El número de objetos catalogados como basura espacial crecerá en el futuro si no se toman medidas para removerlos de su órbita. En efecto, en un estudio reportado en el número de enero de 2010 de la publicación de la NASA “Orbital Derbis” –“Basura Orbital”– se concluye que el número de objetos en órbita con un tamaño mayor a 10 centímetros de incrementaría en los próximos 200 años aún si se suspendieran los lanzamientos de nuevos satélites. La razón de esto es la multiplicación de fragmentos por el choque entre objetos en órbita.        La basura espacial constituye un gran peligro para los satélites activos. Aún pequeños fragmentos de pintura desprendida de las superficies de los satélites puede representar cierto riesgo por las altas velocidades de decenas de miles de kilómetros por hora con que pueden colisionar. La energía de un objeto metálico de 10 centímetros viajando a 30,000 kilómetros por hora,  por ejemplo, es equivalente a la energía de un objeto de una tonelada viajando a 2,000 kilómetros por hora. Es motivo de preocupación que el espacio satelital esté cada vez más contaminado de basura orbital. En ocasiones la contaminación se ha dado incluso sin ninguna justificación. Esto ocurrió, por ejemplo, al inicio de la década de los sesenta, cuando los satélites de comunicación no estaban todavía desarrollados y las comunicaciones militares internacionales dependían de cables submarinos, que se consideraban vulnerables, o de señales de radio que eran reflejadas en la parte alta de la atmósfera, esquema que también se consideraba poco confiable. Para resolver esta situación, se llevó a cabo el proyecto conocido como West Ford, mediante el cual fueron lanzadas al espacio a 3,700 kilómetros de altura, cerca de 500 millones de pequeñas agujas de cobre de 1.8 centímetros de longitud a fin de formar una nube metálica que reflejara las señales de radiocomunicación, lo que generó protestas de manera inmediata. Por otro lado, si bien es de gran peligro para los satélites activos, la basura espacial no constituye un peligro mayor en la superficie de la Tierra, debido a que los fragmentos, excepto los muy grandes, se desintegran al reingresar a la atmósfera. Así, sucesos como el acaecido el pasado miércoles en la frontera de Hidalgo y Puebla no deben ser motivo de mayor preocupación, ni deben llevar a la pérdida del sueño a ninguna persona, si es que en verdad se trató de basura espacial.    Después de 60 años de actividades espaciales, pareciera ser que los países desarrollados, quienes son los que han mayormente colocado satélites en órbita, no solamente han contaminado la superficie de la Tierra y su atmósfera, sino incluso más arriba.",
    "En las últimas semanas han arreciado las críticas de los activistas anti-cambio climático en contra de la actuación del Panel Intergubernamental sobre Cambio Climático (PICC), en especial contra su presidente Rajendra Pauchari. La andanada se inició el pasado  noviembre cuando fueron “subidos” a Internet  numerosos correos electrónicos robados –“hackeados”– del Centro de Investigación del Clima de la Universidad de East Anglia en el Reino Unido. Dichos correos forman parte de la correspondencia entre diferentes expertos climáticos que trabajan para el PICC y de los mismos se puede interpretar que dichos expertos no siempre actuaron con estricto apego a las normas de ética científica de libre discusión de la ideas. En algunos correos se habla incluso de ocultar datos sobre el clima que no apoyan la tesis del calentamiento global.  Algo que vino a empeorar la situación para el PICC es el reconocimiento por parte de su presidente relativo a que una predicción del panel incluida en su informe de 2007 -en el sentido de que la nieve de los Himalaya se habrá fundido en el año 2035 por efectos del calentamiento global-, en realidad no tiene una base científica. Con esto, los críticos del PICC, que están pidiendo la renuncia de Pachauri, ponen en duda la seriedad de los trabajos y reportes del organismo.Una característica esencial de la ciencia es que sus afirmaciones nunca tienen una certeza absoluta y son siempre susceptibles de ser refutados por investigaciones posteriores. La libre discusión de sus resultados es lo que ha dado a la ciencia un poder de predicción sin precedentes en la historia de la civilización. Esto su vez ha posibilitado la aparición de tecnologías de una gran y creciente sofisticación. Ha llevado, por ejemplo, al desarrollo de computadoras, de satélites artificiales, de materiales sintéticos con propiedades no existentes en la naturaleza y de drogas para curar enfermedades. La construcción de un dispositivo tan complejo como lo es una computadora, o el desarrollo nuevos antibióticos para atacar enfermedades infecciosas, requiere de un conocimiento detallado de las leyes de la física y de la biología. El poder de predicción de la ciencia, sin embargo, no es absoluto, y depende de la complejidad del objeto bajo estudio o del campo científico que se considere. Así, podemos estar razonablemente seguros de que al apretar el botón de encendido de un televisor -producto de nuestro avanzado conocimiento en el campo de la física-, éste se comportará como esperamos y aparecerá una imagen en la pantalla. En cambio, al administrar una droga a un paciente hay una probabilidad no despreciable de que la misma no funcione. Esto refleja la diferencia de complejidad entre la materia viva y la inanimada, que hace intrínsicamente más difícil descubrir las leyes que gobiernan a la primera en comparación con la segunda. En el caso del cambio climático, el objeto bajo estudio –el planeta entero– es extremadamente complejo y el poder de predicción de la ciencia climática es sólo relativo. Existe entonces una cierta incertidumbre sobre los que le ocurrirá a la temperatura de nuestro planeta en los años por venir y de esto se han valido los detractores del cambio climático para impugnarlo.  Hay, sin embargo, evidencias científicas incontestables de que la superficie de la Tierra se está calentando. En un reporte de la Academia  Nacional de Ciencias de los Estados Unidos, por ejemplo, se llega a la conclusión de que las temperaturas terrestres en las últimas décadas han sido mayores en promedio a las observadas en los últimos 400 años. El incremento en la temperatura del planeta se ve, además, reflejado en el deshielo del polo norte durante los últimos veranos y en la retracción del hielo en los glaciares. Sabemos, por otro lado, que la concentración de dióxido de carbono en la atmósfera se incrementó paulatinamente en la segunda mitad del siglo pasado. Mediciones de dicha concentración realizadas en Mauna Loa, Hawai, muestran un aumento paulatino de más de 20 % a lo largo de los últimos 50 años. La causa probable de dicho aumento es la quema de combustibles fósiles.Puesto que se sabe que el dióxido de carbono en la atmósfera genera un efecto invernadero, el aumento de dicha concentración concurrentemente con una elevación de la temperatura de la superficie terrestre sugiere que ambos incrementos están relacionados y que tienen un origen común en el uso de combustibles fósiles. El calentamiento global constituye un problema que requiere de una urgente atención. Se necesita, de manera específica, sustituir a los combustibles fósiles por fuentes de energía renovable. En un mundo que ha basado su desarrollo en el petróleo, el gas y el carbón, la propuesta de sustituirlos por fuentes no contaminantes ha dado origen a resistencias y problemas políticos por parte de los intereses creados alrededor los combustibles fósiles. Esto se ve reflejado en el activismo anti-cambio climático. En este respecto, la contribución de algunos miembros del PICC no ha sido, definitivamente, la más conveniente para la salud del planeta.",
    "La noticia científica de la semana fue sin duda el descubrimiento del color que en vida lució el dinosaurio “Sinosauropterix”, que habitó la provincia de Liaoning en el norte de China hace unos 120 millones de años. En un artículo aparecido el pasado 24 de enero en la revista Nature, un grupo de investigadores del Instituto de Paleontología y Paleoantropología de Beijing, China y de la Universidad de Bristol en el Reino Unido, reportaron haber encontrado evidencia científica que indica que el Sinosauropterix –del tamaño de un gato pequeño– tenía plumas de colores café rojizo. En referencia a este reporte, en días pasados pudimos ver en los medios impresos dibujos que nos mostraban como podrían haber lucido estos dinosaurios: caminando en dos patas y con una cola muy larga adornada con anillos de colores naranja y blanco. Al igual que sus parientes más conocidos, el Velociraptor y el Tiranosaurio Rex, el dinosaurio Sinosauropterix pertenece al suborden de los Terópodos. Éstos son los antecesores evolutivos de las aves modernas y existe controversia sobre si, al igual que éstas, estaba cubierto de plumas. En el artículo referido de Nature, se sostiene que un análisis por microscopía electrónica del fósil estudiado muestra que el Sinosauropterix definitivamente sí tenía plumas. Éstas, no obstante, eran tan pequeñas que no le servían para volar, lo que tiene implicaciones científicas importantes pues resultaría que evolutivamente las plumas de las aves no surgieron con el propósito de ayudarlas a volar –ya que el Sinosauropterix no lo hacía–, sino con algún otro motivo; como un aislante térmico para protegerlas del frío, o como un atractivo visual para conseguir pareja, por ejemplo.  Otra noticia científica importante aparecida en esta semana fue el anuncio hecho por la NASA de que renunciaba a liberar a la sonda “Spirit” de la trampa de arena en la que cayó en la superficie del planeta Marte el pasado mes de mayo. El Spirit, juntamente con su gemelo “Opportunity”, ha estado explorando la superficie de Marte desde que arribaron en enero de 2004. Ambas sondas fueron diseñadas para moverse sobre la superficie del planeta y así lo hicieron por varios años: el Opportunity recorrió cerca de 19 kilómetros, mientras que el Spirit lo hizo por casi 8 kilómetros antes de quedar inmovilizado. De todas las misiones a la superficie de Marte, la de las sondas Spirit y Opportunity ha sido la más larga y exitosa, enviando a la Tierra una gran cantidad de datos de valor científico que han contribuido a incrementar considerablemente nuestro acervo de conocimientos sobre Marte. Cabe preguntarse sobre los beneficios sociales que aportan investigaciones como las anteriormente referidas. ¿Por qué habría de importarnos lo que sucedió en nuestro planeta hace más de cien millones de años o lo que ocurra o deje de ocurrir en un planeta a cientos de millones de kilómetros de la Tierra? Esta pregunta es importante dado los grandes recursos que puede consumir un proyecto científico –la misión que puso a las sondas Spirit y Opportunity en la superficie de Marte, por ejemplo, tuvo un costo de 850 millones de dólares. La respuesta a la pregunta anterior es que cualquier conocimiento científico que acumulemos tiene una utilidad práctica potencial, que podría hacerse efectiva en un tiempo más o menos corto. Tomemos, por ejemplo, el desarrollo de lo que hoy conocemos como la Mecánica Cuántica, ocurrido en las primeras décadas del siglo XX y que cambió por completo nuestras ideas acerca de la estructura atómica de los materiales. Aunque en su momento la Mecánica Cuántica  no prometía aplicaciones prácticas, a la postre llevó, entre otras muchas cosas, al desarrollo del transistor que produjo la revolución en computación y telecomunicaciones que han cambiado drásticamente el mundo en el que vivimos. Gracias a las computadoras, por ejemplo, es posible predecir el clima con varios días de anticipación y también diseñar mejores aviones. Igualmente, la unión de las computadoras con las telecomunicaciones por fibra óptica ha posibilitado el desarrollo de la red Internet. A quienes vivimos la transición entre las eras pre y post Internet, nos cuesta trabajo entender como pudimos vivir sin esta  red de comunicación.La inversión en ciencia y tecnología, sin embargo, no puede ser ilimitada, aun en los países ricos. El Presidente Obama, por ejemplo, acaba de cancelar el proyectado regreso de los Estados Unidos a la Luna. En los países en desarrollo como es el nuestro, las inversiones en ciencia y tecnología deben ser aun más cuidadosamente aplicadas. Todo nuevo conocimiento científico tiene posibilidades potenciales de desarrollar tecnología, pero es obvio que los tiempos involucrados para que esto suceda dependerán de la naturaleza de dicho conocimiento. Una investigación destinada, por ejemplo, a optimizar la eficiencia de una celda solar podrá tener un impacto tecnológico en un plazo de tiempo menor al que tendría un estudio de naturaleza más básica. Las anteriores consideraciones, no obstante, resultan sólo de importancia relativa para nuestro país, que invierte sólo el 0.4 % de producto interno bruto en ciencia y tecnología,  Esta inversión está muy por debajo, no solamente de los países industrializados, sino también de algunos en desarrollo.",
    "En una entrevista concedida el pasado 6 de enero a la revista de difusión científica Scientific American, Robert Yeats, profesor de Geociencias de la Universidad Estatal de Oregon, afirmó que le preocupaba menos el posible sismo de gran magnitud que desde hace años se ha pronosticado que ocurrirá en el sur de California, que el gran terremoto que con gran probabilidad pronto tendría lugar en Haití. Como todos sabemos, Yeats –al igual que otros expertos que opinaban en el mismo sentido– desgraciadamente tuvo voz de profeta y menos de una semana después se cumplió su predicción.Los terremotos se producen cuando se libera energía de porciones de la corteza terrestre sujetas a enormes fuerzas de tensión o compresión. Esto sucede, por ejemplo, a lo largo de la frontera entre dos placas tectónicas. Se sabe que la corteza terrestre está formada  por masas –las placas tectónicas– que se mantienen en movimiento unas con respecto a las otras. De este modo, las fuerzas de rozamiento que se producen en la frontera entre dos placas moviéndose en sentido contrario pueden acumular una  enorme cantidad de energía, la cual se libera cuando las rocas en la frontera se rompen cediendo a la tensión. Existen a lo largo de la superficie de la Tierra fallas geológicas bien identificadas en donde ocurren estas tensiones y que por tanto son fuente de movimientos telúricos. Una de las más estudiadas es la falla de San Andrés, que atraviesa de norte a sur a lo largo de 1300 kilómetros el estado de California y que corresponde a la frontera entre las placas tectónicas del Pacífico y de Norteamérica. Esta falla fue la causante del terremoto que en 1906 destruyó la ciudad de San Francisco, ocasionando entre 700 y 3000 muertes, en lo que constituye la peor catástrofe en los Estados Unidos debida a un terremoto. En años más recientes, una falla geológica cercana a la ciudad de Los Ángeles originó en 1994 un terremoto que ocasionó 72 muertes y produjo daños por 20,000 millones de dólares. Una medida del poder destructivo de un terremoto lo da su magnitud en la escala de Richter. El terremoto de San Francisco de 1906, por ejemplo, tuvo una magnitud entre 7.7 y 7.9, mientras que el recientemente ocurrido en Haití fue de magnitud 7. Para apreciar la diferencia en intensidades, hay que considerar que un punto en la escala de Richter representa un factor multiplicativo por 30 en poder de destrucción. Un terremoto de magnitud 8 es entonces 30 veces más destructivo que uno de magnitud 7. El terremoto que destruyó Puerto Príncipe el 12 de enero pasado causando hasta 200,000 muertos, tuvo su origen en la falla de Enriquillo situada cerca de la frontera entre las placas tectónicas de Norteamérica y del Caribe. No se había producido un terremoto de gran magnitud cerca de Puerto Príncipe en 150 años y lo expertos consideraban como alta la probabilidad de que esto sucediera en un plazo corto.Comparando los terremotos de California y de Haití, llama la atención el contraste entre el número de muertos y los daños materiales ocurridos en San Francisco y en Puerto Príncipe. Esto, sin embargo, es fácilmente explicable por la mala calidad de las construcciones de la ciudad de Puerto Príncipe y por la falta de prevención del gobierno de Haití en relación a la ocurrencia de un desastre como el sufrido, a pesar de las evidencias científicas disponibles. Todo esto a su vez, refleja pobreza de un país que en este respecto ocupa el último lugar en Latinoamérica. Yeats no fue el único que esperaba una catástrofe en Haití como la que finalmente ocurrió. Otros expertos en el mundo la consideraban muy probable, dada la gran cantidad de energía acumulada en la falla de Enriquillo por las muchas décadas de inactividad sísmica. La desgracia fue, sin embargo, que no obstante que se tenían evidencias firmes de lo inminente de la tragedia, en realidad había muy poco que hacer para aminorarla dado lo pobre del país. Otras regiones en el mundo se encuentran en situación similar a la de Haití, por contar con ciudades cercanas a fallas geológicas con una gran actividad sísmica. Entre estas se encuentran Lima, Karachi y Teherán. Al igual que Puerto Príncipe, estas ciudades se encuentran en gran medida inermes con una espada de Damocles sobre la cabeza.En contraste, países con recursos como los Estados Unidos cuentan con reglamentos de construcción estrictos para incrementar la seguridad sísmica de edificios y construcciones y gastan enormes sumas de dinero en materia de prevención de desastres naturales. Con seguridad, en caso de ocurrir, el esperado gran temblor en California no tendrá ni remotamente las mismas consecuencias que sufrió Haití. Dice el dicho que al perro más flaco se le cargan las pulgas. Esto aplicado a los terremotos no pasa de ser una verdad de Perogrullo: si un perro es flaco es porque no tiene que comer y si no tiene para comida menos tendrá para desinfectante.",
    "En los últimos días del año que acaba de terminar, el director de la agencia espacial rusa Anatoly Perminov, convocó a sus pares en la Unión Europea, los Estados Unidos, China y Japón, a llevar a cabo una iniciativa conjunta para prevenir una posible colisión del  asteroide “Apophis” con nuestro planeta. Este asteroide fue descubierto en el año de 2004 y se estima que tiene unos 250 metros de longitud y una masa de unas veinte millones de toneladas. Tiene una órbita alrededor del Sol cercana a la órbita de la Tierra y se sabe que se acercará a nuestro planeta en el año 2029 a una distancia de aproximadamente 30,000 kilómetros, distancia que es menor que la altura a la que se encuentran los actuales satélites de comunicaciones. Apophis tendrá también acercamientos posteriores en los años 2036 y 2068.  Las colisiones de asteroides de gran tamaño con la Tierra son eventos frecuentes en una escala geológica. Los rastros que dejan, sin embargo, son desvanecidos en buena medida por los procesos de erosión que ocurren en la superficie de nuestro planeta. Posiblemente la más famosa colisión de un asteroide con la Tierra fue la que ocurrió hace 65 millones de años y dejó un cráter de 180 kilómetros de diámetro en la península de Yucatán. Dicho cráter está parcialmente sumergido en el mar y tiene su centro en el pueblo de Chicxulub, cercano de Mérida. Este impacto es famoso porque se piensa que la nube de polvo emitida a la atmósfera como resultado del choque provocó un oscurecimiento global que impidió el paso de los rayos solares por largo tiempo, lo que llevó a una catástrofe ambiental que a su vez causó la extinción masiva de los dinosaurios. Se estima que el asteroide de Chicxulub tenía una dimensión de unos 10 kilómetros.  En épocas menos remotas, en 1908, en la cuenca del Río Podkamennaya Tunguska en Siberia, se produjo una explosión de gran magnitud –conocida como el evento de Tunguska– que derribó árboles en un área de más de 2000 kilómetros cuadrados –que como comparación es más de 15 veces el área comprendida dentro del anillo periférico de la ciudad de San Luís Potosí y es mayor que toda el área de la ciudad de México. La explosión de Tunguska tuvo efectos en lugares tan lejanos como Inglaterra, en donde hubo claridad nocturna por varios días después de ocurrida. Es aceptado que el evento de Tunguska fue producido por un asteroide o cometa con una dimensión de decenas de metros, que explotó en el aire a una altura de 5-10 kilómetros.El evento de Tunguska, juntamente con otros similares pero de menor magnitud, ha provocado preocupación sobre las posibles consecuencias de la colisión de un objeto celeste con la Tierra, la cual podría ser catastrófica de ocurrir en una zona densamente poblada. En particular, ha sido motivo de alarma la eventual colisión con Apohis que es varias veces mayor que el cuerpo que produjo el evento de Tunguska y que podría tener un efecto destructivo 100 veces más grande.  Se estima, por otro lado, que una misión espacial para desviar la trayectoria de Apophis costaría hasta 80,000 millones de dólares. Cabe preguntarse, entonces, que tan peligroso debería ser Apophis para merecer gastos tan elevados. Una manera de abordar esta pregunta toma en cuenta lo que subjetivamente percibimos como peligroso para nuestra seguridad personal –percepción que por supuesto varía entre diferentes culturas y épocas–, lo que podemos poner en función de la probabilidad de que nos ocurra un determinado accidente.Todo lo que hacemos implica un cierto riesgo de morir. Algo que, por ejemplo, la inmensa mayoría juzga peligroso y por lo tanto evitaría sin reserva es el juego de la ruleta rusa, por la gran probabilidad de resultar con una bala en la cabeza –1 en 6 si el revólver tiene una sola bala. Por otro lado, aunque somos conscientes de que viajar por carretera implica un cierto riesgo para nuestra seguridad, pocas personas evitarían realizar un viaje de 1000 kilómetros en automóvil alegando que resultaría peligroso. Esto último, por supuesto, tiene que ver con la probabilidad relativamente pequeña de morir en un accidente carretero, que en los Estados Unidos es aproximadamente 1 en 100,000 para un viaje de 1000 kilómetros.En algún momento después de su descubrimiento, se estimó que la probabilidad de que Apophis chocara con la Tierra era de 1 en 37. Actualmente, sin embargo, nuevas observaciones de su trayectoria indican que esta probabilidad es  solamente 1 en 250,000. Si la primera estimación fuera cierta, el peligro de colisión se acerca a lo que subjetivamente consideramos como muy peligroso y cualquier gasto que se realizara para evitarla sería justificable. Por el contrario, si la estimación más reciente fuera la correcta, una posible colisión con Apophis no debería ser motivo de preocupación pues estamos acostumbrados a vivir en medio de peligros sensiblemente mayores.  A reserva de que en el futuro se actualice al alza la probabilidad de colisión con Apophis, pareciera que el llamado del director de la agencia espacial rusa resulta por el momento prematuro.",
    "Después del fracaso de la cubre climática de Copenhague en diciembre pasado, en la que no pudo lograrse un acuerdo para extender el Protocolo de Kioto de reducción de  emisiones de gases de invernadero más allá del año 2012, la siguiente oportunidad para alcanzar un acuerdo se dará en la siguiente cumbre climática a llevarse a cabo en México en el presente año. El tiempo para limitar dichas emisiones se está agotando pues al ritmo actual, la concentración de gases de invernadero en la atmósfera alcanzará pronto valores no sostenibles, si no es que ya lo ha hecho. Los gases de invernadero son producidos por la quema de combustibles fósiles que hoy en día son la fuente de aproximadamente el 85 % de la energía que consume el mundo. Cabe preguntarse, entonces, si desde el punto de vista técnico existen opciones para sustituir dichos combustibles, a corto plazo, por fuentes de energías renovables y no contaminantes. La energía llega al usuario final en varias formas: como gasolina para automóviles, como  carbón o combustóleo para plantas termoeléctricas generadoras de electricidad, como fluido eléctrico para toda serie de aparatos electrodomésticos, etc. Una estrategia de sustitución de los combustibles fósiles debe entonces tomar en cuenta la diversidad de usos de la energía y generar soluciones para cada una de sus principales aplicaciones.Una forma muy conveniente de energía es la eléctrica, pues es fácilmente manipulable y transportable a grandes distancias y, sobre todo, porque su consumo no genera contaminación. En un futuro, la energía eléctrica debe ser entonces usada en forma tan amplia como sea posible: en automóviles eléctricos, en calentadores domésticos, en sistemas de transporte colectivo y, por supuesto, en todas las aplicaciones en las que es actualmente empleada. Habrá, por supuesto, aplicaciones en las que la electricidad no sea la fuente energética adecuada. Este es el caso de la aviación, en donde se requiere liberar rápidamente grandes cantidades de energía en motores de propulsión a chorro. En este caso, sin embargo, será posible emplear hidrógeno como combustible, el cual podrá ser obtenido descomponiendo agua por medio de electricidad. De este modo, aunque no de manera directa, la electricidad podría ser en último término el origen de la energía que impulse a los aviones jet del futuro.Hay que notar que hoy en día la mayor parte de la energía eléctrica que consumimos es producida en plantas termoeléctricas que queman combustibles fósiles y por tanto generan gases de invernadero. La electricidad del futuro deberá ser entonces generada por medios diferentes, y no contaminantes, a los preponderantes en la actualidad. Una fuente no contaminante de electricidad que jugará un papel central en un mundo futuro libre de combustibles fósiles es el viento. Aunque actualmente representa sólo un porcentaje pequeño de la generación mundial de energía, el aprovechamiento de la energía eólica está en rápido crecimiento, además de que existen en el planeta grandes reservas de este tipo de energía. En relación a esto último, de acuerdo con cifras citadas por M. Jacobson y M. Delucchi en el número de noviembre de 2009 de la revista Scientific American en su versión electrónica, las reservas explotables de energía eólica representan entre 3 y 7 veces el actual consumo mundial de energía. Otra fuente de energía no contaminante que será primordial en el futuro, y que de hecho es la que cuenta con las mayores reservas, es al Sol. De acuerdo con datos citados en el artículo referido, la cantidad de energía solar explotable es cerca de 50 veces el consumo mundial de energía.  Existen entonces reservas suficientes de energías no contaminantes para reemplazar de manera total a los combustibles fósiles y de hecho, en su artículo Jacobson y Delucchi delinean un plan para este propósito. De acuerdo con el mismo, el 90 % de la energía del futuro sería proporcionado por el Sol y el viento. Para el restante 10 % se emplearían las energías hidroeléctrica y geotérmica, así como aquellas asociadas a mareas y olas. Se necesitarían en el año 2030 las siguientes plantas y dispositivos: 3.8 millones de turbinas de viento, 1700 millones de instalaciones solares en casas-habitación, 89,000 plantas solares, 5350 plantas geotérmicas, 900 plantas hidroeléctricas y 720,000 dispositivos para capturar la energía de las olas. Una vez construidas todas estas las plantas y dispositivos, con tecnologías que están disponibles en la actualidad, tendríamos un mundo libre de emisiones de gases de invernadero.     Como lo sugiere el plan anterior, y otros que han sido igualmente propuestos,  desde un punto de vista técnico existe una solución al problema del calentamiento global. Desde un punto de vista \tpolítico y de intereses económicos, sin embargo, la solución no parece ser  tan simple, como lo atestigua el fracaso de la reunión de Copenhague. Esperemos que en la cumbre climática de México, a celebrarse el presente año de 2010, tengamos mejor suerte.",
    "Es una opinión generalizada que la cumbre climática realizada en Copenhague, Dinamarca, del 7 al 18 de diciembre pasado fue un fracaso. Algunos esperaban que como resultado de dicha cumbre se extendiera el Protocolo de Kioto que expira en el año 2012 y que se establecieran metas cuantitativas de reducción de emisión de gases de invernadero por parte de los países industrializados para los años 2020 y 2050. Después de muchas dificultades, sin embargo, solamente se alcanzó un acuerdo no obligatorio de última hora en el que se establecen compromisos de fijar cuotas voluntarias de emisión de gases de invernadero, en espera a que un acuerdo obligatorio con metas específicas se firme en 2010. De acuerdo con reportes periodísticos, la reunión de Copenhague estuvo mal organizada y resultó caótica. Fue dominada por el enfrentamiento entre los países industrializados y aquellos en desarrollo –que forman el llamado grupo de los 77–, particularmente entre los Estados Unidos y China, los dos principales emisores de gases de invernadero. Al final, el acuerdo de último momento fue producto de negociaciones solamente entre cinco países, los Estados Unidos, China, Brasil, la India y Sudáfrica.      El protocolo de Kioto fue originado en el año de 1997 y en el mismo se establecen cuotas  de emisión de gases de invernadero –dióxido de carbono y metano, entre otros– para los países industrializados, entre los que se encontraban los Estados Unidos, Japón y la Unión Europea. Los países en desarrollo no fueron incluidos en el Protocolo de Kioto en el entendido de que esto limitaría su crecimiento económico. Los Estados Unidos, sin embargo, no ratificaron el acuerdo, precisamente porque no establecía un compromiso general –que incluyera tanto a países industrializados como a países en desarrollo– de luchar contra el calentamiento global.Es ahora cada vez más evidente que la temperatura terrestre promedio se está 1ncrementando lentamente debido, fundamentalmente, a la cada vez mayor concentración de dióxido de carbono en la atmósfera;  esto, por la quema de combustibles fósiles así como también debido a la deforestación que ha ocurrido en las selvas de países como Brasil e Indonesia –la disminución del área ocupada por los bosques elimina los sumideros de dióxido de carbono en la atmósfera debidos a la actividad fotosintética de las plantas. La temperatura de la Tierra se ha elevado aproximadamente 0.75 grados centígrados desde el inicio de la revolución industrial y se estima que un aumento mayor a los 2 grados centígrados será catastrófico.No es la primear vez en la historia del mundo que se han predicho catástrofes por actividades humanas que a fin de cuentas no han ocurrido. Esta vez, no obstante, existe una evidencia científica firme del calentamiento global y de que si no es controlado podría tener consecuencias graves para la población del mundo. Una prueba de lo serio del problema es precisamente la cumbre climática de Copenhague a la que asistieron 193 países y jefes de estado de naciones tanto industrializadas como en desarrollo.        Podemos entender que llegar a un acuerdo sobre reducción en las emisiones de gases de invernadero presenta muchas dificultades. Esto, sobre todo, por la disparidad, tanto en el desarrollo económico, como en los intereses de los países del mundo. Tenemos, por ejemplo, a los Estados Unidos, un país con un alto nivel de vida, que es el segundo más contaminante del planeta y que tiene una de las mayores emisiones per cápita de gases de invernadero, y por otro lado tenemos a los países en desarrollo que de manera justa reclaman su derecho a alcanzar el mismo el mismo nivel de vida del mundo industrializado, lo que hoy en día implica un uso intensivo de los combustibles fósiles. Esto último estuvo reconocido en el Protocolo de Kioto, mismo que los norteamericanos rehusaron ratificar.Gran parte del problema es que algunos países en desarrollo están creciendo de manera acelerada y están contribuyendo con un porcentaje cada vez más grande a la emisión global de gases de invernadero. En un estudio realizado con datos hasta el año 2000, por ejemplo, se encontró que el 70 % de las emisiones globales de gases de invernadero provenían de 20 países, 12 de los cuales están incluidos en el protocolo de Kioto, mientras que el resto –en el que se incluye a México como el décimo país más contaminante del mundo– fue catalogado como no desarrollado y por lo tanto no fue incluido en dicho protocolo. El estudio encontró que de ese 70 % de emisiones un tercio pertenece a los países en desarrollo y el resto a los incluidos en el Protocolo de Kioto. El porcentaje de emisiones de los países en desarrollo, además, está aumentando y de hecho hoy en día el país más contaminante es China. No parece haber entonces una manera fácil de resolver el problema de reducción de emisiones dentro de un esquema como el del protocolo de Kioto, en el que la responsabilidad recae enteramente en los países industrializados –causantes, por otro lado, del problema de cambio climático–, esquema que los Estados Unidos no han apoyado en el pasado.",
    "El 23 de diciembre de 1947, la víspera de la Nochebuena, los investigadores norteamericanos Walter Brattain y John Bardeen de los Laboratorios Bell de la compañía telefónica AT&T en Murray Hill, New Jersey, mostraban a sus directivos el primer transistor de la historia operando como tal. Brattain y Bardeen habían trabajado a lo largo de varios años en el desarrollo de un dispositivo compacto que sustituyera a los bulbos al vacío usados entonces en toda suerte de aparatos electrónicos y el transistor que entonces mostraban era el fruto de sus esfuerzos. Aunque el modelo original de Brattain y Bardeen era crudo en extremo, demostraba que era efectivamente posible encontrar un sustituto de los bulbos al vacío, con una gran cantidad de ventajas potenciales, incluyendo un tamaño reducido, una mayor confiabilidad y un menor consumo de energía. William Schockey, también investigador de los Laboratorios Bell y que actuaba nominalmente como jefe de Brattain y Bardeen, hizo posteriormente contribuciones significativas que llevaron a una versión de transistor más elaborada y cercana a la actual. Por el desarrollo del transistor, a Brattain, Bardeen y Schockley les fue otorgado el premio Nobel de Física en el año 1956.El transistor, que ha sido clave para el advenimiento de la “Era de la Información”, es  catalogado como uno de los inventos más trascendentes del siglo XX por sus implicaciones sociales profundas. La red de comunicaciones Internet, por ejemplo, que es en último término producto del transistor, ha acercado al mundo haciendo posible el envío de grandes cantidades de información en forma de voz e imágenes, de manera instantánea y a prácticamente cualquier parte habitada del planeta. Por medio de Internet es factible igualmente encontrar información sobre esencialmente cualquier tema, lo cual constituye una contribución sin precedentes a la diseminación del conocimiento humano. De la misma manera, Internet ha posibilitado el nacimiento de las llamadas redes sociales que han revolucionado la comunicación personal.Después de la demostración del transistor en diciembre de 1947, en el curso de dos décadas este dispositivo sustituyó a los bulbos al vacío en radios, televisores y en general en todo tipo de aparatos electrónicos –excepto en los muy especializados. Aquellos que tengan la edad suficiente recordarán que los radios y televisores de hace medio siglo eran voluminosos, usaban  una gran cantidad de energía eléctrica y requerían de un tiempo considerable para encender. Todo esto cambió con el transistor. Los primeros radios de transistores fueron creados por la compañía Texas Instruments y salieron al mercado en 1954. Al siguiente año, una pequeña compañía japonesa, que posteriormente se convertiría en la gigante Sony, empezó a vender igualmente radios de transistores en lo que constituyó la primera comercialización exitosa de un producto basado en estos dispositivos y con esto selló la suerte de los bulbos de vacío como elementos centrales de los aparatos electrónicos. El transistor es el origen de toda la industria electrónica moderna. En particular, es el punto de partida del llamado “Valle de Silicio” en California, en donde se asienta una gran cantidad de industrias de alta tecnología y de hecho, la primera industria establecida en dicho valle fue “Schockley Transistor” –hoy desaparecida– fundada precisamente por uno de los inventores del transistor. Actualmente se producen desde transistores individuales hasta “chips” de silicio con cientos de millones de transistores. El transistor en muchos sentidos modeló al mundo actual, no solamente posibilitando tecnologías como las de las telecomunicaciones y la computación, entre muchas otras, sino también por el impacto social que dichas tecnologías han tenido. Podemos, efectivamente, considerarlo entre las más grandes invenciones de la historia. Por otro lado, el que la invención del transistor haya ocurrido circunstancialmente en la víspera de una Nochebuena nos motiva a una reflexión. Como sabemos, alrededor de las fiestas de Navidad y Año Nuevo nuestro país se semi-paraliza durante lo que en los últimos años hemos llamado, no sin sarcasmo, el “puente Guadalupe-Reyes”. El transistor es producto de una sociedad educada y tecnificada –con universidades y centros de investigación que se cuentan entre los mejores del mundo–, y con una marcada vocación por el trabajo que incluso produjo el transistor en la víspera de una Nochebuena. En México, en donde estamos a mucha distancia de alcanzar los niveles de desarrollo de los países industrializados, debemos de tomar las lecciones que nos proporcionan no solamente los Estados Unidos y demás países avanzados, sino también otros países subdesarrollados, como es el caso de China, que están superando su condición de manera acelerada a base de trabajo. Lejos de esto, en los últimos años tal parece que nos hemos afanado en crear más “puentes” los fines de semana. En lo que va del año, además del Guadalupe-Reyes, contabilizamos cinco.",
    "Desde el pasado día 7 y hasta el próximo 18 de diciembre se estará llevando a cabo en Copenhague, Dinamarca, la Conferencia sobre Cambio Climático organizada por la Organización de las Naciones Unidas con la participación de 192 países. Durante esta reunión se espera que se alcancen acuerdos sobre reducción de emisiones de gases de invernadero a la atmósfera que extiendan el protocolo de Kioto más allá del año 2012. Como es del dominio público, los gases de invernadero que se han emitido a la atmósfera de manera creciente desde el inicio de la revolución industrial hace 200 años –mayormente dióxido de carbono–, han provocado un lento incremento en la temperatura de la Tierra que se anticipa será catastrófico si no se toman las medidas pertinentes. En efecto, la concentración de dióxido de carbono en la atmósfera se ha elevado desde 280 partes por millón (ppm), valor que tenía en la etapa preindustrial, hasta 385 ppm en la actualidad. Esto ha provocado que a lo largo del último siglo se haya elevado la temperatura de la superficie de la Tierra por 0.75 grados centígrados. De seguir la tendencia actual, en poco más de treinta años la temperatura terrestre se habrá elevado dos grados centígrados por arriba del valor que tenía hace doscientos años, lo que muchos expertos consideran es el límite permisible.El protocolo de Kioto establece el compromiso de reducir en el periodo 2008-2012 la emisión de gases de invernadero por un 5% con respecto a los niveles de 1990. A pesar de lo modesto de esta meta, sin embargo, el protocolo de Kioto no fue ratificado por los Estados Unidos, país que emite el 20% del total global de gases de invernadero con apenas el 4% de la población mundial. Por otro lado, los países que más rápidamente están elevando su emisión de gases de invernadero son aquellos que están creciendo económicamente de manera más acelerada, como es el caso de China y la India. China, de hecho, recientemente superó a los Estados Unidos como el mayor emisor de dióxido de carbono del mundo, que ahora ocupa el segundo lugar por un margen estrecho. Los Estados Unidos, sin embargo, emiten per cápita cuatro veces más dióxido de carbono que China, y más del doble que el promedio de los países de la Unión Europea. La emisión de dióxido de carbono a la atmósfera se reducirá en la medida en la que se racionalice el uso de la energía y se sustituyan a los combustibles fósiles por fuentes de energía no contaminantes, como es el caso de las energías solar y eólica. Algunos países de la Unión Europea son los que han mostrado mayor sensibilidad en este respecto. Dinamarca, por ejemplo, sede de la actual cumbre climática, obtiene de la energía del viento el 20 % de la electricidad que produce, además de que desalienta el uso del automóvil  imponiendo altos impuestos. Los combustibles fósiles, sin embargo, no podrán ser eliminados totalmente, al menos al mediano plazo, y la acumulación de dióxido de carbono en la atmósfera se incrementará irremediablemente, en mayor o menor medida, en el futuro.Por otro lado, aun con la incorporación paulatina de nuevas formas de energía renovable no contaminante, no podemos esperar que el actual modelo de industrialización basado en los combustibles fósiles pueda ser extendido al mundo subdesarrollado, que comprende cuatro quintas partes de la población mundial. Es decir, si el desarrollo industrial de una quinta parte de la población de mundo nos ha llevado al desastroso estado actual de contaminación ambiental y cambio climático, es impensable que nuestro planeta pueda soportar un desarrollo similar de las cuatro quintas partes restantes. En estas condiciones, muy probablemente los países subdesarrollados se mantendrán por largo tiempo en esa condición, mientras el mundo industrial cambia lentamente hacia una economía libre de carbón. El 18 de noviembre pasado, el Fondo de Población de las Naciones Unidas publicó el informe del Estado de la Población Mundial 2009 en donde establece una relación entre el crecimiento de la población del planeta en el último siglo y el cambio climático. Aunque la ONU ha sido acusada de aprovechar el problema climático para impulsar políticas neo-malthusianas  de control natal, es claro que nuestro planeta no tiene la capacidad suficiente para proporcionar a toda la población del mundo, presente y futura, el mismo nivel de vida –y el consumo energético que conlleva- del que disfrutan los habitantes de los países desarrollados. En estas condiciones, tiene sentido el llamado que hace en el documento de referencia el Fondo de Población de las Naciones Unidas para redefinir el concepto de progreso social.En el contexto de las consideraciones anteriores, uno esperaría que en la cumbre climática de Copenhague hubiera consideración de parte de los países industrializados, causantes del desastre climático, hacia el mundo en desarrollo, que no va a tener las mismas oportunidades que ellos tuvieron. Esto, sin embargo, probablemente no sucederá.     no obstante, en emisión de dióxido de carbono per cápita, los Estados Unidos ocupan por mucho el primer lugar, superando por poco a Canadá.",
    "El 18 de noviembre pasado, durante un congreso sobre supercomputadoras celebrado en Portland, Oregon, la compañía IBM emitió un comunicado de prensa en el que anunciaba que un grupo investigadores de su laboratorio de Almaden, California, encabezados por Dharmendra Modha, había logrado simular en una supercomputadora el cerebro de un gato. Dicha simulación se llevó a cabo por medio de la supercomputadora IBM “Blue Gene” del Laboratorio Nacional de Berkeley, California, la cual cuenta con cerca de 150,000 procesadores. Se simularon mil millones de neuronas con 10 millones de millones de conexiones entre ellas. De acuerdo con el comunicado de IBM, los avances alcanzados constituyen una plataforma para entender el funcionamiento de cerebro y para eventualmente construir una computadora que emule las habilidades de sensación, percepción, acción, interacción y cognición. A los pocos días del anuncio de IBM, sin embargo, el investigador Henry Markram de la Escuela Politécnica Federal de Lausana, Suiza, le hizo severos cuestionamientos, calificándolo sin tapujos de fraudulento. De acuerdo con Markram, el modelo de cerebro empleado por Modha está muy lejos de emular la complejidad del cerebro de un gato. En particular, y otra vez según Markram, las conexiones entre las neuronas empleadas en el modelo están demasiado simplificadas y no reflejan a las neuronas reales.Henry Markram encabeza el proyecto “Blue Brain” (Cerebro Azul) en Lausana que busca desarrollar un cerebro sintético con la capacidad del cerebro de un mamífero, y es por lo tanto un competidor científico de Modha. El proyecto Blue Brain esta basado igualmente en una supercomputadora “Blue Gene” pero de solamente 10,000 procesadores. Hasta ahora, como parte del proyecto Blue Brain han logrado simular una sección del cerebro de una rata con 10,000 neuronas. Según declaraciones de Markram a la BBC, sin embargo, esperan en 10 años ser capaces de construir un cerebro artificial en una supercomputadora con las funciones equivalentes a las de un cerebro humano, incluyendo potencialmente la capacidad de experimentar emociones. La posibilidad de fabricar un cerebro sintético con la capacidad del nuestro, incluyendo la autoconciencia, es por supuesto algo perturbador, y de llevarse a cabo echaría por la borda 2000 años de especulaciones religiosas y filosóficas acerca de nuestra propia naturaleza. Demostraría que nuestro cerebro no es más que una máquina, de una complejidad y eficiencia extremas, pero a fin de cuentas sólo una máquina que puede ser replicada por un conjunto de “chips” de computadora si el arreglo de éstos es lo suficientemente complejo y si están además programados de la manera adecuada. La inteligencia artificial, además de sus implicaciones filosóficas y religiosas, tiene por supuesto una enorme importancia práctica. En consecuencia, en la investigación en este campo se han invertido una gran cantidad de recursos. Podemos mencionar, por ejemplo, que según el comunicado de IBM el proyecto de Modha consiguió recientemente un apoyo de 16 millones de dólares del Departamento de Defensa de los Estados Unidos. A lo largo de su historia la inteligencia artificial ha sufrido altibajos por las grandes expectativas que ha despertado y que finalmente no se han concretado en el tiempo esperado. El nacimiento de este campo de investigación normalmente se sitúa en el año de 1956, durante una conferencia llevada a cabo en el Darmouth College en el estado de New Hampshire en los Estados Unidos. En ese tiempo había una enorme confianza entre los expertos en que en el curso de una generación los problemas que presentaba la inteligencia artificial serían resueltos y que las computadoras podrían llevar a cabo cualquier actividad propia del cerebro humano. Esta visión optimista está reflejada en la película de ciencia ficción “2001 Odisea del Espacio” dirigida por Stanley Kubrick, la cual fue estrenada en el año de 1968. En esta película, situada en 1999, la computadora “Hal” a cargo de una misión tripulada a Júpiter se rebela por propia decisión y mata a todos los tripulantes con excepción de uno. En contra del optimismo desbordado, sin embargo, llegó 1999 y la tecnología de la época estaba muy lejos de producir algo semejante a Hal, con su capacidad de razonamiento y toma de decisiones propias. De hecho, por no cumplir la inteligencia artificial con las expectativas que despertó, en la década de los setentas le fue disminuido el apoyo gubernamental para investigación, lo que volvió a ocurrir en la siguiente década.  En la actualidad, la enorme velocidad a la que está creciendo el poder de las computadoras ha posibilitado proyectos como los de simulación de las funciones cerebrales de mamíferos. En estas condiciones, la perspectiva cercana de fabricar computadoras con la capacidad del cerebro humano ha resultado un fuerte argumento para conseguir fondos para investigación. En los siguientes diez años sabremos si todos estos esfuerzos de investigación finalmente tienen éxito, con todas sus implicaciones científicas, tecnológicas, filosóficas y religiosas; o bien, si el optimismo fue una vez más demasiado grande.",
    "Brasil es el quinto país más grande del mundo, tanto en territorio como en población. Muchas cosas en Brasil son de grandes proporciones. Se localiza en su territorio la mayor parte del más grande bosque tropical del planeta –la selva amazónica–, en una extensión que es dos veces la superficie de nuestro país y que constituye el mayor pulmón del mundo. Cruza su territorio el Río Amazonas, el más caudaloso del mundo, además de ser también el más largo. Brasil cuenta con las mayores reservas de agua dulce del planeta –casi dos veces más grandes que las de Rusia, país que ocupa el segundo lugar. Es también, y con mucho, el mayor productor mundial de caña de azúcar y del biocombustible etanol obtenido a partir de la misma. En otro orden de ideas, el estadio Maracaná, situado en Río de Janeiro, albergó en 1950 a 200,000 espectadores en el partido final de la copa del mundo de fútbol entre Brasil y Uruguay, lo que constituye el mayor número de personas que hayan asistido a un estadio a presenciar un partido de fútbol. Con la excepción de los trajes de baño en las playas de Río de Janeiro y de las tazas en las que se sirve el omnipresente “cafezinho”, todo en Brasil, incluyendo el carnaval de Río, es verdaderamente de grandes proporciones. Brasil, sin embargo, y hasta hace pocas décadas, era menos conocido en el mundo que México, a pesar de la gran diferencia en tamaños territoriales entre los dos países. Con seguridad que a esto contribuyó el que nuestro país haya organizado los Juegos Olímpicos de 1968 con toda la publicidad y exposición mundial que implicaron. Esta situación, no obstante, está cambiando rápidamente y en la actualidad Brasil es un país de gran notoriedad mundial por el desarrollo económico que ha experimentado en los últimos años. En 2001, el economista Jim O´Neill del grupo Goldman Sachs acuñó el acrónimo “BRIC” como referencia a Brasil, Rusia, India y China, cuatro países que de acuerdo a O´Neill, en conjunto alcanzarán en 2039 un Producto Interno Bruto mayor que el del total de países del grupo G6 –Estados Unidos, Japón, Inglaterra, Alemania, Francia e Italia. En particular, según Goldman Sachs, el tamaño de la economía brasileña sobrepasará a la de Alemania en el año 2037. Uno de los factores que están impulsando el desarrollo económico brasileño es el petróleo. Brasil era un importador de petróleo en las décadas de los 70 y 80 y sufrió particularmente por las crisis petroleras de esos años que elevaron considerablemente el precio de petróleo –lo que lo llevó a desarrollar la industria del bioetanol como combustible para vehículos, en la que los brasileños tienen actualmente el liderazgo mundial. Desde 1980, sin embargo, Brasil ha incrementado más de diez veces su producción de petróleo y la actualidad es autosuficiente, además de ser el treceavo productor mundial. En 2006 fue descubierto el yacimiento submarino de petróleo de Tupi, en al cuenca de Santos enfrente de la costa brasileña entre Río de Janeiro y el puerto de Santos. Con reservas estimadas en 8 mil millones de barriles, este yacimiento constituyó el descubrimiento de petróleo más importante en los últimos años. El petróleo  yace bajo una capa de agua de 2 kilómetros y de 6 kilómetros de roca, arena y sal.  Se espera que el yacimiento esté en operación en 2011. Un segundo yacimiento con una capacidad similar al de Tupi fue descubierto en sus cercanías.Con el descubrimiento de los yacimientos de petróleo de la cuenca de Santos, las perspectivas económicas para Brasil lucen prometedoras y en este punto una comparación con México es inevitable. A diferencia de Brasil, México ha sido por mucho tiempo un país petrolero. En los últimos años, sin embargo, nuestra producción de petróleo ha estado declinando después de alcanzar un máximo en 2004, y a esta baja en producción le atribuye el Gobierno Federal –en forma parcial– los problemas económicos por los que atravesamos. El contraste entre México y Brasil resulta evidente. Mientras que nosotros, a pesar de haber gozado de décadas de ingresos petroleros, no fuimos capaces de descubrir y abrir nuevos yacimientos a la explotación, los brasileños, después de décadas de esfuerzos, están trabajando en la extracción de petróleo en condiciones extremadamente difíciles, empleando métodos que se sitúan en la frontera de la tecnología. Brasil en estos momentos tiene un futuro promisorio basado, entre otras cosas, en el petróleo que no tenía hace treinta años. Este futuro está simbolizado por la copa del mundo de fútbol que organizará en el año 2014, así como por los Juegos Olímpicos del año 2016 que igualmente organizará. Está también simbolizado por el tren de alta velocidad que tiene planeado construir entre las ciudades de Río de Janeiro y Sao Paulo y que espera tener listo para el la copa del mundo de fútbol de 2014. En numerosos  artículos  y publicaciones se menciona que el “gigante brasileño” está finalmente despertando, después de haber sido por mucho tiempo llamado el “país de futuro”. En el caso de México, en contraste, hay incertidumbre en el futuro económico del país. Entre otros factores, por nuestra falta de previsión en materia petrolera.",
    "Un día como hoy pero de 1859, las primeras 1250 copias de el “Origen de las especies” de Charles Darwin fueron distribuidas a los vendedores de libros. La demanda por el libro era tal que dos días después, cuando salió a la venta, estaba ya sobrevendido. Como sabemos, el Origen de las especies está basado en las observaciones que hizo Darwin durante su viaje de 5 años alrededor del Mundo a bordo del HMS Beagle. El viaje se inició en el puerto de Plymouth, Inglaterra, el 27 de diciembre de 1831 y después de tocar las costas oriental y occidental de Sudamérica, cruzó los océanos Pacífico e Índico y retornó a Inglaterra vía el Cabo de Buena Esperanza. Entre otros temas, el Origen de la especies trata de la evolución de plantas y animales y de la selección natural como el mecanismo que la explica. Es uno de los textos científicos más influyentes que se hayan publicado. Fue escrito, además, en un lenguaje sencillo y accesible a audiencias amplias. La evolución de los seres vivos no era un concepto nuevo en la época de Darwin. Al principio del siglo XIX, por ejemplo, Lamarck postuló un mecanismo evolutivo mediante el cual los organismos vivos sufren pequeños cambios para adaptarse al medio ambiente en el que viven. Según Lamarck, estos cambios adquiridos pueden transmitirse a su descendencia, produciéndose de esta manera una transformación gradual acumulativa a través del tiempo. La evolución, sin embargo, no era un hecho aceptado de manera unánime en la primera mitad del siglo XIX. El Origen de las especies contribuyó de manera decisiva a cambiar este estado de cosas. Darwin, no obstante, tuvo más éxito en persuadir a sus contemporáneos de que la evolución era real que en lograr que aceptaran a la selección natural como su causa. La selección natural está basada en el hecho que los organismos vivos tienen más descendientes que los que podrían sobrevivir. Aquellos que están mejor adaptados a su medio ambiente tendrán, entonces, mayores probabilidades de supervivencia y por lo tanto de reproducirse, heredando características ventajosas a sus descendientes. Se genera así un proceso evolutivo en el que las nuevas generaciones están en promedio cada vez mejor adaptadas a su medio ambiente, lo que eventualmente genera la aparición de una nueva especie. Darwin, sin embargo, no postuló un mecanismo específico –provisto hoy en día por la ciencia de la genética– por el cual se producen variaciones entre los distintos miembros de una generación y eso dificultó la aceptación de la selección natural como el mecanismo responsable de la evolución. En su fascinante ensayo “Darwin´s delay”, Stephen Jay Gould nos ofrece una explicación alternativa. En dicho ensayo, Gould trae a colación el  retraso de casi un cuarto de siglo entre el final del viaje del Beagle y la publicación del Origen de las especie. Gould atribuye este retraso, al menos en forma parcial, a la reluctancia de Darwin en provocar controversia con sus ideas. Mas que por sus afirmaciones sobre la evolución de las especies, Darwin temía a las reacciones que pudiera provocar el mecanismo –la  selección natural– que postulaba como su causa. Este mecanismo implica que la evolución no tiene mas fin que el de adaptar a las especies al medio en el que viven. Implica, además, que la Naturaleza misma se encarga de todo el proceso, sin la intervención de fuerza o agente sobrenatural alguno. La existencia de un dios que le dé sentido a la evolución de las especies, incluyendo la nuestra, resulta de este modo innecesaria. Es a propagar esta herejía materialista, como la llama Gould, a lo que Darwin temía por la controversia que le acarrearía. Las consecuencias materialistas implícitas en el texto de Darwin aplicadas a nuestra propia especie resultan particularmente graves: si somos a fin de cuentas producto de un proceso de selección natural, difícilmente podemos asumirnos cualitativamente diferentes de otras especies. Perderíamos así nuestro supuesto lugar de privilegio en el Universo. En realidad, hoy en día estamos en peligro real de perder este lugar por otra vía, y no filosófica. El cerebro humano, que supuestamente es la característica más distintiva de nuestra especie y la que nos hace superiores a todas las demás, según algunas opiniones podría ser superado en capacidad de procesamiento de información por máquinas computadoras en un futuro tan cercano como el año 2050. Estas máquinas cumplirían todas las funciones del cerebro humano, incluyendo las emotivas. Cuando esto suceda, no tendremos ya razón para declararnos la máxima creación en el Universo. Al mismo tiempo, el Mundo posiblemente esté celebrando el 200 aniversario de la publicación del Origen de la especies.",
    "El pasado mes de septiembre dieron inicio oficialmente los festejos por el bicentenario de la independencia de México. Como sabemos, una vez consumada ésta, nuestro país se sumió en un periodo de gran inestabilidad política que se prolongó hasta la restauración de la República en 1867, y que incluyó luchas continuas entre liberales y conservadores, así como guerras e invasiones extranjeras. Por otro lado, si bien la historia política de nuestro país en el siglo XIX está ampliamente documentada, no es igualmente conocida la suerte que tuvo la ciencia colonial en los primeros tiempos del México independiente.La ciencia en nuestro país en la segunda mitad del siglo XVIII, al final del periodo colonial, contaba con investigadores con un alto nivel científico. Entre estos destacaba, José Antonio Alzate (1737-1799), botánico, astrónomo y geógrafo, a quién se considera el más prolífico de los científicos criollos de la época. Otros investigadores mexicanos de gran vuelo de la segunda mitad del siglo XVIII –incluidos por C. Arias y C. Fernández en el libro “Historia de la Ciencia en México, Siglo XVIII”– son, José Ignacio Bartolache (1739-1790), fundador en 1772 de la primera revista médica aparecida en América; Joaquín Velásquez de León (1736-1786), quién realizó los primeros trabajos geodésicos del Valle de México, y Antonio de León y Gama (1735-1802), astrónomo y autor de la obra “Descripción ortográfica universal del eclipse de Sol del  día 24 de junio de 1778”.          El Real Seminario de Minería de la Nueva España, inaugurado el 1 de enero de 1792, tenía como misión el entrenamiento de técnicos e ingenieros metalurgistas en conocimientos de avanzada para fortalecer a la industria minera en México, entonces en declive. Como primer director de la nueva institución fue nombrado Fausto Elhuyar, químico español, descubridor del tungsteno. El Real Seminario de Minería es reconocido como la institución científica más significativa del México colonial que, además, sobrevivió a la guerra de independencia. Andrés Manuel del Río fue un destacado mineralogista español –educado en centros de investigación europeos de vanguardia, en los que tuvo contacto con investigadores de la talla de Antoine Lavoisier–, que fue enviado a México por el gobierno español para cubrir la cátedra de mineralogía del Real Seminario de Minería. Del Río arribo a México el 18 de diciembre de 1794, con todo lo necesario para montar un gabinete de mineralogía e iniciar de inmediato su encomienda.La labor de Andrés del Río en México, país que adoptó como segunda patria y en el que murió en 1849, fue extremadamente fructífera. Se dedicó a transmitir de manera entusiasta sus conocimientos sobre la ciencia de la mineralogía a los estudiantes del Real Seminario de Minería. Para este propósito, escribió el libro “Elementos de Orictognosia” y tradujo al español otros textos sobre el tema.Como investigador, analizando minerales provenientes de una mina en el Estado de Hidalgo, descubrió en 1801 un nuevo elemento químico que llamo “eritronio” y que es hoy conocido como “vanadio”. El descubrimiento del vanadio, no obstante, es comúnmente  atribuido al químico sueco Nils Sefstroem quién lo reporto en 1831, –30 años después del descubrimiento original de del Río–, dándole su nombre actual en honor a una diosa escandinava. A pesar de la polémica que se generó cuando quedó claro que del Río tenía la primicia del descubrimiento del vanadio y se propuso cambiarle de nombre, esto no prosperó.Además de haber sido un científico de primera línea, del Río era un ingeniero notable. Se le encargó, por ejemplo, diseñar una técnica para extraer el agua de una mina inundada en el mineral del Real del Monte, así como el diseño y la construcción de la primera fundición de hierro en América, proyectos que completó exitosamente. Dado que una actividad como la investigación científica requiere de condiciones muy especiales para prosperar, no es de sorprender que la ciencia mexicana, que había alcanzado un buen estado de desarrollo al final de la colonia, sufriera grandes problemas durante la guerra de independencia. En efecto, según C. Arias y C. Fernández: “Con la lucha se rompieron los nexos con instituciones extranjeras  y se hizo difícil mantener relaciones con los hombres de ciencia europea. La Universidad fue convertida en cuartel; las clases del Seminario y del Jardín Botánico se vieron afectadas por la asistencia irregular debida al servicio militar, además de que la mala situación económica no permitía gastar en proyectos de investigación”. Una vez consumada la independencia, la agitación del país no fue tampoco favorable para la continuación de la actividad científica y, por ejemplo, Fausto Elhuyar regresó a España en 1930. Andrés del Río estuvo ausente del país por algún tiempo, y aunque finalmente regresó a México, ya no tuvo logros científicos del nivel de los anteriores.En el siglo XIX se produjeron desarrollos tecnológicos basados en la ciencia, tan importantes como la máquina de combustión interna, la luz eléctrica y la telegrafía, por mencionar solamente algunos. Durante el periodo de inestabilidad política, el país perdió un tiempo precioso para seguir el desarrollo científico del mundo e integrase a la era tecnológica del siglo XX. Podemos afirmar que desde este punto de vista la guerra de independencia ocurrió a “destiempo”.",
    "El número de estudiantes inscritos en carreras profesionales en las universidades en México es uno de los parámetros que más se ha incrementado a lo largo de los últimos cien años. En efecto, tenemos que al final de la primera década del siglo pasado, menos de 10,000 estudiantes realizaban estudios universitarios en comparación de los más de dos millones que así lo hacen en la actualidad, lo que representa un incremento por un factor superior a 200. Este incremento no puede ser explicado solamente por el aumento de la población del País, que creció por un factor de 7 en el mismo periodo.De hecho, la matrícula de las universidades en México no creció en el siglo pasado de manera uniforme, sino que lo hizo por saltos. Así, tenemos que en 1950 el número de estudiantes universitarios en el País no pasaba todavía de 30,000, mientras que para 1970 este número superó los 200,000, según datos de la Asociación Nacional de Universidades e Instituciones de Educación Superior (ANUIES). Durante la década de los setenta e inicio de los ochenta la matrícula de las universidades creció de manera acelerada, superando los 900,000 estudiantes en 1984, año a partir del cual la tasa de crecimiento se desaceleró para volver a incrementarse desde el año de 1994 hasta la actualidad. De acuerdo también con datos de la ANUIES, el crecimiento de la matrícula de postgrado ha procedido también por saltos. Así, observamos que el número de estudiantes inscritos en programas de maestría en México aumentó 270 % entre 1992 y 1998, después de haberse mantenido relativamente estable desde 1980. Este comportamiento fue incluso más marcado, en el caso de la matrícula en programas doctorales, que aumentó un 460 % entre 1992 y 1998, sin haber prácticamente cambiado por más de una década.  Las universidades en nuestro País se expandieron sobre todo a partir de 1970, cuando se inició un proceso de masificación de la educación superior que se extendió hasta la mitad de la década de los ochenta. Este proceso está caracterizado por el hecho que el Gobierno Federal, soporte principal de la universidad pública en México, puso más énfasis en el crecimiento de la matrícula que en la calidad de los programas académicos apoyados.    En contraste, a partir de la década de los noventa la Secretaría de Educación Pública estableció diversos mecanismos para la evaluación y la acreditación del trabajo en las universidades, y con base en los resultados de la evaluación otorgarles apoyos específicos. Dicha evaluación fue dirigida tanto a los programas académicos como a los profesores de los mismos. Se estableció de este modo una clasificación de carreras profesionales con tres niveles de competencia, así como un organismo de acreditación por cada disciplina. A los profesores, por su lado, se les dio acceso a programas de estímulos económicos con base en su desempeño académico. A partir de la década de los noventa las universidades públicas en México han estado sometidas a una serie de presiones que responden a visiones, muchas veces contrapuestas, sobre la función que debe cumplir la universidad pública en México, y que se han reflejado en los apoyos que les ha otorgado el Gobierno Federal en los últimos años. Por un lado, se concibe a la universidad pública en términos puramente mercantilistas como una fábrica de profesionales y técnicos. Según esta visión extrema, las universidades deben ser autosuficientes económicamente y los estudiantes beneficiados con la educación que reciben deben pagar por la misma. No habría espacio de esta manera para la investigación científica y humanística –excepto para la muy escasa que apoyara la iniciativa privada– y las universidades se limitarían a trasmitir a sus estudiantes los conocimientos generados en otros lugares del mundo. En estas condiciones, más que como entes públicos las universidades funcionarían mejor como instituciones privadas.Hay, por otro lado, quienes piensan que la universidad pública debe cumplir una función social e impartir educación de calidad a todos aquellos que cumplan con ciertas exigencias académicas mínimas, sin importar si pueden o no pueden pagarla. La universidad pública, además, debe ser asiento de proyectos de investigación que tengan como propósito avanzar el conocimiento científico y desarrollar tecnologías para resolver problemas de impacto social, y para esto obligadamente se requiere del subsidio público. El grado de  desarrollo económico de un País hoy en día está en gran medida ligado a su infraestructura de investigación científica y tecnológica y de formación de científicos e ingenieros de alta calificación, y dentro de esta perspectiva la universidad pública es un elemento esencial.  Mucho se ha avanzado en México desde la década de los setenta cuando el Gobierno Federal redobló el apoyo a la universidad pública en México y propició su crecimiento. Sin embargo, como lo demuestra la actual discusión sobre las asignaciones presupuestales a las universidades públicas para el siguiente año, es claro que aún queda mucho por hacer para que estas alcancen finalmente el lugar que México requiere.",
    "Las violentas fluctuaciones en el precio del petróleo que se han dado desde la crisis petrolera del año  de 1973 –cuando se  cuadruplicó el precio del petróleo en el curso de unos cuantos meses–, han impulsado el desarrollo de diversas fuentes alternas de generación de energía. Entre estas fuentes destacan los biocombustibles, obtenidos a partir de materia orgánica de origen vegetal. El biocombustible de uso más extendido es el alcohol etílico o etanol, empleado como substituto de la gasolina. En Brasil, por ejemplo, el 50% del combustible para automóviles es etanol fabricado a partir de caña de azúcar. Los Estados Unidos, por su lado, han incrementado en los últimos años la producción de alcohol a partir del maíz, al grado que en la actualidad el 25 % de la producción de este grano se dedica a la fabricación de bioetanol. Otro biocombustible de uso extendido, –aunque no en la medida del etanol– es el biodiesel obtenido a partir de semillas oleaginosas como el girasol y la soya. La producción de biodiesel está también en aumento y en 2007 el 7% de la producción mundial de oleaginosas se dedicaba a su producción.  Los biocombustibles son una opción que en principio luce muy atractiva como fuente de energía, por ser renovable y por no enfrentar por lo tanto un futuro agotamiento como es el caso de los combustibles fósiles. Los biocombustibles fueron, ciertamente, una opción energética atractiva para Brasil en las décadas de los años setenta y ochenta, cuando este país enfrentó la  crisis petrolera. De hecho, Brasil estableció su programa de sustitución de gasolina por etanol, que tan exitoso le ha resultado, precisamente por esta crisis. Los biocombustibles son también muy atractivos por ser no contaminantes del medio ambiente –o al menos en un grado menor en que lo son  los combustibles fósiles. No todo es, sin embargo, miel sobre hojuelas en cuanto a los biocombustibles. Por ejemplo, según una publicación del Banco Mundial de julio de 2008, el incremento en la producción de etanol registrada en los últimos años fue la principal causa del aumento del precio de los alimentos que hizo crisis el año pasado. Según dicha publicación, la demanda de maíz en los Estados Unidos para la producción de alcohol ocasionó que tierras dedicadas al cultivo de soya migraran al cultivo de maíz, ocasionando una escasez de soya y un consecuente aumento en su precio. De la misma manera, respondiendo al incremento en la demanda de aceite vegetal para la producción de biodiesel, países exportadores de trigo como Argentina, Canadá y la Unión Europea disminuyeron la superficie agrícola destinada este a cereal para dedicarla al cultivo de semillas oleaginosas, lo que también llevó a un aumento en el precio del trigo.   La producción de biocombustibles para transportación está en competencia con la producción de alimentos. Esto se hace patente si tomamos en cuenta que para producir 100 litros de bioetanol –apenas lo necesario para que un automóvil recorra algunos cientos de kilómetros– son necesarios unos 250 kilogramos de maíz, es decir lo que consumiría una persona en un año.Para hacer las cosas peores en el panorama de los bioenergéticos, en un artículo publicado por Robert Service el pasado 23 de octubre en la revista Science de la Asociación Americana para el Avance de la Ciencia, se menciona la posibilidad de que una expansión masiva de la producción de maíz en los Estados Unidos para biocombustibles, demande de una cantidad tal de agua para irrigación que genere problemas de escasez del líquido en ciertos lugares. Service señala que los biocombustibles requieren para su producción de volúmenes de agua considerablemente mayores a los demandados por otros medios de generación de energía. Además, se tiene el problema de contaminación del agua por fertilizantes y pesticidas agrícolas, contaminación que llega hasta el Golfo de México vía el Río Misisipi. De este modo, sin bien los biocombustibles contribuirían a resolver el problema energético en los Estados Unidos, crearían por otro lado un problema de abasto y contaminación de agua. La experiencia brasileña, no obstante, ha mostrado que si es posible desarrollar una industria de bioetanol autosustentable. A diferencia de la industria norteamericana basada en el maíz, Brasil fabrica su etanol a partir de la caña de azúcar. Esta diferencia, de entrada es una ventaja, pues el rendimiento en la producción de etanol por hectárea de caña de azúcar es más del doble del correspondiente rendimiento para el maíz. Todas las fuentes de energía alternativas los combustibles fósiles tienen ventajas y desventajas y en este respecto los biocombustibles no son la excepción. En el futuro, sin embargo, aparecerán sin duda bioenergéticos que superarán algunos que los problemas actuales. Está en curso, por ejemplo, el desarrollo de biocombustibles fabricados a partir de aquellas partes de las plantas ricas en celulosa que no se aprovechan actualmente en la industria del bioetanol. De esta manera podrá aumentarse su producción sin incrementar ni la superficie cultivada ni el agua para irrigación. Lo que si es seguro, sin embargo, es que la sustitución de los combustibles fósiles no se hará con base en una sola tecnología, sino con tecnologías diversas basadas en el sol, el viento y los biocombustibles, entre muchas otras fuentes de energía.",
    "En días pasados nos encontramos en los medios impresos con una fotografía en la que aparece el Presidente de las islas Maldivas y su gabinete firmando documentos durante una reunión de trabajo –las Maldivas son un archipiélago de 1190 islas situado al suroeste de la India. La imagen no tendría nada de particular si no fuera porque presidente y ministros aparecen enfundados en trajes de buzo –escafandras y tanques de oxígeno incluidos–, implementos que resultaron indispensables ya que la reunión se llevó a cabo a seis metros bajo el nivel del mar.La exhibición tuvo como objetivo llamar la atención sobre el incremento paulatino en el nivel de los océanos que está ocasionando el calentamiento global. De acuerdo al Panel Intergubernamental sobre Cambio Climático (PICC), este incremento alcanzará alrededor de medio metro en el año 2100 y en esta perspectiva los habitantes de las Maldivas tienen razón de estar preocupados, pues más del 80% de su territorio tiene una elevación inferior a un metro. De este modo, de cumplirse las predicciones del PICC el país desaparecería en buena medida –en realidad podría desaparecer casi por completo de acuerdo con otros especialistas que consideran que el medio metro de incremento en el nivel del mar reportado por el PICC es demasiado conservador y que es más probable que dicho incremento alcance un metro o incluso más. En contraste con la percepción que tienen en las Maldivas y de acuerdo con una encuesta realizada por el “Pew Research Center” dada a conocer el pasado 22 de octubre, en los Estados Unidos la creencia en el calentamiento global está disminuyendo entre la población. Así, actualmente el 57% de los norteamericanos considera que hay evidencias sólidas de dicho calentamiento, en contraste con el 77 % que así pensaba hace dos años. Existen hoy en día, no obstante, muchas evidencias de que la temperatura de la Tierra está aumentando. El calentamiento global, por ejemplo, está derritiendo el hielo del Polo Norte, como lo demuestra el hecho de que en agosto de 2007 el célebre Paso de Noroeste –el canal a lo largo de la costa norte de Canadá que comunica a los océanos Atlántico y Pacífico– quedó libre de hielos por primera vez desde que se tenga memoria.   El calentamiento de la Tierra, ahora se sabe, está ligado al incremento paulatino en la concentración de bióxido de carbono en la atmósfera que ha ocurrido desde el inicio de la revolución industrial hace doscientos años –el bióxido de carbono produce el llamado “efecto invernadero”, mediante el cual la atmósfera refleja el calor que emite la Tierra evitando que se pierda en el espacio. Tomó tiempo, no obstante, llegar a esta conclusión, como lo  relata Spencer Weart en su interesante libro “El descubrimiento del calentamiento global”. De acuerdo con Weart, la conclusión de que la Tierra se está calentando como resultado de nuestra actividad industrial se originó en estudios que pretendían explicar las causas de las glaciaciones. Como sabemos, la Tierra ha sufrido a lo largo de millones de años periodos de bajas temperaturas durante los cuales grandes áreas de superficie terrestre permanecieron cubiertas de hielo. Los estudios sobre el origen de las glaciaciones se remontan al siglo XIX. En 1896 el científico sueco Svante Arrhenius se peguntó por las causas que pueden modificar la temperatura de la Tierra, llegando a la conclusión que dicha temperatura está controlada por la concentración de bióxido de carbono en la atmósfera.Arrhenius, sin embargo, no contaba con herramientas de investigación lo suficientemente sofisticadas para ir más allá de sus conclusiones –que eran en gran medida especulativas–, y hubimos de esperar hasta la segunda mitad del siglo XX para obtener pruebas convincentes, tanto de la relación entre el calentamiento de la Tierra y la concentración de bióxido de carbono en la atmósfera, como del incremento de dicha concentración como  resultado de nuestras actividades industriales.     El incremento de la temperatura global ha sido de aproximadamente medio grado centígrado desde 1975 a la fecha. Esto parecería ser muy poco y por lo tanto no constituir un motivo de alarma. En materia de clima, sin embargo, pequeños cambios producen grandes consecuencias. Por ejemplo, ya que el hielo refleja en gran medida la radiación solar, una disminución en las capas de hielo de los polos incrementará la cantidad de radiación que es absorbida por la Tierra, elevando así su temperatura; esto, a su vez, provocaría un mayor deshielo y un mayor calentamiento. Se establecería de este modo un círculo vicioso que podría ocasionar un cambio importante de temperatura.En el último medio siglo se ha producido un incremento de temperatura global, que aunque pequeño, se ha dado en un tiempo muy corto medido en una escala geológica. Este incremento, aunado a la sensibilidad del clima global a pequeñas perturbaciones, hace incierta su evolución futura que bien pudiera tener resultados catastróficos. Esto, ciertamente, es motivo de preocupación.",
    "A más de 3,600 metros de altura en el suroeste del altiplano boliviano, sobre una superficie de más de 10,000 kilometros cuadrados se extiende el Salar de Uyuni, el mayor desierto de sal del planeta. Debido a que se inunda de manera periódica y a que el agua disuelve cada vez la capa superior de sal, el Salar de Uyuni es extraordinariamente plano, teniendo variaciones de altura menores a un metro a lo largo de toda su superficie. Esta circunstancia lo ha convertido en una de las mayores atracciones turísticas de Bolivia.Se estima que el Salar de Uyuni contiene unas 10,000 millones de toneladas de sal y grandes cantidades de potasio y manganeso. Su mayor riqueza mineral, sin embargo, son los 5 millones de toneladas de litio que yacen disueltos en agua, en forma de cloruro de litio, bajo la costra superficial de sal y que constituyen la mitad de las reservas mundiales de este metal según el United Sates Geological Survey. Bolivia no es, sin embargo, el mayor productor de litio del mundo, lugar que le corresponde a Chile que cuenta con la segunda mayor reserva mundial de este mineral –Bolivia, de hecho, no ha iniciado aún la explotación de sus yacimientos. Chile contribuye con un 45 % a la producción mundial de litio que totaliza unas 27,000 toneladas por año. La producción chilena proviene del Salar de Atacama en el norte del País. Otros productores importantes son Australia, China y Argentina. Si hemos de atender a noticias recientes, México podría ingresar al club de productores de litio en fecha próxima. En efecto, el 7 de octubre pasado el diario Imagen de Zacatecas publicó una entrevista con el geólogo Martín Sutti Courtade, director de la empresa minera zacatecana Piero Sutti, en la que afirma que esta empresa descubrió grandes yacimientos de litio en los municipios potosinos de  Salinas y Villa de Ramos, y en los de Villa Hidalgo, Villa de Cos y Pánfilo Natera en Zacatecas, los cuales se contarían entre los más grandes del mundo. Aseguró también que la compañía Piero Sutti obtuvo una concesión minera para explotar yacimientos de litio en una superficie de unos 360 kilómetros cuadrados. Esta noticia fue posteriormente reproducida y ampliada por diversos medios nacionales y del extranjero. Después del hidrógeno y el helio, el litio es el tercer elemento químico más ligero que existe en la naturaleza. Los primeros, no obstante, se presentan en forma gaseosa, de modo que el litio resulta ser el elemento sólido con el menor peso específico existente. El uso más importante que se da al litio hoy en día es en la fabricación de baterías –tanto recargables como desechables– para diversas aplicaciones, desde computadoras y teléfonos celulares, hasta automóviles eléctricos. Dado que las industrias de las computadoras y los teléfonos celulares se encuentran en una fase de rápido crecimiento, la industria de las baterías recargables se está igualmente expandiendo. Otro ámbito en la que las baterías recargables de litio tienen un gran potencial de crecimiento es la industria de los automóviles eléctricos, los cuales requieren de baterías para almacenar energía eléctrica. En este respecto, la revista Forbes en su número del 24 de noviembre del pasado año llamó a Chile la “Arabia Saudita del litio”, en referencia a que los automóviles eléctricos bien pueden hacer del litio lo que los trasportes de gasolina hicieron del petróleo. Es de esperarse que dados lo problemas de contaminación ambiental que los automóviles de combustión interna están generando, los vehículos eléctricos –y las baterías recargables, en consecuencia– jugarán un papel cada vez más importante en la industria del transporte. De confirmarse los hallazgos de litio en México, nuestro País estaría entonces bien situado como productor de una materia prima que tendrá una gran demanda en el futuro, de manera similar a como hoy lo está con respecto al petróleo. Los reportes del hallazgo de litio en México, no obstante, son aún confusos. Se menciona en diversos medios, por ejemplo, que los yacimientos descubiertos son de los más grandes del mundo –incluso los más grandes– y que, según Sutti, por cada 10 metros de profundidad de excavación se podrán producir 80 millones de toneladas de litio, lo que es aproximadamente ocho veces las reservas mundiales. Todo esto en un área de 360 kilómetros cuadrados que es treinta veces menor que la del Salar de Uyuni. Un factor adicional que habría que considerar es que los costos de producción de litio más bajos se obtienen a partir de materia prima en forma líquida, como es el caso del Salar de Uyuni y del Salar de Atacama en donde Chile obtiene su producción de litio. En el caso de los yacimientos de Zacatecas-San Luís Potosí la materia prima es aparentemente sólida, lo que incrementaría probablemente los costos de producción.En las próximas semanas tendremos con seguridad un panorama más preciso sobre los hallazgos de litio en San Luís Potosí y Zacatecas. Esperemos que estos efectivamente se confirmen y México se convierta en productor de un material que con seguridad será estratégico en el futuro.",
    "El pasado 6 de octubre fueron anunciados los ganadores del Premio Nobel de Física correspondiente al presente año. El premio en esta ocasión fue otorgado a Charles Kao por su trabajo sobre transmisión de luz en fibras ópticas y a Williard Boyle y George Smith por la invención del sensor de imagen CCD –el dispositivo que sustituye al rollo de película en las cámaras digitales. El Premio Nobel de Física se otorga a investigadores que hayan realizado descubrimientos trascendentes en algún área de la física; esto, independientemente del impacto tecnológico inmediato que dicho descubrimiento haya tenido. En algunos casos, sin embargo, el premio ha sido otorgado por la realización de investigaciones que además de hacer avanzar el conocimiento científico, llevaron a desarrollos tecnológicos de gran impacto. Este es el caso, por ejemplo, de los premios Nobel de Física 1909 y 1956 otorgados, en forma respectiva, a Guglielmo Marconi y a Kart Ferdinand Braun por el desarrollo de la telegrafía inalámbrica –antecedente inmediato de la radio–, y a William Shockley, Walter Brattain y John Bardeen por la invención del transistor. Este es igualmente el caso del Premio Nobel de Física del presente año. La mención de la Royal Swedish Academy of Sciences que otorga los premios Nobel en el caso de Kao fue: “por sus revolucionarios logros relativos a la transmisión de luz en fibras para comunicación óptica”, mientras que Williard Boyle y George Smith fueron distinguidos por: “la invención de un circuito semiconductor para la formación de imágenes –el CCD”.  Cuando Alexander Graham Bell estableció en 1876 el primer enlace telefónico de la historia –en el que hizo llegar a su ayudante de laboratorio Thomas Watson el mensaje: “Señor Watson, venga aquí, necesito verlo”–, no imaginó el tremendo desarrollo que las telecomunicaciones alcanzarían un siglo después. El mensaje de Bell a su ayudante, que se encontraba en un cuarto colindante, fue trasmitido a través de un alambre de cobre por medio de pulsos eléctricos generados por un primitivo micrófono. A partir del trabajo de científicos e inventores como Graham Bell, durante la primera mitad del siglo XX la telefonía creció rápidamente. Así, en 1956 entró en operación el primer cable submarino trasatlántico con 34 canales de comunicación y en 1962 el primer satélite de comunicaciones.   Una nueva fase de desarrollo de la tecnología de las telecomunicaciones se inició en la década de los sesentas, cuando Charles Kao en 1965 publicitó la idea –que acaba de redituarle el Premio Nobel– de que la pobre transparencia de las fibras ópticas de la época, que atenuaban rápidamente la luz que viajaba a lo largo de las mismas, era debida a las impurezas que contenía el vidrio del que estaban hechas. Dicha transparencia sería entonces susceptible de aumentarse mediante la eliminación de dichas impurezas. De ser esto posible, las fibras ópticas podrían utilizarse como medio de transmisión de información –una llamada telefónica, por ejemplo– mediante pulsos de luz.La idea de enviar información por medio de la luz –la comunicación óptica– no era nueva en realidad; los indios norteamericanos, por ejemplo, usaron señales de humo –una versión primitiva de comunicación óptica– para trasmitir mensajes a kilómetros de distancia. En tiempos más recientes, Alexander Graham Bell inventó el fotófono, dispositivo que le permitía trasmitir sonidos por medio de un rayo de sol. Más allá de estas primeras aplicaciones, sin embargo, en 1965 la luz se veía como un vehículo de transmisión de información con muchas ventajas sobre la comunicación eléctrica convencional. Entre estas ventajas  se contaban una mayor capacidad de transmisión de información e inmunidad a la interferencia electromagnética. La predicción de Kao acerca  de las fibras ópticas fue acertada y hacia finales de la década de los sesentas fueron fabricadas las primeras fibras de vidrio con la suficiente transparencia para ser usadas en sistemas de comunicación óptica. Cinco años después, en 1975, entró en operación en los Estados Unidos el primer sistema comercial de comunicación óptica. Hay que hacer notar, no obstante, que para el desarrollo de la tecnología de las comunicaciones ópticas fue necesario además otro invento fundamental: el láser semiconductor, que es el generador de la luz que transporta la información a través de la fibra. Este dispositivo fue desarrollado por Zhores Alferov en 1966 y por el mismo recibió el Premio Nobel de Física en el año 2000.  A partir de 1975 la red de telecomunicaciones por fibra óptica creció de manera acelerada, tanto en extensión como velocidad de transmisión de datos, y hoy en día constituye un elemento central en el sistema de telefonía a nivel global, sobre todo en lo que se refiere a las comunicaciones de larga distancia. Las comunicaciones ópticas han sido, además, un factor fundamental para el desarrollo de la red Internet, que está produciendo una revolución social y cultural en el mundo.Las fibras ópticas para telecomunicaciones, los láseres semiconductores, los sensores CCD y la telegrafía inalámbrica, son algunos ejemplos de desarrollos que han sido distinguidos al más alto nivel científico, que han tenido un enorme impacto social, y que ilustran el papel central que la ciencia juega en la tecnología moderna.",
    "El pasado 2 de octubre nos enteramos por el periódico New York Daily News del penoso destino del cuerpo de Ted Williams –uno de los más grandes jugadores de las ligas mayores de beisbol, jugador toda su vida de los Medias Rojas de Boston, miembro del Salón de la Fama, dos veces ganador de la triple corona de bateo y el último en batear en una temporada por arriba de los .400 en porcentaje–, el cual fue entregado a su muerte en 2002 a la compañía Alcor Life Extension Foundation, de Scottdale, Arizona, para su preservación. El curso que siguió el cuerpo de Williams una vez que fue ingresado a las instalaciones de dicha compañía, según el New York Daily News, está relatado en el libro “Frozen”, escrito por Larry Johnson, un antiguo ejecutivo de Alcor. El diario tuvo acceso anticipado al libro, el cual será presentado el próximo martes en el noticiario televisivo “Nightline” de la cadena norteamericana ABC.        Alcor se especializa en mantener cadáveres en nitrógeno líquido a una temperatura de 196 grados centígrados bajo cero, con la esperanza de preservarlos sin deterioro físico hasta el tiempo en que exista la tecnología necesaria para resucitarlos y curarlos de la enfermedad que los llevó a la muerte. La compañía ofrece dos alternativas de preservación: el cuerpo completo o solamente la cabeza. La primera cuesta alrededor de 120,000 dólares, mientras que la segunda resulta más económica, pues solamente hay que desembolsar unos 50,000 dólares. Los deudos de Ted Williams se decidieron por la segunda opción.Una vez en Alcor, según Larry Johnson, la cabeza de Williams fue separada del cuerpo por técnicos sin las certificaciones médicas necesarias y después de congelarla sumergiéndola en nitrógeno líquido, fue golpeada con una llave de tuercas en un intento para separarla de la lata de atún en la que había sido colocada para suspenderla en el líquido congelante. Con los golpes se desprendieron varios pedazos de la cabeza que se regaron por el suelo. Johnson relata, además, que antes de la sumersión, a la cabeza de Ted Williams le fueron practicados varios orificios en los que se colocaron micrófonos, los cuales registraron un total de 16 “cracks” de la masa encefálica conforme su temperatura descendía hasta los 196 grados centígrados bajo cero. La preservación de cuerpos a bajas temperaturas, que recibe el nombre de Criónica, es considerada por sus defensores como una alternativa viable para detener el deterioro natural de un cadáver hasta un tiempo futuro –indeterminado en la actualidad– en el que sea posible revertir la muerte del “paciente”, la cual no consideran como definitiva sino como el inicio de un proceso que ha sido detenido con el congelamiento.  Muchos, sin embargo, colocan a la Criónica más en el campo de la ciencia ficción que en el del conocimiento científico. Apuntan, por ejemplo, que a temperaturas por abajo del punto de congelamiento del agua –no se diga a la temperatura del nitrógeno líquido– dentro de las células se forman cristales de hielo que las destruyen. Aun considerando que el agua es expulsada de interior de las células durante el proceso de enfriamiento, como se argumenta en respuesta, los cristales de hielo que se forman en su exterior destruyen estructuras, tales como vasos sanguíneos, que son esenciales para el funcionamiento de los tejidos orgánicos que de este modo quedan destruidos de manera irreversible. Así, se argumenta que la Criónica está más cerca de la religión que de la ciencia, pues resulta más una cuestión de fe que de conocimiento científico objetivo creer que en el futuro será posible resucitar cadáveres congelados, dada la poca evidencia científica con la que actualmente se cuenta.  La Criónica, sin embargo, tiene un enorme atractivo pues ofrece la perspectiva de una de vida eterna, al igual que lo hace la mayor parte de las religiones. Los egipcios de la antigüedad, por ejemplo, practicaban hace 3,500 años complejas ceremonias que duraban 70 días, en las que los cuerpos de los muertos –los que contaban con suficientes recursos, por supuesto– eran momificados y preparados para el viaje al más allá, en donde tendrían que enfrentar un juicio sobre sus actos terrenales. De ser absueltos, su “ka” y su “ba” –algo así como su fuerza vital, y su alma o personalidad–, separadas al momento de la muerte, podían reunirse nuevamente y de esta manera alcanzar la vida eterna. La preservación de cuerpo físico era, sin embargo, indispensable pues sería habitado nuevamente por el “ba” y de ahí lo elaborado del proceso de momificación.Las creencias egipcias sobre la muerte y los planteamientos de la Criónica tienen efectivamente puntos de contacto. En ambos casos se busca la inmortalidad y la preservación del cuerpo de la persona muerta. La momificación egipcia requería de la remoción de los órganos internos, incluyendo el cerebro que era extraído por la nariz. La excepción era el corazón, que no era removido pues pensaban que ahí residía el alma. En la Criónica, en contraste, el órgano esencial a preservar es el cerebro, que ahora sabemos es el lugar de residencia de lo que llamamos conciencia. Finalmente, como otro punto de contacto, si hemos de creer lo relatado por Larry Johnson, en ambas prácticas, las egipcias y las de la compañía Alcor, se maltrataba o maltrata a los cadáveres más allá de lo recomendable para su eventual resucitación futura.",
    "El 19 de octubre de 1833, doce años después de consumada la independencia de México, el Vicepresidente Valentín Gómez Farías emite un decreto mediante el cual suprime la Real y Pontificia Universidad de México, por “inútil, irreformable y perniciosa”. La Real y Pontificia Universidad, que fue fundada por Felipe II en el año de 1551 y que constituyó el principal centro de educación de la colonia, había quedado obsoleta a la luz de las ideas liberales del siglo XVIII y se encontraba bajo una intensa presión al inicio del México independiente.El decreto que suprimía la Real y Pontificia Universidad, sin embargo, no significó su extinción, pues la misma fue restablecida en el año de 1834, con el nombre de Universidad Pontificia y Nacional, una vez que el Vicepresidente Gómez Farías fue expulsado del poder. Posteriormente, la constante lucha entre liberales y conservadores llevó a que la Universidad fuera cerrada y abierta varias veces, hasta que Maximiliano la clausuró de manera definitiva el 30 de noviembre de 1865.En forma paralela a los avatares sufridos por la Universidad Pontificia el siglo XIX, poco después de consumada la independencia en los estados nacieron instituciones de estudios superiores que pretendían innovar la educación haciendo énfasis en la enseñanza de las ciencias modernas y en el estudio de las lenguas vivas. Así, por ejemplo, en 1826 se creó el Instituto de Ciencias de Jalisco, mientras que en 1827 se fundaron, el Instituto de Ciencias y Artes de Oaxaca, el Instituto Literario del Estado de México y el Colegio del Estado de Guanajuato. Los institutos de educación de los estados, sin embargo, sufrieron con la inestabilidad política del país y no prosperaron sino hasta que el país se pacificó en la segunda mitad del siglo XIX y se deslindaron de la herencia educativa de la colonia.  En San Luís Potosí, el Gobernador del Estado Vicente Chico Séin creó en 1859, hoy hace 150 años, el Instituto Científico y Literario que, sin embargo, inició operaciones sólo hasta el 23 de mayo de 1861 una vez finalizada la Guerra de Reforma. Posteriormente, el Instituto cerró sus puertas entre 1863 y 1867 a causa de la intervención francesa. La misión del Instituto era básicamente la formación profesional y en 1862 se establecieron los  requisitos para los estudios preparatorios y para las carreras de Licenciado en Jurisprudencia, Ingeniero de Minas, Ingeniero Topógrafo, Ingeniero Civil y Medicina.En 1904, según datos del director del Instituto de la época citados por María Gabriela Torres Montero en su libro “Los primeros pasos de la autonomía universitaria en San Luís Potosí, 1922-1994”, el Instituto tenía una matrícula de 150 estudiantes -148 hombres y dos mujeres-, de los cuales ese año se graduaron diez -dos abogados, seis médicos, una partera y un ensayador de metales. Para 1909 la matrícula había crecido a 257 con 19 graduados -diez abogados, un médico, un farmacéutico, cinco profesores de obstetricia, un ensayador de metales y un ingeniero topógrafo. El 10 de enero de 1923, durante el gobierno de Rafael Nieto, el Congreso del Estado  transforma el Instituto Científico y Literario  en la Universidad de San Luís Potosí, dándole autonomía de gobierno. La Universidad quedó integrada por las escuelas Preparatoria, Jurisprudencia, Medicina, Ingeniería, Química y Enfermería. Posteriormente, en 1934 el congreso estatal ratifica la autonomía de la Universidad, que cambia su nombre a Universidad Autónoma de San Luís Potosí.Otras universidades estatales en México se gestaron igualmente en el siglo XX a partir de colegios de estudios profesionales o de institutos científicos y literarios del siglo XIX. Este es el caso de la Universidad de Guanajuato, la Universidad Michoacana y la Universidad de Nuevo León, por mencionar algunas.La Universidad Autónoma de San Luís Potosí se cuenta de esta manera entre las más antiguas del País. Se puede decir incluso que es medio siglo más antigua que la Universidad Nacional Autónoma de México, fundada por iniciativa de Justo Sierra en 1910. Cabe mencionar que, en función de su carácter nacional, algunas veces el origen de la UNAM se lleva hasta la fundación de la Real y Pontificia Universidad en el siglo XVI. Esto, sin embargo, es sujeto de discusión pues transcurrieron 45 años entre el cierre definitivo de la universidad por Maximiliano en 1865 y la fundación de la UNAM en 1910 y cabe la pregunta de si se trata de la misma institución. Además, en 1895 se fundo la Pontificia Universidad de México, que puede igualmente reclamar a la Real y Pontificia Universidad como su antecesora directa.El papel que los institutos científicos y literarios estatales tuvieron en el desarrollo de la educación superior en México fue ciertamente muy importante, pues dichos institutos dieron origen a un buen número de universidades estatales. El tema, no obstante, no ha sido suficientemente explorado y requeriría de una mayor reflexión.",
    "El pasado 12 de septiembre murió a los 95 años en Dallas, Texas, Norman Borlaug, conocido como el padre de la “Revolución Verde”. Esta revolución significó un incremento espectacular en la producción de trigo en los años cincuenta en México –y posteriormente en varios países asiáticos–, el cual fue producto del desarrollo de especies de trigo mejorado llevado a cabo en nuestro País por un grupo de investigadores encabezado por Borlaug. La Revolución Verde fue considerada en su momento la solución al problema de producción de alimentos derivado de la explosión demográfica mundial.A mediados del siglo XX la población del mundo aumentaba exponencialmente duplicándose cada 35 años, en marcado contraste con el periodo de duplicación de 150 años del siglo XIX. La transición en la tasa de crecimiento demográfico, ocurrida en  la primera mitad del siglo pasado, fue producto de la mejora en las condiciones de higiene de la población y de la disponibilidad de antibióticos al término de la Segunda Guerra Mundial. En 1798 Thomas Robert Malthus predijo que la diferencia entre las velocidades de crecimiento demográfico y de producción de alimentos llevaría en algún momento a que estos últimos fueran insuficientes para sostener al creciente número de personas, lo que ocasionaría una catástrofe humanitaria de alcance global. En la segunda mitad del siglo XX, con una tasa de crecimiento poblacional sensiblemente más grande que la del siglo XIX, renacieron las preocupaciones maltusianas. La solución al problema de alimentar al mundo, se pensó, tendría que darse por dos vías: una reducción en la tasa de crecimiento demográfico y un incremento en la eficiencia de producción de alimentos.En 1943, el Gobierno de México bajo la presidencia de Manuel Ávila Camacho estableció un programa de cooperación con la Fundación Rockefeller para realizar estudios sobre patología de plantas y producción de trigo y maíz, con el fin de incrementar la producción de estos cereales en nuestro País. La Fundación Rockefeller posiblemente decidió apoyar el programa previendo el problema alimentario mundial que sería patente años después. Borlaug vino a México en 1944 como parte de dicho programa, con la misión de desarrollar variedades de trigo resistente a las plagas y con una mayor capacidad de aprovechamiento de fertilizantes, lo que redundaría en un mayor rendimiento por hectárea. Obtuvo variedades de plantas enanas que, sin embargo, producían más trigo que sus contrapartes más altas. El éxito de estos estudios fue tan grande que México pasó de tener un déficit de producción de trigo en 1944 a ser un exportador neto en 1963, incrementando la producción de este cereal por un factor de seis.      Las semillas de trigo desarrolladas en México por Borlaug fueron exportadas a la India y Pakistán, en donde tuvieron igualmente un gran éxito. Entre 1965 y 1970 ambos países casi doblaron su producción de trigo y alcanzaron la autosuficiencia, Pakistán en 1968 y la India en 1974. Las técnicas desarrolladas por Borlaug fueron también extendidas a otros cereales como el arroz, con resultados igualmente positivos. Por su trabajo en México y en otros países, a Norman Borlaug le fue otorgado el Premio Nobel de la Paz en 1970. Hablando de la contribución de Norman Borlaug a la paz del mundo, la presidenta del comité Nobel mencionó en su discurso de presentación del premiado: “Durante los 25 años que han pasado desde el fin de la guerra, aquellos de nosotros que vivimos en sociedades industrializadas hemos debatido casi en pánico acerca de la carrera entre la explosión demográfica mundial y la comida de que dispone el mundo. La mayor parte de los expertos que han expresado una opinión sobre esta carrera han sido pesimistas. El mundo ha oscilado entre el miedo a dos catástrofes –la explosión de población y la bomba atómica. Ambas son una amenaza mortal. En esta situación intolerable, con la amenaza del fin del mundo sobre nuestras cabezas, el Dr. Borlaug aparece en escena y corta el nudo gordiano. Nos ha dado una esperanza fundada, una alternativa de paz y de vida –la Revolución Verde.” Borlaug, no obstante, tiene sus críticos, quienes aducen, por ejemplo, que las técnicas desarrolladas por él dependen de un uso intensivo de fertilizantes y pesticidas, y que esto inevitablemente tiene un impacto ambiental. Además, sostienen que se desplaza a los pequeños agricultores y se crea una dependencia con respecto a las grandes empresas agroindustriales, que resultan al final ser las grandes ganadoras. Por su lado, Borlaug argumentaba que las críticas las hacían personas instaladas en cómodas oficinas en Washington o Bruselas, y que si alguno de ellos viviera un mes en medio de la miseria de un país subdesarrollado, no dudaría en apoyar sus técnicas agrícolas.     Al margen de la controversia que su trabajo ha generado, lo que es incontrovertible es que Borlaug fue un investigador admirable que decidió venir a México en lugar de aceptar el trabajo que le ofrecía una compañía transnacional en los Estados Unidos, y que hubiera resultado mucho más cómodo. Además, alcanzar un éxito de investigación y de impacto social como el que tuvo, en un país como era el México de los años cuarenta, no es algo que podamos encontrar a la vuelta de la esquina.",
    "Uno de los mayores desafíos que encara el mundo hoy en día es el relativo al incremento en la producción de alimentos para una población creciente. De las tierras que en nuestro planeta son consideradas arables, aproximadamente la mitad están cultivadas, lo que en principio daría margen para aumentar la producción agrícola. Al abrir más tierras al cultivo, sin embargo, se generan efectos negativos en el medio ambiente por la deforestación que conlleva. Además, los eventos climáticos extremos -sequías e inundaciones- que estamos padeciendo en los últimos años, y que son producto de la alteración del medio ambiente por actividades humanas, impactan negativamente a la producción de alimentos. Dadas estas circunstancias, algunos expertos piensan que la agricultura tal como la conocemos tendrá que cambiar en el futuro.Se estima que 12,000 años atrás, al final del periodo paleolítico y antes de la invención de la agricultura, había unos cinco millones de personas en el mundo que subsistían de la caza y la recolección. En ese tiempo los grupos humanos eran nómadas, pues el alimento que necesitaban para subsistir se encontraba disperso en grandes extensiones de terreno lo que no propiciaba su permanencia en un mismo lugar por periodos prolongados. La agricultura puso a disposición de nuestros ancestros alimentos en cantidades mayores a las que podían obtener en forma silvestre, y esto llevó a un incremento gradual de la población mundial. La agricultura, además, permitió cultivar alimentos en áreas relativamente pequeñas, dando origen a asentamientos humanos permanentes y eventualmente al nacimiento de las primeras ciudades. Después de la invención de la agricultura, la población del mundo creció de manera continua, llegando a unos 300 millones al inicio de nuestra era y a 1,600 millones en el año 1900. En la actualidad somos poco menos de 7,000 millones y si dividimos la cantidad de tierra arable de que disponemos entre la población mundial, encontramos que a cada uno de los habitantes de la Tierra nos corresponde aproximadamente media hectárea. Así, parece ser que la actividad de la que hemos dependido por miles de años para nuestra subsistencia tendrá efectivamente que cambiar y aumentar su eficiencia. Dickson Despommier, quién es profesor de la Universidad de Columbia en la ciudad de Nueva York, tiene una propuesta sorprendente para la agricultura del futuro: cultivar la comida que necesitemos en edificios de gran altura en medio de los centros urbanos. En su propuesta, Despommier imagina estructuras verticales con invernaderos apilados en niveles múltiples, en los que se cultivarán alimentos empleando técnicas de invernadero. Crecer vegetales en espacios cerrados ha sido, por supuesto, practicado por muchos años. Lo que es novedoso en la propuesta de Despommier es la perspectiva de multiplicar la producción de alimentos en una superficie dada apilando invernaderos en estructuras verticales, y además hacerlo dentro de las ciudades.  Según Despommier, una de las virtudes de su propuesta es la de suprimir el transporte de alimentos desde los lugares de producción a los centros urbanos, con el consecuente ahorro de combustible y reducción de la contaminación atmosférica. En no pocos casos los alimentos se consumen hoy en día a cientos o miles de kilómetros de donde son producidos, en una suerte de proceso inverso al que practicaban los grupos nómadas del periodo paleolítico; es decir, los alimentos viajan ahora hacia los consumidores en lugar de que éstos lo hagan hacia los primeros. Con la propuesta de Despommier regresaríamos a la época en la que los alimentos se producían en la vecindad de los centros urbanos. Las granjas verticales producirían todo el año y no tendrían afectaciones por sequías o inundaciones. Además, empleando técnicas aeropónicas se lograría un ahorro considerable de agua en comparación con la agricultura tradicional. Por medio de estas técnicas, que no requieren de tierra, los nutrientes que necesitan las plantas para su crecimiento son proporcionados a través de un flujo de agua atomizada. Despommier también ha considerado el aspecto estético y escribe: ”Las estructuras por si mismas deberían tener gracia y belleza. Con el objeto de permitir que las plantas capturen la luz del Sol, las paredes y los techos deben ser completamente transparentes. De este modo, a la distancia, parecería que fueran jardines suspendidos en el espacio”, en lo que constituye una especie de reedición de los jardines colgantes de Babilonia en gran escala.Existen, no obstante, críticos a la idea de las granjas verticales que consideran, por ejemplo, que en una estructura de invernaderos apilados habrá áreas en los niveles inferiores a las que no llegue la luz solar y tendrán que ser iluminadas artificialmente. Esto implicaría enormes costos de energía eléctrica que harían inviable la idea completa. Aunque es todavía muy pronto para saber si la propuesta de Despommier se hará realidad algún día, lo que si parece seguro es que las prácticas agrícolas actuales tendrán que adecuarse a la capacidad de nuestro planeta para alojar a un número creciente de habitantes, la cual aparenta estar llegando a un límite.",
    "“¿Pero acaso la mujer está condenada a no recibir educación filosófica y científica? ¿Por qué no recibirla si la naturaleza que se complació en hacerla su obra de arte más perfecta, la dotó también de inteligencia? Eminentes pensadores han creído que más débil físicamente que el hombre, y teniendo en la naturaleza una función que la absorbe toda, la mujer sufriría por la instrucción los efectos del surmenage intelectual más fácilmente que aquel, lo cual traería a la larga la esterilidad, la degeneración de la raza y su extinción.”El anterior párrafo fue tomado de la transcripción del discurso pronunciado por el Dr. Antonio F. López –descubierta por la Lic. Lucía Delgado Oviedo en los archivos bibliotecarios de la UASLP– durante una ceremonia de premiación de alumnos llevada a cabo en el año de 1901 en el Instituto Científico y Literario de San Luís Potosí, en la que estuvo presente el Gobernador del Estado. El Dr. Antonio López, que fue Director de dicho Instituto de 1901 a 1907, no dejó duda de lo que pensaba al respecto cuando párrafos más adelante afirmó: “De todo esto se deduce que a la mujer se le dará la instrucción lo más basta posible proporcionalmente a su resistencia cerebral, para no llegar al surmenage”.  De una lectura completa del discurso de referencia queda claro que el Dr. Antonio López era una persona culta, versado en la filosofía positivista en boga en México en aquel tiempo. Como tal, estaba convencido que la Ciencia era la llave para resolver todos nuestros problemas. Refiriéndose a esta última, reconoce que el primer paso del método científico consiste en: “observar los hechos con precisión despojándose de todo prejuicio”. No obstante, al igual que Aristóteles quién afirmaba que las mujeres tienen menos dientes que los hombres, de manera prejuiciosa concluyó que la relativa debilidad muscular de las mujeres en comparación con la de los hombres se extendía al cerebro.  Afortunadamente, no todas las mujeres estuvieron de acuerdo con este tipo de opiniones. Un ejemplo prototípico al respecto es Marie Curie, quién recibió el premio Nobel de Física en el año de 1903 –más o menos en la época del discurso referido – y el premio Nobel de Química ocho años más tarde, convirtiéndose en la primera persona –hombre o mujer– en recibir dos veces un premio Nobel.   Otro ejemplo a destacar, ocurrido medio siglo después y que tiene el ingrediente adicional de haber sido repetidamente puesto en el contexto de la discriminación de género, es el de la química británica Rosalind Franklin, quién tuvo una participación decisiva en el revelación del “secreto de la vida”, es decir en el descubrimiento de la estructura molecular del ADN, que ha sido considerado como el más importante del siglo XX. En el año de 1953, Rosalind Franklin era investigadora del King´s College de la Universidad de Londres y estaba dedicada al estudio del ADN empleando rayos x. El King´s College en esa época era una institución conservadora que practicaba la discriminación de género –tenía, por ejemplo, un comedor exclusivo para hombres– y con la cual Franklin no se identificó. Además, casi a su llegada al laboratorio en 1951 entró en conflicto con Maurice Wilkins, director asistente del mismo, quién aparentemente malinterpretó su contratación asumiendo que venia a trabajar como asistente suyo. Al inicio de la década de los cincuenta varios laboratorios de investigación en el mundo competían por ser los primeros en revelar la estructura de la molécula de ADN, entre ellos el de King´s College. Los ganadores fueron Francis Crick y James Watson de la Universidad de Cambridge, a los cuales, juntamente con Wilkins, les fue otorgado el premio Nobel de Medicina y Fisiología en 1962. Franklin no fue incluida pues había muerto en 1958 a los 37 años, víctima de cáncer de ovario, y por reglamento el premio Nobel no puede ser otorgado de manera póstuma.Watson y Crick, sin embargo, han sido sujetos a muchas críticas debido a que la inspiración para su descubrimiento les llegó después de observar la fotografía de un experimento de rayos x de ADN obtenido por Franklin. Esto en si no tiene nada de incorrecto. El problema fue, no obstante, que la fotografía les fue mostrada por Wilkins sin el conocimiento de Franklin. Watson y Crack, además, no reconocieron este hecho sino hasta años después. La temprana muerte de Rosalind Franklin y las circunstancias que rodearon al descubrimiento de la estructura del ADN, la han convertido en un ícono de los grupos feministas. Esto fue, además, fomentado por el libro “La doble hélice” publicado” por James Watson en 1968, cuando Rosalind ya había muerto, en donde se refiere a ella como “Rosy” –un sobrenombre que no tuvo en vida– y como una investigadora que no sabía  interpretar sus propios experimentos.    Quizá nos parezcan sorprendentes los conceptos referidos al inicio de este artículo. Debemos, sin embargo, juzgarlos a la luz de la discriminación de género prevaleciente hace cien años y de la que encontramos restos en el caso de Rosalind Franklin y en el hecho de que una universidad como el King´s College tuviera comedores exclusivos para hombres hace apenas medio siglo. Aún hoy está discriminación persiste, con lo que el mundo se priva de la mitad de la inteligencia humana.",
    "En el mes de octubre de 1973 los países árabes productores de petróleo decretaron un embargo petrolero a los Estados Unidos por su apoyo a Israel en la guerra del Yom Kippur. Este embargo provocó una crisis energética que cuadruplicó el precio del petróleo en unos cuantos meses y disparó señales de alarma en los países industrializados, que repentinamente dejaron de tener asegurado el flujo de petróleo del Golfo Pérsico que necesitaban para mantener funcionando sus economías. La situación fue alarmante no solamente para países no productores de petróleo, sino incluso para los Estados Unidos, que no obstante su gran nivel de producción  de crudo, dependía del petróleo árabe en buena medida. Como respuesta a la crisis energética, los gobiernos de países como los Estados Unidos, Alemania, Francia y Japón buscaron mitigar su dependencia con respecto al petróleo impulsando el desarrollo de fuentes alternas de energía. Así, por ejemplo, Francia privilegió la energía nuclear e incrementó por un factor de diez su capacidad nucleoelectrica, al grado que actualmente alrededor del 80 % de la electricidad consumida en ese país es de origen nuclear. Japón y Alemania también incrementaron substancialmente su capacidad nucleoeléctrica –aunque no al grado en el que lo hizo Francia– y en la actualidad alrededor del 30% de la energía eléctrica en esos países proviene de centrales nucleares. Aunque la energía nuclear ha reducido la dependencia energética con respecto al petróleo del Medio Oriente, la producción del uranio necesario para las plantas nucleares está concentrada, al igual que el petróleo, en algunos pocos países, notablemente Canadá y Australia. Además, el combustible nuclear es no renovable y eventualmente escaseará y aumentará de precio. Como medio de lograr una independencia energética, la energía nuclear no parece ser entonces una opción viable al largo plazo. En contraste con el petróleo y el uranio, algunas fuentes de energía renovable, –como el sol y el viento, por ejemplo– están más “democráticamente” distribuidas y constituyen una opción energética altamente atractiva. Algunos países han tomado muy en serio esta opción, notablemente Alemania y España, que generan hoy en día a partir de fuentes renovables del orden de un 10 % de la electricidad que consumen. En el caso de Alemania se prevé que en el año 2020 un 15 % de la electricidad generada provenga del viento y se habla incluso de la posibilidad de que para el año 2050 el país obtenga su energía totalmente de fuentes renovables. España, por su lado, está incrementando el uso de energías renovables a una velocidad todavía más grande que la de Alemania. Aunque México sigue siendo un país petrolero de primer orden, después de alcanzar una producción máxima de poco menos de cuatro millones de barriles diarios en el año de 2004 nuestra producción de crudo ha declinando de manera constante y, peor aun, las reservas probadas apenas nos alcanzan para algunos 10 años a los niveles actuales de extracción. Pareciera ser entonces que podríamos perder nuestra condición de país petrolero en el futuro. Por su posición geográfica y por su orografía, por otro lado, México cuenta con grandes recursos solares. Esto es sobre todo cierto en el norte del país. En particular, la región del altiplano de San Luís Potosí presenta condiciones de insolación solar por arriba del promedio nacional. Cada día inciden en el altiplano potosino por cada metro cuadrado de superficie aproximadamente 6 kilowatts-hora de energía. Esto es equivalente a, por ejemplo, la energía necesaria para mantener encendida  día y noche una bomba de agua pequeña. En algunas regiones en el límite de nuestro estado con Zacatecas la radiación solar es incluso más alta.  La explotación de los recursos solares del país, sin embargo, requiere del desarrollo o la adaptación de tecnologías solares para lo que son necesarios ingenieros e investigadores que no estamos produciendo en número suficiente. Los expertos solares que necesitamos deben ser formados en nuestras universidades y éstas por lo general no cuentan con los recursos suficientes. En el contexto actual de crisis económica –debida en buena medida a la baja en las divisas que el país recibe por la venta de petróleo–, las universidades públicas incluso enfrentan reducciones presupuestarias. En materia de investigación y estudios de postgrado, el porcentaje del producto interno bruto que dedica el país a la ciencia y a la tecnología es inferior al 0.4 por ciento que, como reconoció el director del CONACYT en su reciente visita a San Luís Potosí, es incluso inferior al promedio latinoamericano. Más tarde o más temprano el petróleo de México llegará a su fin. Cuando eso ocurra, y si no tomamos ahora providencias para sustituirlo como fuente de energía, nos lamentaremos no solamente por las divisas que no recibiremos por su exportación, sino por las que necesitaremos para importar la energía que no habremos de producir. Países sin petróleo como Alemania y España nos están mostrando el camino. Por los recortes anunciados en educación y apoyo a la ciencia y la tecnología, no pareciera, sin embargo, que estemos en la ruta correcta.",
    "A partir del inicio de la revolución industrial a finales del siglo XVIII, el mundo ha utilizado cantidades crecientes de energía que ha obtenido fundamentalmente de la quema de combustibles fósiles. En la actualidad, el 80 % de la energía que consumimos proviene de dichos combustibles. Los combustibles fósiles –petróleo, gas y carbón– tienen su origen en materia orgánica vegetal y animal muerta hace mucho tiempo. Se cree que el carbón combustible proviene de árboles y arbustos que existieron en el periodo carbonífero hace unos 300 millones de años, los cuales fueron sepultados por procesos geológicos en rocas sedimentarias. El gas y el petróleo se acepta que se generaron a partir de algas y animales marinos del periodo jurásico, que igualmente fueron sepultados por sedimentos en el fondo del mar. Las altas presiones y temperaturas a las que fue sometida la materia orgánica bajo la superficie de la tierra, a lo largo de decenas o centenas de millones de años, hicieron su trabajo y generaron los combustibles fósiles que hoy conocemos y usamos.Al extraer estos combustibles llevamos a la superficie terrestre compuestos químicos que de otra manera hubieran permanecido aislados. Peor aun, al quemarlos generamos residuos –notablemente dióxido de carbono– que están cambiando lentamente la composición química de la atmósfera dando lugar al fenómeno del calentamiento global. Durante la combustión del carbón, el petróleo o el gas, el carbono que contienen se combina con el oxígeno del aire, generándose dióxido de carbono que se emite a la atmósfera. Remontándonos unos 300 millones de años al periodo carbonífero, las plantas que posteriormente dieron origen al carbón crecieron mediante la fotosíntesis. Por medio de este proceso, absorbieron dióxido de carbono de la atmósfera y lo convirtieron en tejido vegetal. Los animales que fueron la fuente del petróleo y el gas fósil no pudieron por ellos mismos fijar dióxido de carbono de la atmósfera. En último término, sin embargo, el carbono de sus tejidos orgánicos provino también de la atmósfera a través de las plantas. De este modo, al quemar combustibles fósiles estamos regresando a la atmósfera el dióxido de carbono que fue extraído de la misma mediante la fotosíntesis, lo que nos llevaría a  pensar que no constituye un problema. Los combustibles fósiles, sin embargo, están liberando un dióxido de carbono que fue fijado por las plantas en épocas muy remotas y por lo tanto en la actualidad hay un exceso de emisión de este gas y un consecuente desequilibrio atmosférico.   No obstante los problemas ambientales, y aunque las fuentes de energía renovables están adquiriendo una importancia creciente, se anticipa que el uso de combustibles fósiles se incrementará en el mediano plazo. Dada esta situación, se ha desarrollado interés en las tecnologías de “secuestro de carbono”, mediante las cuales el dióxido de carbono producido, por ejemplo por una planta carboeléctrica –que genera energía a partir del carbón fósil–, es capturado y aislado de la atmósfera en lugar de emitirlo a la misma. Los árboles y las plantas en general nos proporcionan el medio natural para capturar y almacenar dióxido de carbono, por medio del proceso de fotosíntesis. El incremento de la superficie terrestre cubierta por bosques es entonces un medio para promover el secuestro de carbono. La captura y almacenaje de carbono también puede hacerse por medios artificiales. Por ejemplo, la planta carboeléctrica “Schwarze Pumpe” en el este de Alemania tiene operando desde 2008 una pequeña planta piloto de producción de vapor, que captura y almacena en tanques el dióxido de carbono que produce, el cual es vendido posteriormente a diferentes compañías –a fabricantes de bebidas gaseosas, por ejemplo. Las plantas carboeléctricas contribuyen con un 40% de la producción mundial de energía eléctrica. Las técnicas de captura y almacenaje de carbón son entonces especialmente importantes para este tipo de plantas. Se piensa que el dióxido de carbono puede almacenarse en grandes cantidades  en depósitos de roca porosa bajo tierra. Puede también inyectarse en campos petroleros que han bajado su producción por una baja de presión. Los proyectos de captura y almacenaje de carbón, sin embargo, han sido criticados por organizaciones ambientalistas, que consideran que contribuyen a perpetuar las plantas carboeléctricas y a inhibir el desarrollo de las energías renovables. En todo caso, uno de los grandes obstáculos que actualmente encuentran dichos proyectos en plantas carboeléctricas son los altos costos para capturar el dióxido de carbono emitido, lo que incrementa substancialmente el costo de la energía eléctrica producida.   No existe una fuente de energía que nos proporcione una solución mágica al problema del calentamiento global. En el futuro la energía que consumirá el mundo provendrá de fuentes renovables –ciertamente en un porcentaje mayor al actual– pero también de los combustibles fósiles, en particular del carbón, del cual países como los Estados Unidos, Rusia, China y la India tienen considerables reservas. La mitigación de los efectos ambientales por la quema del carbón fósil, incluyendo las tecnologías de secuestro de carbono, jugarán entonces un papel fundamental en el mediano plazo.",
    "La noche del 18 de marzo de 1987, durante el congreso de la Sociedad Americana de Física que tuvo lugar en las instalaciones de un hotel en la ciudad de Nueva York, se llevó a cabo una sesión técnica por demás inusual en este tipo de eventos. Dicha sesión inició a las 19:30 –con un considerable retraso pues el número de personas en la sala de conferencias era mayor a la permitida por los reglamentos y hubo que esperar a desalojar a algunas de ellas– y terminó después las tres de la mañana. Fue seguida por cerca de tres mil participantes, en su mayoría a través de monitores de televisión dispersos  a lo largo de los pasillos del hotel. La razón de tanto interés fue que en dicha sesión se discutirían los últimos resultados de investigación obtenidos en relación a una clase muy especial de materiales conocidos como superconductores. Los superconductores son materiales asombrosos, que pueden alcanzar una resistencia eléctrica igual a cero bajo ciertas condiciones. Todos los materiales, con la excepción de los superconductores, se oponen al paso de corriente eléctrica a través de ellos; algunos de manera férrea, como es el caso del vidrio o el plástico, y otros en forma tímida como sucede con los metales. Los materiales superconductores, en cambio, no se oponen –o lo hacen en grado ínfimo– al paso de la electricidad. Para que esto suceda, sin embargo, deben ser enfriados a temperaturas muy bajas. El plomo, por ejemplo, se convierte en superconductor a una temperatura de -266 grados centígrados. Estas bajas temperaturas se obtienen empleando helio líquido como refrigerante, a gran costo y dificultad de manejo. Los materiales superconductores fueron descubiertos en 1911 por el físico holandés Heike Kamerlingh Onnes en la Universidad de Leiden. Desde ese año se han descubierto un buen número de materiales superconductores. Hasta 1986, sin embargo, todos requerían de muy bajas temperaturas para actuar como tales. Los superconductores prometen un gran número de aplicaciones, algunas de las cuales son ya una realidad. Estos materiales, por ejemplo, se utilizan para la generación de campos magnéticos de gran intensidad en los sistemas de diagnóstico médico por resonancia magnética. Muchas de las aplicaciones de la superconductividad son, sin embargo,   económicamente inviables en la actualidad por las bajas temperaturas requeridas. Una de las aplicaciones potenciales más importantes de los superconductores se encuentra en el campo de los trenes de alta velocidad de levitación magnética, o “Maglev” como son universalmente conocidos. Este tipo de trenes avanzan suspendidos por fuerzas magnéticas sobre un riel de guía. Las fuerzas magnéticas son generadas por electroimanes de gran potencia que requieren de corrientes eléctricas extremadamente elevadas. Los superconductores, que no se oponen al flujo de electricidad, son entonces ideales para esta aplicación. Para la fabricación de los electroimanes superconductores es posible usar alambres de una aleación de niobio y estaño. Desgraciadamente, el helio líquido necesario para que esta aleación se convierta en superconductora ha sido un factor para que la superconductividad no haya tenido un mayor impacto en el transporte de alta velocidad.   En septiembre de 1986, los físicos Georg Bednorz y Karl Muller trabajando en los laboratorios de la compañía IBM en Zurich, Suiza, reportaron el descubrimiento de una nueva clase de superconductores con una temperatura de operación de -238 grados centígrados, que fue incrementada poco tiempo después por un grupo de investigadores de las universidades de Houston y Alabama hasta -183 grados centígrados. Aunque esta última temperatura es todavía extremadamente baja, representa un gran avance pues es mayor que la temperatura del nitrógeno líquido –el mismo que emplean los dermatólogos para eliminar lunares de la piel–, que de este modo puede sustituir ventajosamente al helio líquido como refrigerante, a un costo y dificultad de manejo mucho menores.El descubrimiento de superconductores a temperaturas más altas que las del nitrógeno líquido –los llamados superconductores de alta temperatura– expandió repentinamente el universo de aplicaciones potenciales de la superconductividad, lo que en buena medida explica la excitación de la comunidad de físicos en la reunión de marzo de 1987 en Nueva York. Desgraciadamente y aunque en la actualidad se han obtenido superconductores a temperaturas hasta de -135 grados centígrados, su tecnología metalúrgica –para la fabricación de alambres, por ejemplo– se ha topado con numerosas dificultades y no ha progresado al mismo ritmo. El resultado ha sido que, pese a todo el ruido que provocaron hace 22 años, lo superconductores de alta temperatura no han estado a la altura de las expectativas.Recientemente, un grupo de investigadores japoneses del Instituto de Tecnología de Tokio reportó un nuevo tipo de superconductores de alta temperatura, del que hay esperanzas sean más amigables desde el punto de vista tecnológico. De este modo, se han reavivado las expectativas de que a superconductividad irrumpa algún día de manera masiva en el campo tecnológico. La respuesta nos la darán lo años por venir.",
    "Como es ahora evidente, el incremento en el consumo de energía per cápita que se dio en los países desarrollados a partir del inicio de la Revolución Industrial, un par de siglos atrás, ha provocado serios problemas ecológicos a nivel global. En particular, la emisión de bióxido de carbono a la atmósfera por el uso indiscriminado de combustibles fósiles ha llevado al problema -ahora bien conocido- del calentamiento global, que anticipa desastres ecológicos y climáticos en los años por venir. Hoy en día, el 80% de la energía que se consume a nivel mundial se obtiene a partir de combustibles fósiles. En consecuencia, para paliar el problema del cambio climático es necesario reducir el consumo de energía per cápita y al mismo tiempo buscar sustituir a los combustibles fósiles por opciones que no emitan bióxido de carbono a la atmósfera.Una forma de energía que es particularmente útil es la eléctrica, que se puede trasmitir por grandes distancias desde la central generadora a los centros de consumo -fábricas, edificios públicos, casas habitación, etc.-. Si consideramos solamente la energía eléctrica, encontramos que su generación a nivel mundial proviene en un 66% de los combustibles fósiles, mientras que el restante 34% se divide entre varias fuentes que no producen bióxido de carbono.Dos de estas fuentes son la energía hidroeléctrica y la energía nuclear, que han sido usadas ya por décadas y que de hecho son las más importantes después de los combustibles fósiles: la energía hidroeléctrica contribuye con un 17% del total mundial, mientras que la energía nuclear lo hace con un 15%.  El restante 2% se divide entre varias tecnologías, algunas de ellas, como la eólica y la solar, en rápido crecimiento. Notamos que con la excepción de la energía nuclear, las restantes energías libres de carbono son renovables, en contraste con los combustibles fósiles o nucleares que eventualmente se agotarán.Tres de las energías renovables de más potencial presente y futuro son la hidroeléctrica, la eólica y la solar. La central hidroeléctrica más grande del mundo, después de la de Itaipú en la frontera entre Brasil y Paraguay, es la de “Tres gargantas” sobre el río Yangtse en la zona central de China, que generará cuando se termine totalmente en 2011 una potencia eléctrica de 18,000 mega watts -esto es aproximadamente el 40% de la capacidad total de generación de electricidad de México-. La represa de Tres gargantas inundará una superficie de 1,000 kilómetros cuadrados que se extenderá cientos de kilómetros río arriba, lo que ha motivado que el proyecto haya sido criticado por su impacto ambiental. Además, fue necesario relocalizar a 1.3 millones de personas que habitaban en las regiones inundadas. Tres gargantas tendrá un costo total de 24,000 millones de dólares.       El viento es una de las fuentes de energía renovable de más rápido crecimiento. La “granja” de energía eólica más grande del mundo es la de Horse Hollow, a 160 kilómetros de Dallas, Texas. Dicha granja está compuesta de 421 molinos de viento que en conjunto generan 735 mega watts de energía eléctrica. Esta potencia es aproximadamente equivalente a la de la planta termoeléctrica de Villa de Reyes, San Luís Potosí. La planta de Horse Hollow ocupa un área de 190 kilómetros cuadrados, que en números redondos es una y media veces el área que encierra el anillo periférico que rodea a San Luís Potosí.En las centrales hidroeléctricas y eólicas la potencia eléctrica es producida en generadores movidos por turbinas, que a su vez son accionadas por la fuerza del agua o del viento, según sea el caso. En contraste, un sistema de generación de energía eléctrica por celdas solares es más simple, en virtud de que la energía eléctrica se obtiene directamente del Sol, sin ningún paso intermedio. Aunque pudiera uno pensar que esta característica le da una ventaja decisiva sobre otras fuentes de energía renovable, no ocurre así por el alto costo de fabricación de celdas solares. No obstante, y debido precisamente a la simplicidad del esquema de generación de energía empleando celdas solares, éstas están teniendo una importancia creciente en el mercado energético. La granja de celdas solares más grande en la actualidad es la de Olmedilla  de Alarcón en la provincia de Cuenca, España, con una potencia máxima de 60  mega watts  -aproximadamente un décimo de la central de Villa de Reyes- generada por medio de 270,000 paneles de celdas solares. La planta tuvo un costo de 530 millones de dólares y ocupa un área de poco menos de 1.5 kilómetros cuadrados.Todas las tecnologías de generación de energía renovable disponibles en estos momentos tienen ventajas y desventajas y ninguna nos dará una solución mágica al problema de sustitución de combustibles fósiles en un futuro inmediato. La generación masiva de energía a partir del Sol, el viento o las caídas de agua, requiere de grandes extensiones de terreno -y en este respecto la energía eólica es la  más demandante-, lo que necesariamente tiene un impacto social y/o ecológico negativo. No obstante, y aunque en un futuro  inmediato no veremos una sustitución total de los combustibles fósiles por energías renovables, por imperativos ecológicos éstas tomarán un papel cada vez más relevante en los años por venir.",
    "La madrugada del 6 de agosto de 1945 -el próximo jueves se cumplirán 64 años- el bombardero B29 bautizado “Enola Gay” por su capitán Paul Tibbets en honor a su madre, despegó de la pequeña isla de Tinian en el archipiélago de las Marianas, 2,500 kilómetros al sureste del Japón. Enfiló rumbo a la ciudad de Hiroshima en la isla de Honshu, que en esos momentos tenía una población de alrededor de 300,000 habitantes. La misión de Tibbets: hacer explotar sobre Hiroshima la primera bomba nuclear de la historia en ser usada como arma de ataque. Una vez sobre la ciudad, a una altura de 10,000 metros, el Enola Gay arrojó la bomba y viró bruscamente para alejarse lo más rápido posible de los efectos de la explosión. A 600 metros del suelo -así fue planeado para que causara el mayor daño posible-, la bomba explotó con una fuerza equivalente a 15,000 toneladas de TNT. Como resultado, murieron de 80,000 a 140,000 personas y 100,000 resultaron heridas. Más de dos tercios de lo edificios de la ciudad fueron demolidos y todo, en un radio de 1.5 kilómetros, fue incinerado. Las muertes se produjeron fundamentalmente por tres causas: 1) por la onda de calor de la explosión, que alcanzó temperaturas de varios miles de grados centígrados al llegar a la ciudad y que incineró a todos aquellos que encontró a su paso; 2) por los muchos fuegos que la onda de calor encendió y que fueron dispersados por la onda expansiva que le siguió, derivando en un incendio generalizado que devoró todo en un área de 7 kilómetros cuadrados, y 3) por efectos de la radiación de alta energía producto de la reacción nuclear. Mientras que las dos primeras causas de muerte ocurren también con bombas convencionales -si bien en Hiroshima se presentaron en una escala nunca antes vista-, la muerte por radiación fue un fenómeno nuevo, que incluso no fue notado durante las primeras horas inmediatamente después de la explosión.  La fuerza liberada en una explosión convencional proviene de la transformación química del material combustible y de los demás elementos participantes. La energía que libera una reacción nuclear, por el contrario, es resultado de la transmutación -el sueño de los alquimistas- de un elemento químico en otros diferentes. En la bomba que explotó sobre Hiroshima, la energía liberada provino de la fragmentación -fisión- de átomos de uranio en átomos más ligeros, lo que originó una bola de fuego de 300 metros de diámetro, con una temperatura en su centro de millones de grados centígrados, que engulló a la ciudad en la medida que se expandió.El proyecto Manhattan, mediante el cual fue desarrollada la bomba atómica, fue una empresa de gran magnitud que requirió de la capacidad tecnológica de los Estados Unidos y del concurso de algunos de los más prominentes físicos estadounidenses de la época. El éxito de este proyecto hizo evidente, de manera dramática, el poder del método científico empleado de una manera organizada y sistemática. Con este éxito, la ciencia adquirió un lugar de prominencia.Por otro lado, y quizá porque estamos a más de seis décadas del fin de la Segunda Guerra Mundial, nos es difícil entender como fue que algunos de los más notables científicos de la época pudieron unir esfuerzos para crear algo que sabían podría derivar en algo tan terrible como los bombardeos de Hirsoshima y, tres días después, Nagasaki. Dichos bombardeos han sido justificados por los estadounidenses aduciendo que acortaron la duración de la guerra y evitaron la muerte de cientos de miles de solados norteamericanos y japoneses, que habría ocurrido si la inminente invasión del Japón se hubiera dado. En contra de este argumento, otros sostienen que el Japón no hubiera podido mantener la guerra mucho más tiempo y que, en todo caso, no había necesidad de lanzar bombas atómicas contra la población civil. Hubiera bastado hacer una demostración en despoblado del poder destructivo de las nuevas armas, lo que haría evidente la inutilidad de resistir.     En realidad, aparte de terminar la guerra lo más pronto posible, los militares norteamericanos tenían como objetivo probar las nuevas armas en una situación real. Para esto escogieron cuidadosamente los blancos. Hiroshima fue elegida porque prácticamente no había sido tocada por los bombardeos aéreos, de modo que la destrucción que produciría la bomba sería mejor evaluada. Así, acompañando al Enola Gay, otro bombardero B29 llevó a bordo observadores científicos que realizaron mediciones de los efectos de la explosión.Hiroshima y Nagasaki fueron víctimas de una tragedia de la que hay que culpar a la guerra en lo abstracto, y en lo concreto a norteamericanos y japoneses por igual. Los aterradores resultados del bombardeo nuclear sobre la población civil han convertido a ambas ciudades en un símbolo de aquellos que abogan por eliminar a la guerra y en particular a las armas nucleares. Esto, sin embargo, pareciera ser una empresa imposible, ya que a partir de 1945 el número  de países que poseen arman nucleares se ha ido incrementando paulatinamente, hasta sumar nueve en la actualidad. Tal pareciera que con los bombardeos de Hiroshima y Nagasaki los norteamericanos abrieron una caja de Pandora que no sabemos como revertir.",
    "El pasado 20 de julio, como fue ampliamente difundido por los medios escritos y electrónicos, se cumplieron 40 años del arribo de los primeros seres humanos a la superficie de la Luna. El acontecimiento coronó el ofrecimiento del Presidente John F. Kennedy, hecho ante el Congreso de los Estados Unidos el 25 de mayo de 1961, de hacer llegar un hombre a nuestro satélite antes de finalizar la década. Poner un hombre en la Luna constituyó una empresa tecnológica de gran magnitud, que dio una medida del poderío tecnológico de los Estados Unidos. Es ampliamente aceptado que una de las motivaciones del Presidente Kennedy para anunciar los planes del gobierno de llegar a la Luna, fue el atraso de los Estados Unidos en la carrera espacial con la Unión Soviética. La URSS puso en órbita el primer satélite artificial, el Sputnik 1, el 4 de octubre de 1957, y con ello se adelantó a los Estados Unidos y asestó un fuerte golpe en el ánimo del público norteamericano. La aparente desventaja tecnológica de los Estados Unidos fue percibida, en el marco de la Guerra, como una seria amenaza para la seguridad nacional. Un mes después, la inquietud pública aumentó aún más cuando fue puesto en órbita el Sputnik 2 con un pasajero a bordo: la perra Laika, que sobrevivió algunas horas en el espacio. El shock norteamericano llegó a un nuevo nivel cuando el cohete Vanguard, que debía poner en órbita al primer satélite artificial norteamericano, explotó durante su lanzamiento el 6 de diciembre de 1957. La prensa norteamericana, más allá de la alarma, reaccionó con sarcasmo. En su edición del 16 de diciembre de 1957 la revista Time escribió: “Después de que el agua y el dióxido de carbono liberados por los extinguidores automáticos apagaron el fuego, los atribulados operadores del cohete encontraron al único sobreviviente: el diminuto satélite norteamericano, intacto, arrojado desde la nariz del cohete, radiando las señales que se suponía emitiría desde el espacio. El Sputnik norteamericano emitía desde el suelo la frecuencia correcta: 108 megaciclos”.   El primer satélite norteamericano, el Explorer I, competencia del Vanguard, fue puesto en órbita el 31 de enero de 1958, seguido del Vanguard 1 el 17 de marzo de ese mismo año. Había, no obstante, una gran disparidad de peso entre los satélites norteamericanos y los soviéticos: el Explorer 1 pesaba 14 kg, mientras que el Vanguard 1 solamente 1.4 kg, en comparación con la media tonelada del Sputnik 2. Esta disparidad de pesos motivó que Nikita Kruschev se refiriera de manera sarcástica al Vanguard 1 como el “satélite toronja”. Al final, sin embargo, lo norteamericanos superaron ampliamente a los soviéticos en la carrera espacial, al menos en el ánimo de la opinión pública, con el arribo de los astronautas del Apolo 11 a la Luna. El lanzamiento del Sputnik tuvo también un impacto considerable en el sistema educativo norteamericano. En la década de los cincuentas el sistema de enseñanza de las ciencias y las matemáticas en los Estados Unidos estaba bajo presión. Se afirmaba que  los jóvenes norteamericanos no estaban siendo adecuadamente educados en estas materias, que se percibían como fundamentales para el progreso del país. En este clima de críticas, la puesta en órbita del Sputnik aceleró el desarrollo de nuevos planes de estudio en ciencias y matemáticas con el apoyo de varias agencias gubernamentales, y produjo lo que se conoce como la reforma educativa de la “Era post Sputnik”. Participaron en esta tarea, de manera prominente, destacados investigadores y profesores universitarios, que desarrollaron textos innovadores para la enseñanza de las ciencias y las matemáticas. Podría pensarse que los éxitos del proyecto Apolo, con todo su glamour y fuerte difusión mediática, aunado a los nuevos planes de enseñanza de las ciencias y las matemáticas diseñados por profesionales de la investigación científica, constituyeron un poderoso incentivo para que los jóvenes norteamericanos escogieran carreras profesionales en campos científicos y tecnológicos. Éste, sin embargo, no fue el caso y, por el contrario, después del proyecto Apolo se observo una marcada baja en el interés por estos campos. Por ejemplo, según el Instituto Americano de Física, a partir de la década de los setentas ocurrió un descenso en el número de estudiantes norteamericanos que se enrolaron en los programas de maestría y doctorado en física ofrecidos por las universidades norteamericanas. En otra medida del cambio de interés profesional de los norteamericanos, según datos de la Fundación Nacional de la Ciencia, en 2005 solamente un 50% de los grados doctorales concedidos por las universidades en los Estados Unidos en las áreas de ingeniería, matemáticas, ciencias de la computación, física y economía, fueron otorgados a estudiantes de ese país. No hay, por supuesto, una conexión directa entre el proyecto Apolo y la pérdida de interés de los jóvenes norteamericanos por la ciencia y la tecnología, que de cualquier manera se hubiera dado. Notamos, sin embargo, la ironía de que dicha pérdida se haya dado precisamente después de un éxito tecnológico de la magnitud del proyecto Apolo, lo que es una muestra de la complejidad de la sociedad norteamericana.",
    "A lo largo del siglo XX la ciencia jugó un papel fundamental en el desarrollo económico de los países industrializados. El conocimiento científico acumulado por estos países en el último siglo, fue la base para la creación de tecnologías de gran sofisticación que originaron nuevas industrias y que dieron ventajas estratégicas y competitivas a los países que las desarrollaron. En la medida en que las tecnologías de base científica adquirieron relevancia para el desarrollo económico, la carrera de investigador científico se hizo más popular. En los Estados Unidos, por ejemplo, el número de personas que realizaban profesionalmente actividades clasificadas en la categoría de ciencia e ingeniería se incremento por un factor de 25 entre los años 1950 y 2000, hasta alcanzar cinco millones en la actualidad.   La investigación científica tiene características propias que la hacen única. El objetivo de la ciencia es descubrir las leyes de la naturaleza, de modo que la validación de un descubrimiento científico debe provenir de un experimento u observación sistemática del fenómeno bajo estudio. La ciencia, además, requiere de la difusión de los resultados alcanzados por los investigadores, así como de una abierta discusión de los mismos.  El conocimiento científico en un determinado campo está en continua evolución  y bajo el escrutinio severo de los especialistas. Como parte de la dinámica de desarrollo de la ciencia, el sistema internacional de revistas en el que los científicos comunican los resultados de sus investigaciones es fundamental. La publicación de un artículo está comúnmente precedida por una revisión crítica del mismo por uno o varios árbitros, los cuales pueden aceptarlo o rechazarlo. El sistema de revistas arbitradas es uno de los foros en los que se validan o refutan los resultados de la investigación científica. Los científicos están obligados a publicar sus resultados pues de otro modo pasarían sin ser notados. Un investigador está entonces bajo fuerte presión por publicar el mayor número de artículos, ya que de esto también depende el que consiga fondos para investigación. Como en toda actividad humana y debido a esta presión, sin embargo, se dan de cuando en vez casos de publicaciones fraudulentas que pueden tomar diversas formas. Por ejemplo, un artículo puede incluir datos falsos de un experimento o incluso reportar resultados de un experimento no realizado. Existe también el plagio de trabajos de otros autores. A lo largo del último siglo hay ejemplos notables de fraudes científicos. Uno de los más famosos es el llamado “Hombre de Piltdown”. En 1912, el arqueólogo amateur británico Charles Dawson anunció el descubrimiento de restos fósiles -una mandíbula y un cráneo- en el poblado de Piltdown en East Sussex, Inglaterra, los cuales fueron tomados por algunos como el “eslabón perdido” en la evolución entre los simios y el hombre moderno. Aunque los fósiles de Piltdown fueron muy controvertidos desde un inicio, no fue sino hasta 1953 que se demostró que eran en realidad falsos: la mandíbula pertenecía a un orangután y el cráneo a un hombre moderno. Dawson, que murió en 1938, disfrutó sin embargo y hasta su muerte de la celebridad que le dieron sus fósiles.Se podría pensar que un caso como el de Dawson no tendría posibilidad de repetirse en la actualidad dado el gran desarrollo de la ciencia en el último siglo. No hay, sin embargo, nada más alejado de la realidad. A manera de ejemplo, en mayo de 2002 los directivos de los Bell Laboratories en Nueva Jersey, uno de los centros de investigación industrial de mayor prestigio en el mundo, formaron un comité de científicos para investigar el trabajo de uno de sus investigadores, Jan Hendrik Schön, acusado por algunos de sus colegas de conducta científica fraudulenta. Schön era en esos momentos un brillante científico de 32 años que había publicado una cantidad asombrosa de artículos científicos en el campo de la nanotecnología, reportando resultados de alta relevancia que anticipaban una revolución tecnológica. No pocos veían a Schön como un seguro ganador del Premio Nobel de física. El reporte del comité fue, sin embargo, catastrófico para este investigador, pues fue encontrado culpable de falsificar datos experimentales y de reportar resultados de experimentos que no pudieron ser reproducidos, y que por lo tanto carecían de valor científico. Como resultado, y a diferencia de Dawson quién murió sin que su fraude fuera demostrado de manera plena, Schön fue despedido de su trabajo en Bell Laboratories el mismo día que se recibió el reporte del comité  y cayó en el mayor descrédito.    El fraude científico cometido por Jan Hendrik Schön ha sido considerado como el más grande jamás ocurrido y puso bajo crítica el sistema de arbitraje de las revistas científicas que dejaron pasar uno tras otro los artículos fraudulentos. A pesar de todo, sin embargo, tanto los fraudes de Dawson como los de Schön fueron finalmente descubiertos gracias al carácter abierto de la discusión científica. Siendo humanos, los científicos no están ciertamente exentos de buscar el mayor provecho con el mínimo esfuerzo. Al final, sin embargo, la ciencia se corrige por si misma y deja poco espacio para las debilidades humanas.",
    "El uso del fuego se remonta cientos de miles de años en el tiempo, a una época en la que no había aparecido todavía el hombre moderno. En la ribera del lago Hula en el norte de Israel, se han encontrado evidencias que sugieren que homínidos, posibles antecesores nuestros, habían adquirido la habilidad de usar el fuego hace unos 800,000 años. Aunque nunca llegaremos a saber como fue que adquirieron esta habilidad, podríamos quizá imaginar que inicialmente tomaron el fuego de una fuente natural -por ejemplo de un árbol incendiado por un rayo- y lo mantuvieron encendido en una fogata hasta que la lluvia o alguna otra contingencia lo extinguieran. Posteriormente, a lo largo de cientos de miles de años, aprendieron a generar su propio fuego -quizá a partir de las chispas producidas al chocar dos pedernales- y de este modo se libraron de depender de los incendios naturales fortuitos para tener acceso al fuego. El fuego resulta de la reacción química de materiales orgánicos con el oxígeno del aire, lo que libera energía en forma de calor. Podemos considerar que una fogata es un arreglo o dispositivo para producir energía calorífica a partir de la energía química acumulada en el material combustible. Así, las fogatas encendidas hace 800,000 años representan el ejemplo más antiguo conocido de generación de energía calorífica para un uso específico, quizá para calentar espacios habitables.   Ochocientos mil años después, en el año de 1876, el ingeniero alemán Nicolaus Otto desarrolló el motor de combustión interna empleado en automóviles. Este motor es un dispositivo que convierte la energía química almacenada en la gasolina en energía mecánica. Para lograr esto, dentro de los cilindros del motor se hace explotar de manera controlada una mezcla de gasolina con aire. Los gases producto de la explosión se expanden dentro de los cilindros y la fuerza de esta expansión, transmitida a las ruedas del automóvil, es la que lo hace moverse. Al igual que una fogata, un motor de combustión interna está diseñado como un dispositivo para convertir una forma de energía en otra para un uso específico. Tanto en una fogata como en un motor de combustión interna esta conversión debe darse en forma controlada a fin de que los dispositivos sean prácticos. En caso contrario, la fogata podría extenderse a sus alrededores y provocar un incendio catastrófico, o bien el motor podría explotar con resultados también catastróficos. Si bien tanto la fogata como el motor de combustión interna son dispositivos para obtener energía, la diferencia en sofisticación tecnológica entre los dos es muy grande. De hecho, el motor de combustión interna tal como lo conocemos ahora, es un ejemplo de una tecnología de base científica que requirió de avances científicos que se dieron en la primera mitad de siglo XIX. En contraste, el descubrimiento del procedimiento para producir fuego a voluntad hace cientos de miles de años, se realizó seguramente por accidente y empleando un procedimiento de prueba y error. El desarrollo industrial de los últimos dos siglos ha requerido de cantidades crecientes de energía que obtenemos hoy en día de muy diversas fuentes. Una de éstas es la nuclear, que representa aproximadamente el 6 % de la energía total generada a nivel mundial. La generación de energía nuclear es un ejemplo prototípico de tecnología de base científica, que fue posible gracias al conocimiento científico del núcleo atómico que se alcanzó en la primera mitad del siglo XX. La energía nuclear proviene de la fisión de átomos de uranio, proceso que libera una enorme cantidad de energía. Como sabemos, al final de la Segunda Guerra Mundial los Estados Unidos usaron la fisión nuclear para fabricar bombas atómicas. En una bomba de este tipo, se produce una explosión por la fisión incontrolada de átomos de uranio. La fisión nuclear, sin embargo, puede también emplearse para generar energía de manera controlada. Esto se logra en los reactores nucleares que son instalaciones para la generación de energía eléctrica a partir de la energía liberada por la fisión controlada de átomos de uranio. La energía ha sido un elemento esencial para el desarrollo de la civilización humana. A nuestro alrededor encontramos energía en diferentes formas: solar, eólica, hidráulica y como combustibles orgánicos o fisionables, por nombrar sólo algunas. Esta energía está a nuestra disposición y es suficiente para cubrir nuestras necesidades, pero hay que saber como controlarla y aprovecharla. Desde la adquisición del fuego por nuestros ancestros hace alrededor de un millón de años hemos logrado grandes avances en esta dirección, sobre todo a partir de la introducción del conocimiento científico en el proceso de desarrollo tecnológico. La diferencia en sofisticación tecnológica entre un reactor nuclear y una fogata es enorme y nos da una medida de cuanto hemos progresado.No debemos, sin embargo, menospreciar la tecnología del fuego desarrollada hace cientos de miles de años, la cual en su momento constituyó un desarrollo tecnológico formidable. Baste pensar que la gran mayoría de nosotros seríamos incapaces en la actualidad de encender un fogata sin cerillos.",
    "El año de 1980 Brasil no era un país petrolero. Su producción de petróleo en esa época era apenas de 182,000 barriles diarios. En comparación, México, un país más pequeño, produjo ese año casi dos millones de barriles por día. No es de extrañar, entonces, que Brasil sufriera de manera severa las crisis del petróleo de las décadas 70 y 80. La primera de estas crisis se dio a finales del año de 1973, cuando los países árabes decretaron un embargo petrolero a los Estados Unidos y a otros países industrializados, lo que provocó que se cuadruplicara el precio del petróleo en el curso de unos cuantos meses. Posteriormente, la revolución islámica en Irán en 1979, que en esos momentos era el segundo productor mundial de petróleo, produjo una segunda crisis que incrementó todavía más el precio del crudo.Todo esto impactó considerablemente la economía de Brasil, que tuvo incluso que recurrir a préstamos en dólares para importar el petróleo que necesitaba para mantenerse funcionando. Como respuesta, y a fin de reducir su dependencia del petróleo importado, Brasil se dio a la tarea de desarrollar tecnologías para la producción masiva de alcohol etílico, o etanol, para ser usado como combustible en automóviles. La materia prima para la producción de etanol fue la caña de azúcar. El proyecto resultó muy exitoso y como resultado, Brasil es el país que en estos momentos tiene la tecnología más avanzada para la producción de alcohol etílico, así como la mayor experiencia en su uso, puro o mezclado con gasolina, en automóviles. Así, el 50 % del combustible que actualmente consumen los  automóviles en Brasil es etanol. Brasil no es el máximo productor mundial de etanol, lugar que le corresponde a los Estados Unidos, país que lo produce no de la caña de  azúcar sino del maíz. Los brasileños son, sin embargo, considerablemente más eficientes que los norteamericanos en la producción de alcohol etílico. En efecto, Brasil tiene una productividad de etanol por área cultivada que es aproximadamente el doble de la de los Estados Unidos. Además, el área empleada por Brasil para la producción de alcohol etílico es aproximadamente el 1 % del total de su área cultivada, en contraste con los Estados Unidos que emplea casi el 4 %. Como resultado, el costo de producción del etanol brasileño es menor que el correspondiente costo del alcohol estadounidense; y lo más importante, Brasil no subsidia su producción mientras que los Estados Unidos sí lo hace.Al margen de lo anterior, la experiencia brasileña resulta de gran relevancia en el contexto de la presente crisis ambiental -léase calentamiento global- debida al uso indiscriminado de los combustibles fósiles -petróleo, carbón y gas natural-. Esta crisis demanda la sustitución de los combustibles fósiles no renovables por versiones renovables. Los biocombustibles, obtenidos a partir de materia orgánica y de los cuales el etanol es un ejemplo, constituyen un grupo de energéticos renovables de la mayor importancia desde el punto de vista ambiental, pues son neutros desde el punto de vista de la emisión de carbono a la atmósfera. Es decir, la cantidad de bióxido de carbono que liberan al quemarse es -al menos en teoría- la misma que fue absorbida por las plantas que los sintetizaron.No podríamos, por supuesto, esperar, que los biocombustibles supriman por completo la emisión de dióxido de carbono a la atmósfera, pues para obtenerlos se requiere de procesos que emplean combustibles fósiles. Los brasileños han demostrado, sin embargo, que el uso de etanol en automóviles reduce en un 90% la cantidad de bióxido de carbono emitido a la atmósfera. Entre los países latinoamericanos, Brasil es uno de los pocos que ha tomado serio a la ciencia y la tecnología como un factor de desarrollo económico. Esto nos lo demuestra el proyecto del etanol que, por otro lado, no es el único ejemplo que nos ofrece. México, en contraste, difícilmente hubiera tenido la capacidad para desarrollar un proyecto de esta envergadura y podemos sentirnos afortunados de que las sucesivas crisis del petróleo nos hayan afectado solamente de manera indirecta por ser un país petrolero. Teniendo México una historia y una cultura cercanas a las de Brasil, no existe ninguna justificación ni tampoco explicación para la gran diferencia en desarrollo científico y tecnológico que muestran hoy en día los dos países, excepto quizás por la poca consistencia que siempre ha caracterizado a nuestros gobiernos en cuanto al apoyo a la ciencia y la tecnología. En la actualidad, por ejemplo, México invierte en este respecto un porcentaje del producto interno bruto que el la mitad del brasileño. Por cierto, Brasil ha multiplicado por diez su producción de petróleo en los últimos treinta años y es hoy en día el quinceavo productor mundial. México, por su parte, muestra un declive en su producción de crudo, la cual ha disminuido 20% en los últimos cinco años. Es como si México estuviera jugando el papel de la cigarra y Brasil el de la hormiga en la fábula en la que la cigarra muere de hambre y frío en invierno por su falta de previsión en los días de verano, mientras la hormiga está a buen abrigo y con suficiente comida. Esperemos que el apoyo a la ciencia y la tecnología en México alcance el nivel que merece en un futuro no muy lejano.",
    "La energía que hoy en día mueve al mundo se obtiene en un 86 % de los combustibles fósiles, petróleo, gas natural y carbón. Estos combustibles, que fueron generados en el transcurso de cientos de millones de años bajo la superficie de la Tierra, son, por supuesto, no renovables y verán su fin en un futuro más o menos cercano. Por otro lado, el uso indiscriminado de combustibles fósiles desde el inicio de la Revolución industrial dos siglos atrás, ha producido un incremento en el contenido de bióxido de carbono de la atmósfera, que a su vez ha generado el calentamiento global que nos amenaza con desastres ecológicos. El desarrollo de energías alternativas que no generen bióxido de carbono y que además sean renovables, es entonces un tema de importancia primordial.El Sol es la fuente de vida de nuestro planeta. Sin el Sol, la Tierra sería un lugar frío, oscuro y muerto. El secreto de la vida es la fotosíntesis, que es posible gracias al Sol. Como sabemos, a través de este proceso plantas y algas absorben bióxido de carbono de la atmósfera y generan materia orgánica. Sin el Sol no habría fotosíntesis y no existirían plantas sobre la faz de la Tierra; ni tampoco animales y humanos que dependen de las plantas para subsistir.  El Sol es y ha sido también la fuente de la mayor parte de la energía consumida en la Tierra desde tiempo inmemorial. La energía de nuestros músculos, por ejemplo, la obtenemos del alimento vegetal o animal que ingerimos y en consecuencia del Sol en último término. La energía del viento, que nos permitió navegar en tiempos pasados en buques veleros de todo tipo y que ahora se aprovecha en  “granjas” de molinos de viento, tiene también al Sol como causa primaria. Es éste también el caso de las caídas de agua empleadas para producir energía en plantas hidroeléctricas, cuya operación depende del ciclo del agua evaporación-lluvia que a su vez es consecuencia del calentamiento solar. Los mismos combustibles fósiles, que se generaron a partir de materia orgánica sepultada bajo la superficie de la Tierra en tiempos geológicos, fueron producto de la actividad fotosintética ocurrida hace cientos de millones de años. Tenemos, por otro lado, que los mayores usuarios de la radiación solar que alcanza la Tierra, son y han sido las plantas y algas verdes, las cuales mediante el proceso de fotosíntesis generan anualmente materia orgánica con un contenido de carbón que en peso es diez veces mayor que todo el peso del carbón consumido por año en actividades industriales a nivel mundial. La fabricación de materia orgánica por la naturaleza a través de la fotosíntesis excede, y con mucho, la dimensión de cualquiera de nuestras actividades industriales.         Podemos ver entonces que el Sol, que hace llegar a la superficie de la Tierra una cantidad de energía que es 10,000 veces más grande que el total del consumo mundial, es sin duda la primera opción de energía renovable. Para esto, hay varios esquemas que pueden ser utilizados. Podemos, por ejemplo, aprovechar la energía solar directamente para calentar agua, o bien para generar energía eléctrica por medio de celdas solares. Podemos también aprovechar al Sol indirectamente, a través de molinos de viento o en centrales hidroeléctricas.Una opción que luce particularmente atractiva como fuente de energía renovable es la que nos proporciona la fotosíntesis misma, a través de la cual se fabrica material combustible - la llamada biomasa-, convirtiendo a la energía solar en energía química almacenada en el material sintetizado. De hecho, el primer material de este tipo utilizado por nuestros ancestros para hacer fuego -la madera- fue fabricado por la naturaleza empleando la fotosíntesis. Una ventaja de fabricar el combustible que necesitamos de esta manera, es que no incrementaríamos el balance de carbono en nuestro planeta; es decir, al quemar la biomasa y arrojar de manera inevitable bióxido de carbono a la atmósfera, solamente estaríamos regresándole a la misma el carbono que se empleó con la fotosíntesis para fabricar dicha biomasa. Por otro lado, una desventaja de concebir a la fotosíntesis como fábrica de combustible es su baja eficiencia de conversión de energía solar en energía química de biomasa, la cual es de unas pocas unidades porcentuales. Hay que hacer notar, sin embargo, que para algunos cultivos esta eficiencia es significativamente mayor. En el caso de la caña de azúcar, por ejemplo, es del orden del 8% y de ahí su popularidad como fuente de energía renovable. Existe la actualidad mucho interés en investigar los mecanismos detallados por los cuales ocurre la fotosíntesis. Una mejor compresión de dichos mecanismos llevará con seguridad al desarrollo de procesos de fotosíntesis artificiales con eficiencias mayores a las de los correspondientes procesos naturales. Entre las energías renovables que en un futuro -que esperamos no sea muy lejano- sustituirán a los combustibles fósiles, encontraremos, sin duda, a aquellas basadas en la producción de biomasa. Después de todo, ésta constituye la forma más primitiva de obtener energía, aparte de la nos brinda nuestra propia fuerza muscular.",
    "Quienes cursaron la escuela secundaria hace algunas décadas posiblemente recuerden que los libros de geografía nos enseñaban que el mar de Aral -situado en Asia Central en las ex repúblicas soviéticas de Kazakistán y Uzbekistán- era por su tamaño el cuarto mayor lago del planeta, solamente superado por su vecino el mar Caspio, por el lago Superior en la frontera entre Canadá y los Estados Unidos y por el lago Victoria en África. En los últimos 50 años, sin embargo, el mar Aral ha ido encogiéndose dramáticamente por la disminución del volumen de los ríos que lo alimentan. En 1960 el mar de Aral cubría un área de 68,000 kilómetros cuadrados y su superficie tenía una forma ovalada con casi 300 kilómetros en sus partes más anchas. En la actualidad, el mar de ha dividido en tres cuerpos de agua no conectados y su superficie se ha reducido en un 90%. De estos tres lagos, el mar Aral Norte se encuentra en Kazakistán, mientras que los dos restantes al sur  lo están en Uzbekistán.  La ciudad de Muinak en Uzbekistán, que una vez fue un próspero puerto pesquero en la costa sur del mar de Aral, es una de las principales víctimas del desastre ecológico y está hoy a 100 kilómetros de la costa. Efectivamente, como lo describe la periodista española Pilar Bonet, corresponsal del diario El País, “Muinak, el centro de un distrito poblado por 28.000 habitantes, es una localidad deprimida. Su principal atracción es el cementerio de barcos. Distribuidas sobre la arena ante un mirador, las barcazas que un día surcaron el mar son hoy cadáveres de chatarra y símbolos de la tragedia del Aral”.Los afluentes del mar de Aral, los rios Amu Dariá y Sir Dariá, nacen en las montañas cerca de la frontera china, y cruzan campos agrícolas en Uzbekistán el primero y Kazakistán el segundo. Estos campos, que son irrigados con agua desviada de los afluentes del mar de Aral, fueron creados para el cultivo de algodón en gran escala durante la era soviética, cuando Kazakistán y Uzbekistán formaban parte de la URSS. El uso del agua de los ríos Amu Dariá y Sir Dariá para propósitos agrícolas fue, sin embargo, tan masivo que el nivel del mar de Aral comenzó a decrecer paulatinamente a partir de los años sesentas hasta llegar al estado actual. El agua del mar de Aral fue, además, severamente contaminada por los pesticidas empleados en los campos agrícolas y por el incremento en la concentración de sal que ha hecho imposible la vida de peces en dos de los lagos actuales. El desastre ecológico del Aral no solamente ha tenido consecuencias económicas por la afectación de la actividad pesquera, sino también severas consecuencias sanitarias y en la región se encuentra una incidencia significativamente alta de enfermedades tales como la tuberculosis, debido a la contaminación atmosférica por al polvo y la sal descubiertos al retraerse las aguas. Ha tenido también consecuencias climáticas y ahora los inviernos son más fríos y los veranos más calurosos de lo que solían ser.Aunque no en la escala del mar de Aral, en México no nos quedamos atrás en materia de desastres ecológicos. Tenemos, por ejemplo, que el lago de Chapala -el más grande del País- almacenó en promedio en los últimos 20 años, según datos de la Comisión Estatal del Agua de Jalisco,  solamente alrededor del 60 % del volumen de agua que contenía en la primera mitad del siglo XX. Después de llegar a un mínimo de nivel en el año 2002, por abajo del que se considera crítico, el lago de Chapala se ha recuperado en los últimos años debido a un incremento en la captación por lluvias. Hay quienes consideran, sin embargo, que el lago está en peligro de desaparecer de sobrevenir un periodo de sequía. Por considerarlo en situación crítica, el lago de Chapala fue declarado “Lago amenazado del año 2004” por el Global Nature Fund.De manera similar al mar de Aral, la disminución del nivel del lago de Chapala es debida a la desviación de sus afluentes, en este caso el río Lerma para fines agrícolas y de consumió humano. Este río nace en el Estado de México, cruza la zona agrícola al sur del Estado de Guanajuato y desemboca en el lago de Chapala. A lo largo de su recorrido de 700 kilómetros, el río Lerma es contaminado por desechos urbanos y agrícolas, mismos que vierte finalmente en el lago de Chapala.  El mar de Aral y el lago de Chapala son dos ejemplos de uso del agua para lograr un crecimiento económico, que tiene, por supuesto, un beneficio social. En el caso del mar de Aral, por ejemplo, la desviación del río Amu Dariá condujo a un considerable crecimiento económico en Uzbekistán, tan importante que incluso han hecho difícil tomar medidas para revertir la situación y regresar al mar de Aral a su estado original. De hecho, los dos lagos remanentes que se encuentran en su territorio han sido abandonados a su suerte y se prevé la desaparición de uno de ellos en pocos años. Dados los desastres ecológicos que se han generado por el uso inadecuado del agua, y de los cuales el del mar de Aral es quizá el ejemplo más saliente, es ahora claro que tenemos la responsabilidad de manejar el agua de manera planificada. De otra manera provocaremos desastres todavía mayores.",
    "En el año de 1950 el escritor estadounidense Ray Bradbury publicó su famosa novela de ciencia ficción  “Crónicas marcianas” en la que relata una serie de expediciones desde la Tierra al planeta Marte que eventualmente llevaron a su colonización. La novela se sitúa entre los años 1999 y 2006, medio siglo adelante del tiempo en que fue escrita, y en la misma se describe como los viajeros encuentran una civilización marciana altamente desarrollada con nativos capaces de comunicarse telepáticamente, así como ruinas de gran antigüedad. Además, aunque la atmósfera marciana en la novela era muy tenue, contenía suficiente oxígeno de modo que los exploradores pudieron respirar y realizar actividades físicas moderadas sin necesidad de equipo especial. Los viajeros encontraron también agua en abundancia, así como los canales que el astrónomo italiano Giovanni Schiaparelli creyó observar a través del telescopio en la superficie marciana en 1877. Bradbury nos describe la llegada de colonizadores en número creciente y la construcción de pueblos con materiales llevados de la Tierra. Los marcianos, por su lado, fueron exterminados en masa por la varicela llevada a Marte por las primeras expediciones -de manera similar a como la población en México fue diezmada por la viruela traída por los conquistadores españoles en el siglo XVI-.  El Marte que encontraron las sondas Spirit y Opportunity de la NASA después de arribar a la superficie del planeta en el año 2004 -medio siglo después de la aparición de Crónicas marcianas- fue, sin embrago, muy diferente al Marte de Bradbury. Las primeras imágenes panorámicas enviadas por la sonda Spirit en enero de 2004 desde el cráter Gusev cerca del ecuador marciano, mostraron una planicie pedregosa y desolada con algunas elevaciones a kilómetros de distancia, sin rastros visibles de agua. La sonda Opportunity, que amartizó en el lado opuesto del planeta pocos días después del arribo del Spirit, lo hizo en el fondo de un pequeño cráter y de acuerdo con un comunicado de prensa de la NASA del 25 de enero de 2004, encontró un “paisaje surrealista y oscuro como nunca antes se había visto en Marte”.    Como sabemos, Marte es el cuarto planeta del sistema solar y se encuentra aproximadamente 1.5 veces más alejado del Sol que la Tierra. Marte recibe entonces menos radiación solar que nuestro planeta y es por lo tanto más frío, con una temperatura promedio de 63 grados centígrados bajo cero en comparación con la temperatura promedio terrestre que es de unos 14 grados centígrados. Por otro lado, la atmósfera en Marte es muy tenua con una presión que es solamente un 0.6 % de la presión atmosférica en nuestro planeta. Además, está compuesta en un 95 % de bióxido de carbono, con un porcentaje muy pequeño de oxígeno. Nos sería, pues, imposible respirar en la superficie de Marte sin la ayuda de un traje de astronauta. Es entonces impensable que pudiéramos colonizar Marte tal como lo describe Bradbury en su novela.    Sabemos también que, aunque la sonda Phoenix de la NASA que arribó al polo norte marciano en 2008 encontró hielo enterrado a poca profundidad, no hay agua en la superficie de Marte en grandes cantidades. La sonda Opportunity encontró, no obstante, que esto no ha sido siempre así y que en un pasado remoto sí existió agua líquida en Marte.Bradbury sabía, por supuesto, que Marte no era como él lo describió en su novela y que, por ejemplo, los canales de Schiaparelli -que en algún momento se sostuvo fueron construidos por una civilización avanzada para transportar agua- en realidad habían sido producto de una ilusión óptica. Lejos de esto, Bradbury utilizó el tema de los viajes espaciales -que en la época no se veían lejanos por los avances en la tecnología de los cohetes que se habían dado durante y después de la Segunda Guerra Mundial- para tratar una serie de temas que le preocupaban, haciéndolo de una manera magistral y en un escenario inusual. Algunos de estos temas tenían que ver con críticas a la sociedad norteamericana de su tiempo e incluían al racismo, la guerra, las armas atómicas y la censura. También incluyó críticas a la sociedad capitalista que “si no ha puesto carritos de hot dogs en medio de templo de Karnak en Egipto, es porque está muy alejado y no sería redituable”.   Este último tema lo abordó con el argumento de que el planeta Marte podría ser severamente dañado por los colonizadores norteamericanos y en este punto Bradbury se adelanta varias décadas a su tiempo, dados los graves los problemas de contaminación ambiental que actualmente nos aquejan. Estos problemas son producto de una desenfrenada actividad industrial que, no obstante, no ha sido producto exclusivo de las economías capitalistas sino de las socialistas también.   Marte está en realidad demasiado alejado de la Tierra para llevar a cabo una colonización en un futuro cercano y las Crónicas marcianas no nos proporcionan en este respecto un panorama factible. Los temas que Bradbury trata en su novela, sin embargo, son en realidad problemas propios de nuestra civilización y dicha novela bien pudiera en cambio haberse titulado “Crónicas Terrestres”.",
    "En agosto de 1609, hace ya 400 años, Galileo Galilei hizo una demostración pública de un instrumento construido por él -el telescopio, como fue posteriormente conocido- que permitía acercar visualmente objetos lejanos.  La demostración la hizo desde el campanario de la plaza de San Marcos en Venecia. Si tomamos en cuenta que aún ahora nos cuesta resistir la tentación de echar un vistazo a través de un telescopio si se nos presenta la ocasión, no es difícil imaginar la curiosidad y el asombro que experimentaron los espectadores con la demostración del instrumento de Galileo, que aumentaba las imágenes unas diez veces. No obstante que el telescopio se asocia comúnmente a Galileo Galilei, él no fue el primero en construir un instrumento de este tipo, y fue solamente después de enterarse que en Holanda existía un dispositivo que acercaba los objetos lejanos, que  Galileo se dio a la tarea de reproducirlo. El telescopio de Galileo resultó, sin embargo,  superior al holandés y, sobre todo, mucho mejor manejado desde el punto de vista de las relaciones públicas. En efecto, con la demostración de la plaza de San Marcos, realizada ante miembros del Senado veneciano, Galileo no sólo logró que su nombre quedara por siempre asociado al telescopio, sino que además obtuvo un importante aumento de sueldo como profesor de la Universidad de Padua, debido al interés del Senado en las potenciales aplicaciones militares del instrumento.En la segunda mitad de año de 1609, Galileo realizó numerosas observaciones astronómicas con versiones mejoradas del telescopio de la plaza de San Marcos, descubriendo que la Luna tiene montanas, que hay muchas más estrellas en el cielo de las que podemos ver a simple vista, que el Sol tiene manchas y que el planeta Venus presenta fases análogas a las de la Luna. Aun más, en enero de 1610 dirige su telescopio hacia el planeta Júpiter y descubre cuatro puntos luminosos a su alrededor, que cambian de posición de manera continua y que Galileo identifica correctamente como satélites de dicho planeta -los actuales satélites galileanos, Io, Ganímedes, Europa y Calixto-.El año de 1609, en el que en adición a las observaciones de Galileo apareció la obra “Astronomia nova” de Johannes Kepler, marca el inicio de la astronomía moderna. Por esta razón, el presente año, en el que se celebra el 400 aniversario de este hecho, ha sido denominado “Año Internacional de la Astronomía” por la Unión Astronómica Internacional.    El microscopio, un instrumento que es pariente cercano del telescopio, está también asociado al nombre de Galileo Galilei. Como todos sabemos, el microscopio, que permite ver objetos pequeños -microbios y células, por ejemplo- imposibles de percibir a simple vista, ha tenido un impacto mayor en el desarrollo de nuestra civilización. Por medio de este instrumento, por ejemplo, el holandés  HYPERLINK \"http://es.wikipedia.org/wiki/Anton_Van_Leeuwenhoek\" \\o \"Anton Van Leeuwenhoek\" Anton Van Leeuwenhoek observó por primera vez las bacterias en la segunda mitad del siglo XVII, lo que dio origen a la bacteriología, ciencia que con el tiempo permitió identificar el origen de las enfermedades infecciosas y su cura en un gran número de casos.  Al igual que el microscopio, el telescopio ha tenido un enorme impacto científico, permitiéndonos descubrir y observar objetos que están más allá de nuestra capacidad visual.  El telescopio, además, jugó un papel crucial en la revolución científica de los siglos XVI y XVII, papel que no tuvo el microscopio. En efecto, en los tiempos de Galileo Galilei la ciencia y su método, tal como los conocemos ahora, no eran universalmente aceptados -en  realidad hoy en día tampoco lo son, aunque la situación de la ciencia en nuestro tiempo es sensiblemente mejor que la que era 400 años atrás- y la tradición medieval, fuertemente influenciada por las ideas de Aristóteles y basada en el principio de autoridad para dirimir cualquier controversia, tenía una gran fuerza. Siguiendo a Aristóteles, por ejemplo, se suponía que todos los objetos celestiales tenían formas esféricas perfectas y eran ellos mismos perfectos, afirmación que quedó hecha añicos con el descubrimiento de las montañas de la Luna y de las manchas del Sol. Otro punto de gran discusión al inicio del siglo XVII era la teoría heliocéntrica de Nicolás Copérnico, negada por la Iglesia y defendida, entre otros, por Galileo. La observación de Júpiter y sus satélites, a los que Galileo vio como un sistema solar en miniatura, le proporcionó argumentos objetivos para defender que la Tierra no era por necesidad el centro del Universo como lo afirmaba la Iglesia. A la postre, como es bien conocido, la defensa de la teoría heliocéntrica llevó a Galileo a ser enjuiciado por la Inquisición, y como resultado a sufrir reclusión domiciliaria de por vida y a abjurar de sus ideas. A la larga, sin embargo, las evidencias proporcionadas por el telescopio de Galileo ayudaron de manera crucial a demoler las ideas medievales y a construir la ciencia de la que hoy somos beneficiarios. Esto último basta para colocar al telescopio entre los más grandes inventos jamás desarrollados.",
    "La epidemia de influenza A (H1N1) que hemos padecido en México a lo largo del último mes ha constituido una experiencia inédita para la gran mayoría de nosotros. La paralización de gran parte de las actividades del País en los primeros días de mayo, la solicitud del Presidente Calderón de permanecer en casa, las calles desiertas, el temor al contagio, el rechazo a los mexicanos en varios países y la declaración del nivel 5 de pandemia por parte de la Organización Mundial de la Salud, fueron acontecimientos que nos tomaron por sorpresa y que se dieron a una velocidad frenética. La vida de pronto nos cambió: mientras que el miércoles 22 de abril pocos habían oído hablar de la epidemia, el viernes 24 existía ya una alarma generalizada y el cubrebocas se hizo parte de nuestra indumentaria.   Por su parte, la Secretaría de Salud sabía de la existencia de brotes de influenza en algunas localidades del País desde hacía varias semanas. Estos, sin embargo, fueron considerados como una extensión de la epidemia estacional y no fueron causa de alarma. Todavía el 17 de abril el Subsecretario de Prevención y Promoción de la Salud aseguró que no existía peligro de propagación de ninguno de los brotes de influenza observados. Uno de estos, que a la postre se hizo célebre como posible origen del la epidemia de influenza, fue el que ocurrió en la comunidad de La Gloria, en el municipio de Perote en el Estado de Veracruz, en donde, de 1575 habitantes 616 padecieron de influenza entre 15 de febrero  el 14 de abril. Un recuento de los principales acontecimientos relacionados con la epidemia de influenza A (H1N1) puede ser encontrado en la edición electrónica de la revista “Science”, publicada por la Sociedad Americana para el Avance de la Ciencia. En dicho recuento se asienta que fue precisamente el brote de La Gloria el que alertó a la Organización Mundial de la Salud (OMS) sobre el posible inicio de una epidemia de influenza en México. La OMS supo de este brote el 10 de abril a través de la compañía Veratect, especializada en vigilancia epidemiológica.Ya encendidas las luces rojas, el 18 de abril el Instituto nacional de Diagnóstico y Referencia Epidemiológicos solicitó ayuda a la Agencia de Salud Pública de Canadá para identificar al virus causante de la influenza. Las muestras a analizar fueron recibidas por esta última institución el 21 de abril. El jueves 23 de abril la agencia canadiense juntamente con el Centro de Control y Prevención de Enfermedades de los Estados Unidos -que también recibió muestras de México- identificaron al virus causante de la epidemia en nuestro País, que además coincidió con el virus encontrado los días 15 y 17 de abril en dos pacientes del estado de California. Esta información fue la que disparó la rápida respuesta del Gobierno FederalDe acuerdo con datos de la Secretaría de Salud, la epidemia de influenza en México alcanzó su máximo entre el 26 y el 27 de abril, es decir, solamente tres días después de que se declaró la contingencia sanitaria, mientras que el 1 de mayo, cuando se inició el paro de actividades no esenciales, la epidemia iba ya en descenso. Hasta el 23 de abril, según la Secretaría de Salud, se habían dado poco más de 600 casos de influenza A (H1N1) en México. Esta cifra, sin embargo, es probablemente una subestimación del número real de casos, como se concluye en una publicación aparecida el 11 de mayo en la revista “Science”, realizada por un grupo internacional de expertos bajo el liderazgo de investigadores de la Facultad de Medicina del Imperial College en Londres, Inglaterra. Dicho grupo estimó el número total de casos de influenza en México de manera indirecta, a partir del número de extranjeros que visitaron México y que una vez de regreso a su País descubrieron que habían contraído la enfermedad. Esto es, si conocemos cuantas personas en promedio cada enfermo contagia, a partir del número de nuevos infectados es posible saber el número fuentes de contagio. Empleando éste y otros razonamientos, y tomando en cuenta los casos de influenza que se presentaron en el extranjero, se estimó que al 23 de abril el número de personas que habían sido infectadas en México era de 23,000. Este número es 30 a 40 veces superior al que reconoce la Secretaría de Salud. Un dato a notar es que entre los autores del artículo de referencia se incluyen a funcionarios de esta Secretaría.     El Gobierno Federal ha sido criticado por haber tardado en reaccionar al brote epidémico -lo cual probablemente es cierto-, pero también de haber reaccionado desmedidamente una vez que recibió la noticia de que estábamos en medio de una epidemia. Esto último es, sin embargo, discutible y como lo afirmó Adolfo Martínez Palomo, coordinador del Consejo Consultivo de Ciencias de la Presidencia de la República, al diario español El País, lo más probable es que nunca lo lleguemos a saber con seguridad.Lo que sí sabemos es que México mostró una pobre capacidad de respuesta a la epidemia. Esto no es sorprendente dada la poca importancia que en nuestro País se da a la investigación científica, que es apoyada con un porcentaje inferior al 0.5 % del PIB nacional, cifra que es varias veces menor que las de los países desarrollados y aun de países latinoamericanos como Brasil.",
    "El 1 de noviembre de 1952 los Estados Unidos hicieron estallar la primera bomba de hidrógeno jamás construida -la bomba H, como fue entonces conocida- en el atolón de Eniwetok en el Océano Pacífico sur.  La explosión, equivalente a 10 millones de toneladas de TNT, arrasó la pequeña isla de Elugelab en dicho atolón, dejando un cráter de 2 kilómetros de diámetro y una profundidad de 50 metros. Años antes, el 16 de julio de 1945, la primera explosión nuclear de la historia se realizó en el desierto de Nuevo México. El poder de destrucción de esta primera bomba nuclear –conocida como bomba A- fue, sin embargo, mucho menor que el poder de la bomba de Elugelab. De hecho, ambas eran de naturaleza diferente: la bomba A se basaba en la fisión de átomos de uranio o plutonio, mientras que la bomba H obtenía su energía mediante la fusión de átomos de hidrógeno.La bomba A fue desarrollada por los Estados Unidos durante la Segunda Guerra Mundial y usada sobre las ciudades japonesas de Hiroshima y Nagasaki en agosto de 1945. En los años subsecuentes, en el marco de la Guerra Fría, los Estados Unidos y la Unión Soviética se lanzaron a una carrera armamentista que llevó no solamente al desarrollo de la bomba H por ambos países, sino también de otras armas de sofisticación creciente. En este clima de enfrentamiento entre las dos superpotencias, el 24 de marzo de 1951 Juan Domingo Perón, Presidente de la Argentina, anunció en una conferencia de prensa que su país había conseguido realizar una reacción termonuclear por fusión bajo condiciones controladas. Aunque Perón hizo énfasis en usos pacíficos de la energía nuclear, quedó claro que los experimentos reportados ponían a la Argentina en posibilidad de fabricar una bomba H. Lo anunciado por el presidente argentino resulta sorprendente si consideramos que en el momento en que se dio, la energía atómica por fusión estaba todavía en fase de investigación. Como resultó a la postre, sin embargo, lo afirmado por Perón no tenía ningún sustento objetivo y el supuesto desarrollo nuclear argentino resultó un fiasco.   La historia de la bomba nuclear argentina se inició con el arribo a Buenos Aires del físico austriaco Ronald Richter en agosto de 1948.  Richter venía recomendado por el prestigiado ingeniero aeronáutico alemán Kurt Tank, quién trabajaba en la Argentina desde 1947 después de dejar su país natal. Gracias a esta recomendación, Richter pronto tuvo acceso a Perón, quién quedó impresionado por las posibilidades que aquel le planteó en el sentido de desarrollar en la Argentina la energía nuclear por fusión –todavía no demostrada-, como una alternativa de bajo costo a la fisión nuclear, tecnología que ya era del dominio de los Estados Unidos. Lo que siguió está relatado en el libro “El secreto atómico de Huemul” del físico argentino Mario Mariscotti. Decir que Perón quedó impresionado por Richter es decir muy poco. En realidad recibió un impacto tan fuerte que de inmediato lo contrató y puso a su disposición todo lo necesario para llevar a cabo su proyecto nuclear. El apoyo de Perón llegó al extremo de construir para Richter instalaciones de gran magnitud en la isla de Huemul, en el lago Nahuel Huapi en la Patagonia argentina, quedando la isla en algún momento bajo la autoridad del austriaco como representante de Perón. Se construyó un “reactor nuclear” de hormigón de 12 metros de altura, un diámetro de 12 metros y paredes de 4 metros de espesor, el cual fue, sin embargo, posteriormente demolido por órdenes de Richter con el argumento de que tenía una fisura de 50 cm de largo y unos pocos centímetros de ancho y de profundidad. Para sustituirlo, Richter ordenó la construcción de una instalación similar, pero ahora enterrada en la roca viva. Todo lo que demandó Richter en contratación de personal y compra de equipo científico le fue concedido. Tuvo, además, a su disposición aviones oficiales para traer del extranjero, con la máxima celeridad, cualquier pieza de equipo o material que considerara necesario e introducirlo de contrabando a la Argentina. Al final, sin embargo, todos los recursos invertidos en el proyecto se fueron a la basura, pues Ronald Richter resultó ser un farsante que tomó ventaja de la disposición de Perón a creer todo lo que viniera de un físico de tradición alemana. Aunque, como todos los farsantes de éxito, Richter era una persona inteligente y estaba al tanto de la investigación en física nuclear entonces en boga, sus planteamientos estaban equivocados y sus experimentos de ninguna manera podrían dar origen a una reacción nuclear, como lo demostró en su momento el físico argentino José Antonio Balseiro. De hecho, lo único que generaba el “reactor nuclear” de Richter eran unas chispas enormes, que quizá pudieran impresionar a un lego en la materia, pero que estaban muy lejos de producir la temperatura necesaria para lograr que dos átomos se fusionaran. Richter terminó siendo desenmascarado y removido del proyecto nuclear argentino. Perón, por su parte, quedó en ridículo y con algunos pesos de menos. Una enseñanza que podríamos extraer del episodio de Perón y Richter es que en países como el nuestro, en donde la ciencia ocupa un lugar secundario, la única defensa contra posibles farsantes del estilo de Ronald Richter, que aún hoy en día pudieran andar por ahí, es precisamente construir una sólida tradición científica.",
    "En su “Ensayo sobre el principio de la población”, publicado en 1798, el economista inglés Thomas Robert Matlhus sostuvo que de continuar el ritmo de incremento geométrico de la población mundial que se observaba en ese momento, en algún punto se llegaría a una catástrofe por falta de alimentos, los cuales crecían a un ritmo aritmético; esto es, de manera mucho más lenta que la población. Matlhus no pensaba, por supuesto, que la población del mundo podría crecer geométricamente de manera indefinida, sino que en algún momento su ritmo de crecimiento se detendría por diversas causas, tales como hambrunas, guerras, epidemias, etc. De esta manera, tenía la visión de un mundo permanentemente en lucha por lograr la supervivencia.    Thomas Malthus notó hace 200 años una diferencia fundamental entre los crecimientos geométrico y aritmético, que aún ahora a veces perdemos de vista a pesar de las graves consecuencias que nos puede acarrear. Un ejemplo típico de crecimiento geométrico nos lo da el número de bacterias en un cultivo biológico, el cual se duplica a un ritmo constante mientras haya suficiente provisión de alimento. No es difícil entender por qué el número de bacterias debe crecer de este modo si tomamos en cuenta que cada una de ellas se divide en dos en un cierto tiempo característico. De este modo, partiendo de una bacteria, después de transcurrido este tiempo tendremos dos, las cuales a su vez se convertirán en cuatro y éstas en ocho, y así de manera sucesiva. Por supuesto que cuando los microorganismos no dispongan de sustento suficiente, el crecimiento geométrico no podrá ser mantenido y se desacelerará. Algo que crece aritméticamente, en contraste, se incrementa por una cantidad fija en cada intervalo de tiempo. Un ejemplo de esto es el kilometraje que marca el medidor de un automóvil a medida que se desplaza en una carretera a velocidad constante.   Una cosa que podemos decir del crecimiento geométrico es que es muy engañoso. Pensemos, por ejemplo, en la fábula del rey que queriendo recompensar a quién inventó el juego de ajedrez le ofreció lo que le pidiera. La solicitud fue por la cantidad de arroz necesaria para llenar un tablero de ajedrez en una progresión geométrica; es decir, colocando un grano de arroz en el primer cuadro del tablero, dos en el segundo, cuatro en el tercero y así de manera sucesiva hasta completar los 64 cuadros del tablero. Los encargados de cumplir el deseo no tuvieron por supuesto ninguna dificultad para colocar un grano de arroz en el primer cuadro, ni dos en el segundo, ni cuatro en el tercero. Si consideramos que cada gramo de arroz hay unos 40 granos, el cuadro número ocho al final de la primera línea requirió poco más de 3 gramos y el cuadro 16 al final de la segunda cerca de un kilogramo, lo cual, por supuesto, tampoco causó problemas. En el cuadro 29 en la cuarta línea, sin embargo, ya la cuenta iba en casi siete toneladas, mismas que se duplicaron en el cuadro 30 y cuadruplicaron en el 31. En este punto los encargados de acarrear el arroz debieron empezar a sospechar que algo iba mal, cosa que habrían podido corroborar haciendo el cálculo de cuanto arroz necesitarían para el último cuadro: ¡más de 200,000 millones de toneladas!, algo por supuesto imposible de satisfacer.Hay muchas cantidades importantes que crecen de manera geométrica. Tenemos, por ejemplo, que la población mundial en los siglos XVIII y XIX creció geométricamente, duplicándose cada 150 años -lo que dio sustento a la tesis de Malthus-. En otro ejemplo, se estima que el número de infectados de la actual epidemia de influenza en México, creció geométricamente en los últimos días de abril pasado con un periodo de duplicación cercano a una semana. En un ejemplo en otro orden, el número de transistores en los microcircuitos empleados en las computadoras se duplica cada dos años. Este crecimiento ha sido observado a lo largo de los últimos 50 años y es conocido como la ley de Moore. El crecimiento geométrico puede en algunos casos tener consecuencias positivas. La ley de Moore, por ejemplo, ha incrementado de manera sostenida la capacidad de las computadoras al mismo tiempo que ha disminuido su precio, al grado que en la actualidad una gran proporción de la población mundial tiene acceso a computadoras, cuya capacidad crece, además, de manera geométrica. Esto ha llevado a una revolución tecnológica con implicaciones sociales profundas.Por otro lado, el crecimiento geométrico puede también resultar peligroso por engañoso. En efecto, una cantidad que crece geométricamente puede parecer inofensiva por un cierto tiempo, como nos lo enseña la fábula del arroz y el tablero de ajedrez, y de manera repentina convertirse en un problema. Un ejemplo claro nos lo da la epidemia de influenza, que a lo largo de las primeras semanas o meses de gestación, cuando tenía un tamaño pequeño, no produjo alarma pública, situación que sí se dio, y de manera abrupta, en la última semana de abril. Hay quienes sostienen que los humanos tendemos a pensar de manera aritmética y no geométrica y que por lo mismo a veces cometemos errores de juicio cuando evaluamos las consecuencias de un crecimiento geométrico. Esto nos indica que debemos estar alerta, pues el número de situaciones en la que se presenta una cantidad que crece geométricamente es quizá mayor al que nos imaginamos en primera instancia. Esto no vaOtro ejemplo nos lo proporcionan los problemas de tráfico vehicular que sufre la ciudad de San Luís Potosí. Aunque no hemos encontrado estadísticas del incremento en el número de automóviles en las últimas décadas, es claro que la tasa de incremento de automóviles sobrepasó con mucho al crecimiento de la capacidad de calles y avenidas para canalizar el tráfico creciente, situación que hizo crisis de manera más o menos abrupta hace algunos años.",
    "Debido al decremento en la mortalidad infantil, producto de la invención de los antibióticos y de la mejora en las condiciones de higiene, en la segunda mitad del siglo XX la población mundial cambió de forma marcada el ritmo de crecimiento que había mantenido desde la Edad Media. Así, entre 1950 y 2009 la población del mundo creció en más de un 150 %, hasta llegar a la cifra actual de 6,800 miles de millones en números redondos. En este mismo lapso de tiempo, la población de México se multiplicó por un factor mayor a  4, al pasar de unos 25 millones de personas en 1950 a más de 110 millones en 2009. México, además, estuvo caracterizado en ese periodo por procesos de migración del campo a la ciudad, lo que hizo que la población de los centros urbanos más importantes creciera por factores todavía más grandes. A manera de ejemplo, San Luís Potosí, que en conjunto con Soledad de Graciano Sánchez constituye la décima concentración urbana en el País, creció alrededor de 600 % entre 1950 y 2009. El crecimiento poblacional acelerado de los centros urbanos en México ha demandado de toda clase de servicios en volúmenes cada vez más grandes. Entre estos servicios se cuenta el agua para uso doméstico e industrial que, en el caso de San Luís Potosí y otras ciudades del centro de México, mayormente se extrae del subsuelo. Se reconoce, por otro lado, que muchos acuíferos que abastecen centros urbanos en el País, entre los que se encuentra la ciudad de  San Luís Potosí, están sobreexplotados; es decir, se les extrae más agua que la que reciben como recarga por medios naturales. Una consecuencia dramática de la sobreexplotación del acuífero de San Luís Potosí ha sido la aparición de grietas en diversos puntos de la ciudad. Estas grietas, que se generan por la compactación del terreno debido a la disminución del volumen del agua en el subsuelo, han estudiadas por más de una década por investigadores del Instituto de  Geología de la Universidad Autónoma de San Luís Potosí. Se tienen identificadas una gran cantidad de grietas, cruzando algunas de ellas por el centro de la ciudad y poniendo en peligro edificios históricos, como el Museo de la Máscara, la Iglesia del Espíritu Santo, el Museo Federico Silva, y el Museo Regional Potosino y la Capilla de Aranzazú.De acuerdo con el Dr. Rafael Barboza Gudiño, Director del Instituto de Geología de la UASLP, las grietas son producidas por la compactación irregular del terreno al extraer grandes cantidades de agua. Esta compactación, a su vez, es debida a las irregularidades del lecho rocoso sobre el que se asienta la ciudad. De este modo, en áreas en las que el lecho rocoso está a una profundidad uniforme, la extracción de agua produce, en condiciones ideales, un asentamiento también uniforme. En contraste, en áreas en las cuales existe una variación en la profundidad de dicho lecho, el terreno se hundirá más en aquellos puntos en los que la profundidad sea mayor. Aquellas localizaciones en las que haya un cambio abrupto en la profundidad del lecho rocoso serán propicias para la aparición de grietas. El asentamiento del terreno urbano por la sobreexplotación de un acuífero no es exclusivo de la ciudad de San Luís Potosí. Por el contrario, este problema lo encontramos también en otras ciudades del centro de México entre las que se cuentan Aguascalientes, Celaya, Torreón, Toluca, Morelia y Querétaro, y por supuesto en la Ciudad de México, que lo ha sufrido desde finales del siglo XIX y que le está produciendo en la actualidad hundimientos  a una razón de varios centímetros por año. La sobreexplotación de los acuíferos en centros urbanos no solamente pone en peligro edificios  y construcciones, sino que también afecta a la red subterránea de distribución de agua y al sistema de drenaje, entre otros servicios subterráneos. El hundimiento del terreno puede llevar a la fractura de los conductos de agua, con el consecuente desperdicio de líquido e intensificación de las demandas sobre el acuífero. Puede llevar también a la fractura de tubos de drenaje lo que, además del costo que significa repararlos, tiene el potencial de contaminar el manto acuífero con desechos orgánicos. Aunque presentes en todo el mundo, los asentamientos de terreno en áreas urbanas por la sobreexplotación de acuíferos subterráneos son particularmente agudos en países no desarrollados, los cuales, o no cuentan con reglamentaciones adecuadas sobre extracción de agua, o bien, si estas reglamentaciones existen, no son observadas puntualmente. En San Luís Potosí hay evidencias científicas que indican que el acuífero de la ciudad está sobreexplotado. Es posible que estas evidencias, expresadas en lenguaje científico, sean de difícil comprensión a los legos en la materia. Hay, sin embargo, una evidencia visual -las grietas-, que todo mundo puede entender y que es una manifestación clara de que algo está mal y que si no tomamos la acciones adecuadas la ciudad de hundirá literalmente ante nuestros ojos.",
    "El pasado lunes 27 de abril el Presidente Barack Obama pronunció un discurso durante el congreso anual de la Academia Nacional de Ciencias de los Estados Unidos en el que delineó su política para impulsar la ciencia estadounidense. Obama expresó preocupación por la fuerte competencia científica que están experimentando los Estados Unidos por parte de otros países, y decidió dar a la ciencia norteamericana un impulso similar, o mayor, al que tuvo en la década de los sesentas cuando el Presidente Kennedy estableció como un objetivo nacional la puesta de un hombre en la superficie de la Luna y su regreso sano y salvo a la Tierra.  Aunque en términos del porcentaje del Producto Interno Bruto (PIB) invertido en Investigación y Desarrollo (IyD) los Estados Unidos no es el país que ocupa el primer lugar  -éste corresponde a Suecia que invierte aproximadamente el 3.8 % de su PIB en actividades de IyD-, en términos absolutos sí ocupa el liderazgo de manera abrumadora. Los Estados Unidos invierten en IyD alrededor de 350,000 millones de dólares, lo que representa aproximadamente el 2.6% de su PIB. La administración de Obama planea incrementar hasta el 3% este último porcentaje, con lo que doblará el presupuesto de la Fundación Nacional de Ciencias y del Instituto Nacional de Estándares y Tecnología, así como de la Oficina Científica del Departamento de Energía. Un aspecto que llama la atención es el énfasis que puso en la investigación científica básica, la cual calificó como “increíblemente importante” para el futuro de los Estados Unidos. El proyecto de energía será el proyecto científico más importante de la administración de Obama, el cual pretende doblar en pocos años la capacidad del país de generación de energía renovable y reducir 80% la contaminación atmosférica de dióxido de carbono en el año 2050. Los planes son invertir 150,000 millones de dólares en investigación en energía en los próximos 10 años. Otras áreas notables a impulsar son las de la salud y el medio ambiente.La educación fue otro de los puntos contemplados por el Presidente Obama, expresando preocupación por el pobre desempeño de los estudiantes estadounidenses de secundaria en las pruebas PISA de la OCDE. En particular resaltó el hecho de que ocuparon el lugar 25 en matemáticas y el 21 en ciencias entre los treinta miembros de la OCDE. La deficiente educación que reciben los estudiantes estadounidenses en la escuela secundaria, apuntó, es una de las razones por las que ha disminuido su interés por las carreras en ciencias e ingeniería.  Con respecto a esto último, se hace notar que uno de los aspectos característicos del sistema científico y tecnológico norteamericano es que está integrado en buena medida por investigadores no nacidos en los Estados Unidos. Esto, por un lado, puede verse como una medida de la fortaleza del país, que es capaz de atraer a científicos que se encuentran entre los mejores del mundo; es, además, consecuente con el hecho que los Estados Unidos es un país de inmigrantes. Por otro lado, es también un signo de debilidad indicativo del poco atractivo que una carrera en ciencia o ingeniería tiene entre los jóvenes estadounidenses. En efecto, según datos del Instituto Americano de Física, el ingreso de estudiantes norteamericanos a los programas de maestría en física en los Estados Unidos es en la actualidad un tercio del que era al inicio de los años setenta. De la misma manera, aunque no con una caída tan dramática, encontramos que el ingreso de estudiantes norteamericanos a programas doctorales disminuyó en un 25% en el mismo lapso de tiempo. Esta caída, no obstante, fue en buena medida compensada con el ingreso de estudiantes extranjeros que constituyen en la actualidad casi el 50% de la matrícula.   En consonancia con lo anterior, según datos de la Fundación Nacional de la Ciencia, en 2005 cerca del 50% de los grados doctorales otorgados por las universidades norteamericanas en las áreas de ingeniería, matemáticas, ciencias de la computación, física y economía, lo fueron a estudiantes extranjeros. Igualmente, se encuentra, según la misma fuente, que la fuerza de trabajo actual en ciencia e ingeniería está integrada en más de un 40% por profesionales no nacidos en el País.Los Estados Unidos, el país económicamente más poderoso del mundo, está tomando providencias para enfrentar la competencia con países de crecimiento económico acelerado como los son China y la India, entre otros. Esto contrasta con la situación que prevalece en México, en donde a la ciencia no se le da el lugar que le corresponde. Aunque ha habido indudablemente progreso en los últimos 40 años, la ciencia mexicana no tiene una dimensión acorde con el tamaño de la economía del País, que dedica escasamente un 0.5 % del PIB a actividades de IyD. Este porcentaje es un quinto del norteamericano, casi siete veces menor que el japonés, y la mitad del brasileño. En educación no estamos mejor, y en las pruebas PISA México ocupó el último lugar entre los 30 países de la OCDE.La importancia que la ciencia y la tecnología tienen en el mundo actual se ha hecho dramáticamente patente en la última semana con la crisis de salud que enfrentamos. Esperemos que esta circunstancia nos haga receptivos a la lección que los Estados Unidos y su plan de impulso a la ciencia nos están dando.",
    "La Revolución Industrial, que tuvo su origen en las últimas décadas del siglo XVIII, marca el inicio de un periodo de grandes cambios y desarrollos tecnológicos que transformaron drásticamente al mundo. Un invento de la mayor relevancia, que jugó un papel central en la Revolución Industrial, fue la máquina de vapor de James Watt, la cual proporcionó la fuerza motriz necesaria para mecanizar la industria e incrementar su productividad. La máquina de vapor dio igualmente origen al buque de vapor y al ferrocarril, que revolucionaron la industria del transporte en la segunda mitad del siglo XIX. Ese siglo fue también testigo de la aparición del refrigerador, que resolvió el milenario problema de la conservación de los alimentos, y de la luz eléctrica que nos liberó de la oscuridad de la noche. El siglo XX, por su parte, nos trajo el motor de combustión interna que a su vez posibilitó el desarrollo de las industrias del automóvil y de la aviación, que nos dieron una movilidad sin precedente. Ciertamente, las tecnologías desarrolladas en los dos últimos siglos han contribuido a elevar de manera notable nuestra calidad de vida. Se han eliminado o controlado, por ejemplo, un gran número de enfermedades infecciosas para las que antes no había cura y como resultado nuestra esperanza de vida al nacer es ahora el doble de lo que era hace cien años -debido fundamentalmente a un abatimiento de la mortalidad infantil-. Otra medida de nuestro bienestar es el tiempo que dedicamos a actividades recreativas, el cual ha aumentado de forma considerable al grado que la industria del entretenimiento -sin incluir a la televisión, el cine y los videojuegos- ocupa en la actualidad en los Estados Unidos alrededor de dos millones de personas. Ahora también viajamos mucho más, en algunos casos a lugares lejanos que antes nos tomaba semanas alcanzar y que ahora por avión están a menos de un día de viaje. Nuestras facilidades actuales de transportación, de iluminación nocturna de espacios públicos y privados, de acondicionamiento climático de casas y edificios, así como el crecimiento económico acelerado que significó la producción industrial en masa, han tenido su costo en energía, cuyo consumo mundial ha crecido de manera continua desde el inicio de la revolución industrial. La relación bienestar-energía se hace patente por el hecho que una medida de la calidad de vida en un determinado país es precisamente su consumo de energía per cápita.El incremento en la calidad de vida en los últimos dos siglos no ha sido, sin embargo, uniforme y existen grandes diferencias entre países. La mortalidad infantil en África, por poner un ejemplo, es unas 15 veces mayor que la correspondiente mortalidad infantil en la Unión Europea. De manera concurrente, el consumo de energía per cápita muestra también grandes contrastes. Así, el grupo de 36 países con mayor nivel de ingreso, que incluye a los Estados Unidos, Canadá, Australia, Japón y a los países más ricos de la Unión Europea, tienen un consumo per cápita de energía que es diez veces más grande que el correspondiente consumo de los 37 países con menores ingresos, entre los que se incluyen un buen número de países africanos. La energía que ha impulsado el desarrollo del mundo industrializado en los últimos dos siglos se ha obtenido en un porcentaje mayoritario -80% en la actualidad- de la combustión de carbón, petróleo y gas natural. Esta práctica ha incrementado considerablemente la concentración de bióxido de carbono en la atmósfera, ocasionado el bien conocido fenómeno de calentamiento global que amenaza con grandes desastres climáticos. Para prevenir el desastre ecológico los países industrializados están planeando medidas para frenar las emisiones de gases de invernadero a mediano plazo. En el caso de los Estados Unidos, que es el segundo país -después de Canadá- con mayor índice de consumo de energía per cápita, y que produce más del 20 % de los gases de invernadero a nivel global, una reducción significativa podrá posiblemente llevarse a cabo sin sacrificar su nivel de vida. Esto resulta claro si consideramos que los países europeos más ricos tienen un nivel de vida comparable al de los Estados Unidos con la mitad de consumo per cápita de energía. Siguiendo el mismo razonamiento, los países europeos tendrán que hacer un esfuerzo mayor para hacer un uso eficiente de la energía sin comprometer su nivel de vida.No es claro, por otro lado, cual será el curso de los países en desarrollo que no han alcanzado un nivel adecuado de consumo energético. Si en 200 años de desarrollo industrial -sin preocupación alguna por los gases de invernadero emitidos a la atmósfera en cantidades crecientes- no existieron mecanismos eficaces para lograr un crecimiento económico armónico que no produjera las contrastantes diferencias entre países que observamos en la actualidad, con las preocupaciones energéticas actuales la situación probablemente empeorará, y los países en desarrollo tendrán mayores restricciones para crecer económicamente. Esto no será el caso de China y la India que se convertirán sin duda en dos de las potencias económicas del siglo XXI, pero si lo será posiblemente de países que no cuenten con suficientes recursos científicos y tecnológicos propios para autodefinir su destino.",
    "La invención de la luz eléctrica en la segunda  mitad del siglo XIX cambió por completo el estilo de vida en las zonas urbanas, permitiendo la realización de actividades nocturnas que antes estaban reservadas a las horas de sol. Un ejemplo de este tipo de actividades, entre muchas otras, son los eventos deportivos nocturnos, que hoy en día son comunes y que hubieran sido impensables hace 150 años. En relación a esto, es ilustrativo recordar que el primer partido nocturno de béisbol de grandes ligas se llevó a cabo en Cincinnati, Ohio, en el año de 1935, entre los equipos Rojos y Filis, y que el evento generó mucho escepticismo entre los demás equipos de la liga que rechazaron realizar el experimento en sus estadios. Uno de los hechos clave para el desarrollo de la luz eléctrica en el siglo XIX como una industria exitosa fue la aparición en 1879 de la lámpara incandescente de Thomas Alva Edison. En la actualidad, sin embargo, 130 años después, las condiciones mundiales han cambiado -fundamentalmente en relación al problema del calentamiento global- y la lámpara incandescente está sujeta a muchas críticas por su baja eficiencia. En efecto, encontramos que la mayor parte de la radiación que emite dicha lámpara es infrarroja, con el resultado de que solamente entre el 2 % y el 3 % de la energía eléctrica que consume se convierte finalmente en energía luminosa. Los críticos de las lámparas incandescentes apuntan que, ya que un 10 % la energía eléctrica consumida se emplea para iluminación, se obtendría un substancial ahorro de energía sustituyendo a dichas lámparas por fuentes de luz más eficientes, con el consecuente impacto ambiental positivo.Entre los candidatos para sustituir a las lámparas incandescentes se encuentran las lámparas fluorescentes. La luz en este tipo de lámparas, que contienen vapores de mercurio en su interior, se genera en dos etapas. En un primer paso, el mercurio se excita por medio de una descarga eléctrica produciendo luz ultravioleta -que no podemos ver-, la cual es absorbida en el material que recubre el tubo por su parte interior; enseguida, como respuesta, el material del recubrimiento emite luz visible que es la que finalmente escapa de la lámpara. Las lámparas fluorescentes tienen eficiencias luminosas en el rango del 7% al 10%, es decir, cuatro o cinco veces mayores  que la eficiencia de las lámparas incandescentes. Las primeras, sin embargo, tienen también sus críticos, quienes objetan la contaminación ambiental con mercurio que producen una vez que son desechadas.Una segunda opción de reemplazo de las lámparas incandescentes la constituyen las lámparas basadas en los diodos emisores de luz, también conocidos como LEDs por sus siglas en inglés. Los LEDs son dispositivos de “estado sólido”, que son fabricados con materiales “semiconductores”. La tecnología de los LEDs es pariente cercana de aquella empleada en la fabricación de microprocesadores y memorias de computadora, aunque usando materiales semiconductores distintos. Comparados con las lámparas incandescentes y fluorescentes, los LEDs son mucho más compactos, duraderos,  resistentes al impacto y con eficiencias luminosas comparables, y con una clara tendencia al crecimiento, a las de las lámparas fluorescentes. Los LEDs, además, tienen características únicas, que las hacen diferentes de las lámparas eléctricas convencionales. Entre estas ventajas se encuentra la posibilidad de controlar la tonalidad de la luz emitida por la lámpara como se verá enseguida. Sabemos –desde tiempos de Isaac Newton 350 años atrás- que si combinamos colores en las proporciones adecuadas es posible obtener luz blanca. Sí, por otro lado, modificamos estas proporciones obtendremos una tonalidad que dependerá del grado de modificación que hagamos. La luz del Sol, por ejemplo, tiene al amarillo y al verde como colores dominantes y de aquí resulta su tonalidad amarilla. Una lámpara fluorescente, por otro lado, tiene una tonalidad ligeramente azul, también por su composición de colores. De este modo, integrando en una lámpara tres LEDs con colores, rojo, verde y azul, es posible controlar la tonalidad de la luz que emite dicha lámpara cambiando las intensidades de los haces de luz de colores de los LEDs en una manera predeterminada. Esta posibilidad no existe, o no es práctica de realizar, en las lámparas fluorescentes e incandescentes.  Hoy en día encontramos a los LEDs en un buen número de aplicaciones: semáforos para el control de tráfico urbano, pantallas para el despliegue de información, fuentes de luz fría para aplicaciones médicas y de otro tipo, etc.  Los LEDs, sin embrago, no han irrumpido de manera masiva en el mercando de la iluminación de espacios públicos y privados. Esto es debido a factores tanto económicos –altos precios- como tecnológicos. En el mediano plazo, sin embargo, los LEDs con seguridad dominarán las aplicaciones masivas de iluminación artificial, no solamente por sus ventajas y superioridad tecnológica sobre otras fuentes de luz eléctrica, sino también por los problemas de contaminación de mercurio de las lámparas fluorescentes, que hoy en día son su más fuerte competencia.",
    "En días pasados nos enteramos por la prensa de la oferta que hizo el expresidente Vicente Fox a la Federación Mexicana de Fútbol para organizar en las instalaciones del llamado Centro Fox un seminario a fin de “convertir a la Selección Nacional en una triunfadora”. El seminario sería impartido por una organización que se especializa en métodos educativos para incentivar la superación personal, que Fox enfatizó son “científicos” y no son “charlatanería”.  Sin calificar a los métodos de la organización de referencia como científicos o no científicos -por desconocerlos-, sí podemos apuntar que el calificativo “científico” se emplea frecuentemente de manera bastante liberal. Por ejemplo, no es algo extraño toparnos con pruebas “científicas” de visitas de seres extraterrestres a nuestro planeta, o de dietas para adelgazar que se anuncian como probadas científicamente pero que no siempre pueden acreditarlo a cabalidad. Por otro lado, el amplio uso que se da al término “ciencia” y a todos sus derivados, es un indicativo de la enorme importancia que la actividad científica y la tecnología que de la misma resulta han adquirido en la actualidad. El desarrollo científico del último siglo ha dado origen a un progreso sin precedentes, traducido, entre muchos otros aspectos, en un incremento notable en nuestro promedio de vida, en una potenciación de nuestras capacidades intelectuales a través de la red Internet de telecomunicaciones y en un desarrollo industrial y económico acelerado.La ciencia, tal como la conocemos, tuvo sus inicios en la Europa de los siglos XVI y XVII, y en su momento constituyó una revolución intelectual de grandes dimensiones que se sobrepuso al pensamiento medieval europeo, influenciado fuertemente por las ideas aristotélicas. Una de las características más sobresalientes del método científico es su demanda de validar, mediante la experimentación, cualquier afirmación que se haga. Así, la aseveración de que todos los objetos sin importar su masa caen con la misma velocidad, no tuvo una validez científica sino hasta que se demostró mediante experimentos. Estos experimentos, se menciona frecuentemente, fueron llevados a cabo por Galileo Galiei arrojando objetos de diferente masa desde lo alto de la torre de Pisa y midiendo cuidadosamente el tiempo que tardaban en llegar al suelo. Hoy en día, se acepta ampliamente que, despreciando la resistencia del aire, todos lo objetos caen efectivamente a la misma velocidad sin importar su masa. Al margen de su validez histórica, el episodio de Galileo Galilei y la torre de Pisa tiene un gran simbolismo, pues ataca directamente a los métodos de investigación de la naturaleza puestos en boga por Aristóteles, quién afirmaba que la velocidad que adquieren los objetos al caer es proporcional a su masa. El que Aristóteles daba poco crédito a la experimentación lo demuestra su afirmación de que las mujeres tienen menos dientes que los hombres. Como lo apuntaba el filósofo y matemático británico Bertrand Russell, Aristóteles hizo esta afirmación a pesar de haber tenido dos mujeres, no habiéndosele ocurrido contarle los dientes a ninguna de ellas.       Otra característica fundamental del método científico es el de la generalización; es decir, la obtención de leyes generales a partir de observaciones particulares. Así, Galileo Galilei  pudo generalizar que todos los objetos caen a  la misma velocidad, a pesar de que solamente hizo experimentos con algunos pocos de ellos y, por supuesto, no con todos los posibles. En este punto también hay discordancia con lo sostenido por Aristóteles, quién afirmaba que los objetos celestiales tienen una esencia diferente a la de los objetos terrestres y por lo tanto están sujetos a leyes físicas distintas. Así, cualquier ley física que pudiéramos descubrir para los objetos en la Tierra no sería válida para, por ejemplo, los planetas. Una consecuencia natural de los resultados obtenidos empleando el método científico es la posibilidad de desarrollar tecnologías de gran sofisticación, más allá de la que es factible obtener mediante simples procedimientos de prueba y error. Una demostración de esta capacidad nos la proporcionan las asombrosas tecnologías actuales, desde la microelectrónica, la computación y las telecomunicaciones, hasta las modernas terapias genéticas y técnicas quirúrgicas para curar enfermedades. Ninguna de las tecnologías modernas hubiera sido posible sin el concurso del conocimiento científico.    Volviendo al punto con el que abrimos este artículo, para validar la afirmación de que los métodos propuestos para convertir a la selección nacional de fútbol en una triunfadora tienen una base científica, habría que poner bajo el escrutinio estricto de expertos en la materia el contenido del seminario de referencia, así como los resultados obtenidos con anterioridad con otros deportistas -los cuales, de hecho, se mencionan en las notas periodísticas-. Asumiendo que nuestros seleccionados tomasen el seminario, una prueba sólida de la validez científica del mismo sería que en el próximo campeonato mundial de fútbol, -en caso de calificar- nuestro País tuviera un desempeño fuera de lo común.",
    "Uno de los tópicos de mayor actualidad en el campo de la ciencia y la tecnología es sin duda el de las energías renovables. Esto, debido a que los combustibles fósiles empleados en la generación de energía son no renovables y por lo tanto tendrán un fin inexorable en algún momento futuro, además de que su uso intensivo está incrementando la concentración de gases de invernadero en la atmósfera con el consecuente incremento en la temperatura de la superficie terrestre.         La generación de energía a nivel mundial proviene en aproximadamente un 66 % de la quema de combustibles fósiles: petróleo, gas natural y carbón, por lo que la sustitución de los mismos en un porcentaje significativo por fuentes renovables no contaminantes representa un reto formidable. Existen varias energías renovables que podrían contribuir en esta dirección, entre la que se encuentran la solar, la del viento, la hidroeléctrica, la geotérmica y la asociada a las mareas.De las diferentes alternativas de energías renovables, la que pareciera ser la opción natural a escoger es la solar, particularmente aquella que emplea celdas solares que transforman directamente la energía del Sol en energía eléctrica. En efecto, tenemos por un lado que la radiación solar que incide sobre la superficie de la Tierra excede con mucho nuestros requerimientos energéticos y solamente sería necesario aprovechar una parte minúscula de la misma para satisfacerlos. Por otro lado, la energía solar es una fuente limpia no contaminante, que no emite gases de invernadero a la atmósfera y que es además prácticamente inagotable. En la práctica, sin embargo, las celdas solares no han constituido la primera opción entre las energías renovables, debido fundamentalmente a factores económicos que superan todas las consideraciones técnicas. Así, se encuentra que el costo de generación de electricidad por kilowatt-hora de una celda solar es unas 5 veces mayor al de, por ejemplo, la energía del viento. Como resultado, encontramos que mientras que esta última contribuye con un 1.5 % al total mundial de energía eléctrica generada, la celdas solares solamente aportan el 0.08 %, es decir, unas veinte veces menos. La otra fuente renovable significativa es la energía geotérmica que constituye el 0.5 % del total mundial generado, porcentaje que es también significativamente mayor al correspondiente a la energía solar.Hay que ser justos, sin embargo, con las celdas solares, pues si bien en términos globales su contribución a la generación mundial de energía eléctrica es significativamente menor que la correspondiente contribución de las energías eólica y geotérmica, hay ventajas intrínsecas a las celdas solares que las hacen únicas. Estas ventajas tienen que ver con la escalabilidad de una instalación solar, que lo mismo puede tener una potencia de kilowatts, adecuado para una casa-habitación, que decenas de megawatts, suficientes para energizar a un pueblo pequeño. Las celdas solares trabajan, además, de manera silenciosa y sin partes móviles que lleven a un desgaste mecánico. Así, una vez instalado, una planta de celdas solares se espera que funcione por al menos 20 años, sin más mantenimiento que el de limpiar periódicamente el polvo acumulado en su superficie.Hay, además, quienes ven a las celdas solares proporcionando por si mismas un porcentaje mayoritario de la energía eléctrica mundial. Por ejemplo, en un artículo aparecido en la revista Scientific American el pasado año -“A solar grand plan”, K. Zweibel, J. Mason y V. Fthenakis, 2008- se delinea un plan para generar por medio de celdas solares un 69 % de la energía eléctrica que necesitaran los Estados Unidos en el año 2050. El plan requeriría cubrir una superficie de aproximadamente 50,000 kilómetros cuadrados con celdas solares -un área de aproximadamente 220 por 220 kilómetros-, en la región de mayor insolación en el suroeste de los Estados Unidos. Para almacenar la energía generada y usarla en las horas en que no hay sol, los autores del artículo de referencia proponen bombear aire a presión en cavernas durante el día y emplear dicho aire para mover una turbina y un generador de electricidad durante la noche. Sin duda, en el futuro las energías renovables no contaminantes jugarán un papel cada vez más significativo en la generación mundial de energía, y si bien en la actualidad la energía eólica seguida de la geotérmica ocupan los lugares preponderante en este respecto, con seguridad desarrollos tecnológicos futuros en el área de las celdas solares harán que estas adquieran una importancia cada vez mayor y eventualmente dominen la escena. La tecnología de celdas solares dominante en la actualidad es la basada en el silicio cristalino, que acapara el 90 % del mercado mundial de módulos solares. El silicio está, no obstante, en intensa competencia con tecnologías basadas en otros materiales y no queda claro cual de todas saldrá finalmente triunfante. Lo que si es seguro es que la simplicidad conceptual, flexibilidad y ausencia de contaminación del esquema de producción de energía eléctrica por medio de celdas solares, aunada a la abundancia de la energía solar, dará mucho de que hablar en el futuro.",
    "En 1859, hace 150 años, Carlos Darwin publicó su famoso libro “El origen de la especies”, y con esto desató una controversia que dura hasta nuestros días. Dicha controversia gira en buena medida en torno a la teoría de la evolución, -universalmente aceptada en el medio científico-, según la cual todas las especies animales evolucionaron a partir de un origen común, y por lo tanto están emparentadas en mayor o menor medida. Aplicada al Homo sapiens, la teoría de la evolución ha tenido un enorme poder de polarización y ha generado posiciones fuertemente encontradas e innumerables debates.  En los Estados Unidos la controversia se ha dado como un enfrentamiento entre “creacionistas” y “evolucionistas” y se ha centrado en la enseñanza de las ideas de Darwin en las escuelas públicas norteamericanas. Los creacionistas rechazan la idea de la evolución de las especies y sostienen que estas fueron originadas en un único acto de creación divina, o  por un “diseño inteligente”, obra de un ente superior, como se sostiene en la actualidad. La confrontación creación-evolución se inició en el año de 1925, cuando la legislatura del Estado de Tenesí aprobó lo que se conoce como la ley Butler. De acuerdo con esta ley, era ilegal enseñar en escuelas públicas del Estado “cualquier teoría que niegue la historia de la Creación Divina del hombre tal como se enseña en la Biblia y enseñar en su lugar que el hombre desciende de animales de orden más bajo”. El lanzamiento del primer satélite artificial, el Sputnik, por la Unión Soviética en el año de 1957, disparó señales de alarma sobre el bajo nivel que la enseñanza de las ciencias tenía en los Estados Unidos y el peligro que esto implicaba en el contexto de la Guerra Fría. En estas circunstancias, en el año de 1968 la Suprema Corte de Justicia norteamericana declaró inconstitucional la prohibición de enseñar las ideas evolucionistas en las aulas.  En los inicios de la década de los 80, coincidiendo con arribo de Ronald Reagan a la presidencia de los Estados Unidos, los creacionistas reanudaron su embestida en contra de la enseñanza de la evolución en escuelas públicas. Debido a que no era ya posible intentar prohibirla, el movimiento creacionista buscó en su lugar establecer que a ambas posturas, creacionista y evolucionista, se les diera el mismo tratamiento. Como resultado, en el año de 1981 la legislatura de Louisana aprobó una ley en este sentido y lo mismo hicieron otros estados. En respuesta, en 1987 la Suprema Corte resolvió que el creacionismo no es una ciencia sino una religión y que por lo tanto, darle el mismo espacio que al evolucionismo en las escuelas públicas, violaba el principio de separación iglesia-estado.Ante esta situación, el movimiento creacionista cambió de estrategia y evitó usar en sus iniciativas cualquier alusión al creacionismo y a Dios como creador. En vez de esto, originó el término “Diseño inteligente” como una alternativa a la evolución y se dio a la tarea de presentar a la teoría de la evolución como algo no definitivo y lleno de fallas, bajo el supuesto de que la imperfección de dicha teoría apoya de manera tácita las ideas creacionistas. El último capítulo de la confrontación creación-evolución se dio esta última semana en Texas, cuando el consejo de educación estatal se reunió para decidir sobre el contenido de los libros de texto para los próximos 10 años. El punto a discutir fue si las “debilidades y fortalezas” de la teoría de la evolución deben enseñarse en las escuelas públicas. La controversia surgió debido a que los críticos del creacionismo consideran que esto constituye una vía para la introducción de las ideas creacionistas entre los alumnos. En una decisión dada a conocer el pasado 27 de marzo, el consejo de educación votó por no enseñar las debilidades de la teoría de la evolución, aunque estableció que los maestros deberán poner bajo escrutinio de los estudiantes todas las facetas de las teorías científicas. Esto último ha sido visto por los críticos del Diseño inteligente como un camino para la penetración de las ideas creacionistas en la educación.Sorprende que en los Estados Unidos, que ha prosperado en no poca medida impulsado por su propio progreso científico y por la aplicación del mismo al desarrollo de tecnología de gran sofisticación, florezcan movimientos como el creacionista y de hecho subsistan dos universos paralelos. Por un lado, encontramos a un país con un impresionante sistema de universidades que se encuentran entre las mejores de mundo y que son líderes en investigación científica en un gran número de áreas. Por otro, y en marcado contraste, de acuerdo con encuestas realizadas por The Pew Forum on Religion and Public Life,  42% de los norteamericanos no cree en la evolución de las especies, contra el 48 % que sí lo hace. Aun más, dentro de este último grupo solamente un 54% cree en la evolución por selección natural tal como Darwin la propuso, mientras que el 38% considera que se produjo guiada por un poder supremo.                    Dada la perseverancia de los grupos creacionistas y la enorme influencia de la cultura norteamericana en nuestro País, vale la pena estar atentos al desarrollo de la confrontación creación-evolución, a la que poca difusión se ha dado en México.",
    "El próximo  24 de marzo la Organización para la Cooperación y el Desarrollo Económico (OCDE) llevará a cabo en México una evaluación de estudiantes de secundaria, como parte del “Programa Internacional de Evaluación de Estudiantes” (PISA, por sus siglas en inglés). Dicha evaluación se lleva a cabo cada tres años, participando estudiantes de escuelas secundarias de todo el País con una edad de l5 años. La evaluación mide conocimientos, habilidades y actitudes en las áreas de ciencias, matemáticas y lectura. Más que simplemente medir los conocimientos del estudiante en dichas áreas, la evaluación PISA mide su capacidad para emplearlos en la resolución de problemas, capacidad que será relevante para su desempeño futuro en la sociedad de la información, en la que la ciencia y la tecnología juegan un papel preponderante. De manera específica, PISA mide lo que la OCDE llama “Scientific literacy”, término que define como la posesión de conceptos y conocimientos científicos, aunada a la capacidad de usarlos para adquirir nuevos conocimientos, explicar fenómenos científicos y llegar a conclusiones en temas relacionados con la ciencia. PISA igualmente mide “Mathematical literacy”, entendida esta como la capacidad de identificar y entender el papel que las matemáticas tienen en el mundo actual y el empleo de las mismas para tomar decisiones bien fundamentadas, y “Reading literacy”, definida como la capacidad de entender, usar y reflexionar sobre textos escritos para alcanzar metas propias. Se han llevado a cabo pruebas PISA en los años 2000, 2003 y 2006. En todas las ocasiones se evaluaron las tres áreas que comprende el examen: ciencias, matemáticas y lectura. De manera rotativa, sin embargo, se ha hecho énfasis en un área en particular: lectura en 2000, matemáticas en 2003 y ciencias en 2006. En el presente año, el énfasis se pondrá en el área de lectura. En la evaluación PISA 2006, se consideraron 6 niveles en competencia científica. En el nivel 6 un estudiante tiene la capacidad de aplicar de manera consistente información científica e información acerca de la ciencia en situaciones complejas en la vida diaria. En el nivel 1, en contraste, el estudiante tiene un conocimiento científico limitado y la  capacidad de emplearlo solamente en algunas pocas situaciones que le son familiares.  En la prueba del año 2006, los cinco países que alcanzaron la máxima puntuación fueron, en ese orden, Finlandia, Corea del Sur, Canadá, Nueva Zelanda y Holanda. En el otro extremo de la escala, México ocupó el último lugar en cada una de  las tres categorías del examen; le siguieron, en orden ascendente, Turquía, Grecia, Italia y Portugal. Hay que hacer notar, además, que las calificaciones promedio de México estuvieron bastante por abajo del promedio general de todos los treinta países miembros de la OCDE. En cuanto a la distribución de estudiantes por nivel de calificación, en Finlandia más del 60% de los estudiantes se colocó en los niveles 3 y 4, mientras que el 3.9% calificó en el nivel 6 y solamente el 0.5 % tuvo una calificación inferior al nivel 1. En México, en cambio, el 18 % de los estudiantes calificó por abajo del nivel 1 y más del 60% en los niveles 1 y 2.    Los resultados obtenidos por nuestros estudiantes de secundaria son ciertamente decepcionantes, pues implican que una mayoría de ellos no está recibiendo el entrenamiento adecuado para tener éxito como adultos en una sociedad en la que la ciencia y la tecnología juegan un papel primordial. En relación a esto, es quizá válido traer a colación el llamado “Efecto Flynn”, descubierto en la década de los 80 por James R. Flynn, profesor de la Universidad de Otago en Nueva Zelanda. El efecto Flynn se refiere al incremento paulatino en los puntajes de la pruebas de coeficiente intelectual (CI) que se ha dado en muchos países a lo largo del último siglo. Se han aventurado varias hipótesis para explicar este incremento; entre estas se incluyen las de una mejor alimentación, familias con menos hijos y cambios en la educación. Flynn mismo ofreció recientemente una explicación. De acuerdo con esta, el desarrollo científico y tecnológico del último siglo ha transformado la educación de  modo tal que se privilegiado el desarrollo de la capacidad de abstracción en los estudiantes. Así, esta capacidad se ha incrementado de manera paulatina a lo largo del siglo pasado y las pruebas de CI, dirigidas en buena medida a evaluar esta capacidad, han arrojado en consecuencia puntajes paulatinamente crecientes.  El efecto Flynn es motivo de controversia y no es claro cuales son sus implicaciones reales. La explicación ofrecida por Flynn, sin embargo, parece razonable. Asumiéndola cierta, concluiríamos que, dada la relativa pobre presencia de la ciencia y la tecnología México -en comparación con las naciones desarrolladas- nuestro sistema educativo no ha fomentado el desarrollo de la capacidad de abstracción entre los estudiantes al mismo ritmo en que se ha dado en los países avanzados. Estaría aquí quizá una de las explicaciones al pobre desempeño de México en las pruebas PISA.",
    "El término “Perpetuum mobile” o máquina de movimiento perpetuo, hace referencia a un dispositivo o mecanismo que, una vez puesto a funcionar, lo seguirá haciendo por tiempo indefinido sin necesidad de un impulso adicional. Una máquina de este tipo funcionaria y haría un trabajo sin necesidad de combustible, excepto el necesario para ponerla inicialmente a trabajar. El perpetuum mobile ha sido buscado afanosamente desde hace muchos siglos. Durante la Edad Media se idearon un gran número de dispositivos mecánicos compuestos de ruedas, ejes, bandas, péndulos, etc., que según sus autores funcionaban como máquinas de movimiento perpetuo. Incluso un genio como Leonardo da Vinci estuvo interesado en  estas máquinas, aunque parece ser que no creía realmente que funcionaran. En el último par de siglos, en la medida que avanzó el conocimiento científico y la tecnología se hizo más compleja, los diseños de maquinas perpetuas se hicieron correspondientemente más sofisticados, pudiendo incluir, además de partes mecánicas, elementos eléctricos, magnéticos, químicos e incluso nucleares. Una versión actual de un perpetuum mobile -si bien poco sofisticado-, podría ser el arreglo formado por un motor eléctrico que impulsa a un generador de electricidad, el cual a su vez impulsa al motor. De este modo, bastaría con poner a funcionar el motor eléctrico para que el ingenio trabajase sin interrupción y sin necesidad de intervención externa adicional.   Una máquina que funcione sin impulso externo resulta, por supuesto, muy atractiva, pues virtualmente eliminaría el gasto de combustible en actividades industriales y de transporte. Con máquinas de movimiento perpetuo no tendríamos que preocuparnos por nuestro futuro energético, al mismo tiempo que eliminaríamos en gran medida la emisión de los gases de invernadero responsables del calentamiento global y del cambio climático asociado. Las maquinas de movimiento perpetuo, ciertamente, resolverían muchos de los problemas que está actualmente enfrentado nuestra civilización. Desgraciadamente, los móviles perpetuos son una imposibilidad física, pues violan ya sea el principio de conservación de la energía o la segunda ley de la termodinámica, que son leyes físicas firmemente establecidas. Así, no importando lo sofisticado de su diseño, un supuesto perpetuum mobile nunca funcionará como tal. En el ejemplo del motor eléctrico que impulsa un generador de electricidad que a su vez impulsa al motor, aquel nunca generará la potencia necesaria para mantener al arreglo funcionando.Todos los reportes de perpetuum mobile exitosos han sido entonces, por necesidad, fraudulentos, y al respecto hay algunos casos notables. En el siglo XVIII, Johann Ernst Elias Bessler, también conocido como Orfirius, construyó una rueda automotora, según él de movimiento perpetuo. Orfirius era un charlatán de gran habilidad que logró convencer de su invención incluso al prestigiado matemático y filósofo Gottfried Leibnitz, inventor del cálculo infinitesimal juntamente con Isaac Newton. Convenció también a Carlos I de Hesse-Kasssel, quién organizó una demostración de la máquina en su castillo de Weissenstein. Para llevar a cabo las pruebas, la máquina de Orfirius fue colocada y puesta en marcha en una habitación aislada. La puerta de la habitación se cerró con candado, quedando bajo la vigilancia de dos guardas. Después de catorce días, se abrieron las puertas y se encontró a la rueda girando sin disminuir su velocidad. Se repitió la prueba, ahora por cuarenta días, con los mismos  resultados. Carlos I quedó tan impresionado que le extendió a Orfirius un certificado sobre los resultados de las pruebas, mismo que por supuesto empleó en su beneficio. Al final, sin embargo, se descubrió que el supuesto perpetuum mobile no era autónomo, sino que giraba impulsado por una sirvienta de Orfirius escondida cerca de la máquina.   Por otro lado, aunque un perpetuum mobile sea una imposibilidad física, existen dispositivos que en términos prácticos son equivalentes a una máquina de este tipo. Entre estos dispositivos se encuentran aquellos que trabajan a base de energía solar o de energía eólica. En efecto, ya que estas fuentes de energía son prácticamente inagotables, un motor eléctrico impulsado por energía solar trabajará de manera continua empleando una fuente gratuita de energía -en tanto, naturalmente, que las partes móviles no tengan que ser reemplazadas por desgaste-. Por supuesto, para la fabricación de las componentes del sistema -celdas solares, motor eléctrico, acumulador para almacenar energía, cables de conexión, etc.-, fue necesario emplear una cierta cantidad de energía; esta, no obstante,  puede ser incluida como parte del impulso inicial necesario para poner la máquina en funcionamiento. De manera adicional, hay que hacer notar que el tiempo de vida del sistema no es infinito. Así, para propósitos prácticos -aunque no teóricos- una máquina solar o eólica es equivalente a un perpetuum mobile que trabaja con un combustible renovable que para nosotros no representa costo alguno. De máquinas de esta clase -solares, eólicas, geotérmicas, de fusión nuclear, etc.- dependerá, muy probablemente y en buena medida, nuestro futuro energético.",
    "El 17 de febrero de 1600, en la plaza Campo de' Fiori en Roma, murió en la hoguera Giordano Bruno, acusado por la Inquisición romana de ser, herético, impenitente, pertinaz y obstinado. Una de las acusaciones contra Bruno se centraba en su defensa de la cosmología helio-centrista de Nicolás Copérnico, así como en sus propias ideas sobre la multiplicidad de los mundos. Según estas últimas, nuestro sistema solar no es único en el Universo; por el contrario, existen una infinidad de sistemas solares y de planetas similares a la Tierra, capaces, además, de albergar vida inteligente. Para honrar la memoria de Giordano Bruno, a iniciativa de un comité internacional de escritores, historiadores y científicos, entre los que se encontraba Víctor Hugo, en el año de 1889 la municipalidad romana le erigió un monumento en el sitio de su ejecución en Campo de' Fiori.Giordano Bruno nació en el año de 1548 en el Reino de Nápoles. Por su manera libre de pensar y por la costumbre de expresar sus ideas de manera abierta, tuvo una vida -y por supuesto una muerte- muy agitada. A partir de 1576 viajó por de manera extensiva por Europa, inicialmente por varias ciudades de lo que ahora es Italia y después a  Ginebra. Ahí tuvo problemas con los calvinistas por los que fue encarcelado. Una vez liberado se trasladó a Francia, en donde se convirtió en profesor de la Universidad de París, y posteriormente a Inglaterra en donde enseñó en la Universidad de Oxford. Regresó a Venecia en 1592 invitado por el noble veneciano Giovanni Mocenigo, quién deseaba ser instruido en el arte de la memoria en el que Bruno era experto. Pronto, sin embargo, tuvo dificultades con Mocenigo, quién lo denunció a la Inquisición veneciana. Fue encarcelado y posteriormente entregado a la Inquisición romana, la que lo sometió al juicio del que resultaría su ejecución. Aunque es hoy motivo de controversia el impacto real que las ideas de Bruno tuvieron en la revolución intelectual de los siglos XVI y XVII que dio origen a la ciencia moderna -y que tuvo en Nicolás Copérnico, Galileo Galilei e Isaac Newton a sus figuras más notables-, es interesante señalar que 400 años después el tema de la multiplicidad de los mundos, que contribuyó a su sentencia de muerte, es de una gran actualidad. En efecto, la noche del pasado viernes 6 de marzo fue lanzada al espacio por la NASA la sonda Kepler (ver la página web, http://kepler.nasa.gov/), destinada a investigar la existencia de planetas similares a la Tierra fuera del sistema solar (exoplanetas).El proyecto Kepler consistirá en la observación continua y simultánea, durante un mínimo de tres años y medio, de 100,000 estrellas localizadas en la Vía Láctea. Para esto, la sonda Kepler se mantendrá en una órbita alrededor del Sol siguiendo cercanamente a la Tierra. Dicha sonda cuenta con un telescopio de 95 centímetros de diámetro, que se apuntará durante toda la misión a un mismo grupo de estrellas. La luz que provenga de estas será captada por un sistema de 42 detectores CCD -similares a los que se usan en las cámaras fotográficas digitales, pero más especializados- equivalentes a 95 megapixeles. La técnica de detección de planetas que usará la sonda Kepler es muy simple y está basada en la disminución del brillo de una estrella cuando uno de sus planetas se interpone entre esta y la Tierra. De este modo, la observación de que la luminosidad de una determinada estrella fluctúa de manera periódica será un indicativo de la existencia de un planeta. Además, el tiempo entre dos fluctuaciones en luminosidad corresponderá al año solar del planeta, mientras que la magnitud de dichas fluctuaciones nos indicará su tamaño. La NASA espera detectar por este método alrededor de 30 exoplanetas con un tamaño y una órbita similares a las de Júpiter.  De interés especial, sin embargo, será la detección de planetas habitables, es decir, de planetas que tengan una temperatura en el rango adecuado de modo que puedan contener agua líquida, la cual es indispensable para la vida tal como la conocemos. Un planeta muy cerca de su estrella será demasiado caliente y toda el agua que pudiera contener en su superficie estaría convertida de vapor. Por el contrario, si estuviera muy alejado el agua se congelaría. Además, la masa del planeta debe ser aproximadamente igual a la de la Tierra. Si fuera muy pequeña no podría retener una atmósfera, como le sucede a nuestra luna y a al planeta Marte, y si por el contrario fuera muy grande retendría todos los gases incluyendo los más ligeros y el planeta se parecería a Júpiter, que no es adecuado para la vida. Se requieren entonces muchas condiciones para la existencia de planetas capaces de desarrollar vida. No obstante, la NASA espera detectar con la sonda Kepler cuando menos 50 planetas de este tipo.Según la NASA, los resultados sobre la detección de exoplanetas similares a la Tierra probablemente se obtengan sólo hasta al final de los tres años y medio que durará la misión. En tanto esto ocurre, vale la pena tener presente a Giordano Bruno, pionero (y mártir) en la búsqueda de vida extraterrestre.",
    "Según una noticia aparecida en los medios de comunicación la pasada semana, el Parlamento Europeo reafirmó su decisión de desaparecer gradualmente los focos incandescentes empleados para iluminación y sustituirlos por fuentes de luz más eficientes, como un medio de reducir la emisión de gases de invernadero causantes del calentamiento global. La decisión europea, juntamente con iniciativas similares en otras partes del Mundo, equivalen a una sentencia de muerte para las lámparas incandescentes, desarrolladas por Thomas Alva Edison hace 130 años.  El foco incandescente cambió drásticamente el ambiente nocturno de los centros urbanos a finales del siglo XIX y constituyó un excelente negocio para la compañía Edison Electric Light, fundada por Edison en 1878, que instaló sistemas de iluminación pública en Nueva York y otras ciudades de Este de los Estados Unidos. Además de la lámpara incandescente, Edison es conocido por la invención del fonógrafo y por muchos otros desarrollos -al morir tenia registradas más de mil patentes- que llevó a cabo en su laboratorio de Menlo Park, Nueva Jersey, al que se considera precursor de los modernos laboratorios de investigación industrial. Edison es uno de los más grandes héroes norteamericanos y un icono del capitalismo, notable no solamente por sus inventos sino por su actividad como empresario.Un aspecto del trabajo de Edisón que es menos conocido y que lo pinta, no obstante su fama pública, como un empresario carente de escrúpulos, es el relativo a sus experimentos para desarrollar un método de ejecución de criminales por choques eléctricos. Estos experimentos, que a la postre dieron origen a la silla eléctrica -usada por primera vez en Auburn, Nueva York en 1890 en la persona de William Kemmler-, fueron conducidos en animales por Harold Brown y Arthur Kennelly, este último empleado de Edison, en las instalaciones del laboratorio de Menlo Park. En notas de laboratorio de julio de 1888 de estos dos investigadores se consigna que, después de haber llevado cabo pruebas de electrocución con perros fox terrier, bulldog y pastor, experimentaron con un perro negro de raza híbrida al que le aplicaron inicialmente una descarga de 300 voltios, con lo que el animal “aulló por un minuto y comenzó a forcejear violentamente”; que con subsecuentes choques de 400 y 500 voltios el perro “chilló y luchó” y, finalmente, con una descarga de 600 voltios el infortunado animal “chilló, gimió y murió a los noventa segundos”.  El interés de Edison en la electrocución de condenados tuvo que ver con la competencia feroz que sostenía con George Westinghouse -fundador en 1886 de la compañía Westinghouse Electric- por el control de mercado de la electrificación de los centros urbanos en los Estados Unidos. Edison y Westinghouse competían por dicho mercado cada uno con su propia tecnología: Edison con la tecnología de corriente directa y Westinghouse con la de corriente alterna. La tecnología de Edison tenía la desventaja de requerir de cables de cobre más gruesos que los demandados por la tecnología de Wesitinghouse; en contraposición, esta última involucraba el empleo de voltajes de miles de voltios en parte de la red de distribución eléctrica, voltajes que son letales para los seres humanos.Aprovechando esta última característica, Edison se lanzó a una campaña para presentar a la tecnología de corriente alterna como peligrosa e incompatible con la vida, y como parte de esta campaña, se pronunció a favor de la electrocución como un método de ejecución más “humano” que la horca prevaleciente en la época. Para expresar sus opiniones, Edison se basó en sus experimentos de electrocución de animales, no solamente con perros sino también con terneras y caballos. Su verdadera intención fue, sin embargo, hacer que la imagen de los electrocutados quedara asociada en la mente del público a Westinghouse, ya que para conseguir los altos voltajes necesarios para operar la silla eléctrica se requería emplear corriente alterna. De hecho, en la ejecución de Auburn se empleó un generador de electricidad marca Westinghouse, en gran medida gracias a maniobras de Brown apoyado por Edison. Es quizá innecesario mencionar que, no obstante los testimonios y opiniones de Edison, la silla eléctrica estuvo lejos de matar sin dolor y que, por el contrario, en las primeras ejecuciones hubo muchos contratiempos. Kemmler, por ejemplo, sobrevivió a una primera descarga de 1000 voltios por 17 segundos y sólo murió después de recibir una segunda descarga por un tiempo entre uno y dos minutos, misma que le quemó la cabeza e incendió los cabellos.     Irónicamente, y a pesar de todos sus esfuerzos, Edison perdió finalmente la batalla con Westinghouse y la corriente alterna se impuso a la corriente directa, situación que prevalece hasta nuestros días.Edison ejemplifica a un sistema económico capitalista con claroscuros, que lo mismo ha generado avances tecnológicos que han elevado substancialmente nuestra calidad de vida -incluyendo las fuentes de iluminación más eficientes que sustituirán a los focos de Edison en el futuro próximo- que fraudes multimillonarios y crisis económicas como la que estamos padeciendo. Sólo el futuro nos dirá si esta situación podrá ser superada.",
    "El pasado 12 de febrero, durante la reunión anual de la Sociedad Americana para el Avance de la Ciencia celebrada en Chicago, IL, un grupo internacional de investigadores encabezado por el Instituto Max Planck de Leipzig, Alemania, reportó haber obtenido un “borrador” del genoma del Neandertal. El anuncio –y la reunión misma, que llevó por tema: Nuestro Planeta y su Vida: Orígenes y Futuro- se hizo coincidir con el 200 aniversario del nacimiento de Carlos Darwin, el creador de la teoría de la evolución de las especies.Los primeros restos fósiles del Neandertal fueron encontrados en el Valle del Neander en Alemania en 1856 y se cree que tienen una antigüedad de unos 50,000 años. Curiosamente, algunos investigadores especularon entonces que los restos correspondían a los de un jinete cosaco con las piernas arqueadas –hay que recordar que el descubrimiento se produjo tres años antes de que Carlos Darwin publicara su libro sobre el origen de las especies-. Con el tiempo, sin embargo, se reconoció que dichos restos no eran humanos y a los neandertales se les asignó su propia especie: “Homo neanderthalensis”.Hoy en día, en virtud de los numerosos restos fósiles que han sido encontrados, sabemos que los neandertales eran un poco más bajos y más robustos que el promedio nuestro y que contaban con una musculatura considerablemente mayor. Sabemos también que carecían de mentón y que tenían frente huidiza en contraste con la frente vertical humana. La característica más notable del Neandertal era, sin embargo, el tamaño de su cerebro, que tenía un volumen comparable al nuestro. En concordancia con esta última característica, se sabe que los neandertales fabricaban y usaban herramientas de piedra y que dominaban el uso del fuego. Se ha incluso sugerido que tenían la capacidad de hablar.        Como es ahora aceptado, el Neandertal es un pariente cercano nuestro y con el que compartimos un ancestro común que vivió en África hace unos 500,000 años. El Homo sapiens y el Homo neanderthalensis salieron de África en distintas épocas y se reencontraron en Europa y en el Medio Oriente en donde convivieron por decenas de miles de años.  Los neandertales sobrevivieron  en Europa hasta hace unos 30,000 años y a partir de entonces se extinguieron como especie. No hay acuerdo sobre cual fue la causa de la extinción del Neandertal. La creencia tradicional es que había perecido a manos del Homo sapiens, con quién no pudo competir por su inferioridad como especie. En actualidad, sin embargo, se acepta que las capacidades del Neandertal fueron grandemente subestimadas -posiblemente por nuestros prejuicios antropocéntricos- y que otras tesis más elaboradas tienen que emplearse para explicar su extinción. Una de estas tesis sostiene que hubo una cruza entre el Homo sapiens y el Homo neanderthalensis y que, en virtud de que el número de aquellos era mucho mayor, los neandertales fueron absorbidos por nuestra especie. Quienes así opinan se apoyan, entre otros descubrimientos, en los restos de un niño encontrados en el valle Lapedo en Portugal en 1999 y a los que se les calcula una antigüedad de unos 25,000 años. Dichos restos, que son fechados en una época en el que había ya desaparecido el Homo neanderthalensis, muestran rasgos que parecen ser una mezcla de características humanas y neandertales. No obstante, de acuerdo con los resultados del estudio del genoma neandertal presentado el pasado febrero en Chicago, la cruza de las especies Sapiens y Neanderthalensis parece improbable. Resultaría entonces que el Neandertal se extinguió y que nosotros sobrevivimos por las diferencias en capacidad intelectual y de adaptación a un medio ambiente cambiante. Por otro lado, independientemente de que se hayan cruzado o no, el que humanos modernos y neandertales hayan convivido e interactuado de algún modo durante la Edad de Piedra resulta por lo menos fascinante.  Si los neandertales hubiesen sobrevivido hasta nuestros días –y no les faltó mucho para hacerlo- y si efectivamente, a pesar del tamaño de su cerebro, hubiesen sido intelectualmente inferiores, no hay duda de lo que hubiera resultado de nuestra interacción con ellos, dadas las tradiciones de discriminación racial y de prácticas de esclavismo que nos han caracterizado. En la Edad de Piedra, sin embargo, esta interacción tuvo que ser muy diferente, y en caso de no haber sido amigable, no debió de ir más allá de que unos y otros se vieran, ya sea como competidores por alimentos o como objetos susceptibles de ser convertidos en alimentos.  Como quiera que sea, aunque en estos tiempos no tendríamos la oportunidad de conocer a un Neandertal en persona, su influencia en al percepción que tenemos de nosotros mismos ha de ser profunda. En palabras de J. Zilhao y F. d´Errico, refiriéndose a la discusión sobre la capacidad del Neandertal para manejar símbolos (Scientific American, abril de 2000): “Independientemente de cual hipótesis  resulte correcta, la barrera de comportamiento que parecía que separaba a los modernos humanos del Neandertal y que nos daba la impresión de ser únicos y particularmente agraciados -con la habilidad de producir culturas simbólicas- ha colapsado de manera definitiva”.",
    "Muchos de los actuales dispositivos de alta tecnología parecen verdaderamente cosa de magia. Pensemos, por ejemplo, en los discos compactos empleados para grabar y reproducir  música. A quienes acostumbramos, ya hace algún tiempo, escuchar música en discos de acetato o “Long Play”, no dejó de sorprendernos la calidad de sonido de los CDs cuando estos aparecieron en el mercado. Aun más, como muchos de nosotros recordamos, los discos de acetato eran muy frágiles y una raya en su superficie podía inutilizarlos. En contraste, un CD resiste maltratos y rayas sin aparente demérito en la calidad del sonido que reproduce, algo que efectivamente parece mágico. En un ejemplo en otro orden, empleando técnicas de Carbono 14 los arqueólogos son capaces de fechar con gran precisión restos de material orgánico, animal o vegetal, con una antigüedad de hasta decenas de miles de años. Para un lego en la materia esto es algo tan sorprendente que en cierta medida se acerca a lo mágico.  Los anteriores ejemplos corresponden a tecnologías de gran sofisticación basadas en el conocimiento científico, que contrastan grandemente con las tecnologías empíricas desarrolladas por medio de un procedimiento de prueba y error. Como un ejemplo de estas últimas, consideremos las técnicas tradicionales para fabricar espadas de acero. Después de numerosos intentos de prueba y error, los artesanos aprendieron a lo largo del tiempo que si golpeaban con un martillo una espada al rojo vivo y después la enfriaban rápidamente en agua, la podían endurecer de manera considerable. Podemos comparar el proceso de desarrollo de una tecnología empírica con el juego de tratar de romper a palos una piñata con los ojos vendados, cosa que pudiéramos quizá lograr después de numerosos golpes fallidos En contraste, una tecnología de base científica, si bien hace uso del procedimiento de prueba y error en cierto grado, se enfoca de manera primordial en el conocimiento científico, que puede ser sofisticado en extremo. El desarrollo de una tecnología basada en la ciencia sería equivalente, en nuestra alegoría, a tratar de darle a la piñata viéndola de reojo y con los ojos parcialmente cubiertos.  En referencia a la técnica de fechado de material orgánico, esta se basa en el conocimiento científico que se tiene sobre el Carbono 14, que se sabe se transforma lentamente y de manera espontánea en Carbono 12 por un proceso de desintegración radiactiva. El fechado se hace a partir de la medición de la cantidad de Carbono 14 que se encuentra en los restos vegetales o animales bajo estudio, cantidad que se sabe empieza a disminuir de manera paulatina una vez que un organismo muere. De este modo, entre menos cantidad de Carbono 14 tenga una muestra arqueológica mayor será su antigüedad. La existencia de tecnologías de base científica, entre las que se cuentan a la computación, las comunicaciones, la biotecnología, la medicina, entre muchas otras, es una de las pruebas más palpables del poder que nos da el conocimiento científico. A lo largo de la historia se ha buscado de muchas maneras influir y adquirir poder sobre la naturaleza. Los alquimistas de la Edad Media, por ejemplo, se propusieron, entre otras cosas, encontrar el elixir de la vida para alcanzar la inmortalidad y la piedra filosofal para transformar plomo en oro. No alcanzaron, sin embargo, ninguna de sus metas debido a que los métodos de trabajo que emplearon no estaban basados en la ciencia. No fue sino hasta el siglo XX, empleando el conocimiento que se tenía sobre física nuclear, que se logró transformar plomo en oro -aunque, es necesario mencionarlo, el costo del procedimiento para hacerlo sobrepasa cualquier potencial beneficio económico y por lo tanto carece de interés práctico.Hoy en día, y no obstante el avance tecnológico del que somos testigos -producto en último término del empleo sistemático del método científico-, subsisten muchas prácticas y creencias que podemos catalogar como mágicas. Pensemos, por ejemplo, en las predicciones astrológicas y los horóscopos que cuentan con numerosos adeptos y para los cuales no existe una evidencia firme de que tengan utilidad práctica. Se encuentran también muy difundidas creencias que están en franca contradicción los resultados de la ciencia. Un ejemplo notable es la doctrina creacionista que tuvo un gran auge en los Estados Unidos a lo largo del siglo XX. La doctrina creacionista sostiene una interpretación literal de la creación del Mundo tal como lo relata la Biblia, lo que esta en abierta contradicción con numerosos resultados de la Biología y la Geología que apoyan la evolución de las especies y no una creación súbita de las mismas.  El que aún en los países económicamente desarrollados subsistan muy extendidas formas de pensar anticientíficas posiblemente signifique que en nuestro contexto cultural el razonamiento científico no es algo natural. Podemos esperar, sin embargo, que en la medida en que el mundo real, producto de la alta tecnología, supere de manera abrumadora al mundo mágico, esta situación cambie de manera drástica y la magia quede definitivamente erradicada de nuestra vida.",
    "El 8 de julio de 1853 cuatro barcos de guerra estadounidenses al mando del comodoro Mathew C. Perry anclaron en la bahía de Edo (actual Tokio); el propósito: obligar al Japón a abrirse al comercio con los Estados Unidos. Por dos siglos Japón había rehusado el contacto con occidente, con la única excepción de Holanda, país con el que mantenía  un intercambio limitado a través del puerto de Nagasaki en el sur del país. Los cañones de los barcos de Perry fueron, sin embargo, lo suficientemente convincentes para que Japón aceptara un tratado comercial con los Estados Unidos, mismo que firmó el 31 de marzo de 1854. Japón fue forzado por los estadounidenses debido a su gran atraso en tecnología militar en comparación con Occidente producto de su política aislacionista de dos siglos. Con gran sentido práctico, en respuesta, los japoneses se dieron a la tarea de poner remedio a la situación. El resultado de sus esfuerzos, como es bien conocido, fue sorprendente y Japón es ahora, a poco más de 150 años de la llegada de Mathew C. Perry a sus costas, una de las potencias tecnológicas del Mundo.    La estrategia seguida por Japón para alcanzar tecnológicamente a Occidente fue sorprendentemente simple: contrataron a profesores visitantes en Europa y los Estados Unidos para que le enseñaran lo que no sabían, al mismo tiempo que enviaron a grupos de estudiantes al extranjero a capacitarse en las tecnologías que les interesaba introducir al país. El plan fue tomado tan en serio que en 1874 el Ministerio de Industria japonés empleó más del 30% de su presupuesto en pagar profesores extranjeros. Llevaron también a cabo una reforma educativa a todos niveles, creando el llamado sistema de universidades imperiales a fin de educar a los líderes que necesitaban. La primera universidad imperial fue la Universidad de Tokio, fundada en 1886, con departamentos de Química, Matemáticas, Física y Astronomía, Biología, Ingeniería, y Geología y Minas. Le siguieron las universidades de Kyoto (1897), Tohoku (1907), Kyushu (1910) y Hokkaido (1918). El avance del Japón fue tan vertiginoso que en el año de 1905 derrotó militarmente a Rusia, una de las potencias europeas de la época.  Comparar las historias de México y Japón en los últimos 150 años resulta tentador. Ambos países no solamente sufrieron a mediados del siglo XIX intervenciones por parte de los Estados Unidos -circunstancialmente, Mathew C. Perry participó en el bombardeo de Veracruz durante la guerra entre México y los Estados Unidos en 1847, además de que estuvo al mando de ataques a varias ciudades mexicanas en la costa del Golfo de México en ese mismo año-, sino que en esa época los dos países estaban en una situación de gran atraso tecnológico con respecto a las potencias económicas contemporáneas. En contraste con Japón, sin embargo, México no tuvo la suficiente claridad -o los recursos y condiciones adecuadas- para entender el papel que la educación y la tecnología tendrían en el desarrollo de un país en los años entonces por venir. Con la ley de Instrucción Pública emitida por el Presidente Juárez en 1868, México ciertamente se embarcó en la segunda mitad del siglo XIX en una reforma educativa. Dicha reforma, no obstante, estuvo muy lejos de alcanzar el éxito de la japonesa. Por un lado, la educación profesional en México estuvo a cargo de escuelas en la Ciudad de México y de institutos creados por los gobiernos, y no fue sino hasta el siglo XX que se crearon las universidades públicas mexicanas de una manera un tanto tardía. Por otro lado, en una situación que en gran medida subsiste a la fecha, fue muy difícil establecer una conexión universidad-industria. Los caminos seguidos por México y Japón desde mediados del siglo XIX, si bien partieron de puntos con cierto grado de similitud, han sido violentamente divergentes en cuanto a desarrollo  científico y tecnológico. Baste citar algunas estadísticas. Por ejemplo, según el Ministerio de Educación de Japón, ese país cuenta actualmente con alrededor de 700,000 investigadores científicos; en nuestro país, de acuerdo al Sistema Nacional de Investigadores, este número es 15,000. En otro ejemplo, todavía más contrastante, según la Organización Mundial de la Propiedad Intelectual el número de patentes concedidas por Japón a personas radicadas en ese país fue alrededor de 145,000 en 2007; en México, el número correspondiente es 201. En las últimas décadas México ha reconocido la importancia de la ciencia y la tecnología para el desarrollo económico de país y a partir de la década de los setentas en el siglo pasado se está dando impulso a la educación de postgrado y a la investigación científica por parte del Gobierno Federal. También, a partir de 1971 con la creación del CONACYT, se ha apoyado un programa de becas para estudios de postgrado tanto en México como en el extranjero, tal como lo hiciera Japón en el siglo XIX. Esto es ciertamente loable, aunque resulta de lamentar que nos hayamos tardado un siglo en reaccionar. No obstante, y por supuesto, más vale tarde que nunca.",
    "El celebre paso del Noroeste, el pasaje helado al norte de Canadá que conecta al océano Atlántico con el océano Pacífico, ha sido objeto de gran interés y fascinación por más de quinientos años. Esta fascinación ha sido tal que dicho pasaje ha tenido incluso presencia en la literatura. Julio Verne, por ejemplo se refiere al mismo en su novela Las Aventuras del Capitán Hatteras publicado en 1866. El paso del Noroeste alcanzó relevancia en el siglo XVI debido a que las rutas de navegación por los mares del sur entre Europa y Asia eran dominadas por España y Portugal, lo que motivó que otras potencias europeas, notablemente Inglaterra, buscaran afanosamente una ruta adicional por el Norte viajando en dirección Este-Oeste. A lo largo de los siglos XVI-XIX se hicieron numerosos intentos por encontrar el paso del Noroeste, intentos que, sin embargo, resultaron infructuosos por los hielos árticos que dificultaban enormemente la navegación.  No fue sino hasta 1906 en que el noruego Roald Amundsen logró navegarlo por primera vez, desde la Bahía de Baffin en el este, hasta el estrecho de Bering en el oeste, en un viaje que le tomó tres años. A lo largo de Siglo XX, el paso del Noroeste fue practicable solamente para buques rompehielos. Al inicio del presente siglo, sin embargo, tal parece que esta situación está cambiando rápidamente por efecto del calentamiento global. En efecto, en una noticia ampliamente difundida por la prensa mundial, el 21 de agosto de 2007 se dio a conocer que el paso del Noroeste había quedado libre de hielos por primera vez desde que hay memoria, y que días después, el 15 de septiembre del mismo año, la superficie cubierta por los hielos árticos alcanzó el valor más bajo jamás observado, lo que hubiera permitido la navegación sin dificultad entre los océanos Atlántico y Pacífico. El calentamiento global es producto del incremento en la concentración de dióxido de carbono en la atmósfera debido al uso intensivo de combustibles fósiles en diversas actividades humanas. El mecanismo que produce dicho calentamiento -conocido como efecto invernadero- es similar a aquel por medio del cual los rayos del Sol elevan la temperatura en el interior de un automóvil con los vidrios cerrados. En este caso, el efecto invernadero es básicamente debido a que el parabrisas y las ventanas del automóvil son transparentes a la radiación solar y por lo tanto permiten su paso hacia el interior, al mismo tiempo que no permiten la salida del calor generado por la absorción de dicha radiación en las vestiduras y recubrimientos internos. De este modo, en la medida que transcurre el tiempo más y más calor es atrapado dentro del automóvil, incrementando paulatinamente su temperatura. El calentamiento de la Tierra puede ser entendido con un símil en el cual el interior del automóvil es la superficie terrestre, mientras que el parabrisas y las ventanas corresponden a la atmósfera.   En realidad el efecto invernadero en la atmósfera es crucial para controlar las variaciones de temperatura entre el día y la noche, lo que es esencial para la existencia de la vida en la Tierra tal como la conocemos -en la superficie de la Luna, que no cuenta con una atmósfera, la variación de temperatura entre día y noche es superior a los 250 grados centígrados-. Existen en las atmósfera diferentes gases que producen un efecto invernadero y de estos el más importante es el vapor de agua, seguido del dióxido de carbono. El efecto invernadero del vapor de agua resulta evidente si pensamos en las grandes variaciones de temperatura que se producen entre el día y la noche en regiones con clima seco. El efecto correspondiente del dióxido de carbono quizá no nos resulta igual de evidente, pero su incremento paulatino en la atmósfera desde el inicio de la revolución industrial es señalado como el principal culpable del actual calentamiento global y el cambio climático asociado.   Algunos expertos piensan que el calentamiento global puede cambiar drásticamente las condiciones sobre la superficie de la Tierra. Por ejemplo, es motivo de intensa atención la posible fusión de los hielos polares que podría dar lugar a un incremento en el nivel de los océanos. De especial preocupación son los hielos antárticos, que representan el 70% de agua dulce que existe en la Tierra y que potencialmente podrían incrementar hasta en 5 metros el nivel del mar, lo que sumergiría bajo el agua buena parte de la península de Florida. No existe un acuerdo generalizado sobre como en detalle será afectada la superficie de la Tierra por el calentamiento global en el largo plazo. Hay, no obstante, señales inequívocas y preocupantes de que un cambio está en curso como lo indican la apertura del paso del Noroeste en 2007 y el record de mínimo recubrimiento de hielo ártico ocurrido ese mismo año. Es posible que en algunos años el paso del Noroeste sea navegable por periodos largos en el verano, con lo que perderá, sin duda, la fascinación que ha despertado en los últimos cinco siglos.",
    "Uno de los grandes temas actuales es, sin duda, el del calentamiento global y el cambio climático asociado. Como es ampliamente aceptado, las concentraciones crecientes de gases de invernadero en la atmósfera, producto del uso de combustibles fósiles, han generado en las últimas décadas un incremento paulatino de la temperatura terrestre. Estos combustibles son empleados intensivamente por la industria y el transporte, así como en plantas termoeléctricas generadoras de electricidad. Según cifras del Departamento de Energía de los Estados Unidos, la generación de energía eléctrica mediante combustibles fósiles representó en el año de 2005 el 66% del total generado a nivel mundial. Una estrategia para reducir la emisión de gases de invernadero debe entonces incluir planes para sustituir a los combustibles fósiles por energías renovables no contaminantes. Existe una gran variedad de energías renovables que no producen desechos contaminantes. Entre estas podemos mencionar a las energías solar, eólica, geotérmica,  hidráulica y aquellas asociadas al movimiento de agua en los océanos. Sin considerar a la energía hidráulica, sin embargo, la contribución de la energías renovables a la generación global de electricidad apenas sobrepasa el 2%, siendo la eólica la más significativa con aproximadamente el 1.4% del total mundial.  Hasta hace algunas décadas, el aprovechamiento de la energía eólica la asociábamos comúnmente con los molinos de viento holandeses empleados para desecar las tierras bajas ganadas al mar, o bien con las bombas extractoras de agua impulsadas por el viento que todavía podemos ver en algunos lugares en el campo. En contraste, hoy en día nos es cada vez más familiar la imagen de los modernos y enormes molinos de viento, o aerogeneradores como es más propio llamarlos, con cuerpos esbeltos y con un rotor formado de tres paletas girando lentamente por efecto del viento. En un aerogenerador, el movimiento de rotación del rotor impulsa un generador de electricidad, obteniéndose de esta manera energía eléctrica limpia. En una planta termoeléctrica, en contraste, el generador de electricidad es impulsado por una turbina accionada por un fluido caliente. Para obtener este último, es necesario quemar combustible, con la consecuente emisión de gases de invernadero.Si bien la energía eólica contribuye todavía con un porcentaje pequeño a la generación total de electricidad a nivel mundial, en algunos países este porcentaje es notablemente alto. En Dinamarca, por ejemplo, el 19% de la electricidad es de origen eólico, mientras que en España y Alemania los correspondientes porcentajes son 9% y 6%, en forma respectiva. En México –en donde más del 80% de la electricidad es generada en plantas termoeléctricas-, la energía eólica es todavía poco significativa, representando menos del 0.1 % del total generado. En enero de 2007 la Comisión Federal de Electricidad puso en operación la planta eólica La Venta II en la región del Istmo de Tehuantepec. Dicha planta consta de 98 areogeneradores de 44 metros altura con rotores de 52 metros de diámetro. La planta produce suficiente energía para abastecer a una ciudad pequeña. Además, en días pasados se puso en operación una segunda planta, denominada Eurus, con una capacidad inicial semejante a la de La Venta II, en la misma región de Itsmo de Tehuantepec. A diferencia de la primera, sin embargo, esta segunda planta fue construida con capital privado. Según un estudio de los Laboratorios Nacionales de Energía Renovable de los Estados Unidos, el potencial de México para la generación de energía eólica pudiera alcanzar los 43,000 MW, lo que es casi igual a la capacidad total instalada del País de 50,000 MW. Esto indicaría que la energía eólica puede contribuir substancialmente a reducir nuestro consumo de combustibles fósiles para la generación de electricidad. Hay que hacer notar, no obstante, que la energía eólica tiene sus críticos, quienes, entre otros argumentos, objetan la gran extensión de terreno que es necesaria para la instalación de una planta eólica. La Venta II, por ejemplo, ocupa 960 hectáreas, área demandada por las grandes dimensiones de los 98 aerogeneradores que requieren de una separación de 130 metros entre ellos. Por otro lado, debido a la intermitencia aleatoria de la energía eólica, no es posible pensar que podamos depender de la misma para satisfacer todas nuestras necesidades de generación de energía eléctrica; es decir, en los momentos en que no haya viento, una planta eólica necesita ser respaldada por una fuente de energía no intermitente, que necesariamente será contaminante. Así, la energía eólica no es estrictamente limpia, al igual que no lo son otras fuentes de energía renovables. No obstante, y a pesar de todos los problemas que tiene asociados, podemos esperar que en el futuro el aprovechamiento de la energía eólica sea cada mayor y que la figura de los esbeltos molinos de viento sea cada vez más parte del paisaje.",
    "El próximo 24 de noviembre se cumplirán 150 años de la publicación de “El origen de las especies” por Carlos Darwin, en donde el naturalista británico presentó su teoría de la evolución por selección natural. Las ideas de Darwin en esta materia se originaron en las observaciones realizadas durante su viaje de 5 años alrededor de Mundo a bordo del HMS Beagle.  El viaje partió de Plymouth, Inglaterra y después de cruzar el Atlántico tocó varios puntos de Brasil, cruzó el estrecho de Magallanes y prosiguió a lo largo de la costa occidental de América del Sur, para después atravesar el Pacífico, tocar Australia y retornar a Inglaterra vía el Cabo de Buena Esperanza. Durante este viaje,  Darwin recogió un gran número de especimenes vegetales y animales. De capital importancia fueron las muestras colectadas en las Islas Galápagos -así bautizadas por la cantidad de tortugas gigantes que ahí habitan- localizadas aproximadamente a 1000 kilómetros de la costa de Ecuador. Estas islas constituyen un laboratorio natural para el estudio de la evolución, pues debido a su lejanía de la costa las especies evolucionaron ahí de manera aislada. La teoría de la evolución de las especies de Darwin ha tenido un enorme impacto intelectual y asestó un golpe demoledor a nuestros prejuicios antropocéntricos, según los cuales ocupamos un lugar especial en el Universo. Después de “El origen de la Especies”, es ampliamente -si bien no universalmente- aceptado que los humanos estamos en realidad sujetos a las mismas reglas evolutivas que el resto de las especies y por lo tanto ni somos inmutables ni constituimos una clase cualitativamente diferente entre las mismas.Las revolucionarias ideas de Darwin tuvieron, por supuesto, tanto detractores como defensores. En este respecto es famoso el debate realizado el 30 de junio de 1860 entre Thomas Henry Huxley –apodado el “bulldog de Darwin”-, defensor de la teoría de la evolución y el Obispo anglicano de Oxford  HYPERLINK \"http://es.wikipedia.org/w/index.php?title=Samuel_Wilberforce&action=edit&redlink=1\" \\o \"Samuel Wilberforce (aún no redactado)\" Samuel Wilberforce -conocido por el mote de “Sam el Jabonoso”-, detractor de la misma. El debate, que de acuerdo a la mayor parte de los presentes fue ampliamente ganado por Huxely, se llevó a cabo durante la reunión anual de Asociación Británica para el Avance de la Ciencia. Es sobre todo recordado por el intercambio entre Huxley y Wilberforce quién habría preguntado al primero si en su caso el parentesco con los monos se daba por parte de su abuelo o de su abuela, a lo que Huxley contestó que si le daban a escoger entre ser pariente de un mono o ser pariente de un hombre brillante que usaba su don de palabra para tratar de oscurecer una discusión científica, sin duda habría escogido lo primero.El golpe de Darwin a nuestro antropocentrismo en realidad fue el segundo de su tipo. 350 años antes, Nicolás Copérnico había asestado el primero al quitar a la Tierra su lugar preponderante como centro del Universo. En su modelo de sistema solar, Copérnico puso al Sol en el centro y a los planetas, incluyendo a la Tierra, girando a su alrededor. Después de Copérnico, la Tierra, nuestro hábitat, no es sino uno más de un grupo de planetas, todos obedeciendo las mismas leyes físicas. Al igual que Darwin, Copérnico encontró gran oposición a sus ideas. De hecho, su libro “De las Revoluciones de las Esferas Celestes”, en el que describe su modelo de sistema solar,  no vio la luz sino hasta el final de sus días por temor a la controversia. Se dice que vio un ejemplar del libro ya publicado sólo hasta el día en que murió. En la actualidad, algunos autores piensan que está en curso un tercer golpe al antropocentrismo, esta vez definitivo. Este golpe tiene que ver con la ritmo exponencial con está incrementándose la capacidad de las computadoras, la cual se duplica cada dos años. Al respecto, el futurista Raymond Kurzweil -el mismo que fundó la compañía que fabrica instrumentos musicales- considera que antes del año 2050 dicha capacidad habrá sobrepasado la del cerebro humano, de modo las computadoras serán capaces no solamente de pensar de manera racional sino también de tener conciencia de ellas mismas e incluso de experimentar emociones. En este punto, en la visión de Kurzweil, las computadoras con ayuda de la Nanotecnología se habrán convertido en máquinas con capacidad de automejorarse, incrementando aun más sus capacidades. La brecha entre las computadoras y los humanos crecerá entonces de manera continua, sin que tengamos nosotros manera de controlar el proceso. Si bien los puntos de vista de Kurzweil, así como los de otros autores que expresan ideas similares, son muy controvertidos, su argumentación está, al menos en forma parcial, basada en un hecho real: la gran velocidad con la que se está incrementando la capacidad de las computadoras, tendencia que cualquiera de nosotros puede atestiguar. ¿Lograrán las computadoras ser más inteligentes que los humanos? La respuesta a esta pregunta la tendrán, quizá, aquellos que nacidos en el Siglo XXI. Si se da el caso, la evolución seguirá un nuevo camino y nuestro antropocentrismo crónico estará definitivamente muerto y sepultado.",
    "Hace más de 2400 años el filósofo griego Demócrito postuló la existencia de los átomos. Razonó que si partimos un objeto cualquiera en dos y una de las dos partes resultantes la dividimos a su vez en dos, y así de manera sucesiva, llegará un momento en que obtengamos un pedazo tan pequeño que no lo podamos dividir más. A este pedazo último lo llamó átomo, o indivisible en griego. Para Demócrito, todos los objetos estaban formados por átomos, agrupados en el vacío siguiendo un cierto patrón; además, las propiedades físicas y químicas del objeto eran determinadas tanto por el tipo de átomos que lo forman -su composición química- como por su ordenamiento. Si bien el procedimiento seguido por Demócrito para alcanzar sus conclusiones no se apega a los métodos de la Ciencia moderna, dichas conclusiones concuerdan sorprendentemente bien con nuestra visión actual de la materia. No por nada la imagen de Demócrito aparecía en billetes griegos hasta hace no mucho tiempo. Hoy en día la existencia de los átomos es aceptada universalmente. Se sabe que los materiales están formados de átomos y que existe un número limitado de clases de estos –alrededor de cien, como nos lo enseña la Tabla Periódica que se estudia en los cursos de Química de la escuela preparatoria-. Sabemos también que, tanto la clase de átomos de que está formado un material como la manera en que se ordenan en el interior del mismo, determinan sus propiedades físicas y químicas.  Por otro lado, si bien Demócrito anticipó por más de dos mil años gran parte de nuestras ideas actuales sobre los materiales, hubo un aspecto que inevitablemente se le escapó. Este tiene que ver con un hecho sorprendente: las propiedades físicas de un material dependen no solamente de su composición química, sino también de su tamaño, si éste es lo suficientemente pequeño. Por ejemplo, una esfera de oro de 1 milímetro de diámetro tornaría su color amarillo característico a rojo rubí si su radio disminuyera unas 40,000 veces. Otras propiedades de los materiales como la maleabilidad y la conducción de electricidad muestran una dependencia similar con el tamaño. Las propiedades de los materiales son sensibles al tamaño cuando este alcanza el rango de los nanómetros. Un nanómetro es un  millonésimo de milímetro, una cantidad extremadamente pequeña. Para tener una idea, el diámetro de un cabello humano es aproximadamente 100,000 nanómetros, mientras la bacteria E. coli, causante de enfermedades intestinales, mide aproximadamente 1000 nanómetros y puede ser observada solamente a través de un microscopio. Las partículas con tamaños que se miden en nanómetros reciben el nombre de nanopartículas.Existen aplicaciones de las nanopartículas desde tiempos remotos. Durante la edad media, los artesanos fabricantes de vitrales para iglesias aprendieron a colorear vidrio mezclándole cloruro de oro, lo que produce esferas de oro de tamaño nanométrico embebidas en vidrio, adquiriendo éste un  color rojo rubí. Los vidrieros en esas épocas no entendían, por supuesto, como en detalle se coloreaba el vidrio. De hecho, fue hasta el Siglo XIX que Michael Faraday comprendió que el efecto estaba asociado al pequeño tamaño de las partículas de oro involucradas. Es, sin embargo, sólo hasta la época actual que se ha alcanzado una amplia comprensión sobre cómo el tamaño de un material afecta sus propiedades físicas y químicas, comprensión que ha contribuido a crear el campo de la Nanotecnología, que se ocupa de la fabricación y aplicación de objetos y materiales de dimensiones nanométricas. El avance del conocimiento científico en el Siglo XX permitió el mejoramiento de las propiedades de algunos materiales naturales, e incluso el desarrollo de materiales artificiales no existentes en la naturaleza, algunos con propiedades novedosas.  Un ejemplo de esto son las fibras sintéticas como el nailon, que sustituyeron a las fibras naturales en muchas aplicaciones. Otro ejemplo notable de material artificial es el silicio, que es la base de buena parte de la industria electrónica, en particular de las memorias y los microprocesadores para computadoras. No obstante todo lo anterior, los avances tecnológicos del Siglo XX serán sin duda ampliamente superados por lo que se verá en el futuro. En la actualidad, los científicos e ingenieros tienen a su disposición toda una serie de nuevos nanomateriales para ser empleados en aplicaciones sorprendentes, tanto por su número como por su variedad. Entre estas aplicaciones, demostradas o potenciales, se incluyen: aprovechamiento de la energía solar, fabricación de nuevas clases de computadoras, terapia de pacientes cancerosos, fabricación de láseres, aceleración de reacciones químicas en la industria, fabricación de materiales ultra duros, entre muchas otras. Sin duda alguna, dadas las maravillas futuras que nos prometen los nanomateriales, podemos afirmar que, efectivamente, el tamaño si importa.",
    "La cereza del pastel de la crisis financiera y económica que estamos padeciendo ha sido sin duda la acusación por fraude a Bernard Madoff por un total de 50,000 millones de dólares. De acuerdo con informaciones periodísticas, Madoff empleó un “Esquema Ponzi” para estafar esta asombrosa cantidad a inversionistas de todo el Mundo. Carlo Ponzi, por quién fue nombrado dicho esquema, fue un inmigrante italiano a los Estados Unidos que en la década de los veintes estafó millones de dólares a numerosos inversionistas en ese país, mediante un esquema  de negocios que ofrecía altos intereses. Ponzi planteaba que podía obtener un beneficio comprando cupones de correo para gastos de devolución de piezas postales adquiridos fuera de los Estados Unidos, y venderlos ahí a un precio mayor dado el costo más bajo del servicio postal. El negocio de Ponzi generaba réditos extremadamente grandes y por lo tanto atrajo a un gran número de inversionistas. Estos réditos, sin embargo,  no provenían del negocio planteado por Ponzi sino de los recursos que aportaban los inversionistas que recién ingresaban. De este modo, el negocio tuvo viabilidad en la medida que hubo un flujo de nuevos inversionistas aportando recursos; una vez que este flujo bajó, no hubo manera de pagar los altos réditos ofrecidos y el negocio se colapsó. El esquema Ponzi es un caso particular de los negocios piramidales que están basados en el supuesto de que existe un número infinito de inversionistas dispuestos a participar. Otro ejemplo lo constituyen las llamadas pirámides o cadenas que de tiempo en tiempo aparecen tomando diferentes formas. Un esquema simple de esta última clase es el siguiente. La pirámide la inicia una persona que constituye su vértice, aportando una cantidad fija e invitando a dos personas a formar el segundo nivel de la pirámide. Los dos nuevos miembros están obligados a aportar la misma cantidad fija y a invitar cada uno de ellos a dos miembros que constituirán el tercer nivel. El proceso continua hasta un cuarto nivel con ocho miembros, punto en el cual el iniciador deja de pertenecer a la pirámide recibiendo íntegra, como beneficio, la aportación de los integrantes del cuarto nivel. A partir de aquí la pirámide se parte en dos de modo que los miembros del segundo nivel pasan a ocupar los vértices de las dos nuevas pirámides, repitiéndose a partir de aquí todo el proceso. Claramente, en cada paso del crecimiento piramidal se requiere doblar el número de participantes, de modo que en el momento que estos se agotan el negocio se colapsa y solamente algunos pocos de los iniciadores de la pirámide obtienen una ganancia, a expensas de aquellos que ingresaron posteriormente.El crecimiento exponencial, que se presenta en las primeras etapas de un gran número de procesos, tanto naturales como producto de la actividad humana, es muy engañoso  y por lo mismo peligroso. Viene aquí a colación la fábula del rey, situado unas veces en China y otras en la India, que quiso recompensar a quién lo enseñó a jugar ajedrez ofreciéndole cualquier cosa que él quisiera. Este hizo una petición que sorprendió al Rey por su aparente pequeñez: le solicitó que le diera tantos granos de arroz como fueran necesarios para colocar uno en la primera casilla del tablero de ajedrez, dos en la segunda, cuatro en la tercera, ocho en la cuarta, y así sucesivamente -doblando el número de granos entre dos casillas sucesivas, o sea siguiendo un crecimiento piramidal- hasta terminar con las 64 casillas de que se compone dicho tablero. La petición hecha al Rey, que en primera instancia pareciera pudiera cumplirse con uno o dos costales de arroz, es en realidad imposible de satisfacer pues requeriría, por ejemplo, de aproximadamente 1000 veces la producción mundial de arroz correspondiente al presente año.        Los esquemas piramidales de negocios son exitosos para los que los inician y fallan para los demás debido a las particularidades del crecimiento piramidal o exponencial. Entre estas particularidades se encuentra que es posible mantener el crecimiento en el número de inversionistas durante un cierto periodo inicial, durante el cual toda parece funcionar a la perfección. Llegado a un cierto valor crítico en el volumen de inversionistas participantes, sin embargo, no es posible seguir incrementándolo a la velocidad requerida y el esquema se colapsa. Cabe hacer notar que este es el fin inexorable de cualquier negocio piramidal, aunque en algunos casos los negocios terminan prematuramente debido a la intervención del Gobierno o de algún otro factor externo. Aunque la esencia de los negocios piramidales es como se describe líneas arriba, en la práctica se presentan a los potenciales inversionistas de manera elaborada, de tal manera que es difícil en muchos casos identificarlos como tales. En este respecto, tomar conciencia de la naturaleza y las consecuencias del crecimiento piramidal o exponencial quizá nos sea de utilidad para no tomar decisiones las cuales después lamentemos."
]